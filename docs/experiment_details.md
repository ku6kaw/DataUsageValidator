# データ利用検証パイプラインの実験詳細

このドキュメントは、科学論文におけるデータ利用の検証と分析を行うパイプラインが、具体的にどのような実験を行っているのかを詳細に説明します。

## 1. 実験の目的

本実験の主な目的は、科学論文が特定のデータ論文のデータを「実際に使用しているか」を自動的に判定する手法を開発し、その性能を評価することです。特に、大規模言語モデル (LLM) を活用したデータ利用検証の有効性を探り、ルールベースの手法や人間によるレビューと組み合わせることで、堅牢で効率的な検証パイプラインを構築することを目指します。

## 2. 実験の背景

科学研究において、データの再利用と適切な引用は研究の透明性と再現性を確保するために不可欠です。しかし、論文がデータを「使用している」と判断するか、「言及しているだけ」と判断するかは、しばしば曖昧であり、手動での検証には多大な時間と労力がかかります。本実験は、この課題に対し、自動化された検証プロセスを提供することで、研究者の負担を軽減し、データ利用の正確な追跡を可能にすることを目指します。

## 3. 各フェーズにおける実験内容

パイプラインは以下のフェーズで構成され、それぞれが特定の実験的側面を担っています。

### フェーズ1: データ論文収集 (`collect_data_pipeline.py`)

*   **目的**: データ利用検証の出発点となる「データ論文」のリストを収集します。データ論文とは、主にデータセットの公開や記述に焦点を当てた論文を指します。
*   **実験内容**:
    *   Scopus API を使用し、特定のクエリ（例: `DOCTYPE(dp)`）に基づいてデータ論文を検索します。
    *   検索結果から、論文のEID、DOI、タイトル、出版年、被引用数などのメタデータを抽出します。
    *   収集されたデータは `data/processed/data_papers.csv` に保存されます。

### フェーズ2: 引用論文収集 (`collect_citing_papers_pipeline.py`)

*   **目的**: 収集したデータ論文を実際に引用している論文（引用論文）のメタデータと全文XMLを収集します。これらの引用論文がデータを利用しているかどうかが検証の主要な対象となります。
*   **実験内容**:
    *   フェーズ1で収集されたデータ論文の中から、一定の被引用数（例: 10回以上）を持つ論文を対象とします。
    *   各データ論文を引用している論文をScopus APIで検索し、そのメタデータ（EID, DOI, タイトルなど）をリストアップします。
    *   リストアップされた引用論文について、ScienceDirect APIを通じて全文XMLファイルをダウンロードします。レートリミットに対応するため、指数バックオフによるリトライ機構が組み込まれています。
    *   収集されたメタデータとXMLファイルのパスは `data/processed/citing_papers_with_paths.csv` に保存され、XMLファイル自体は `data/raw/fulltext/` に保存されます。

### フェーズ3: データ準備 (`prepare_data_pipeline.py`)

*   **目的**: LLM検証および人間によるアノテーションの準備として、引用論文のサブセットをサンプリングし、そのXMLから必要なテキスト情報を抽出します。
*   **実験内容**:
    *   フェーズ2で全文XMLのダウンロードに成功した引用論文の中から、ランダムサンプリングによりアノテーション対象の論文リスト（例: 200件）を作成します。このリストは `data/ground_truth/annotation_target_list.csv` に保存され、人間による手動アノテーションの基礎となります。
    *   アノテーション対象の論文について、ダウンロードされたXMLファイルからアブストラクトと全文テキストを抽出します。抽出プロセスは、XML構造の多様性に対応するため、複数のパターンを試行するロバストな方法を採用しています。
    *   抽出されたテキストは、元の論文メタデータと結合され、`data/processed/samples_with_text.csv` に保存されます。

### フェーズ4: LLM検証 (`llm_validation_pipeline.py`)

*   **目的**: 大規模言語モデル (LLM) を用いて、引用論文がデータ論文のデータを「使用しているか」を自動的に予測する実験を行います。異なる入力テキスト（アブストラクト vs 全文）やプロンプト戦略（Zero-shot vs Few-shot CoT）の有効性を比較します。
*   **実験内容**:
    *   **API設定**: Google Gemini APIキーを設定し、LLMとの連携を確立します。
    *   **プロンプトテンプレート**: `prompts/` ディレクトリに格納されたプロンプトテンプレート（例: `zero_shot_abstract.txt`, `few_shot_cot_fulltext.txt`）を読み込み、LLMへの指示を定義します。
    *   **予測実行**:
        *   **アブストラクト予測**: 引用論文のアブストラクトのみをLLMに与え、データ利用の有無を予測させます（Zero-shot）。
        *   **全文Zero-shot予測**: 引用論文の全文テキストをLLMに与え、データ利用の有無を予測させます（Zero-shot）。
        *   **全文Few-shot CoT予測**: 引用論文の全文テキストと、思考の連鎖 (Chain-of-Thought) を促すFew-shotの例をLLMに与え、より詳細な推論を経て予測させます。
    *   **結果の保存**: 各予測結果は `data/processed/prediction_llm.csv` に保存され、既存の予測結果があれば更新されます。
    *   **再試行メカニズム**: APIエラーなどで予測が失敗した論文（予測値が-1）については、再試行する機能が提供されます。

### フェーズ5: 評価と分析 (`evaluate_results_pipeline.py`)

*   **目的**: LLMによる予測結果の性能を、人間がアノテーションした正解データと比較し、様々な評価指標を用いて分析します。また、ルールベースの手法やハイブリッドモデルの性能も評価します。
*   **実験内容**:
    *   **データ結合**: 正解データ (`annotation_target_list.csv`)、ルールベースの特徴量データ (`features_for_evaluation.csv` - このパイプラインでは生成されないが、評価に利用されることを想定)、およびLLM予測結果 (`prediction_llm.csv`) を結合します。
    *   **ハイブリッド予測の生成**: ルールベースの予測とLLMの予測を組み合わせたハイブリッドモデル（例: ANDゲート、階層的モデル）の予測値を生成します。
    *   **評価指標の計算**: 各予測モデル（ルールベース、LLM単体、ハイブリッド）について、以下の評価指標を計算します。
        *   Accuracy (正解率)
        *   Precision (適合率)
        *   Recall (再現率)
        *   F1-Score (F値)
        *   Confusion Matrix (混同行列の要素: TP, TN, FP, FN)
    *   **結果の保存と表示**: 計算された評価指標は `results/tables/evaluation_metrics_summary.csv` に保存され、コンソールにも表示されます。

### フェーズ6: レビューと修正 (`review_and_correct_pipeline.py`)

*   **目的**: LLMの予測と人間の判断が食い違った論文を特定し、人間が再レビューするためのプロンプトを生成します。これにより、正解データの品質向上やLLMの改善に役立てます。また、手動で修正された正解データをパイプラインにフィードバックするメカニズムを提供します。
*   **実験内容**:
    *   **データ結合**: 正解データ、LLM予測結果、およびテキスト抽出済みデータを結合し、レビューに必要な情報を集約します。
    *   **食い違いの特定**: 人間がアノテーションしたラベルと、最も性能の良いLLMモデルの予測結果が異なる論文を特定し、リストアップします。
    *   **レビュープロンプトの生成**: 食い違いが特定された各論文について、LLMに再確認を促すための詳細なプロンプトを生成し、コンソールに表示します。このプロンプトには、データ論文と引用論文のタイトル、引用論文の全文テキスト、人間とLLMの初期判断が含まれます。
    *   **手動修正の適用**: 人間によるレビューの結果、正解データに修正が必要と判断された場合、DOIと新しいラベルの辞書をパイプラインに渡すことで、`annotation_target_list.csv` を更新できます。

## 4. 実験の再現性への配慮

本パイプラインは、以下の点に配慮して実験の再現性を高めるように設計されています。

*   **モジュール化**: 各フェーズが独立したスクリプトとして提供され、個別に実行・検証が可能です。
*   **設定の一元化**: `src/config.py` でAPIキー、ファイルパス、モデル名などの設定を一元管理します。
*   **コマンドライン引数**: 実行時に主要なパラメータを調整できるため、異なる設定での実験が容易です。
*   **モックテスト**: APIリクエストなしでロジックを検証できるテストが提供され、開発とデバッグを支援します。
*   **ランダムシード**: サンプリングなどのランダムな処理には `random_state` を設定することで、結果の再現性を保証します。

## 5. 補足事項

*   `src/evaluation.py` で使用される `OUTPUT_FILE_FEATURES_FOR_EVALUATION` (ルールベースの特徴量CSV) は、このパイプラインでは直接生成されません。これは、別途ルールベースの検証プロセスが存在し、その結果がこのファイルに保存されることを想定しています。必要に応じて、このファイルを作成するフェーズを追加するか、既存のノートブックから生成してください。
