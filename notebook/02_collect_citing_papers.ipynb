# 引用論文の収集と全文XMLダウンロード

## 目的
このノートブックの目的は、`01_collect_data_papers.ipynb` で収集したデータ論文を**引用している側の論文（＝被引用論文）**の情報を収集し、全文 XML データをダウンロードすることです。
これは、実験で「どのセクションで引用されたか」を分析するための、最も重要なデータ収集ステップです。

## 達成状況
- `data_papers.csv` を読み込み、引用されている可能性が高い「被引用数 10 以上」のデータ論文を処理対象として選びました。
- 各データ論文について、Scopus API を使い、それを引用している論文のメタデータ（DOI, タイトルなど）をリストアップしました。
- リストアップした全被引用論文に対し、並列処理を用いて高速に ScienceDirect API にアクセスし、全文 XML のダウンロードを試みました。
- ダウンロードの成否（成功、キャッシュ、404 エラー、429 エラーなど）を記録し、最終的な結果を `citing_papers_with_paths.csv` に保存済みです。
- 429（レートリミット）エラーで失敗した論文を対象に、再試行するスクリプトも準備しました。

## 成果物
- `citing_papers_with_paths.csv`: 被引用論文のメタデータと、ダウンロードした XML へのパスや成否ステータスが記録された索引ファイル。
- `data/raw/fulltext/`内の XML ファイル群: 実際にダウンロードされた本文データ。

---

import pandas as pd
import os
import sys

# srcディレクトリをパスに追加
sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))

from src.collect_citing_papers import (
    load_data_papers_for_citing_collection,
    list_citing_papers,
    download_citing_papers_xml,
    save_citing_papers_results,
    retry_failed_downloads
)
from src.config import (
    SCOPUS_API_KEY,
    OUTPUT_FILE_DATA_PAPERS,
    OUTPUT_FILE_CITING_PAPERS_RAW,
    OUTPUT_FILE_CITING_PAPERS_WITH_PATHS,
    XML_OUTPUT_DIR
)

# --- 1. 入力データの読み込みと準備 ---
df_target_data_papers = load_data_papers_for_citing_collection(
    input_csv=OUTPUT_FILE_DATA_PAPERS,
    min_citations=10
)

if not df_target_data_papers.empty:
    # --- 2. ダウンロード対象となる全被引用論文のリストアップ ---
    citing_papers_list = list_citing_papers(
        df_data_papers=df_target_data_papers,
        api_key=SCOPUS_API_KEY
    )

    # --- 3. 本文XMLの並列ダウンロード ---
    # 初回ダウンロード
    df_download_results = download_citing_papers_xml(
        tasks=citing_papers_list,
        api_key=SCOPUS_API_KEY,
        output_dir=XML_OUTPUT_DIR,
        max_workers=10 # 初回は多めのワーカーで高速化
    )

    # --- 4. 最終的なデータの保存 ---
    save_citing_papers_results(
        df=df_download_results,
        output_csv=OUTPUT_FILE_CITING_PAPERS_WITH_PATHS
    )

    # --- 5. 失敗したダウンロードの再試行（特に429エラー） ---
    # 必要に応じて、max_workersを減らしてAPIへの負荷を軽減
    retry_failed_downloads(
        input_csv=OUTPUT_FILE_CITING_PAPERS_WITH_PATHS,
        api_key=SCOPUS_API_KEY,
        output_dir=XML_OUTPUT_DIR,
        max_workers=2 # 再試行時は少なめのワーカーで慎重に
    )
else:
    print("処理対象のデータ論文がありませんでした。")
