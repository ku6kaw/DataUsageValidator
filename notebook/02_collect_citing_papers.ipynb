{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34a2936d",
   "metadata": {},
   "source": [
    "📌 何をしていたか？ (目的)\n",
    "\n",
    "01 で収集したデータ論文を**引用している側の論文（＝被引用論文）**の情報を収集し、全文 XML データをダウンロードしていました。\n",
    "\n",
    "これは、実験で「どのセクションで引用されたか」を分析するための、最も重要なデータ収集ステップです。\n",
    "\n",
    "✅ どこまで行っているか？ (達成状況)\n",
    "\n",
    "data_papers.csv を読み込み、引用されている可能性が高い「被引用数 10 以上」のデータ論文を処理対象として選びました。\n",
    "\n",
    "各データ論文について、Scopus API を使い、それを引用している論文のメタデータ（DOI, タイトルなど）をリストアップしました (ステップ A)。\n",
    "\n",
    "リストアップした全被引用論文に対し、並列処理を用いて高速に ScienceDirect API にアクセスし、全文 XML のダウンロードを試みました (ステップ B)。\n",
    "\n",
    "ダウンロードの成否（成功、キャッシュ、404 エラー、429 エラーなど）を記録し、最終的な結果を citing_papers_with_paths.csv に保存済みです。\n",
    "\n",
    "429（レートリミット）エラーで失敗した論文を対象に、再試行するスクリプトも準備しました。\n",
    "\n",
    "➡️ このノートブックの成果物:\n",
    "\n",
    "citing_papers_with_paths.csv: 被引用論文のメタデータと、ダウンロードした XML へのパスや成否ステータスが記録された索引ファイル。\n",
    "\n",
    "data/raw/fulltext/内の XML ファイル群: 実際にダウンロードされた本文データ。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feff9851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "処理対象のデータ論文（被引用数10以上）: 4320件\n",
      "\n",
      "[ステップA] ダウンロード対象の被引用論文リストを作成しています...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aab75c6743c4d7b88dfa4dfb2aa8038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "データ論文をスキャン中:   0%|          | 0/4320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcursor\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m search_params \u001b[38;5;129;01mand\u001b[39;00m search_params[\u001b[33m'\u001b[39m\u001b[33mcursor\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m         search_response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSEARCH_API_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43msearch_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m         search_response.raise_for_status()\n\u001b[32m     50\u001b[39m         search_data = search_response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kurokawa\\Desktop\\DataUsageValidator\\DataUsageValidator\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kurokawa\\Desktop\\DataUsageValidator\\DataUsageValidator\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kurokawa\\Desktop\\DataUsageValidator\\DataUsageValidator\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kurokawa\\Desktop\\DataUsageValidator\\DataUsageValidator\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kurokawa\\Desktop\\DataUsageValidator\\DataUsageValidator\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kurokawa\\Desktop\\DataUsageValidator\\DataUsageValidator\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kurokawa\\Desktop\\DataUsageValidator\\DataUsageValidator\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kurokawa\\Desktop\\DataUsageValidator\\DataUsageValidator\\Lib\\site-packages\\urllib3\\connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1427\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1430\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# --- 設定項目 ---\n",
    "API_KEY = \"90469972beed34fa2913dc1ad6a644ac\" \n",
    "INPUT_DATA_PAPERS_CSV = '../data/processed/data_papers.csv'\n",
    "OUTPUT_CITING_PAPERS_CSV = '../data/processed/citing_papers_raw.csv'\n",
    "XML_OUTPUT_DIR = '../data/raw/xml/'\n",
    "SEARCH_API_URL = \"https://api.elsevier.com/content/search/scopus\"\n",
    "FULLTEXT_API_URL = \"https://api.elsevier.com/content/article/eid/\"\n",
    "\n",
    "# --- 1. 入力データの読み込みと準備 ---\n",
    "try:\n",
    "    df_data_papers = pd.read_csv(INPUT_DATA_PAPERS_CSV)\n",
    "    df_target = df_data_papers.dropna(subset=['eid'])\n",
    "    df_target['citedby_count'] = pd.to_numeric(df_target['citedby_count'], errors='coerce').fillna(0)\n",
    "    df_target = df_target[df_target['citedby_count'] >= 10].copy()\n",
    "    df_target = df_target.sort_values(by='citedby_count', ascending=False).reset_index(drop=True)\n",
    "    print(f\"処理対象のデータ論文（被引用数10以上）: {len(df_target)}件\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"エラー: '{INPUT_DATA_PAPERS_CSV}' が見つかりません。`01`のノートブックを先に実行してください。\")\n",
    "    df_target = pd.DataFrame()\n",
    "\n",
    "# --- ステップA: ダウンロード対象となる全被引用論文のリストアップ ---\n",
    "tasks = []\n",
    "if not df_target.empty:\n",
    "    print(\"\\n[ステップA] ダウンロード対象の被引用論文リストを作成しています...\")\n",
    "    \n",
    "    # データ論文を一つずつ処理\n",
    "    for index, data_paper in tqdm(df_target.iterrows(), total=len(df_target), desc=\"データ論文をスキャン中\"):\n",
    "        data_paper_eid = data_paper['eid']\n",
    "        data_paper_title = data_paper['title']\n",
    "        \n",
    "        # Scopus Search APIで被引用論文を検索\n",
    "        search_params = {\n",
    "            'apiKey': API_KEY, 'query': f\"REF({data_paper_eid})\",\n",
    "            'cursor': '*', 'count': 25, 'view': 'STANDARD'\n",
    "        }\n",
    "        \n",
    "        # カーソルを使ったページネーションループ\n",
    "        while 'cursor' in search_params and search_params['cursor']:\n",
    "            try:\n",
    "                search_response = requests.get(SEARCH_API_URL, params=search_params)\n",
    "                search_response.raise_for_status()\n",
    "                search_data = search_response.json()\n",
    "                \n",
    "                entries = search_data.get('search-results', {}).get('entry', [])\n",
    "                if not entries:\n",
    "                    break\n",
    "                \n",
    "                # 収集した被引用論文のメタデータをtasksリストに追加\n",
    "                for entry in entries:\n",
    "                    tasks.append({\n",
    "                        'citing_paper_eid': entry.get('eid'),\n",
    "                        'citing_paper_doi': entry.get('prism:doi'),\n",
    "                        'citing_paper_title': entry.get('dc:title'),\n",
    "                        'citing_paper_year': entry.get('prism:coverDate', '')[:4],\n",
    "                        'cited_data_paper_title': data_paper_title,\n",
    "                    })\n",
    "\n",
    "                # 次のカーソル情報を探して更新\n",
    "                next_cursor_url = next((link.get('@href') for link in search_data.get('search-results', {}).get('link', []) if link.get('@ref') == 'next'), None)\n",
    "                \n",
    "                if next_cursor_url:\n",
    "                    parsed_url = urlparse(next_cursor_url)\n",
    "                    query_params = parse_qs(parsed_url.query)\n",
    "                    search_params['cursor'] = query_params.get('cursor', [None])[0]\n",
    "                else:\n",
    "                    break # 次のページがなければこのデータ論文の処理は終了\n",
    "                \n",
    "                time.sleep(1)\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"  - API検索中にエラー (EID: {data_paper_eid}): {e}\")\n",
    "                break # エラーが発生したら次のデータ論文へ\n",
    "    \n",
    "    print(f\"[ステップA] 完了。合計 {len(tasks)} 件の被引用論文をリストアップしました。\")\n",
    "\n",
    "\n",
    "# --- ステップB: 本文XMLの並列ダウンロード ---\n",
    "\n",
    "# 個々のXMLをダウンロードする関数\n",
    "def download_xml(task):\n",
    "    eid = task.get('citing_paper_eid')\n",
    "    if not eid:\n",
    "        return eid, None\n",
    "        \n",
    "    xml_path = os.path.join(XML_OUTPUT_DIR, f\"{eid}.xml\")\n",
    "    \n",
    "    if os.path.exists(xml_path):\n",
    "        return eid, xml_path\n",
    "\n",
    "    try:\n",
    "        url = f\"{FULLTEXT_API_URL}{eid}?apiKey={API_KEY}\"\n",
    "        response = requests.get(url, headers={'Accept': 'application/xml'}, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            with open(xml_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(response.text)\n",
    "            return eid, xml_path\n",
    "    except requests.exceptions.RequestException:\n",
    "        pass\n",
    "        \n",
    "    return eid, None\n",
    "\n",
    "# マルチスレッドで並列実行\n",
    "updated_tasks = []\n",
    "if tasks:\n",
    "    os.makedirs(XML_OUTPUT_DIR, exist_ok=True)\n",
    "    print(f\"\\n[ステップB] {len(tasks)}件の論文XMLのダウンロードを並列で開始します...\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_eid = {executor.submit(download_xml, task): task['citing_paper_eid'] for task in tasks}\n",
    "        \n",
    "        for future in tqdm(as_completed(future_to_eid), total=len(tasks), desc=\"XMLダウンロード中\"):\n",
    "            try:\n",
    "                eid, result_path = future.result()\n",
    "                # 元のタスク情報に結果（ファイルパス）を追記\n",
    "                task_info = next(item for item in tasks if item[\"citing_paper_eid\"] == eid)\n",
    "                task_info['fulltext_xml_path'] = result_path\n",
    "                updated_tasks.append(task_info)\n",
    "            except Exception as e:\n",
    "                eid_for_error = future_to_eid[future]\n",
    "                print(f\"タスク処理中にエラーが発生 (EID: {eid_for_error}): {e}\")\n",
    "\n",
    "\n",
    "# --- 4. 最終的なデータの保存 ---\n",
    "if updated_tasks:\n",
    "    final_df = pd.DataFrame(updated_tasks)\n",
    "    final_df.to_csv(OUTPUT_CITING_PAPERS_CSV, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\n処理完了。XMLのパス情報を更新した {len(final_df)} 件の論文情報を '{OUTPUT_CITING_PAPERS_CSV}' に保存しました。\")\n",
    "    print(\"\\n--- 保存されたデータの先頭5件 ---\")\n",
    "    print(final_df.head())\n",
    "else:\n",
    "    print(\"\\n収集できた被引用論文はありませんでした。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51081d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'../data/processed/citing_papers_raw.csv' を正常に読み込みました。\n",
      "合計 148581 件の論文のXMLダウンロードを開始します。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7361ab86594c64afe4be6e3ab5f7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "XMLダウンロード中:   0%|          | 0/148581 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "処理完了。結果を '../data/processed/citing_papers_with_paths.csv' に保存しました。\n",
      "\n",
      "--- 処理結果サマリー ---\n",
      "download_status\n",
      "failed (Status: 429)    90613\n",
      "failed (Status: 404)    32862\n",
      "success (cached)        17023\n",
      "success (downloaded)     8083\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# --- 設定項目 ---\n",
    "API_KEY = \"90469972beed34fa2913dc1ad6a644ac\" \n",
    "INPUT_CITING_PAPERS_CSV = '../data/processed/citing_papers_raw.csv'\n",
    "OUTPUT_WITH_PATHS_CSV = '../data/processed/citing_papers_with_paths.csv'\n",
    "XML_OUTPUT_DIR = '../data/raw/fulltext/'\n",
    "FULLTEXT_API_URL = \"https://api.elsevier.com/content/article/doi/\"\n",
    "\n",
    "# --- 1. 入力データの読み込みと準備 ---\n",
    "try:\n",
    "    df_citing_papers = pd.read_csv(INPUT_CITING_PAPERS_CSV)\n",
    "    df_target = df_citing_papers.dropna(subset=['citing_paper_doi']).copy()\n",
    "    print(f\"'{INPUT_CITING_PAPERS_CSV}' を正常に読み込みました。\")\n",
    "    print(f\"合計 {len(df_target)} 件の論文のXMLダウンロードを開始します。\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"エラー: '{INPUT_CITING_PAPERS_CSV}' が見つかりません。\")\n",
    "    df_target = pd.DataFrame()\n",
    "\n",
    "# --- 【修正点①】ファイル名から不正な文字を取り除く関数を定義 ---\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"ファイル名として使えない文字をアンダースコアに置換する\"\"\"\n",
    "    invalid_chars = '<>:\"/\\\\|?*'\n",
    "    for char in invalid_chars:\n",
    "        filename = filename.replace(char, '_')\n",
    "    return filename\n",
    "\n",
    "# --- 2. XMLダウンロード用の関数を定義 ---\n",
    "def download_xml_by_doi(task):\n",
    "    doi = task.get('citing_paper_doi')\n",
    "    if not doi or pd.isna(doi):\n",
    "        task['fulltext_xml_path'] = None\n",
    "        task['download_status'] = \"failed (DOI is missing)\"\n",
    "        return task\n",
    "\n",
    "    # 【修正点②】サニタイズ関数を使って安全なファイル名を作成\n",
    "    safe_filename = sanitize_filename(doi) + '.xml'\n",
    "    xml_path = os.path.join(XML_OUTPUT_DIR, safe_filename)\n",
    "    \n",
    "    if os.path.exists(xml_path):\n",
    "        task['fulltext_xml_path'] = xml_path\n",
    "        task['download_status'] = \"success (cached)\"\n",
    "        return task\n",
    "\n",
    "    try:\n",
    "        url = f\"{FULLTEXT_API_URL}{doi}?apiKey={API_KEY}\"\n",
    "        response = requests.get(url, headers={'Accept': 'application/xml'}, timeout=60)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            with open(xml_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(response.text)\n",
    "            task['fulltext_xml_path'] = xml_path\n",
    "            task['download_status'] = \"success (downloaded)\"\n",
    "        else:\n",
    "            task['fulltext_xml_path'] = None\n",
    "            task['download_status'] = f\"failed (Status: {response.status_code})\"\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        task['fulltext_xml_path'] = None\n",
    "        task['download_status'] = \"failed (Request Error)\"\n",
    "        \n",
    "    return task\n",
    "\n",
    "# --- 3. 並列処理でダウンロードを実行 ---\n",
    "# (このセクションは変更ありません)\n",
    "results_list = []\n",
    "if not df_target.empty:\n",
    "    os.makedirs(XML_OUTPUT_DIR, exist_ok=True)\n",
    "    tasks = df_target.to_dict('records')\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        results_list = list(tqdm(executor.map(download_xml_by_doi, tasks), total=len(tasks), desc=\"XMLダウンロード中\"))\n",
    "\n",
    "# --- 4. 結果の集計と保存 ---\n",
    "# (このセクションは変更ありません)\n",
    "if results_list:\n",
    "    df_results = pd.DataFrame(results_list)\n",
    "    df_results.to_csv(OUTPUT_WITH_PATHS_CSV, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\n処理完了。結果を '{OUTPUT_WITH_PATHS_CSV}' に保存しました。\")\n",
    "    print(\"\\n--- 処理結果サマリー ---\")\n",
    "    print(df_results['download_status'].value_counts())\n",
    "else:\n",
    "    print(\"処理対象のデータがありませんでした。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3329c3e",
   "metadata": {},
   "source": [
    "429 の再試行\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c88f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# --- 基本設定（前回と同じ） ---\n",
    "API_KEY = \"90469972beed34fa2913dc1ad6a644ac\" \n",
    "RESULTS_CSV_PATH = '../data/processed/citing_papers_with_paths.csv'\n",
    "XML_OUTPUT_DIR = '../data/raw/fulltext/'\n",
    "FULLTEXT_API_URL = \"https://api.elsevier.com/content/article/doi/\"\n",
    "\n",
    "# download関数は前回と同じものを使用します\n",
    "def download_xml_by_doi_with_retry(task, max_retries=3):\n",
    "    doi = task.get('citing_paper_doi')\n",
    "    if not doi or pd.isna(doi):\n",
    "        task['fulltext_xml_path'] = None\n",
    "        task['download_status'] = \"failed (DOI is missing)\"\n",
    "        return task\n",
    "    safe_filename = doi.replace('/', '_') + '.xml'\n",
    "    xml_path = os.path.join(XML_OUTPUT_DIR, safe_filename)\n",
    "    if os.path.exists(xml_path):\n",
    "        task['fulltext_xml_path'] = xml_path\n",
    "        task['download_status'] = \"success (cached)\"\n",
    "        return task\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            url = f\"{FULLTEXT_API_URL}{doi}?apiKey={API_KEY}\"\n",
    "            response = requests.get(url, headers={'Accept': 'application/xml'}, timeout=60)\n",
    "            if response.status_code == 200:\n",
    "                with open(xml_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(response.text)\n",
    "                task['fulltext_xml_path'] = xml_path\n",
    "                task['download_status'] = f\"success (retry attempt {attempt + 1})\"\n",
    "                return task\n",
    "            elif response.status_code == 429:\n",
    "                wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                task['fulltext_xml_path'] = None\n",
    "                task['download_status'] = f\"failed (Status: {response.status_code})\"\n",
    "                return task\n",
    "        except requests.exceptions.RequestException:\n",
    "            time.sleep(2 ** attempt)\n",
    "    task['fulltext_xml_path'] = None\n",
    "    task['download_status'] = f\"failed (retries exhausted)\"\n",
    "    return task\n",
    "\n",
    "# --- 1. 失敗したタスクの読み込み ---\n",
    "try:\n",
    "    df_results = pd.read_csv(RESULTS_CSV_PATH)\n",
    "    # 【重要】429エラーで失敗したタスクのみを抽出\n",
    "    retry_targets_df = df_results[df_results['download_status'] == 'failed (Status: 429)'].copy()\n",
    "    \n",
    "    if not retry_targets_df.empty:\n",
    "        print(f\"レートリミットで失敗した {len(retry_targets_df)} 件のダウンロードを再試行します。\")\n",
    "        tasks_to_retry = retry_targets_df.to_dict('records')\n",
    "    else:\n",
    "        print(\"429エラーで失敗したタスクはありませんでした。\")\n",
    "        tasks_to_retry = []\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"エラー: '{RESULTS_CSV_PATH}' が見つかりません。\")\n",
    "    tasks_to_retry = []\n",
    "\n",
    "# --- 2. 失敗したタスクのみを、より低速で再実行 ---\n",
    "retry_results_list = []\n",
    "if tasks_to_retry:\n",
    "    # 【重要】max_workersの数を大幅に減らして、APIに優しくする\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        retry_results_list = list(tqdm(executor.map(download_xml_by_doi_with_retry, tasks_to_retry), total=len(tasks_to_retry), desc=\"429エラー再試行中\"))\n",
    "\n",
    "# --- 3. 元のデータと結果をマージして更新 ---\n",
    "if retry_results_list:\n",
    "    # 再試行の結果をDataFrameに変換\n",
    "    df_retry_results = pd.DataFrame(retry_results_list)\n",
    "    \n",
    "    # 元のDataFrameを更新するために、EIDまたはDOIをインデックスに設定\n",
    "    df_results.set_index('citing_paper_doi', inplace=True)\n",
    "    df_retry_results.set_index('citing_paper_doi', inplace=True)\n",
    "    \n",
    "    # 再試行の結果で元のデータを更新\n",
    "    df_results.update(df_retry_results)\n",
    "    \n",
    "    # インデックスをリセットして元の形式に戻す\n",
    "    df_results.reset_index(inplace=True)\n",
    "    \n",
    "    # 更新されたDataFrameを同じファイルに上書き保存\n",
    "    df_results.to_csv(RESULTS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\n再試行完了。'{RESULTS_CSV_PATH}' を更新しました。\")\n",
    "    print(\"\\n--- 最新の処理結果サマリー ---\")\n",
    "    print(df_results['download_status'].value_counts())\n",
    "else:\n",
    "    print(\"再試行するタスクはありませんでした。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataUsageValidator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
