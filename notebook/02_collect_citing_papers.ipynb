{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34a2936d",
   "metadata": {},
   "source": [
    "ğŸ“Œ ä½•ã‚’ã—ã¦ã„ãŸã‹ï¼Ÿ (ç›®çš„)\n",
    "\n",
    "01 ã§åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿è«–æ–‡ã‚’**å¼•ç”¨ã—ã¦ã„ã‚‹å´ã®è«–æ–‡ï¼ˆï¼è¢«å¼•ç”¨è«–æ–‡ï¼‰**ã®æƒ…å ±ã‚’åé›†ã—ã€å…¨æ–‡ XML ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã—ãŸã€‚\n",
    "\n",
    "ã“ã‚Œã¯ã€å®Ÿé¨“ã§ã€Œã©ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§å¼•ç”¨ã•ã‚ŒãŸã‹ã€ã‚’åˆ†æã™ã‚‹ãŸã‚ã®ã€æœ€ã‚‚é‡è¦ãªãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ãƒ†ãƒƒãƒ—ã§ã™ã€‚\n",
    "\n",
    "âœ… ã©ã“ã¾ã§è¡Œã£ã¦ã„ã‚‹ã‹ï¼Ÿ (é”æˆçŠ¶æ³)\n",
    "\n",
    "data_papers.csv ã‚’èª­ã¿è¾¼ã¿ã€å¼•ç”¨ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã€Œè¢«å¼•ç”¨æ•° 10 ä»¥ä¸Šã€ã®ãƒ‡ãƒ¼ã‚¿è«–æ–‡ã‚’å‡¦ç†å¯¾è±¡ã¨ã—ã¦é¸ã³ã¾ã—ãŸã€‚\n",
    "\n",
    "å„ãƒ‡ãƒ¼ã‚¿è«–æ–‡ã«ã¤ã„ã¦ã€Scopus API ã‚’ä½¿ã„ã€ãã‚Œã‚’å¼•ç”¨ã—ã¦ã„ã‚‹è«–æ–‡ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆDOI, ã‚¿ã‚¤ãƒˆãƒ«ãªã©ï¼‰ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ (ã‚¹ãƒ†ãƒƒãƒ— A)ã€‚\n",
    "\n",
    "ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ãŸå…¨è¢«å¼•ç”¨è«–æ–‡ã«å¯¾ã—ã€ä¸¦åˆ—å‡¦ç†ã‚’ç”¨ã„ã¦é«˜é€Ÿã« ScienceDirect API ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€å…¨æ–‡ XML ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’è©¦ã¿ã¾ã—ãŸ (ã‚¹ãƒ†ãƒƒãƒ— B)ã€‚\n",
    "\n",
    "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®æˆå¦ï¼ˆæˆåŠŸã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€404 ã‚¨ãƒ©ãƒ¼ã€429 ã‚¨ãƒ©ãƒ¼ãªã©ï¼‰ã‚’è¨˜éŒ²ã—ã€æœ€çµ‚çš„ãªçµæœã‚’ citing_papers_with_paths.csv ã«ä¿å­˜æ¸ˆã¿ã§ã™ã€‚\n",
    "\n",
    "429ï¼ˆãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆï¼‰ã‚¨ãƒ©ãƒ¼ã§å¤±æ•—ã—ãŸè«–æ–‡ã‚’å¯¾è±¡ã«ã€å†è©¦è¡Œã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚‚æº–å‚™ã—ã¾ã—ãŸã€‚\n",
    "\n",
    "â¡ï¸ ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®æˆæœç‰©:\n",
    "\n",
    "citing_papers_with_paths.csv: è¢«å¼•ç”¨è«–æ–‡ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸ XML ã¸ã®ãƒ‘ã‚¹ã‚„æˆå¦ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒè¨˜éŒ²ã•ã‚ŒãŸç´¢å¼•ãƒ•ã‚¡ã‚¤ãƒ«ã€‚\n",
    "\n",
    "data/raw/fulltext/å†…ã® XML ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤: å®Ÿéš›ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸæœ¬æ–‡ãƒ‡ãƒ¼ã‚¿ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feff9851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‡¦ç†å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿è«–æ–‡ï¼ˆè¢«å¼•ç”¨æ•°10ä»¥ä¸Šï¼‰: 4320ä»¶\n",
      "\n",
      "[ã‚¹ãƒ†ãƒƒãƒ—A] ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®è¢«å¼•ç”¨è«–æ–‡ãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã¦ã„ã¾ã™...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aab75c6743c4d7b88dfa4dfb2aa8038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ãƒ‡ãƒ¼ã‚¿è«–æ–‡ã‚’ã‚¹ã‚­ãƒ£ãƒ³ä¸­:   0%|          | 0/4320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcursor\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m search_params \u001b[38;5;129;01mand\u001b[39;00m search_params[\u001b[33m'\u001b[39m\u001b[33mcursor\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m         search_response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSEARCH_API_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43msearch_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m         search_response.raise_for_status()\n\u001b[32m     50\u001b[39m         search_data = search_response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kurokawa\\Desktop\\DataUsageValidator\\DataUsageValidator\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kurokawa\\Desktop\\DataUsageValidator\\DataUsageValidator\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kurokawa\\Desktop\\DataUsageValidator\\DataUsageValidator\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kurokawa\\Desktop\\DataUsageValidator\\DataUsageValidator\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kurokawa\\Desktop\\DataUsageValidator\\DataUsageValidator\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kurokawa\\Desktop\\DataUsageValidator\\DataUsageValidator\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kurokawa\\Desktop\\DataUsageValidator\\DataUsageValidator\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kurokawa\\Desktop\\DataUsageValidator\\DataUsageValidator\\Lib\\site-packages\\urllib3\\connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1427\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1430\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# --- è¨­å®šé …ç›® ---\n",
    "API_KEY = \"90469972beed34fa2913dc1ad6a644ac\" \n",
    "INPUT_DATA_PAPERS_CSV = '../data/processed/data_papers.csv'\n",
    "OUTPUT_CITING_PAPERS_CSV = '../data/processed/citing_papers_raw.csv'\n",
    "XML_OUTPUT_DIR = '../data/raw/xml/'\n",
    "SEARCH_API_URL = \"https://api.elsevier.com/content/search/scopus\"\n",
    "FULLTEXT_API_URL = \"https://api.elsevier.com/content/article/eid/\"\n",
    "\n",
    "# --- 1. å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨æº–å‚™ ---\n",
    "try:\n",
    "    df_data_papers = pd.read_csv(INPUT_DATA_PAPERS_CSV)\n",
    "    df_target = df_data_papers.dropna(subset=['eid'])\n",
    "    df_target['citedby_count'] = pd.to_numeric(df_target['citedby_count'], errors='coerce').fillna(0)\n",
    "    df_target = df_target[df_target['citedby_count'] >= 10].copy()\n",
    "    df_target = df_target.sort_values(by='citedby_count', ascending=False).reset_index(drop=True)\n",
    "    print(f\"å‡¦ç†å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿è«–æ–‡ï¼ˆè¢«å¼•ç”¨æ•°10ä»¥ä¸Šï¼‰: {len(df_target)}ä»¶\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ã‚¨ãƒ©ãƒ¼: '{INPUT_DATA_PAPERS_CSV}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`01`ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "    df_target = pd.DataFrame()\n",
    "\n",
    "# --- ã‚¹ãƒ†ãƒƒãƒ—A: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã¨ãªã‚‹å…¨è¢«å¼•ç”¨è«–æ–‡ã®ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ— ---\n",
    "tasks = []\n",
    "if not df_target.empty:\n",
    "    print(\"\\n[ã‚¹ãƒ†ãƒƒãƒ—A] ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®è¢«å¼•ç”¨è«–æ–‡ãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã¦ã„ã¾ã™...\")\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿è«–æ–‡ã‚’ä¸€ã¤ãšã¤å‡¦ç†\n",
    "    for index, data_paper in tqdm(df_target.iterrows(), total=len(df_target), desc=\"ãƒ‡ãƒ¼ã‚¿è«–æ–‡ã‚’ã‚¹ã‚­ãƒ£ãƒ³ä¸­\"):\n",
    "        data_paper_eid = data_paper['eid']\n",
    "        data_paper_title = data_paper['title']\n",
    "        \n",
    "        # Scopus Search APIã§è¢«å¼•ç”¨è«–æ–‡ã‚’æ¤œç´¢\n",
    "        search_params = {\n",
    "            'apiKey': API_KEY, 'query': f\"REF({data_paper_eid})\",\n",
    "            'cursor': '*', 'count': 25, 'view': 'STANDARD'\n",
    "        }\n",
    "        \n",
    "        # ã‚«ãƒ¼ã‚½ãƒ«ã‚’ä½¿ã£ãŸãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ—\n",
    "        while 'cursor' in search_params and search_params['cursor']:\n",
    "            try:\n",
    "                search_response = requests.get(SEARCH_API_URL, params=search_params)\n",
    "                search_response.raise_for_status()\n",
    "                search_data = search_response.json()\n",
    "                \n",
    "                entries = search_data.get('search-results', {}).get('entry', [])\n",
    "                if not entries:\n",
    "                    break\n",
    "                \n",
    "                # åé›†ã—ãŸè¢«å¼•ç”¨è«–æ–‡ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’tasksãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "                for entry in entries:\n",
    "                    tasks.append({\n",
    "                        'citing_paper_eid': entry.get('eid'),\n",
    "                        'citing_paper_doi': entry.get('prism:doi'),\n",
    "                        'citing_paper_title': entry.get('dc:title'),\n",
    "                        'citing_paper_year': entry.get('prism:coverDate', '')[:4],\n",
    "                        'cited_data_paper_title': data_paper_title,\n",
    "                    })\n",
    "\n",
    "                # æ¬¡ã®ã‚«ãƒ¼ã‚½ãƒ«æƒ…å ±ã‚’æ¢ã—ã¦æ›´æ–°\n",
    "                next_cursor_url = next((link.get('@href') for link in search_data.get('search-results', {}).get('link', []) if link.get('@ref') == 'next'), None)\n",
    "                \n",
    "                if next_cursor_url:\n",
    "                    parsed_url = urlparse(next_cursor_url)\n",
    "                    query_params = parse_qs(parsed_url.query)\n",
    "                    search_params['cursor'] = query_params.get('cursor', [None])[0]\n",
    "                else:\n",
    "                    break # æ¬¡ã®ãƒšãƒ¼ã‚¸ãŒãªã‘ã‚Œã°ã“ã®ãƒ‡ãƒ¼ã‚¿è«–æ–‡ã®å‡¦ç†ã¯çµ‚äº†\n",
    "                \n",
    "                time.sleep(1)\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"  - APIæ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ (EID: {data_paper_eid}): {e}\")\n",
    "                break # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‚‰æ¬¡ã®ãƒ‡ãƒ¼ã‚¿è«–æ–‡ã¸\n",
    "    \n",
    "    print(f\"[ã‚¹ãƒ†ãƒƒãƒ—A] å®Œäº†ã€‚åˆè¨ˆ {len(tasks)} ä»¶ã®è¢«å¼•ç”¨è«–æ–‡ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸã€‚\")\n",
    "\n",
    "\n",
    "# --- ã‚¹ãƒ†ãƒƒãƒ—B: æœ¬æ–‡XMLã®ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ---\n",
    "\n",
    "# å€‹ã€…ã®XMLã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•°\n",
    "def download_xml(task):\n",
    "    eid = task.get('citing_paper_eid')\n",
    "    if not eid:\n",
    "        return eid, None\n",
    "        \n",
    "    xml_path = os.path.join(XML_OUTPUT_DIR, f\"{eid}.xml\")\n",
    "    \n",
    "    if os.path.exists(xml_path):\n",
    "        return eid, xml_path\n",
    "\n",
    "    try:\n",
    "        url = f\"{FULLTEXT_API_URL}{eid}?apiKey={API_KEY}\"\n",
    "        response = requests.get(url, headers={'Accept': 'application/xml'}, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            with open(xml_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(response.text)\n",
    "            return eid, xml_path\n",
    "    except requests.exceptions.RequestException:\n",
    "        pass\n",
    "        \n",
    "    return eid, None\n",
    "\n",
    "# ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ã§ä¸¦åˆ—å®Ÿè¡Œ\n",
    "updated_tasks = []\n",
    "if tasks:\n",
    "    os.makedirs(XML_OUTPUT_DIR, exist_ok=True)\n",
    "    print(f\"\\n[ã‚¹ãƒ†ãƒƒãƒ—B] {len(tasks)}ä»¶ã®è«–æ–‡XMLã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ä¸¦åˆ—ã§é–‹å§‹ã—ã¾ã™...\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_eid = {executor.submit(download_xml, task): task['citing_paper_eid'] for task in tasks}\n",
    "        \n",
    "        for future in tqdm(as_completed(future_to_eid), total=len(tasks), desc=\"XMLãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­\"):\n",
    "            try:\n",
    "                eid, result_path = future.result()\n",
    "                # å…ƒã®ã‚¿ã‚¹ã‚¯æƒ…å ±ã«çµæœï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼‰ã‚’è¿½è¨˜\n",
    "                task_info = next(item for item in tasks if item[\"citing_paper_eid\"] == eid)\n",
    "                task_info['fulltext_xml_path'] = result_path\n",
    "                updated_tasks.append(task_info)\n",
    "            except Exception as e:\n",
    "                eid_for_error = future_to_eid[future]\n",
    "                print(f\"ã‚¿ã‚¹ã‚¯å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ (EID: {eid_for_error}): {e}\")\n",
    "\n",
    "\n",
    "# --- 4. æœ€çµ‚çš„ãªãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ ---\n",
    "if updated_tasks:\n",
    "    final_df = pd.DataFrame(updated_tasks)\n",
    "    final_df.to_csv(OUTPUT_CITING_PAPERS_CSV, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nå‡¦ç†å®Œäº†ã€‚XMLã®ãƒ‘ã‚¹æƒ…å ±ã‚’æ›´æ–°ã—ãŸ {len(final_df)} ä»¶ã®è«–æ–‡æƒ…å ±ã‚’ '{OUTPUT_CITING_PAPERS_CSV}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚\")\n",
    "    print(\"\\n--- ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­5ä»¶ ---\")\n",
    "    print(final_df.head())\n",
    "else:\n",
    "    print(\"\\nåé›†ã§ããŸè¢«å¼•ç”¨è«–æ–‡ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51081d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'../data/processed/citing_papers_raw.csv' ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\n",
      "åˆè¨ˆ 148581 ä»¶ã®è«–æ–‡ã®XMLãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7361ab86594c64afe4be6e3ab5f7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "XMLãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­:   0%|          | 0/148581 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å‡¦ç†å®Œäº†ã€‚çµæœã‚’ '../data/processed/citing_papers_with_paths.csv' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚\n",
      "\n",
      "--- å‡¦ç†çµæœã‚µãƒãƒªãƒ¼ ---\n",
      "download_status\n",
      "failed (Status: 429)    90613\n",
      "failed (Status: 404)    32862\n",
      "success (cached)        17023\n",
      "success (downloaded)     8083\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# --- è¨­å®šé …ç›® ---\n",
    "API_KEY = \"90469972beed34fa2913dc1ad6a644ac\" \n",
    "INPUT_CITING_PAPERS_CSV = '../data/processed/citing_papers_raw.csv'\n",
    "OUTPUT_WITH_PATHS_CSV = '../data/processed/citing_papers_with_paths.csv'\n",
    "XML_OUTPUT_DIR = '../data/raw/fulltext/'\n",
    "FULLTEXT_API_URL = \"https://api.elsevier.com/content/article/doi/\"\n",
    "\n",
    "# --- 1. å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨æº–å‚™ ---\n",
    "try:\n",
    "    df_citing_papers = pd.read_csv(INPUT_CITING_PAPERS_CSV)\n",
    "    df_target = df_citing_papers.dropna(subset=['citing_paper_doi']).copy()\n",
    "    print(f\"'{INPUT_CITING_PAPERS_CSV}' ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\")\n",
    "    print(f\"åˆè¨ˆ {len(df_target)} ä»¶ã®è«–æ–‡ã®XMLãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ã‚¨ãƒ©ãƒ¼: '{INPUT_CITING_PAPERS_CSV}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
    "    df_target = pd.DataFrame()\n",
    "\n",
    "# --- ã€ä¿®æ­£ç‚¹â‘ ã€‘ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ä¸æ­£ãªæ–‡å­—ã‚’å–ã‚Šé™¤ãé–¢æ•°ã‚’å®šç¾© ---\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã—ã¦ä½¿ãˆãªã„æ–‡å­—ã‚’ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã«ç½®æ›ã™ã‚‹\"\"\"\n",
    "    invalid_chars = '<>:\"/\\\\|?*'\n",
    "    for char in invalid_chars:\n",
    "        filename = filename.replace(char, '_')\n",
    "    return filename\n",
    "\n",
    "# --- 2. XMLãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ã®é–¢æ•°ã‚’å®šç¾© ---\n",
    "def download_xml_by_doi(task):\n",
    "    doi = task.get('citing_paper_doi')\n",
    "    if not doi or pd.isna(doi):\n",
    "        task['fulltext_xml_path'] = None\n",
    "        task['download_status'] = \"failed (DOI is missing)\"\n",
    "        return task\n",
    "\n",
    "    # ã€ä¿®æ­£ç‚¹â‘¡ã€‘ã‚µãƒ‹ã‚¿ã‚¤ã‚ºé–¢æ•°ã‚’ä½¿ã£ã¦å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½œæˆ\n",
    "    safe_filename = sanitize_filename(doi) + '.xml'\n",
    "    xml_path = os.path.join(XML_OUTPUT_DIR, safe_filename)\n",
    "    \n",
    "    if os.path.exists(xml_path):\n",
    "        task['fulltext_xml_path'] = xml_path\n",
    "        task['download_status'] = \"success (cached)\"\n",
    "        return task\n",
    "\n",
    "    try:\n",
    "        url = f\"{FULLTEXT_API_URL}{doi}?apiKey={API_KEY}\"\n",
    "        response = requests.get(url, headers={'Accept': 'application/xml'}, timeout=60)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            with open(xml_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(response.text)\n",
    "            task['fulltext_xml_path'] = xml_path\n",
    "            task['download_status'] = \"success (downloaded)\"\n",
    "        else:\n",
    "            task['fulltext_xml_path'] = None\n",
    "            task['download_status'] = f\"failed (Status: {response.status_code})\"\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        task['fulltext_xml_path'] = None\n",
    "        task['download_status'] = \"failed (Request Error)\"\n",
    "        \n",
    "    return task\n",
    "\n",
    "# --- 3. ä¸¦åˆ—å‡¦ç†ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ ---\n",
    "# (ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å¤‰æ›´ã‚ã‚Šã¾ã›ã‚“)\n",
    "results_list = []\n",
    "if not df_target.empty:\n",
    "    os.makedirs(XML_OUTPUT_DIR, exist_ok=True)\n",
    "    tasks = df_target.to_dict('records')\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        results_list = list(tqdm(executor.map(download_xml_by_doi, tasks), total=len(tasks), desc=\"XMLãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­\"))\n",
    "\n",
    "# --- 4. çµæœã®é›†è¨ˆã¨ä¿å­˜ ---\n",
    "# (ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å¤‰æ›´ã‚ã‚Šã¾ã›ã‚“)\n",
    "if results_list:\n",
    "    df_results = pd.DataFrame(results_list)\n",
    "    df_results.to_csv(OUTPUT_WITH_PATHS_CSV, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nå‡¦ç†å®Œäº†ã€‚çµæœã‚’ '{OUTPUT_WITH_PATHS_CSV}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚\")\n",
    "    print(\"\\n--- å‡¦ç†çµæœã‚µãƒãƒªãƒ¼ ---\")\n",
    "    print(df_results['download_status'].value_counts())\n",
    "else:\n",
    "    print(\"å‡¦ç†å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3329c3e",
   "metadata": {},
   "source": [
    "429 ã®å†è©¦è¡Œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c88f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# --- åŸºæœ¬è¨­å®šï¼ˆå‰å›ã¨åŒã˜ï¼‰ ---\n",
    "API_KEY = \"90469972beed34fa2913dc1ad6a644ac\" \n",
    "RESULTS_CSV_PATH = '../data/processed/citing_papers_with_paths.csv'\n",
    "XML_OUTPUT_DIR = '../data/raw/fulltext/'\n",
    "FULLTEXT_API_URL = \"https://api.elsevier.com/content/article/doi/\"\n",
    "\n",
    "# downloadé–¢æ•°ã¯å‰å›ã¨åŒã˜ã‚‚ã®ã‚’ä½¿ç”¨ã—ã¾ã™\n",
    "def download_xml_by_doi_with_retry(task, max_retries=3):\n",
    "    doi = task.get('citing_paper_doi')\n",
    "    if not doi or pd.isna(doi):\n",
    "        task['fulltext_xml_path'] = None\n",
    "        task['download_status'] = \"failed (DOI is missing)\"\n",
    "        return task\n",
    "    safe_filename = doi.replace('/', '_') + '.xml'\n",
    "    xml_path = os.path.join(XML_OUTPUT_DIR, safe_filename)\n",
    "    if os.path.exists(xml_path):\n",
    "        task['fulltext_xml_path'] = xml_path\n",
    "        task['download_status'] = \"success (cached)\"\n",
    "        return task\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            url = f\"{FULLTEXT_API_URL}{doi}?apiKey={API_KEY}\"\n",
    "            response = requests.get(url, headers={'Accept': 'application/xml'}, timeout=60)\n",
    "            if response.status_code == 200:\n",
    "                with open(xml_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(response.text)\n",
    "                task['fulltext_xml_path'] = xml_path\n",
    "                task['download_status'] = f\"success (retry attempt {attempt + 1})\"\n",
    "                return task\n",
    "            elif response.status_code == 429:\n",
    "                wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                task['fulltext_xml_path'] = None\n",
    "                task['download_status'] = f\"failed (Status: {response.status_code})\"\n",
    "                return task\n",
    "        except requests.exceptions.RequestException:\n",
    "            time.sleep(2 ** attempt)\n",
    "    task['fulltext_xml_path'] = None\n",
    "    task['download_status'] = f\"failed (retries exhausted)\"\n",
    "    return task\n",
    "\n",
    "# --- 1. å¤±æ•—ã—ãŸã‚¿ã‚¹ã‚¯ã®èª­ã¿è¾¼ã¿ ---\n",
    "try:\n",
    "    df_results = pd.read_csv(RESULTS_CSV_PATH)\n",
    "    # ã€é‡è¦ã€‘429ã‚¨ãƒ©ãƒ¼ã§å¤±æ•—ã—ãŸã‚¿ã‚¹ã‚¯ã®ã¿ã‚’æŠ½å‡º\n",
    "    retry_targets_df = df_results[df_results['download_status'] == 'failed (Status: 429)'].copy()\n",
    "    \n",
    "    if not retry_targets_df.empty:\n",
    "        print(f\"ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆã§å¤±æ•—ã—ãŸ {len(retry_targets_df)} ä»¶ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å†è©¦è¡Œã—ã¾ã™ã€‚\")\n",
    "        tasks_to_retry = retry_targets_df.to_dict('records')\n",
    "    else:\n",
    "        print(\"429ã‚¨ãƒ©ãƒ¼ã§å¤±æ•—ã—ãŸã‚¿ã‚¹ã‚¯ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "        tasks_to_retry = []\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"ã‚¨ãƒ©ãƒ¼: '{RESULTS_CSV_PATH}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
    "    tasks_to_retry = []\n",
    "\n",
    "# --- 2. å¤±æ•—ã—ãŸã‚¿ã‚¹ã‚¯ã®ã¿ã‚’ã€ã‚ˆã‚Šä½é€Ÿã§å†å®Ÿè¡Œ ---\n",
    "retry_results_list = []\n",
    "if tasks_to_retry:\n",
    "    # ã€é‡è¦ã€‘max_workersã®æ•°ã‚’å¤§å¹…ã«æ¸›ã‚‰ã—ã¦ã€APIã«å„ªã—ãã™ã‚‹\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        retry_results_list = list(tqdm(executor.map(download_xml_by_doi_with_retry, tasks_to_retry), total=len(tasks_to_retry), desc=\"429ã‚¨ãƒ©ãƒ¼å†è©¦è¡Œä¸­\"))\n",
    "\n",
    "# --- 3. å…ƒã®ãƒ‡ãƒ¼ã‚¿ã¨çµæœã‚’ãƒãƒ¼ã‚¸ã—ã¦æ›´æ–° ---\n",
    "if retry_results_list:\n",
    "    # å†è©¦è¡Œã®çµæœã‚’DataFrameã«å¤‰æ›\n",
    "    df_retry_results = pd.DataFrame(retry_results_list)\n",
    "    \n",
    "    # å…ƒã®DataFrameã‚’æ›´æ–°ã™ã‚‹ãŸã‚ã«ã€EIDã¾ãŸã¯DOIã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨­å®š\n",
    "    df_results.set_index('citing_paper_doi', inplace=True)\n",
    "    df_retry_results.set_index('citing_paper_doi', inplace=True)\n",
    "    \n",
    "    # å†è©¦è¡Œã®çµæœã§å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°\n",
    "    df_results.update(df_retry_results)\n",
    "    \n",
    "    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦å…ƒã®å½¢å¼ã«æˆ»ã™\n",
    "    df_results.reset_index(inplace=True)\n",
    "    \n",
    "    # æ›´æ–°ã•ã‚ŒãŸDataFrameã‚’åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¸Šæ›¸ãä¿å­˜\n",
    "    df_results.to_csv(RESULTS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\nå†è©¦è¡Œå®Œäº†ã€‚'{RESULTS_CSV_PATH}' ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚\")\n",
    "    print(\"\\n--- æœ€æ–°ã®å‡¦ç†çµæœã‚µãƒãƒªãƒ¼ ---\")\n",
    "    print(df_results['download_status'].value_counts())\n",
    "else:\n",
    "    print(\"å†è©¦è¡Œã™ã‚‹ã‚¿ã‚¹ã‚¯ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataUsageValidator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
