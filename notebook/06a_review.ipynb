{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1154f35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 正解データとLLM予測結果の読み込み・結合が完了しました。\n",
      "\n",
      "======================================================================\n",
      "【人間の判断とLLMの判断が食い違った「怪しい」論文リスト】\n",
      "======================================================================\n",
      "合計 35 件の食い違いが見つかりました。\n",
      "                         citing_paper_doi                                                                                                                                                                                        citing_paper_title Human_Label LLM_Prediction\n",
      "1          10.1016/j.jaridenv.2024.105282                                                                                                     Potential effects of climate change on cacti distribution and conservation in North American drylands        Used       Not Used\n",
      "2          10.1016/j.revpalbo.2023.104989                                                                                                                                     Approaches to pollen taxonomic harmonisation in Quaternary palynology    Not Used           Used\n",
      "9         10.1016/j.resconrec.2023.107296                                                                                                      Government resource allocation practices toward carbon neutrality in China: A hybrid system approach        Used       Not Used\n",
      "10          10.1016/j.solener.2025.113509                                                                                                           Predictive modeling and optimization of CIGS thin film solar cells: A machine learning approach        Used       Not Used\n",
      "18              10.1016/j.ces.2024.121101                                                                                                                                           Experimental and modelling study of ammonia-based FGD scrubbers    Not Used           Used\n",
      "24              10.1016/j.ref.2023.100490                                                                                                    Performance and dynamics of California offshore wind alongside Western US onshore wind and solar power    Not Used           Used\n",
      "34             10.1016/j.sftr.2025.100578                                                                              Towards a sustainable future: The interplay of trade globalization and regulatory quality on environmental outcomes in India    Not Used           Used\n",
      "47        10.1016/j.gloplacha.2022.103785                Linking the Mediterranean MIS 5 tephra markers to Campi Flegrei (southern Italy) 109–92 ka explosive activity and refining the chronology of MIS 5c-d millennial-scale climate variability        Used       Not Used\n",
      "58              10.1016/j.str.2024.10.016                                                                                                                              LEDGF interacts with the NID domain of MeCP2 and modulates MeCP2 condensates        Used       Not Used\n",
      "60            10.1016/j.uclim.2025.102409                                                                                                                                 Government-involved urban meteorological networks (UMNs): A global review    Not Used           Used\n",
      "61             10.1016/j.imed.2024.12.001                                                                               Osteosarcoma knowledge graph question answering system: deep learning-based knowledge graph and large language model fusion    Not Used           Used\n",
      "66              10.1016/j.nut.2023.112182                                                                     Trends in socioeconomic inequalities in malnutrition among children under 5 in the Democratic Republic of the Congo from 2001 to 2018    Not Used           Used\n",
      "73           10.1016/j.pestbp.2025.106399                                                                                                                           Cythochrome P450-mediated dinotefuran resistance in onion thrips, Thrips tabaci    Not Used           Used\n",
      "92          10.1016/j.jhydrol.2024.132393                                                               Evaluating SWAT-3PG simulation of hydrologic and water quality processes in a forested watershed: A case study in the St. Croix River Basin    Not Used           Used\n",
      "97          10.1016/j.foodres.2025.116829                                                                             Health risk assessment from heavy metals in crops grown and marketed by family farmers in the mining region of Moquegua, Peru    Not Used           Used\n",
      "99   10.1016/j.theriogenology.2019.12.012                                                     Effect of carboxylated poly L-Lysine as a cryoprotectant on post-thaw quality and in vivo fertility of Nili Ravi buffalo (Bubalus bubalis) bull semen    Not Used           Used\n",
      "101             10.1016/j.rse.2023.113578                                                                        A first Chinese building height estimate at 10 m resolution (CNBH-10 m) using multi-source earth observations and machine learning        Used       Not Used\n",
      "114          10.1016/j.chiabu.2023.106299                                                                                                        Do parenting behaviors intended as discipline vary by household religious affiliation in Cameroon?    Not Used           Used\n",
      "118             10.1016/j.jsg.2025.105445  Multi-millennia slip rate relationships between closely spaced across-strike faults: Temporal earthquake clustering of the Skinos and Pisia Faults, Greece, from in situ 36Cl cosmogenic exposure dating    Not Used           Used\n",
      "122          10.1016/j.compag.2024.108680                                                                                         Harnessing quantum computing for smart agriculture: Empowering sustainable crop management and yield optimization        Used       Not Used\n",
      "126       10.1016/j.quascirev.2023.108368                                                            Holocene vertical velocity fields in the calabrian arc (southern Italy): New insights from uplifted sea-level markers in the Crotone Peninsula        Used       Not Used\n",
      "129           10.1016/j.ijdrr.2024.104605                                                       Nationwide evaluation of changes in fluvial and pluvial flood damage and the effectiveness of adaptation measures in Japan under population decline    Not Used           Used\n",
      "134             10.1016/j.dib.2021.107531                                                                                                                                                        Dataset of seized wildlife and their intended uses        Used       Not Used\n",
      "140       10.1016/j.scitotenv.2022.161157                                                                                         Widespread missing super-emitters of nitrogen oxides across China inferred from year-round satellite observations    Not Used           Used\n",
      "146            10.1016/j.agsy.2024.104243                                                                                         Enhancing simulations of biomass and nitrous oxide emissions in vineyard, orchard, and vegetable cropping systems        Used       Not Used\n",
      "148          10.1016/j.surfin.2025.106134                                                                                        Synthesis and adsorption efficiency of magnetically separable sawdust-based activated carbon for ibuprofen removal        Used       Not Used\n",
      "152       10.1016/j.scitotenv.2024.177626         Effects of sublethal concentration of thiamethoxam formulation on the wild stingless bee, Partamona helleri Friese (Hymenoptera: Apidae): Histopathology, oxidative stress and behavioral changes    Not Used           Used\n",
      "153           10.1016/j.ygeno.2021.09.013                                                                Differential analysis of chromatin accessibility and gene expression profiles identifies cis-regulatory elements in rat adipose and muscle    Not Used           Used\n",
      "168          10.1016/j.envres.2022.112901                                                                          Viral diversity and biogeochemical potential revealed in different prawn-culture sediments by virus-enriched metagenome analysis    Not Used           Used\n",
      "174          10.1016/j.ejmech.2021.113921                                                                                                                           Coumarin–benzimidazole hybrids: A review of developments in medicinal chemistry        Used       Not Used\n",
      "177         10.1016/j.pmatsci.2017.02.001                                                                                                                                   Recent progress in marine foul-release polymeric nanocomposite coatings        Used       Not Used\n",
      "180           10.1016/j.media.2024.103201                                                                                                                                  A comprehensive survey on deep active learning in medical image analysis        Used       Not Used\n",
      "181      10.1016/j.jretconser.2024.103723                                                                                       Importance and performance in PLS-SEM and NCA: Introducing the combined importance-performance map analysis (cIPMA)        Used       Not Used\n",
      "185       10.1016/j.resconrec.2024.107934                                                                                                       Assessment of forest disturbance and soil erosion in wind farm project using satellite observations        Used       Not Used\n",
      "192    10.1016/j.brainresbull.2025.111327                                                                                                                                  Pad2 deletion induces anxiety‑like behaviors and memory deficits in mice    Not Used           Used\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. ファイルパスの設定 ---\n",
    "GROUND_TRUTH_CSV = '../data/ground_truth/annotation_target_list.csv'\n",
    "LLM_PREDICTIONS_CSV = '../data/processed/prediction_llm.csv'\n",
    "\n",
    "# --- 2. データの読み込みと結合 ---\n",
    "try:\n",
    "    df_gt = pd.read_csv(GROUND_TRUTH_CSV)\n",
    "    df_llm = pd.read_csv(LLM_PREDICTIONS_CSV)\n",
    "\n",
    "    # 評価対象のモデルのカラム名\n",
    "    best_model_column = 'prediction_rule3_gemini-2_5-flash' # Few-shot版がF1最高だったのでこれを採用\n",
    "\n",
    "    df_review = pd.merge(\n",
    "        df_gt[['citing_paper_doi', 'citing_paper_title', 'is_data_used_gt']],\n",
    "        df_llm[['citing_paper_doi', best_model_column]],\n",
    "        on='citing_paper_doi',\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # -1やNaNを除外\n",
    "    df_review.dropna(inplace=True)\n",
    "    df_review = df_review[df_review[best_model_column] != -1]\n",
    "    df_review[best_model_column] = df_review[best_model_column].astype(int)\n",
    "    df_review['is_data_used_gt'] = df_review['is_data_used_gt'].astype(int)\n",
    "    \n",
    "    print(\"✅ 正解データとLLM予測結果の読み込み・結合が完了しました。\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ エラー: ファイルが見つかりません。パスを確認してください。 {e}\")\n",
    "    df_review = pd.DataFrame()\n",
    "\n",
    "# --- 3. 判断が食い違った論文を抽出して表示 ---\n",
    "if not df_review.empty:\n",
    "    \n",
    "    # 判断の不一致を検出 (正解ラベルと予測が異なるもの)\n",
    "    disagreements = df_review[df_review['is_data_used_gt'] != df_review[best_model_column]].copy()\n",
    "\n",
    "    # 人間の判断とLLMの判断を分かりやすくラベル付け\n",
    "    disagreements['Human_Label'] = disagreements['is_data_used_gt'].map({1: 'Used', 0: 'Not Used'})\n",
    "    disagreements['LLM_Prediction'] = disagreements[best_model_column].map({1: 'Used', 0: 'Not Used'})\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"【人間の判断とLLMの判断が食い違った「怪しい」論文リスト】\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not disagreements.empty:\n",
    "        print(f\"合計 {len(disagreements)} 件の食い違いが見つかりました。\")\n",
    "        # to_string()ですべての行を省略せずに表示\n",
    "        print(disagreements[['citing_paper_doi', 'citing_paper_title', 'Human_Label', 'LLM_Prediction']].to_string())\n",
    "    else:\n",
    "        print(\"✅ 人間の判断とLLMの判断はすべて一致していました。\")\n",
    "        \n",
    "else:\n",
    "    print(\"評価対象のデータがありませんでした。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a13766b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citing_paper_eid_gt</th>\n",
       "      <th>citing_paper_doi</th>\n",
       "      <th>citing_paper_title_gt</th>\n",
       "      <th>cited_data_paper_title_gt</th>\n",
       "      <th>is_data_used_gt</th>\n",
       "      <th>citing_paper_eid</th>\n",
       "      <th>citing_paper_title</th>\n",
       "      <th>cited_data_paper_title</th>\n",
       "      <th>prediction_rule3_abstract</th>\n",
       "      <th>prediction_rule3_fulltext</th>\n",
       "      <th>prediction_rule3_fulltext_few_shot</th>\n",
       "      <th>prediction_rule3_gemini-2_5-flash</th>\n",
       "      <th>prediction_rule3_gemini-2_5-flash_zeroshot</th>\n",
       "      <th>citing_paper_eid_text</th>\n",
       "      <th>citing_paper_title_text</th>\n",
       "      <th>cited_data_paper_title_text</th>\n",
       "      <th>abstract</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2-s2.0-85210281314</td>\n",
       "      <td>10.1016/j.jaridenv.2024.105282</td>\n",
       "      <td>Potential effects of climate change on cacti d...</td>\n",
       "      <td>The World Checklist of Vascular Plants, a cont...</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85210281314</td>\n",
       "      <td>Potential effects of climate change on cacti d...</td>\n",
       "      <td>The World Checklist of Vascular Plants, a cont...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85210281314</td>\n",
       "      <td>Potential effects of climate change on cacti d...</td>\n",
       "      <td>The World Checklist of Vascular Plants, a cont...</td>\n",
       "      <td>Climate change is expected to adversely impact...</td>\n",
       "      <td>Drylands biodiversity is essential to the well...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2-s2.0-85171680307</td>\n",
       "      <td>10.1016/j.revpalbo.2023.104989</td>\n",
       "      <td>Approaches to pollen taxonomic harmonisation i...</td>\n",
       "      <td>European pollen-based REVEALS land-cover recon...</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-85171680307</td>\n",
       "      <td>Approaches to pollen taxonomic harmonisation i...</td>\n",
       "      <td>European pollen-based REVEALS land-cover recon...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-85171680307</td>\n",
       "      <td>Approaches to pollen taxonomic harmonisation i...</td>\n",
       "      <td>European pollen-based REVEALS land-cover recon...</td>\n",
       "      <td>Pollen taxonomic harmonisation involves the st...</td>\n",
       "      <td>Quaternary pollen analysis demands careful and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2-s2.0-85176563605</td>\n",
       "      <td>10.1016/j.resconrec.2023.107296</td>\n",
       "      <td>Government resource allocation practices towar...</td>\n",
       "      <td>Global Carbon Budget 2022</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85176563605</td>\n",
       "      <td>Government resource allocation practices towar...</td>\n",
       "      <td>Global Carbon Budget 2022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85176563605</td>\n",
       "      <td>Government resource allocation practices towar...</td>\n",
       "      <td>Global Carbon Budget 2022</td>\n",
       "      <td>Government resource allocation practices for a...</td>\n",
       "      <td>The Chinese central government uses a top-down...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2-s2.0-105002637578</td>\n",
       "      <td>10.1016/j.solener.2025.113509</td>\n",
       "      <td>Predictive modeling and optimization of CIGS t...</td>\n",
       "      <td>Modeling and performance analysis dataset of a...</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-105002637578</td>\n",
       "      <td>Predictive modeling and optimization of CIGS t...</td>\n",
       "      <td>Modeling and performance analysis dataset of a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-105002637578</td>\n",
       "      <td>Predictive modeling and optimization of CIGS t...</td>\n",
       "      <td>Modeling and performance analysis dataset of a...</td>\n",
       "      <td>This study employs Machine Learning (ML) techn...</td>\n",
       "      <td>In the pursuit of sustainable energy solutions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2-s2.0-85212982352</td>\n",
       "      <td>10.1016/j.ces.2024.121101</td>\n",
       "      <td>Experimental and modelling study of ammonia-ba...</td>\n",
       "      <td>Dataset of wet desulphurization scrubbing in a...</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-85212982352</td>\n",
       "      <td>Experimental and modelling study of ammonia-ba...</td>\n",
       "      <td>Dataset of wet desulphurization scrubbing in a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85212982352</td>\n",
       "      <td>Experimental and modelling study of ammonia-ba...</td>\n",
       "      <td>Dataset of wet desulphurization scrubbing in a...</td>\n",
       "      <td>In this work, we propose an experimental and m...</td>\n",
       "      <td>The growing energy demand and the need for mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2-s2.0-85173883804</td>\n",
       "      <td>10.1016/j.ref.2023.100490</td>\n",
       "      <td>Performance and dynamics of California offshor...</td>\n",
       "      <td>Developing reliable hourly electricity demand ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-85173883804</td>\n",
       "      <td>Performance and dynamics of California offshor...</td>\n",
       "      <td>Developing reliable hourly electricity demand ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85173883804</td>\n",
       "      <td>Performance and dynamics of California offshor...</td>\n",
       "      <td>Developing reliable hourly electricity demand ...</td>\n",
       "      <td>We use statistical methods to evaluate the coi...</td>\n",
       "      <td>The global emergency to swiftly transition to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2-s2.0-105001429996</td>\n",
       "      <td>10.1016/j.sftr.2025.100578</td>\n",
       "      <td>Towards a sustainable future: The interplay of...</td>\n",
       "      <td>The KOF Globalisation Index – revisited</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-105001429996</td>\n",
       "      <td>Towards a sustainable future: The interplay of...</td>\n",
       "      <td>The KOF Globalisation Index – revisited</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-105001429996</td>\n",
       "      <td>Towards a sustainable future: The interplay of...</td>\n",
       "      <td>The KOF Globalisation Index – revisited</td>\n",
       "      <td>Over the past three decades, India's transform...</td>\n",
       "      <td>Global warming and climate change due to persi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2-s2.0-85126864679</td>\n",
       "      <td>10.1016/j.gloplacha.2022.103785</td>\n",
       "      <td>Linking the Mediterranean MIS 5 tephra markers...</td>\n",
       "      <td>Lake Ohrid’s tephrochronological dataset revea...</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85126864679</td>\n",
       "      <td>Linking the Mediterranean MIS 5 tephra markers...</td>\n",
       "      <td>Lake Ohrid’s tephrochronological dataset revea...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-85126864679</td>\n",
       "      <td>Linking the Mediterranean MIS 5 tephra markers...</td>\n",
       "      <td>Lake Ohrid’s tephrochronological dataset revea...</td>\n",
       "      <td>Explosive activity preceding the ~40 ka Campan...</td>\n",
       "      <td>Near-vent volcanic successions provide fundame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2-s2.0-85210089827</td>\n",
       "      <td>10.1016/j.str.2024.10.016</td>\n",
       "      <td>LEDGF interacts with the NID domain of MeCP2 a...</td>\n",
       "      <td>A catalogue of 863 Rett-syndrome-causing MECP2...</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85210089827</td>\n",
       "      <td>LEDGF interacts with the NID domain of MeCP2 a...</td>\n",
       "      <td>A catalogue of 863 Rett-syndrome-causing MECP2...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85210089827</td>\n",
       "      <td>LEDGF interacts with the NID domain of MeCP2 a...</td>\n",
       "      <td>A catalogue of 863 Rett-syndrome-causing MECP2...</td>\n",
       "      <td>Methyl-CpG-binding protein 2 (MeCP2) is a ubiq...</td>\n",
       "      <td>The methyl-CpG-binding protein 2 (MeCP2) is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2-s2.0-105002149939</td>\n",
       "      <td>10.1016/j.uclim.2025.102409</td>\n",
       "      <td>Government-involved urban meteorological netwo...</td>\n",
       "      <td>The automatic weather stations NOANN network o...</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-105002149939</td>\n",
       "      <td>Government-involved urban meteorological netwo...</td>\n",
       "      <td>The automatic weather stations NOANN network o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-105002149939</td>\n",
       "      <td>Government-involved urban meteorological netwo...</td>\n",
       "      <td>The automatic weather stations NOANN network o...</td>\n",
       "      <td>Studies on urban climate are important to this...</td>\n",
       "      <td>Meteorological observation networks play a cru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2-s2.0-105003949553</td>\n",
       "      <td>10.1016/j.imed.2024.12.001</td>\n",
       "      <td>Osteosarcoma knowledge graph question answerin...</td>\n",
       "      <td>Building a knowledge graph to enable precision...</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-105003949553</td>\n",
       "      <td>Osteosarcoma knowledge graph question answerin...</td>\n",
       "      <td>Building a knowledge graph to enable precision...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-105003949553</td>\n",
       "      <td>Osteosarcoma knowledge graph question answerin...</td>\n",
       "      <td>Building a knowledge graph to enable precision...</td>\n",
       "      <td>Osteosarcoma is a prevalent primary malignant ...</td>\n",
       "      <td>Osteosarcoma is one of the most common bone tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2-s2.0-85167821848</td>\n",
       "      <td>10.1016/j.nut.2023.112182</td>\n",
       "      <td>Trends in socioeconomic inequalities in malnut...</td>\n",
       "      <td>Multiple Indicator Cluster Surveys: Delivering...</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-85167821848</td>\n",
       "      <td>Trends in socioeconomic inequalities in malnut...</td>\n",
       "      <td>Multiple Indicator Cluster Surveys: Delivering...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85167821848</td>\n",
       "      <td>Trends in socioeconomic inequalities in malnut...</td>\n",
       "      <td>Multiple Indicator Cluster Surveys: Delivering...</td>\n",
       "      <td>Malnutrition in the Democratic Republic of the...</td>\n",
       "      <td>Malnutrition is a global concern that retards ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2-s2.0-105001552301</td>\n",
       "      <td>10.1016/j.pestbp.2025.106399</td>\n",
       "      <td>Cythochrome P450-mediated dinotefuran resistan...</td>\n",
       "      <td>Chromosome-level genome assembly of bean flowe...</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-105001552301</td>\n",
       "      <td>Cythochrome P450-mediated dinotefuran resistan...</td>\n",
       "      <td>Chromosome-level genome assembly of bean flowe...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-105001552301</td>\n",
       "      <td>Cythochrome P450-mediated dinotefuran resistan...</td>\n",
       "      <td>Chromosome-level genome assembly of bean flowe...</td>\n",
       "      <td>Onion thrips,</td>\n",
       "      <td>Insecticide resistance has been observed in nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2-s2.0-85210538344</td>\n",
       "      <td>10.1016/j.jhydrol.2024.132393</td>\n",
       "      <td>Evaluating SWAT-3PG simulation of hydrologic a...</td>\n",
       "      <td>The global forest above-ground biomass pool fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-85210538344</td>\n",
       "      <td>Evaluating SWAT-3PG simulation of hydrologic a...</td>\n",
       "      <td>The global forest above-ground biomass pool fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85210538344</td>\n",
       "      <td>Evaluating SWAT-3PG simulation of hydrologic a...</td>\n",
       "      <td>The global forest above-ground biomass pool fo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forests are an integral component of terrestri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2-s2.0-105008325674</td>\n",
       "      <td>10.1016/j.foodres.2025.116829</td>\n",
       "      <td>Health risk assessment from heavy metals in cr...</td>\n",
       "      <td>Health risk assessment of heavy metals (Hg, Pb...</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-105008325674</td>\n",
       "      <td>Health risk assessment from heavy metals in cr...</td>\n",
       "      <td>Health risk assessment of heavy metals (Hg, Pb...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-105008325674</td>\n",
       "      <td>Health risk assessment from heavy metals in cr...</td>\n",
       "      <td>Health risk assessment of heavy metals (Hg, Pb...</td>\n",
       "      <td>This study is the first to assess health risks...</td>\n",
       "      <td>The presence of toxic elements such as arsenic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2-s2.0-85076838242</td>\n",
       "      <td>10.1016/j.theriogenology.2019.12.012</td>\n",
       "      <td>Effect of carboxylated poly L-Lysine as a cryo...</td>\n",
       "      <td>Data supporting the spectrophotometric method ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-85076838242</td>\n",
       "      <td>Effect of carboxylated poly L-Lysine as a cryo...</td>\n",
       "      <td>Data supporting the spectrophotometric method ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85076838242</td>\n",
       "      <td>Effect of carboxylated poly L-Lysine as a cryo...</td>\n",
       "      <td>Data supporting the spectrophotometric method ...</td>\n",
       "      <td>Buffalo bull sperm are more prone to cryo-inju...</td>\n",
       "      <td>The technology of artificial insemination (A.I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2-s2.0-85151781902</td>\n",
       "      <td>10.1016/j.rse.2023.113578</td>\n",
       "      <td>A first Chinese building height estimate at 10...</td>\n",
       "      <td>Global 1 km × 1 km gridded revised real gross ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85151781902</td>\n",
       "      <td>A first Chinese building height estimate at 10...</td>\n",
       "      <td>Global 1 km × 1 km gridded revised real gross ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-85151781902</td>\n",
       "      <td>A first Chinese building height estimate at 10...</td>\n",
       "      <td>Global 1 km × 1 km gridded revised real gross ...</td>\n",
       "      <td>Building height is a crucial variable in the s...</td>\n",
       "      <td>Accurate measurement of building height is ess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>2-s2.0-85163566858</td>\n",
       "      <td>10.1016/j.chiabu.2023.106299</td>\n",
       "      <td>Do parenting behaviors intended as discipline ...</td>\n",
       "      <td>Multiple Indicator Cluster Surveys: Delivering...</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-85163566858</td>\n",
       "      <td>Do parenting behaviors intended as discipline ...</td>\n",
       "      <td>Multiple Indicator Cluster Surveys: Delivering...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85163566858</td>\n",
       "      <td>Do parenting behaviors intended as discipline ...</td>\n",
       "      <td>Multiple Indicator Cluster Surveys: Delivering...</td>\n",
       "      <td>Religious affiliation may account for some var...</td>\n",
       "      <td>Parenting behavior intended for discipline (in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>2-s2.0-105003911484</td>\n",
       "      <td>10.1016/j.jsg.2025.105445</td>\n",
       "      <td>Multi-millennia slip rate relationships betwee...</td>\n",
       "      <td>Fault Scarp Dating Tool - a MATLAB code for fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-105003911484</td>\n",
       "      <td>Multi-millennia slip rate relationships betwee...</td>\n",
       "      <td>Fault Scarp Dating Tool - a MATLAB code for fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-105003911484</td>\n",
       "      <td>Multi-millennia slip rate relationships betwee...</td>\n",
       "      <td>Fault Scarp Dating Tool - a MATLAB code for fa...</td>\n",
       "      <td>This study investigates slip behaviour on over...</td>\n",
       "      <td>Understanding the temporal behaviour of active...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2-s2.0-85184837866</td>\n",
       "      <td>10.1016/j.compag.2024.108680</td>\n",
       "      <td>Harnessing quantum computing for smart agricul...</td>\n",
       "      <td>Crop Origins and Phylo Food: A database and a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85184837866</td>\n",
       "      <td>Harnessing quantum computing for smart agricul...</td>\n",
       "      <td>Crop Origins and Phylo Food: A database and a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-85184837866</td>\n",
       "      <td>Harnessing quantum computing for smart agricul...</td>\n",
       "      <td>Crop Origins and Phylo Food: A database and a ...</td>\n",
       "      <td>Agriculture has undergone progressive transfor...</td>\n",
       "      <td>According to the , the global population has s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2-s2.0-85175166432</td>\n",
       "      <td>10.1016/j.quascirev.2023.108368</td>\n",
       "      <td>Holocene vertical velocity fields in the calab...</td>\n",
       "      <td>Last Interglacial sea-level proxies in the wes...</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85175166432</td>\n",
       "      <td>Holocene vertical velocity fields in the calab...</td>\n",
       "      <td>Last Interglacial sea-level proxies in the wes...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85175166432</td>\n",
       "      <td>Holocene vertical velocity fields in the calab...</td>\n",
       "      <td>Last Interglacial sea-level proxies in the wes...</td>\n",
       "      <td>The study of fossil coastlines is one of the m...</td>\n",
       "      <td>The Calabrian Arc defines the upper plate of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2-s2.0-85196867431</td>\n",
       "      <td>10.1016/j.ijdrr.2024.104605</td>\n",
       "      <td>Nationwide evaluation of changes in fluvial an...</td>\n",
       "      <td>Development of CMIP6-Based Climate Scenarios f...</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-85196867431</td>\n",
       "      <td>Nationwide evaluation of changes in fluvial an...</td>\n",
       "      <td>Development of CMIP6-Based Climate Scenarios f...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85196867431</td>\n",
       "      <td>Nationwide evaluation of changes in fluvial an...</td>\n",
       "      <td>Development of CMIP6-Based Climate Scenarios f...</td>\n",
       "      <td>To implement strategic countermeasures against...</td>\n",
       "      <td>Flood disasters cause severe economic losses. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2-s2.0-85118491607</td>\n",
       "      <td>10.1016/j.dib.2021.107531</td>\n",
       "      <td>Dataset of seized wildlife and their intended ...</td>\n",
       "      <td>United States wildlife and wildlife product im...</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85118491607</td>\n",
       "      <td>Dataset of seized wildlife and their intended ...</td>\n",
       "      <td>United States wildlife and wildlife product im...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-85118491607</td>\n",
       "      <td>Dataset of seized wildlife and their intended ...</td>\n",
       "      <td>United States wildlife and wildlife product im...</td>\n",
       "      <td>The illegal wildlife trade (IWT) threatens con...</td>\n",
       "      <td>The presented data covers the illegal trade (i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2-s2.0-85144817654</td>\n",
       "      <td>10.1016/j.scitotenv.2022.161157</td>\n",
       "      <td>Widespread missing super-emitters of nitrogen ...</td>\n",
       "      <td>Air pollution emissions from Chinese power pla...</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-85144817654</td>\n",
       "      <td>Widespread missing super-emitters of nitrogen ...</td>\n",
       "      <td>Air pollution emissions from Chinese power pla...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85144817654</td>\n",
       "      <td>Widespread missing super-emitters of nitrogen ...</td>\n",
       "      <td>Air pollution emissions from Chinese power pla...</td>\n",
       "      <td>Nitrogen oxides (NO</td>\n",
       "      <td>Nitrogen oxides (NO ≡ NO + NO ) play a central...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2-s2.0-85211712060</td>\n",
       "      <td>10.1016/j.agsy.2024.104243</td>\n",
       "      <td>Enhancing simulations of biomass and nitrous o...</td>\n",
       "      <td>Gridded daily weather data for North America w...</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85211712060</td>\n",
       "      <td>Enhancing simulations of biomass and nitrous o...</td>\n",
       "      <td>Gridded daily weather data for North America w...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85211712060</td>\n",
       "      <td>Enhancing simulations of biomass and nitrous o...</td>\n",
       "      <td>Gridded daily weather data for North America w...</td>\n",
       "      <td>Nitrous oxide (N This study aimed to: 1) condu...</td>\n",
       "      <td>It's critical to mitigate climate change impac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2-s2.0-86000359834</td>\n",
       "      <td>10.1016/j.surfin.2025.106134</td>\n",
       "      <td>Synthesis and adsorption efficiency of magneti...</td>\n",
       "      <td>Ibuprofen removal using coconut husk activated...</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-86000359834</td>\n",
       "      <td>Synthesis and adsorption efficiency of magneti...</td>\n",
       "      <td>Ibuprofen removal using coconut husk activated...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-86000359834</td>\n",
       "      <td>Synthesis and adsorption efficiency of magneti...</td>\n",
       "      <td>Ibuprofen removal using coconut husk activated...</td>\n",
       "      <td>This study aimed to develop innovative activat...</td>\n",
       "      <td>Pharmaceutical emerging contaminants, such as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>2-s2.0-85210120741</td>\n",
       "      <td>10.1016/j.scitotenv.2024.177626</td>\n",
       "      <td>Effects of sublethal concentration of thiameth...</td>\n",
       "      <td>Data supporting the spectrophotometric method ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-85210120741</td>\n",
       "      <td>Effects of sublethal concentration of thiameth...</td>\n",
       "      <td>Data supporting the spectrophotometric method ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-85210120741</td>\n",
       "      <td>Effects of sublethal concentration of thiameth...</td>\n",
       "      <td>Data supporting the spectrophotometric method ...</td>\n",
       "      <td>Bees are pollinators of native and cultivated ...</td>\n",
       "      <td>Approximately 90 % of plants are pollinated by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>2-s2.0-85115603472</td>\n",
       "      <td>10.1016/j.ygeno.2021.09.013</td>\n",
       "      <td>Differential analysis of chromatin accessibili...</td>\n",
       "      <td>An ATAC-seq atlas of chromatin accessibility i...</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-85115603472</td>\n",
       "      <td>Differential analysis of chromatin accessibili...</td>\n",
       "      <td>An ATAC-seq atlas of chromatin accessibility i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85115603472</td>\n",
       "      <td>Differential analysis of chromatin accessibili...</td>\n",
       "      <td>An ATAC-seq atlas of chromatin accessibility i...</td>\n",
       "      <td>Chromatin accessibility is a key factor influe...</td>\n",
       "      <td>Chromatin accessibility across the genome defi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>2-s2.0-85125278381</td>\n",
       "      <td>10.1016/j.envres.2022.112901</td>\n",
       "      <td>Viral diversity and biogeochemical potential r...</td>\n",
       "      <td>Viral metagenomes of Lake Soyang, the largest ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-85125278381</td>\n",
       "      <td>Viral diversity and biogeochemical potential r...</td>\n",
       "      <td>Viral metagenomes of Lake Soyang, the largest ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85125278381</td>\n",
       "      <td>Viral diversity and biogeochemical potential r...</td>\n",
       "      <td>Viral metagenomes of Lake Soyang, the largest ...</td>\n",
       "      <td>As the most numerous biological entities on Ea...</td>\n",
       "      <td>Viruses were estimated population size of 10 v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>2-s2.0-85117787998</td>\n",
       "      <td>10.1016/j.ejmech.2021.113921</td>\n",
       "      <td>Coumarin–benzimidazole hybrids: A review of de...</td>\n",
       "      <td>Synthesis and characterization of 3-[3-(1H-ben...</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85117787998</td>\n",
       "      <td>Coumarin–benzimidazole hybrids: A review of de...</td>\n",
       "      <td>Synthesis and characterization of 3-[3-(1H-ben...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85117787998</td>\n",
       "      <td>Coumarin–benzimidazole hybrids: A review of de...</td>\n",
       "      <td>Synthesis and characterization of 3-[3-(1H-ben...</td>\n",
       "      <td>Coumarin and benzimidazole are privileged stru...</td>\n",
       "      <td>According to the International Agency for Rese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>2-s2.0-85013146982</td>\n",
       "      <td>10.1016/j.pmatsci.2017.02.001</td>\n",
       "      <td>Recent progress in marine foul-release polymer...</td>\n",
       "      <td>Data on photo-nanofiller models for self-clean...</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85013146982</td>\n",
       "      <td>Recent progress in marine foul-release polymer...</td>\n",
       "      <td>Data on photo-nanofiller models for self-clean...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-85013146982</td>\n",
       "      <td>Recent progress in marine foul-release polymer...</td>\n",
       "      <td>Data on photo-nanofiller models for self-clean...</td>\n",
       "      <td>Progress in materials science is associated wi...</td>\n",
       "      <td>Nanocomposites constitute a class of materials...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>2-s2.0-85193837673</td>\n",
       "      <td>10.1016/j.media.2024.103201</td>\n",
       "      <td>A comprehensive survey on deep active learning...</td>\n",
       "      <td>MedMNIST v2 - A large-scale lightweight benchm...</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85193837673</td>\n",
       "      <td>A comprehensive survey on deep active learning...</td>\n",
       "      <td>MedMNIST v2 - A large-scale lightweight benchm...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-85193837673</td>\n",
       "      <td>A comprehensive survey on deep active learning...</td>\n",
       "      <td>MedMNIST v2 - A large-scale lightweight benchm...</td>\n",
       "      <td>Deep learning has achieved widespread success ...</td>\n",
       "      <td>Medical imaging visualizes anatomical structur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>2-s2.0-85185155470</td>\n",
       "      <td>10.1016/j.jretconser.2024.103723</td>\n",
       "      <td>Importance and performance in PLS-SEM and NCA:...</td>\n",
       "      <td>Dataset on an extended technology acceptance m...</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85185155470</td>\n",
       "      <td>Importance and performance in PLS-SEM and NCA:...</td>\n",
       "      <td>Dataset on an extended technology acceptance m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85185155470</td>\n",
       "      <td>Importance and performance in PLS-SEM and NCA:...</td>\n",
       "      <td>Dataset on an extended technology acceptance m...</td>\n",
       "      <td>This research offers a novel approach that ext...</td>\n",
       "      <td>Comparing attributes' performance and importan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>2-s2.0-85204984969</td>\n",
       "      <td>10.1016/j.resconrec.2024.107934</td>\n",
       "      <td>Assessment of forest disturbance and soil eros...</td>\n",
       "      <td>Harmonised global datasets of wind and solar f...</td>\n",
       "      <td>1</td>\n",
       "      <td>2-s2.0-85204984969</td>\n",
       "      <td>Assessment of forest disturbance and soil eros...</td>\n",
       "      <td>Harmonised global datasets of wind and solar f...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-85204984969</td>\n",
       "      <td>Assessment of forest disturbance and soil eros...</td>\n",
       "      <td>Harmonised global datasets of wind and solar f...</td>\n",
       "      <td>The construction of wind farms, involving road...</td>\n",
       "      <td>Accelerating the penetration of renewable ener...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>2-s2.0-105001558082</td>\n",
       "      <td>10.1016/j.brainresbull.2025.111327</td>\n",
       "      <td>Pad2 deletion induces anxiety‑like behaviors a...</td>\n",
       "      <td>Systematic phenotyping and characterization of...</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-105001558082</td>\n",
       "      <td>Pad2 deletion induces anxiety‑like behaviors a...</td>\n",
       "      <td>Systematic phenotyping and characterization of...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2-s2.0-105001558082</td>\n",
       "      <td>Pad2 deletion induces anxiety‑like behaviors a...</td>\n",
       "      <td>Systematic phenotyping and characterization of...</td>\n",
       "      <td>Citrullination, the deimination of peptidylarg...</td>\n",
       "      <td>As one of the important ways to regulate prote...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     citing_paper_eid_gt                      citing_paper_doi  \\\n",
       "1     2-s2.0-85210281314        10.1016/j.jaridenv.2024.105282   \n",
       "2     2-s2.0-85171680307        10.1016/j.revpalbo.2023.104989   \n",
       "9     2-s2.0-85176563605       10.1016/j.resconrec.2023.107296   \n",
       "10   2-s2.0-105002637578         10.1016/j.solener.2025.113509   \n",
       "18    2-s2.0-85212982352             10.1016/j.ces.2024.121101   \n",
       "24    2-s2.0-85173883804             10.1016/j.ref.2023.100490   \n",
       "34   2-s2.0-105001429996            10.1016/j.sftr.2025.100578   \n",
       "47    2-s2.0-85126864679       10.1016/j.gloplacha.2022.103785   \n",
       "58    2-s2.0-85210089827             10.1016/j.str.2024.10.016   \n",
       "60   2-s2.0-105002149939           10.1016/j.uclim.2025.102409   \n",
       "61   2-s2.0-105003949553            10.1016/j.imed.2024.12.001   \n",
       "66    2-s2.0-85167821848             10.1016/j.nut.2023.112182   \n",
       "73   2-s2.0-105001552301          10.1016/j.pestbp.2025.106399   \n",
       "92    2-s2.0-85210538344         10.1016/j.jhydrol.2024.132393   \n",
       "97   2-s2.0-105008325674         10.1016/j.foodres.2025.116829   \n",
       "99    2-s2.0-85076838242  10.1016/j.theriogenology.2019.12.012   \n",
       "101   2-s2.0-85151781902             10.1016/j.rse.2023.113578   \n",
       "114   2-s2.0-85163566858          10.1016/j.chiabu.2023.106299   \n",
       "118  2-s2.0-105003911484             10.1016/j.jsg.2025.105445   \n",
       "122   2-s2.0-85184837866          10.1016/j.compag.2024.108680   \n",
       "126   2-s2.0-85175166432       10.1016/j.quascirev.2023.108368   \n",
       "129   2-s2.0-85196867431           10.1016/j.ijdrr.2024.104605   \n",
       "134   2-s2.0-85118491607             10.1016/j.dib.2021.107531   \n",
       "140   2-s2.0-85144817654       10.1016/j.scitotenv.2022.161157   \n",
       "146   2-s2.0-85211712060            10.1016/j.agsy.2024.104243   \n",
       "148   2-s2.0-86000359834          10.1016/j.surfin.2025.106134   \n",
       "152   2-s2.0-85210120741       10.1016/j.scitotenv.2024.177626   \n",
       "153   2-s2.0-85115603472           10.1016/j.ygeno.2021.09.013   \n",
       "168   2-s2.0-85125278381          10.1016/j.envres.2022.112901   \n",
       "174   2-s2.0-85117787998          10.1016/j.ejmech.2021.113921   \n",
       "177   2-s2.0-85013146982         10.1016/j.pmatsci.2017.02.001   \n",
       "180   2-s2.0-85193837673           10.1016/j.media.2024.103201   \n",
       "181   2-s2.0-85185155470      10.1016/j.jretconser.2024.103723   \n",
       "185   2-s2.0-85204984969       10.1016/j.resconrec.2024.107934   \n",
       "192  2-s2.0-105001558082    10.1016/j.brainresbull.2025.111327   \n",
       "\n",
       "                                 citing_paper_title_gt  \\\n",
       "1    Potential effects of climate change on cacti d...   \n",
       "2    Approaches to pollen taxonomic harmonisation i...   \n",
       "9    Government resource allocation practices towar...   \n",
       "10   Predictive modeling and optimization of CIGS t...   \n",
       "18   Experimental and modelling study of ammonia-ba...   \n",
       "24   Performance and dynamics of California offshor...   \n",
       "34   Towards a sustainable future: The interplay of...   \n",
       "47   Linking the Mediterranean MIS 5 tephra markers...   \n",
       "58   LEDGF interacts with the NID domain of MeCP2 a...   \n",
       "60   Government-involved urban meteorological netwo...   \n",
       "61   Osteosarcoma knowledge graph question answerin...   \n",
       "66   Trends in socioeconomic inequalities in malnut...   \n",
       "73   Cythochrome P450-mediated dinotefuran resistan...   \n",
       "92   Evaluating SWAT-3PG simulation of hydrologic a...   \n",
       "97   Health risk assessment from heavy metals in cr...   \n",
       "99   Effect of carboxylated poly L-Lysine as a cryo...   \n",
       "101  A first Chinese building height estimate at 10...   \n",
       "114  Do parenting behaviors intended as discipline ...   \n",
       "118  Multi-millennia slip rate relationships betwee...   \n",
       "122  Harnessing quantum computing for smart agricul...   \n",
       "126  Holocene vertical velocity fields in the calab...   \n",
       "129  Nationwide evaluation of changes in fluvial an...   \n",
       "134  Dataset of seized wildlife and their intended ...   \n",
       "140  Widespread missing super-emitters of nitrogen ...   \n",
       "146  Enhancing simulations of biomass and nitrous o...   \n",
       "148  Synthesis and adsorption efficiency of magneti...   \n",
       "152  Effects of sublethal concentration of thiameth...   \n",
       "153  Differential analysis of chromatin accessibili...   \n",
       "168  Viral diversity and biogeochemical potential r...   \n",
       "174  Coumarin–benzimidazole hybrids: A review of de...   \n",
       "177  Recent progress in marine foul-release polymer...   \n",
       "180  A comprehensive survey on deep active learning...   \n",
       "181  Importance and performance in PLS-SEM and NCA:...   \n",
       "185  Assessment of forest disturbance and soil eros...   \n",
       "192  Pad2 deletion induces anxiety‑like behaviors a...   \n",
       "\n",
       "                             cited_data_paper_title_gt  is_data_used_gt  \\\n",
       "1    The World Checklist of Vascular Plants, a cont...                1   \n",
       "2    European pollen-based REVEALS land-cover recon...                0   \n",
       "9                            Global Carbon Budget 2022                1   \n",
       "10   Modeling and performance analysis dataset of a...                1   \n",
       "18   Dataset of wet desulphurization scrubbing in a...                0   \n",
       "24   Developing reliable hourly electricity demand ...                0   \n",
       "34             The KOF Globalisation Index – revisited                0   \n",
       "47   Lake Ohrid’s tephrochronological dataset revea...                1   \n",
       "58   A catalogue of 863 Rett-syndrome-causing MECP2...                1   \n",
       "60   The automatic weather stations NOANN network o...                0   \n",
       "61   Building a knowledge graph to enable precision...                0   \n",
       "66   Multiple Indicator Cluster Surveys: Delivering...                0   \n",
       "73   Chromosome-level genome assembly of bean flowe...                0   \n",
       "92   The global forest above-ground biomass pool fo...                0   \n",
       "97   Health risk assessment of heavy metals (Hg, Pb...                0   \n",
       "99   Data supporting the spectrophotometric method ...                0   \n",
       "101  Global 1 km × 1 km gridded revised real gross ...                1   \n",
       "114  Multiple Indicator Cluster Surveys: Delivering...                0   \n",
       "118  Fault Scarp Dating Tool - a MATLAB code for fa...                0   \n",
       "122  Crop Origins and Phylo Food: A database and a ...                1   \n",
       "126  Last Interglacial sea-level proxies in the wes...                1   \n",
       "129  Development of CMIP6-Based Climate Scenarios f...                0   \n",
       "134  United States wildlife and wildlife product im...                1   \n",
       "140  Air pollution emissions from Chinese power pla...                0   \n",
       "146  Gridded daily weather data for North America w...                1   \n",
       "148  Ibuprofen removal using coconut husk activated...                1   \n",
       "152  Data supporting the spectrophotometric method ...                0   \n",
       "153  An ATAC-seq atlas of chromatin accessibility i...                0   \n",
       "168  Viral metagenomes of Lake Soyang, the largest ...                0   \n",
       "174  Synthesis and characterization of 3-[3-(1H-ben...                1   \n",
       "177  Data on photo-nanofiller models for self-clean...                1   \n",
       "180  MedMNIST v2 - A large-scale lightweight benchm...                1   \n",
       "181  Dataset on an extended technology acceptance m...                1   \n",
       "185  Harmonised global datasets of wind and solar f...                1   \n",
       "192  Systematic phenotyping and characterization of...                0   \n",
       "\n",
       "        citing_paper_eid                                 citing_paper_title  \\\n",
       "1     2-s2.0-85210281314  Potential effects of climate change on cacti d...   \n",
       "2     2-s2.0-85171680307  Approaches to pollen taxonomic harmonisation i...   \n",
       "9     2-s2.0-85176563605  Government resource allocation practices towar...   \n",
       "10   2-s2.0-105002637578  Predictive modeling and optimization of CIGS t...   \n",
       "18    2-s2.0-85212982352  Experimental and modelling study of ammonia-ba...   \n",
       "24    2-s2.0-85173883804  Performance and dynamics of California offshor...   \n",
       "34   2-s2.0-105001429996  Towards a sustainable future: The interplay of...   \n",
       "47    2-s2.0-85126864679  Linking the Mediterranean MIS 5 tephra markers...   \n",
       "58    2-s2.0-85210089827  LEDGF interacts with the NID domain of MeCP2 a...   \n",
       "60   2-s2.0-105002149939  Government-involved urban meteorological netwo...   \n",
       "61   2-s2.0-105003949553  Osteosarcoma knowledge graph question answerin...   \n",
       "66    2-s2.0-85167821848  Trends in socioeconomic inequalities in malnut...   \n",
       "73   2-s2.0-105001552301  Cythochrome P450-mediated dinotefuran resistan...   \n",
       "92    2-s2.0-85210538344  Evaluating SWAT-3PG simulation of hydrologic a...   \n",
       "97   2-s2.0-105008325674  Health risk assessment from heavy metals in cr...   \n",
       "99    2-s2.0-85076838242  Effect of carboxylated poly L-Lysine as a cryo...   \n",
       "101   2-s2.0-85151781902  A first Chinese building height estimate at 10...   \n",
       "114   2-s2.0-85163566858  Do parenting behaviors intended as discipline ...   \n",
       "118  2-s2.0-105003911484  Multi-millennia slip rate relationships betwee...   \n",
       "122   2-s2.0-85184837866  Harnessing quantum computing for smart agricul...   \n",
       "126   2-s2.0-85175166432  Holocene vertical velocity fields in the calab...   \n",
       "129   2-s2.0-85196867431  Nationwide evaluation of changes in fluvial an...   \n",
       "134   2-s2.0-85118491607  Dataset of seized wildlife and their intended ...   \n",
       "140   2-s2.0-85144817654  Widespread missing super-emitters of nitrogen ...   \n",
       "146   2-s2.0-85211712060  Enhancing simulations of biomass and nitrous o...   \n",
       "148   2-s2.0-86000359834  Synthesis and adsorption efficiency of magneti...   \n",
       "152   2-s2.0-85210120741  Effects of sublethal concentration of thiameth...   \n",
       "153   2-s2.0-85115603472  Differential analysis of chromatin accessibili...   \n",
       "168   2-s2.0-85125278381  Viral diversity and biogeochemical potential r...   \n",
       "174   2-s2.0-85117787998  Coumarin–benzimidazole hybrids: A review of de...   \n",
       "177   2-s2.0-85013146982  Recent progress in marine foul-release polymer...   \n",
       "180   2-s2.0-85193837673  A comprehensive survey on deep active learning...   \n",
       "181   2-s2.0-85185155470  Importance and performance in PLS-SEM and NCA:...   \n",
       "185   2-s2.0-85204984969  Assessment of forest disturbance and soil eros...   \n",
       "192  2-s2.0-105001558082  Pad2 deletion induces anxiety‑like behaviors a...   \n",
       "\n",
       "                                cited_data_paper_title  \\\n",
       "1    The World Checklist of Vascular Plants, a cont...   \n",
       "2    European pollen-based REVEALS land-cover recon...   \n",
       "9                            Global Carbon Budget 2022   \n",
       "10   Modeling and performance analysis dataset of a...   \n",
       "18   Dataset of wet desulphurization scrubbing in a...   \n",
       "24   Developing reliable hourly electricity demand ...   \n",
       "34             The KOF Globalisation Index – revisited   \n",
       "47   Lake Ohrid’s tephrochronological dataset revea...   \n",
       "58   A catalogue of 863 Rett-syndrome-causing MECP2...   \n",
       "60   The automatic weather stations NOANN network o...   \n",
       "61   Building a knowledge graph to enable precision...   \n",
       "66   Multiple Indicator Cluster Surveys: Delivering...   \n",
       "73   Chromosome-level genome assembly of bean flowe...   \n",
       "92   The global forest above-ground biomass pool fo...   \n",
       "97   Health risk assessment of heavy metals (Hg, Pb...   \n",
       "99   Data supporting the spectrophotometric method ...   \n",
       "101  Global 1 km × 1 km gridded revised real gross ...   \n",
       "114  Multiple Indicator Cluster Surveys: Delivering...   \n",
       "118  Fault Scarp Dating Tool - a MATLAB code for fa...   \n",
       "122  Crop Origins and Phylo Food: A database and a ...   \n",
       "126  Last Interglacial sea-level proxies in the wes...   \n",
       "129  Development of CMIP6-Based Climate Scenarios f...   \n",
       "134  United States wildlife and wildlife product im...   \n",
       "140  Air pollution emissions from Chinese power pla...   \n",
       "146  Gridded daily weather data for North America w...   \n",
       "148  Ibuprofen removal using coconut husk activated...   \n",
       "152  Data supporting the spectrophotometric method ...   \n",
       "153  An ATAC-seq atlas of chromatin accessibility i...   \n",
       "168  Viral metagenomes of Lake Soyang, the largest ...   \n",
       "174  Synthesis and characterization of 3-[3-(1H-ben...   \n",
       "177  Data on photo-nanofiller models for self-clean...   \n",
       "180  MedMNIST v2 - A large-scale lightweight benchm...   \n",
       "181  Dataset on an extended technology acceptance m...   \n",
       "185  Harmonised global datasets of wind and solar f...   \n",
       "192  Systematic phenotyping and characterization of...   \n",
       "\n",
       "     prediction_rule3_abstract  prediction_rule3_fulltext  \\\n",
       "1                            0                          1   \n",
       "2                            0                          0   \n",
       "9                            0                          0   \n",
       "10                           0                          0   \n",
       "18                           0                          0   \n",
       "24                           0                          0   \n",
       "34                           0                          1   \n",
       "47                           0                          0   \n",
       "58                           0                          0   \n",
       "60                           0                          0   \n",
       "61                           0                          0   \n",
       "66                           1                          1   \n",
       "73                           0                          1   \n",
       "92                           0                          0   \n",
       "97                           0                          0   \n",
       "99                           0                          0   \n",
       "101                          0                          0   \n",
       "114                          1                          1   \n",
       "118                          0                          0   \n",
       "122                          0                          0   \n",
       "126                          0                          0   \n",
       "129                          1                          0   \n",
       "134                          1                          1   \n",
       "140                          0                          0   \n",
       "146                          0                          0   \n",
       "148                          0                          0   \n",
       "152                          0                          0   \n",
       "153                          0                          0   \n",
       "168                          0                          0   \n",
       "174                          0                          0   \n",
       "177                          0                          0   \n",
       "180                          0                          0   \n",
       "181                          0                          0   \n",
       "185                          0                          0   \n",
       "192                          0                          0   \n",
       "\n",
       "     prediction_rule3_fulltext_few_shot  prediction_rule3_gemini-2_5-flash  \\\n",
       "1                                     0                                  0   \n",
       "2                                     0                                  1   \n",
       "9                                     0                                  0   \n",
       "10                                    0                                  0   \n",
       "18                                    0                                  1   \n",
       "24                                    1                                  1   \n",
       "34                                    1                                  1   \n",
       "47                                    1                                  0   \n",
       "58                                    0                                  0   \n",
       "60                                    0                                  1   \n",
       "61                                    0                                  1   \n",
       "66                                    1                                  1   \n",
       "73                                    1                                  1   \n",
       "92                                    1                                  1   \n",
       "97                                    0                                  1   \n",
       "99                                    0                                  1   \n",
       "101                                   0                                  0   \n",
       "114                                   1                                  1   \n",
       "118                                   1                                  1   \n",
       "122                                   0                                  0   \n",
       "126                                   0                                  0   \n",
       "129                                   1                                  1   \n",
       "134                                   0                                  0   \n",
       "140                                   1                                  1   \n",
       "146                                   0                                  0   \n",
       "148                                   0                                  0   \n",
       "152                                   0                                  1   \n",
       "153                                   0                                  1   \n",
       "168                                   0                                  1   \n",
       "174                                   0                                  0   \n",
       "177                                   0                                  0   \n",
       "180                                   0                                  0   \n",
       "181                                   0                                  0   \n",
       "185                                   0                                  0   \n",
       "192                                   0                                  1   \n",
       "\n",
       "     prediction_rule3_gemini-2_5-flash_zeroshot citing_paper_eid_text  \\\n",
       "1                                             1    2-s2.0-85210281314   \n",
       "2                                             0    2-s2.0-85171680307   \n",
       "9                                             1    2-s2.0-85176563605   \n",
       "10                                            0   2-s2.0-105002637578   \n",
       "18                                            1    2-s2.0-85212982352   \n",
       "24                                            1    2-s2.0-85173883804   \n",
       "34                                            1   2-s2.0-105001429996   \n",
       "47                                            0    2-s2.0-85126864679   \n",
       "58                                            1    2-s2.0-85210089827   \n",
       "60                                            0   2-s2.0-105002149939   \n",
       "61                                            1   2-s2.0-105003949553   \n",
       "66                                            1    2-s2.0-85167821848   \n",
       "73                                            1   2-s2.0-105001552301   \n",
       "92                                            1    2-s2.0-85210538344   \n",
       "97                                            0   2-s2.0-105008325674   \n",
       "99                                            1    2-s2.0-85076838242   \n",
       "101                                           0    2-s2.0-85151781902   \n",
       "114                                           1    2-s2.0-85163566858   \n",
       "118                                           0   2-s2.0-105003911484   \n",
       "122                                           0    2-s2.0-85184837866   \n",
       "126                                           1    2-s2.0-85175166432   \n",
       "129                                           1    2-s2.0-85196867431   \n",
       "134                                           0    2-s2.0-85118491607   \n",
       "140                                           1    2-s2.0-85144817654   \n",
       "146                                           1    2-s2.0-85211712060   \n",
       "148                                           0    2-s2.0-86000359834   \n",
       "152                                           0    2-s2.0-85210120741   \n",
       "153                                           1    2-s2.0-85115603472   \n",
       "168                                           1    2-s2.0-85125278381   \n",
       "174                                           1    2-s2.0-85117787998   \n",
       "177                                           0    2-s2.0-85013146982   \n",
       "180                                           0    2-s2.0-85193837673   \n",
       "181                                           1    2-s2.0-85185155470   \n",
       "185                                           0    2-s2.0-85204984969   \n",
       "192                                           0   2-s2.0-105001558082   \n",
       "\n",
       "                               citing_paper_title_text  \\\n",
       "1    Potential effects of climate change on cacti d...   \n",
       "2    Approaches to pollen taxonomic harmonisation i...   \n",
       "9    Government resource allocation practices towar...   \n",
       "10   Predictive modeling and optimization of CIGS t...   \n",
       "18   Experimental and modelling study of ammonia-ba...   \n",
       "24   Performance and dynamics of California offshor...   \n",
       "34   Towards a sustainable future: The interplay of...   \n",
       "47   Linking the Mediterranean MIS 5 tephra markers...   \n",
       "58   LEDGF interacts with the NID domain of MeCP2 a...   \n",
       "60   Government-involved urban meteorological netwo...   \n",
       "61   Osteosarcoma knowledge graph question answerin...   \n",
       "66   Trends in socioeconomic inequalities in malnut...   \n",
       "73   Cythochrome P450-mediated dinotefuran resistan...   \n",
       "92   Evaluating SWAT-3PG simulation of hydrologic a...   \n",
       "97   Health risk assessment from heavy metals in cr...   \n",
       "99   Effect of carboxylated poly L-Lysine as a cryo...   \n",
       "101  A first Chinese building height estimate at 10...   \n",
       "114  Do parenting behaviors intended as discipline ...   \n",
       "118  Multi-millennia slip rate relationships betwee...   \n",
       "122  Harnessing quantum computing for smart agricul...   \n",
       "126  Holocene vertical velocity fields in the calab...   \n",
       "129  Nationwide evaluation of changes in fluvial an...   \n",
       "134  Dataset of seized wildlife and their intended ...   \n",
       "140  Widespread missing super-emitters of nitrogen ...   \n",
       "146  Enhancing simulations of biomass and nitrous o...   \n",
       "148  Synthesis and adsorption efficiency of magneti...   \n",
       "152  Effects of sublethal concentration of thiameth...   \n",
       "153  Differential analysis of chromatin accessibili...   \n",
       "168  Viral diversity and biogeochemical potential r...   \n",
       "174  Coumarin–benzimidazole hybrids: A review of de...   \n",
       "177  Recent progress in marine foul-release polymer...   \n",
       "180  A comprehensive survey on deep active learning...   \n",
       "181  Importance and performance in PLS-SEM and NCA:...   \n",
       "185  Assessment of forest disturbance and soil eros...   \n",
       "192  Pad2 deletion induces anxiety‑like behaviors a...   \n",
       "\n",
       "                           cited_data_paper_title_text  \\\n",
       "1    The World Checklist of Vascular Plants, a cont...   \n",
       "2    European pollen-based REVEALS land-cover recon...   \n",
       "9                            Global Carbon Budget 2022   \n",
       "10   Modeling and performance analysis dataset of a...   \n",
       "18   Dataset of wet desulphurization scrubbing in a...   \n",
       "24   Developing reliable hourly electricity demand ...   \n",
       "34             The KOF Globalisation Index – revisited   \n",
       "47   Lake Ohrid’s tephrochronological dataset revea...   \n",
       "58   A catalogue of 863 Rett-syndrome-causing MECP2...   \n",
       "60   The automatic weather stations NOANN network o...   \n",
       "61   Building a knowledge graph to enable precision...   \n",
       "66   Multiple Indicator Cluster Surveys: Delivering...   \n",
       "73   Chromosome-level genome assembly of bean flowe...   \n",
       "92   The global forest above-ground biomass pool fo...   \n",
       "97   Health risk assessment of heavy metals (Hg, Pb...   \n",
       "99   Data supporting the spectrophotometric method ...   \n",
       "101  Global 1 km × 1 km gridded revised real gross ...   \n",
       "114  Multiple Indicator Cluster Surveys: Delivering...   \n",
       "118  Fault Scarp Dating Tool - a MATLAB code for fa...   \n",
       "122  Crop Origins and Phylo Food: A database and a ...   \n",
       "126  Last Interglacial sea-level proxies in the wes...   \n",
       "129  Development of CMIP6-Based Climate Scenarios f...   \n",
       "134  United States wildlife and wildlife product im...   \n",
       "140  Air pollution emissions from Chinese power pla...   \n",
       "146  Gridded daily weather data for North America w...   \n",
       "148  Ibuprofen removal using coconut husk activated...   \n",
       "152  Data supporting the spectrophotometric method ...   \n",
       "153  An ATAC-seq atlas of chromatin accessibility i...   \n",
       "168  Viral metagenomes of Lake Soyang, the largest ...   \n",
       "174  Synthesis and characterization of 3-[3-(1H-ben...   \n",
       "177  Data on photo-nanofiller models for self-clean...   \n",
       "180  MedMNIST v2 - A large-scale lightweight benchm...   \n",
       "181  Dataset on an extended technology acceptance m...   \n",
       "185  Harmonised global datasets of wind and solar f...   \n",
       "192  Systematic phenotyping and characterization of...   \n",
       "\n",
       "                                              abstract  \\\n",
       "1    Climate change is expected to adversely impact...   \n",
       "2    Pollen taxonomic harmonisation involves the st...   \n",
       "9    Government resource allocation practices for a...   \n",
       "10   This study employs Machine Learning (ML) techn...   \n",
       "18   In this work, we propose an experimental and m...   \n",
       "24   We use statistical methods to evaluate the coi...   \n",
       "34   Over the past three decades, India's transform...   \n",
       "47   Explosive activity preceding the ~40 ka Campan...   \n",
       "58   Methyl-CpG-binding protein 2 (MeCP2) is a ubiq...   \n",
       "60   Studies on urban climate are important to this...   \n",
       "61   Osteosarcoma is a prevalent primary malignant ...   \n",
       "66   Malnutrition in the Democratic Republic of the...   \n",
       "73                                       Onion thrips,   \n",
       "92                                                 NaN   \n",
       "97   This study is the first to assess health risks...   \n",
       "99   Buffalo bull sperm are more prone to cryo-inju...   \n",
       "101  Building height is a crucial variable in the s...   \n",
       "114  Religious affiliation may account for some var...   \n",
       "118  This study investigates slip behaviour on over...   \n",
       "122  Agriculture has undergone progressive transfor...   \n",
       "126  The study of fossil coastlines is one of the m...   \n",
       "129  To implement strategic countermeasures against...   \n",
       "134  The illegal wildlife trade (IWT) threatens con...   \n",
       "140                                Nitrogen oxides (NO   \n",
       "146  Nitrous oxide (N This study aimed to: 1) condu...   \n",
       "148  This study aimed to develop innovative activat...   \n",
       "152  Bees are pollinators of native and cultivated ...   \n",
       "153  Chromatin accessibility is a key factor influe...   \n",
       "168  As the most numerous biological entities on Ea...   \n",
       "174  Coumarin and benzimidazole are privileged stru...   \n",
       "177  Progress in materials science is associated wi...   \n",
       "180  Deep learning has achieved widespread success ...   \n",
       "181  This research offers a novel approach that ext...   \n",
       "185  The construction of wind farms, involving road...   \n",
       "192  Citrullination, the deimination of peptidylarg...   \n",
       "\n",
       "                                             full_text  \n",
       "1    Drylands biodiversity is essential to the well...  \n",
       "2    Quaternary pollen analysis demands careful and...  \n",
       "9    The Chinese central government uses a top-down...  \n",
       "10   In the pursuit of sustainable energy solutions...  \n",
       "18   The growing energy demand and the need for mor...  \n",
       "24   The global emergency to swiftly transition to ...  \n",
       "34   Global warming and climate change due to persi...  \n",
       "47   Near-vent volcanic successions provide fundame...  \n",
       "58   The methyl-CpG-binding protein 2 (MeCP2) is a ...  \n",
       "60   Meteorological observation networks play a cru...  \n",
       "61   Osteosarcoma is one of the most common bone tu...  \n",
       "66   Malnutrition is a global concern that retards ...  \n",
       "73   Insecticide resistance has been observed in nu...  \n",
       "92   Forests are an integral component of terrestri...  \n",
       "97   The presence of toxic elements such as arsenic...  \n",
       "99   The technology of artificial insemination (A.I...  \n",
       "101  Accurate measurement of building height is ess...  \n",
       "114  Parenting behavior intended for discipline (in...  \n",
       "118  Understanding the temporal behaviour of active...  \n",
       "122  According to the , the global population has s...  \n",
       "126  The Calabrian Arc defines the upper plate of t...  \n",
       "129  Flood disasters cause severe economic losses. ...  \n",
       "134  The presented data covers the illegal trade (i...  \n",
       "140  Nitrogen oxides (NO ≡ NO + NO ) play a central...  \n",
       "146  It's critical to mitigate climate change impac...  \n",
       "148  Pharmaceutical emerging contaminants, such as ...  \n",
       "152  Approximately 90 % of plants are pollinated by...  \n",
       "153  Chromatin accessibility across the genome defi...  \n",
       "168  Viruses were estimated population size of 10 v...  \n",
       "174  According to the International Agency for Rese...  \n",
       "177  Nanocomposites constitute a class of materials...  \n",
       "180  Medical imaging visualizes anatomical structur...  \n",
       "181  Comparing attributes' performance and importan...  \n",
       "185  Accelerating the penetration of renewable ener...  \n",
       "192  As one of the important ways to regulate prote...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disagreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e208cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "【確認が必要な 35 件の論文について、以下のプロンプトをコピーして私に送信してください】\n",
      "======================================================================\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 2 / DOI: 10.1016/j.jaridenv.2024.105282  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "The World Checklist of Vascular Plants, a continuously updated resource for exploring global plant diversity\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Potential effects of climate change on cacti distribution and conservation in North American drylands\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "Drylands biodiversity is essential to the well-being and development of approximately 2.7 billion people ( ). While drylands provide ecosystem services (e.g., nutrient cycling, primary production, water, and climate regulation) to sustain global biodiversity ( ), the conservation of this major biome, which covers approximately 45–47% of the Globe's terrestrial lands, is far from adequate ( ). Despite its importance, knowledge about threats (e.g., climate change) to dryland biodiversity is still incomplete, limiting sustainable conservation and management practices ( ). Therefore, understanding the impacts of potential threats on dryland biodiversity is crucial to help plan and implement conservation strategies. Drylands are relatively rich environments and are home to over 10,000 mammals, birds, amphibians, and iconic plant groups such as cacti and succulents ( ). The Cactaceae family originated in the American continent and encompasses approximately 127 genera and 1426 species ( ; ). Most of these species are native to deserts and semi-arid lands of North and South America, including Mexico, the United States, Central Andes, Brazil, Paraguay, Uruguay, and Argentina ( ; ). Cacti are well known for their thick and fleshy stem, significantly modified leaves (spine), and various shapes and sizes ( ). They are also essential for semideserts' community dynamics ( ). Many Cacti, however, are considered threatened and have been included in many red lists across the Globe ( ), especially in Mexico, where the world's highest cacti biodiversity is found ( ). Human actions often negatively affect cacti diversity, including habitat loss and degradation (urbanization), poaching, illegal trading, and anthropogenic climate change ( ). Climate change is regarded as one of the key drivers of ecosystem change ( ), especially in dryland ( ). Climate change is expected to change precipitation patterns and water availability by 10–30% in the next 40 years, influencing the intensity and frequency of floods and droughts ( ). Changes in temperature and precipitation regimes in drylands may also result in an expansion of land degradation ( ), the expansion of invasive species (e.g., , ), and influence the distribution range of native cacti species ( ). A recent study by revealed that climate change would impose a significant threat to the giant columnar cactus (Saguaro) in the Sonoran Desert (SW United States and northern Mexico). Despite the significant threats to drylands and cacti diversity, conservation actions aimed at halting cacti loss are scarce in many parts of the Globe, particularly in countries that face socioeconomic crises ( ). A potential aid to conserving dryland biodiversity is defining conservation priorities, such as establishing priority areas for conservation ( ). Priority areas are often used to ensure species' long-term survival and have been used to help managers and conservationists design strategies for implementing conservation actions in particular areas (e.g., biodiversity hotspots maps – ). A challenge to designing and implementing conservation actions is the lack of wall-to-wall inventory about species distribution ( ; ). To overcome the data shortfall, conservationists often use species distribution modeling (SDM) to predict the distribution of terrestrial and aquatic species ( ; ). The SDM process uses known species occurrences or abundances and environmental estimates to map species distribution across the geographic space ( ). Another challenge is incorporating the impacts of potential threats, such as climate change, on biodiversity to produce more effective conservation plants in the future ( ). Incorporating biodiversity threats into conservation plans is crucial to maintaining and increasing conservation planning projects' resilience ( ). This study focused on the spatial configuration of the priority areas for conserving native cacti from North American drylands. Its major goal was to integrate climate change considerations into the conservation planning of native cacti species in North America. Specifically, we investigated the potential impact of climate change on the spatial configuration of priority areas for the conservation of cacti. Over the years, several hypotheses have been proposed to explain the spatial distribution of plant species (e.g., ). They reinforced the idea that the contemporary climate (energy input and measures of precipitation) supports plant diversity. Since climate is a major driver of cacti habitat suitability ( ), we expect climate change will influence the spatial configuration of cacti's habitat suitability. In addition, we tested the ability of the water and energy hypotheses to explain cacti species distribution. We predict that cacti distribution is closely related to climatic variables and that these can be used to predict their potential distribution and the expected distribution under different global change scenarios. Finally, we investigated if the projected spatial configuration of priority areas for the conservation of cacti, in the current and future times, is well represented by protected areas (PAs). Our study area encompasses the North American drylands ( ). The World Atlas of Desertification ( ) defines drylands as ecosystems with a ratio between annual precipitation and potential evapotranspiration, with an aridity index (AI) lower than 0.65 ( ). Based on AI values, drylands are classified into four major subtypes - dry subhumid (0.5 ≤ AI < 0.65), semiarid (0.2 ≤ AI < 0.5), arid (0.05 ≤ AI < 0.2), and hyper-arid (AI < 0.05) ( ). Most of the arid and hyper-arid zones in the study area are deserts ( ). On the arid and hyperarid spectrum, the study area is characterized by four distinctive North American deserts: Great Basin, Mojave, Chihuahuan, and Sonoran. Together, they occupy an area of 1,277,000 km ( ). The Great Basin Desert is the northernmost ecosystem characterized by its cold winters and vegetation – often dominated by small leaf shrubs. The Mojave is the most arid desert, predominating low shrubs and annual ephemerals. The Chihuahuan and Sonoran Deserts are warmer ecosystems with low shrubs, small trees with microphyllous leaves, leaf succulence, and succulents, the most prominent of which are cacti. The Mojave and Chihuahua are warm-temperate deserts, while the Sonora has a subtropical climate ( ). Occurrence data, including current and historical records, were acquired for 838 species from five sources: a global cacti dataset from the Global Information Biodiversity Facility (841,116 - ), Arizona State University (281,766 - ), Desert Botanical Garden (97,453 - ), Herbario de la Universidad de Sonora (6719 ), and the Southwestern Environmental Information Network (102,579, ). Based on the assumption that enrivonmental factors (e.g., climate, topography) are key drivers of plant diversity ( ) and distribution ( ), we selected the following predictors: Climate (19 bioclimate variables from WorldClim, ); Solar radiation (monthly measures from WorldClim, Fick and Hijmans, 2007) and Topography (10 topographic variables from EarthEnv, , ). We also downloaded future climate data from WorldClim (v2.1) for the 2081–2100 period. The data includes future climate prediction (bioclimate variables) from the Coupled Model Intercomparison Project Phase 6 (CMIP6) across four Shared Socioeconomic Pathways (SSPs): 126, 245, 370, and 585 ( ). Bioclimate variables represent measures of temperature and precipitation interpolated climate data for global land areas ( ). Solar radiation includes monthly top-of-atmosphere incident solar radiation values ( ). The monthly radiation values were used to estimate the maximum, minimum, median, mean, and standard deviation (variability). Topography measures included median and standard deviation values for elevation, toughness, slope, topographical position index, and terrain ruggedness index. All variables were obtained at 2.5 min (approximately 5 km). Variables were processed using the Geographic Coordinate System (GCS_WGS_1984). The data cleaning process encompassed five steps. The cleaned regional dataset and the environmental predictors (see above) were used as inputs to Maxent—a maximum-entropy algorithm for modeling species distributions ( ; ) to produce suitability maps for each species across North American drylands and to project such suitability into forecasted future climate. Maxent is a widely used framework for modeling species distributions from presence-only records. The method uses the occurrence records and the background points, also known as pseudo-absences, and their predictors to minimize the relative entropy between probability densities defined in a predictor space ( ). However, the selection of background data may affect model performance, especially when these data are taken from restricted or broad regions ( ; ; ). One way to minimize the potential influence of the background points on prediction accuracy is to restrict the area where these would be sampled ( ; ; ). Thus, we used the function to create a mask around the presence records of each species, using a radius of 200 km ( ). Then, we used the function from the package ( ) to take 10.000 background points within the buffered area. We followed (No Date) to remove duplicate background points and those in the same presence area. Specifically, we overlaid background points to a regular grid cell, with the same resolution of the environmental variables, and used the cells and unique functions in R to find background points and presences that occur in the same cell. We repeated a similar procedure to produce a second set of new background points to evaluate the importance of the selected variables (see below). Because background points may overlay with occurrences, the final number of the former may vary. We used the package from R ( ) and the function to run Maxent models. We used the function ( , ) to avoid mixing highly correlated predictors in the same model for a data-driven variable selection. The function requires occurrences, absences or background points, and environmental variables to find the variables that yield the most accurate prediction. In our case, we used the cleaned records, a second set of 10.000 background points, which differed from those used in the calibration process (see above), and all environmental variables. When correlated, this function performs Jackknife tests to remove those variables, improving the model performance when removed until no high correlation is found—the 0.7 correlation threshold was used to define a high correlation ( ). Once the set of least correlated variables was established, we used the function, training dataset, and background points to predict the habitat suitability of each species across the study area. The variable importance was estimated by the presence and background points and the function ( , ). The importance was estimated as a contribution percentage for each variable. The function uses permutation to compute the decrease in training AUC. We computed the number of times a given variable ranked as the most influential across species distribution models (the variable with the highest percent contribution to the model) and grouped them as Top1. We repeated this procedure to identify the second (Top2) and third (Top3) most influential variables. We used the and functions from the package ( ) to estimate the area under the curve (AUC) and true skill statistics (TSS) values and to evaluate model predictions. Metrics, such as AUC and TSS, are widely used to model prediction accuracy ( ). We used the function ( ) to create four random folds for cross-validation model performance. Models with AUC and TSS greater than 0.7 were used to predict the habitat suitability across the study area (Current suitability model). The current suitability models were used to project habitat suitability under eight different Global Circulation Models (GCMs - ) across four SSPs. The GCM included nineteen bioclimatic variables (the same as used in the current model). GCMs have been used to address the effects of climate change on dryland biodiversity (e.g., , ; ). To project a suitability model, we replaced the selected bioclimatic variable (current time) with the ones provided by the GCM model while keeping the same selected non-climate variables used in the current model. Thus, eight projected suitability maps (one for each GCM) were produced for each species. The projected suitability maps across eight GCMs were then averaged (Future suitability model) and used as input for estimating future priority conservation areas (see below). The current and future suitability maps and Zonation (Zonation 5 v1.0, ) were used to identify priority areas to support the conservation of cacti species (high-value conservation areas, HVCA). Zonation uses distribution data (e.g., species, habitats, etc.) to generate priority ranking maps that define more or less important areas to retain regional biodiversity ( ). Specifically, we used the Core Area Zonation function (CAZ, ) and the projected suitability maps to estimate a priority rank map for each solution (one representing the current configuration and four representing the SSPs). The CAZ function aims to minimize biological loss by selecting areas with a high probability of species occurrence and less overlap among species distribution ( ). Priority values rank values ranged from 0 (low priority) to 1 (high priority). After obtaining the priority rank maps, the top 30% of top-priority rank values were used to find HVCA under the current and future scenarios. This value refers to the target established by the Convention on Biological Diversity (CBD). The CBD expects at least 30% of natural global areas to be protected ( ). Then, the overlay among current and future (SSP126, SSP245, SSP 370, and SSP 585) HVCA was estimated. The congruence (100% agreement) was considered a potential refuge area. We used the World Database on Protected Areas ( ) to estimate the percentage of HVCA represented by protected areas. The WDPA is the most comprehensive protected area database in marine and terrestrial realms ( ). We overlaid the WDPA layer to the HVCA (current) to find critical areas for conservation with no protected area cover. For projected HVCA (future scenarios), we estimated the percent of coverage in refuges and potential expansions. Model simulations indicated a mean AUC value of 0.92, ranging from 0.56 to 0.99 ( ). TSS values ranged from 0.13 to 0.99 (0.78 on average). Of the species with more than 14 records (514), 393 have AUC and TSS values greater than 0.7. Thus, the following results are based on these species ( ). The spatial distribution of cacti in the North American drylands was mainly influenced by climate ( ). A total of 217 (55.2%) species had their habitat suitability primarily influenced by climate, expressed by measures of temperature (77 species) and precipitation (140 species). Among these climate factors, precipitation seasonality (32 species) and minimum temperature of coldest month (26) were the most influential. The second most influential was solar radiation (145 species, 36.9%). Solar radiation minimum and standard deviation were the primary factors for 42 and 34 species, respectively. Topography was identified as the primary driver for 31 species (7.9%). The current geographic distribution of priority areas for conservation and hotspots of native cacti species is displayed in . Most of the hotspots were observed in semiarid (60.1%) and arid lands (28.9%) of the southmost part of the study area (tropical Mexico), the peninsula of Baja California (Baja California and Baja California Sur), Sonora, and Arizona ( ). Regarding the distribution of hotspots in deserts, most were in Chihuahuan (36.3%) and Sonoran (34.0%) deserts ( ), while the lowest were observed in Great Basin (12.1%). The congruence among current and predicted HVCA (refuge) spatial distribution was high (84.5%). The spatial distribution of refuges is located mainly in semiarid lands of the Sonoran and Chihuahuan deserts. The results also display potential HVCAs in the Great Basin and Sonoran Deserts – areas of projected HVCA expansion ( ). The overlay between protected areas and predicted HVCA for the current time was low (14.9%). Gaps were mostly distributed across the Mojave and the Sonoran Deserts ( A). Gaps were also observed in the Great Basin and Chihuahuan Deserts and the southmost part of the study. A similar pattern was observed with projected HVCA in future climate scenarios ( B). Only 16.6% and 6.3% of refuge and projected expansion areas will be potentially represented by protected, respectively. Ours is the first study to explore the major drivers of native cacti distribution across drylands of North America and to assess the impact of climate change on the HVCA of cacti. Results emphasize that climate, represented by measures of precipitation and temperature, is the major driver of cacti distribution. The importance of climate on plant distribution has been explored in previous studies (e.g., ). Nevertheless, their tests were focused on the spatial patterns of species richness. Therefore, results also support the climate hypotheses at drylands on a large scale and strengthen the notion that in marginal, stressful environments, abiotic factors are paramount in shaping biotic interactions ( ). Considering individual factors, the cacti distribution was mainly influenced by energy inputs to the environments, represented by temperature and solar radiation. Altogether, these two predictors were primary factors for 222 species. In general, energy availability is necessary for plants because it helps execute multiple functions and physicochemical and physiological processes (e.g., photosynthesis). While important, extremely high or low energy may impact physical processes and limit a plant's growth and development ( ). However, cacti plants' responses to extreme climate vary considerably among themselves ( ). Previous studies have reported the effect of temperatures on morphological factors (e.g., ; ). They highlighted that ambient temperature and direct solar irradiation are strongly associated with cacti stem surface temperature. Extreme temperatures may affect the saturation of vapor concentration in cacti tissues, leading to increased transpiration water loss ( ). Ice formation can occur in their intercellular spaces when the temperature is too low (e.g., sub-freezing), causing deaths if this freezing condition persists for long periods ( ). High temperatures, on the other hand, can inactivate cacti tissues ( ). Temperature and solar radiation also play an essential role in the success of cacti seed germination. In a recent study, reported that no seed germinated without light, independent of the temperature regime. They also noted that seed germination success is linked to temperature regimes in the presence of light. Therefore, cacti species require a minimum temperature and solar radiation to persist. Also, our results indicated that the influence of energy-related variables on cacti distribution was shared with water availability. This agrees with previous dryland studies that support the idea that water, particularly precipitation seasonality, is a primary controlling factor for dryland plant species ( ). Water is essential to plant physiological requirements. Extreme energy availability (e.g., high temperatures) in conjunction with extended periods of drought may change the evapotranspiration rate and restrict plant distribution across drylands ( ). Also, water restriction may cause plant cell damage, threaten their stem structural integrity, and limit photosynthesis activity ( ). Despite the numerous adaptations that allow cacti to thrive with minimal water requirements, water scarcity and the intensity of droughts are among the major threats to cacti ( ). Water availability is higher under the shade of nurse trees or near objects, by reducing extreme temperatures and insolation, by the phenomenon of hydraulic uplifting found in many desert trees, and by having soils richer in organic matter and litter that provide a mulch holding water for longer periods (Alberto Burquez personal communication). The areas with the highest proportion of HVCAs were semiarid and arid lands of the Chihuahuan and Sonoran Deserts. The patterns of high suitability observed in may be related to extenuating differences in temperature since areas of low suitability are located in the north portion of the study areas. Compared to the Mojave and the Chihuahuan Deserts, the Great Basin Desert has a prolonged frost during winter, significantly reducing cacti species and functional type diversity. Previous studies have recognized that winter temperatures limit the cacti distributional range (e.g., ). Many cacti are cold-intolerant species, and subfreezing temperatures are often fatal ( ; ). While climate change analyses project new HVCA in the future ( ), this study reported a strong overlay among current and future SSP scenarios (84.5%). One possible explanation for such high congruence is cacti's adaptability. The Cactaceae family differs from other plant families by several traits and their physiological tolerance to climate variability. Previous studies have reported that some cacti showed high-temperature tolerance with increased ambient temperature (high-temperature acclimation hypothesis, , ). Another potential explanation is the stem thermal tolerance mechanisms. The broad diversity in volume relative to surface area ( ) is vital for adapting across diverse environments ( ). The ratio relates to the capacity to store water, carbon, and nutrients and is directly related to cacti photosynthetic activity ( ). The areas with high concentrations of HVCA are home to several giant columnar cactus (e.g., , ). Changes in the ratio are also considered morphological adaptations in high temperatures. Thick stem species seem more tolerant to high-temperature stress. investigated high-temperature stress responses of 14 cacti and reported acclimation of high-temperature tolerance in all species. Such adaptations may allow them to thrive in hotter conditions. The high overlay reported herein may also indicate drylands climate refugia, i.e., areas that sustain their high value for the conservation of cacti over time in the face of climate change. Climate refugia areas may help to conserve cacti to adapt to climate change, keep the ecosystem functioning, maintain primary productivity, and allow species to persist despite rapid climate changes ( ). These areas can also support cacti restoration. Because of the lack of connectivity among some climate refuge patches ( ), ecological corridors might be required to facilitate genetic exchange among cacti populations, enhance adaptative capacity, and ensure connectivity. The mismatch between current and future HVCA patterns suggests climate change influences the spatial configuration of cacti's habitat suitability. reported large-scale changes in cacti species in saguaro. They identified a loss and gain of suitability in many regions of the Sonoran Desert. The results reported herein support the tenet that cacti and dryland ecosystems are sensitive to climate change ( ). Results also indicated projected HVCA expansions. These could be related to changes in the quantity and quality of suitable areas ( ). Identifying projected HVCA can help managers and conservationists consider expanding conservation actions to safeguard cacti diversity in the face of climate change. While most current conservation strategies focus on the persistence of static reserves ( ), results also reinforce incorporating climate change into the spatial conservation prioritization of cacti. Previous studies support the full integration of the effects of climate change into all conservation strategies ( ; ). This principle, also known as the climate-smart conservation process, aims to adopt climate change in forward-looking objectives and link conservation actions to climate impacts ( ). In the cases of cacti in North American Drylands, results show that managers and conservationists should explicitly consider and address climate-related shifts in species distribution and the spatial configuration of priority areas. It also highlights that species are not individuals isolated from their interacting species like pollinators, dispersers, and nurse plants, to mention the most relevant. To incorporate these new goals, managers may need to reconsider goals, adapt, and implement novel actions and actions aiming to respond to predictable changes. Their success may also depend on revising current goals and objectives and integrating climate adaptations into existing conservation strategies. Unexpectedly our results revealed an alarming outcome since the overlap between HVCA in the current time and the WDPA network was low (14.9%). This value is inferior to that expected by the Convention on Biological Diversity, which estimates that 30% of natural lands should be protected ( ). This low protection representation could be linked to several factors, including the paucity of comprehensive cacti data ( ). Distribution data is essential for developing conservation solutions ( ). Another factor is the negative perceptions of arid landscapes (Hoover, 2019). Drylands are often perceived as ugly and inhabitable landscapes (wastelands), especially by local populations, because of their barren aspect, lack of water, and scarce vegetation coverage (Hoover, 2019; ). The management of natural resources in dryland landscapes is strongly related to misconceptions, which have helped to undervalue the role of drylands to regional and global extents (Hoover, 2019). Our gap analysis is paramount to helping shift the narrative toward dryland landscapes and developing and implementing actions to conserve cacti diversity in North America. By considering the HVCA in current and future times, managers and conservationists may plan for species persistence, especially because species are likely to persist in suitable habitats ( ). Considering that climate change will potentially favor potential HVCA expansions, results can help forecast eventual changes in the spatial configurations of potential PAs to ensure that the network well represents native cacti ranges. Forecasting the impact of climate change on the distribution of HPCA for cacti would also help to mitigate emerging threats (e.g., invasive species, habitat loss, wildfires). In the last years, the abundance of invasive species (e.g., , ) in drylands of North America and the frequency of wildfires has increased significantly ( ). Habitat and wildfires are known to increase cacti risk of extinction ( ). Identifying and protecting potential HVCA can help develop international protocols and collaborations to monitor the impacts of threats and eventually minimize the risk of extinction. In addition to protecting biodiversity, protecting HVCA in drylands also safeguards the rights of local communities ( ). This combined approach, sometimes known as Community Conserved Areas (ICCAs), is recognized to promote sustainability practices between conservation goals and economic use ( ). ICCAs help to change the negative perception of dryland landscapes, support green jobs for locals, and compensate local communities for maintaining biodiversity and ecosystem services ( ). Establishing PAs can also help boost ecological connectivity, reduce land degradation, and implement sustainable practices to enhance the local economy while targeting critical conservation goals (e.g., Ecotourism). It is crucial to notice that the study's scale and spatial resolution may influence results. First, models did not include some of the predictors of plant diversity, such as biotic interactions and connectivity. Second, this study does not consider important biotic traits, such as plant dispersal. This information is essential because it affects the ability of plants to move and adapt to anthropogenic changes, such as climate and land-cover change ( ). Third, this study did not consider species-specific responses to climate change, which limits the development of ecological strategies to mitigate the potential impacts of anthropogenic change, such as the loss of suitable habitat ( ; ). Fourth, conservation actions and strategies are designed on much finer scales. However, the family Cactaceae distribution varies greatly. For example, the distribution of iconic giant columnar species (i.e., Saguaro) often includes some countries, such as the United States and Mexico. In this sense, large-scale approaches help to create trans-frontier programs and address common responsibilities to conserve cacti efficiently. The results should not be viewed as substituting for finer-scale conservation strategies but as strengthening the climate-smart perspective for preserving biodiversity in a changing world. Our results show climate change will impact cacti species' habitat suitability across all North American drylands. The impact of climate change on conservation often challenges society to maximize conservation efforts in current and future scenarios ( ). To improve the success of conservation actions, managers, conservationists, and society, in general, must be aware of how climate change will affect multiple facets of conservation projects, such as the effectiveness of current priority conservation areas in the future. This study, however, only explored one modification: climate. Future studies should investigate the incorporation of other factors, such as species traits (e.g., dispersal), changes in the physical environment (e.g., water availability), ecological context (e.g., biological invasions), or disturbance (e.g., wildfires, land use). This study highlights that managers and conservationists should account for the influence of climate on selecting areas for conservation and consider the potential impacts of climate change on species distribution and the spatial configuration of priority areas for the conservation of cacti species. Our results also help to fill gaps in cacti protection, especially in the drylands of Mexico. Writing – review & editing, Writing – original draft, Visualization, Methodology, Investigation, Formal analysis, Conceptualization. Writing – review & editing, Writing – original draft, Methodology, Conceptualization. Writing – review & editing, Writing – original draft, Methodology, Conceptualization. Writing – review & editing, Writing – original draft, Methodology, Conceptualization.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Used\n",
      "* **AIモデルの判断:** Not Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 3 / DOI: 10.1016/j.revpalbo.2023.104989  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "European pollen-based REVEALS land-cover reconstructions for the Holocene: Methodology, mapping and potentials\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Approaches to pollen taxonomic harmonisation in Quaternary palynology\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "Quaternary pollen analysis demands careful and critical identification of pollen and spores from ferns and fern allies (hereafter ‘pollen’) and meticulous counting. The taxonomic level of the identifications should be to the lowest (= finest) level possible so as to achieve the maximum amount of botanical and ecological information from such analyses (see, for example, ; ; ; ; ). The ever-improving quality and availability of high-quality optical equipment, the assembly of large and representative modern pollen-reference collections, and advances in pollen morphology (e.g., ; ) make possible the determination of some pollen types that were considered indistinguishable a few decades ago. In view of these developments, it is essential to have consistent and unambiguous identification and nomenclature of the fossil pollen recorded at different depths in the stratigraphical pollen sequence(s) (hereafter ‘sequence(s)’) of interest. To achieve this, taxonomic and nomenclatural harmonisation of pollen names is essential. The importance of harmonisation and of standardising harmonisation protocols and nomenclature is also recognised in neo-ecological research ( ), analytical palaeobiology (sensu ) involving both deep-time and Quaternary-time palaeobiology including Quaternary pollen analysis ( ; ; ; ), Quaternary macrofossil analysis (e.g., ), Quaternary palaeolimnology (e.g., ; ; ), and Quaternary palaeozoology (e.g., ). In Quaternary pollen analysis, the same pollen-morphological entity may have been given different names and be at different taxonomic levels in different pollen-identification manuals (compare ; ; ; ). The names for modern pollen-reference material of the same plant taxon may vary between laboratories or change over time as the name of the parent taxon, especially at the family, sub-family, tribe, or genus level, may change due to plant systematic revisions, resulting in different pollen nomenclatures between analysts, between laboratories, and between publications. An additional cause of variation in nomenclature is differences in how analysts make their determinations. The reasons for such differences can be the different qualities and availability of microscopes, the use of different optical systems, the availability and extent of modern reference material, the availability of the relevant pollen-morphological literature, and the research question(s) being addressed. When comparing pollen-analytical data from different analysts and laboratories, there is the need for rigorous pollen nomenclatural and taxonomic harmonisation (hereafter ‘harmonisation’) to amalgamate synonyms and to assign all determinations of a particular pollen morphotype to the same taxonomic level that should be at the finest possible taxonomic level within the data ( ; ). Prior to discussing harmonisation, we outline pollen identification protocols and basic nomenclatural conventions. Identification of fossil pollen is most rigorously done by comparing the fossil grain of interest with modern pollen-reference material prepared in a similar way to the fossil pollen. The modern reference material should encompass the likely regional pollen flora for the sequences(s) under study. This pollen flora is usually defined on the basis of modern plant geography such as the present occurrences of plant taxa in the area of interest, sometimes with additional knowledge (or assumptions) about past distributions. The concept of a regional pollen flora is, in practice, an abstraction as it contains the local, extra-local, and regional pollen-dispersal components (sensu ). Sometimes it can also contain some extra-regional pollen (sensu ). As warns [that as] “some pollen can be transported considerable distances from its source, there exists a finite probability that pollen of any plant in the world can be encountered during pollen analysis”. All identified pollen should be named in a consistent manner that indicates the degree of certainty of the determination so that equal weight in interpretation or harmonisation is not given to both certain (i.e. beyond all reasonable doubt for the geographical area of study) and doubtful determinations. The recommended conventions given below build on the systems presented by , , and . The use of a consistent nomenclatural convention greatly aids harmonisation if new sequences are considered and provides a sound basis for comparisons within and between study areas. These conventions are as follows. There are several instances where harmonisation is essential. For example, an analyst may count part of a sequence in one year and return to fill in detail a year or two later. A pollen-morphological type may be assigned different names in the years between the analyses due to increased analytical experience, and/or access to more modern reference material and/or new relevant literature. Similarly, two or more analysts may study the same stratigraphic sequence(s). Leaving aside the critical question of analytical quality control between the analysts' pollen counts, different names may be assigned to the same pollen-morphological type by the analysts. To achieve nomenclatural consistency, pollen taxonomic names must be harmonised in such situations when there is more than one analyst. The need for harmonisation becomes increasingly important when pollen-analytical data from many sequences in a geographical area are combined, synthesised, and compared. The extent of the geographical area may range from a single valley (e.g., ) to a large island (e.g., ), a landform unit (e.g., ), a state (e.g., ), part of a country (e.g., ; ; ), an entire country (e.g., ), a biome or a major climate zone (e.g., ; ; ), part of a continent (e.g., ; ; ), an entire continent (e.g., ; ; ; ; ; ; ; ), a hemisphere ( ), or the Earth ( ; ). Harmonisation seeks to find a pollen taxonomic level to which all the pollen of a particular plant taxon (or taxa) have been consistently determined in all the sequences within the geographical area of study and to give an unambiguous and appropriate name to that morphotype ( ). There are two contrasting approaches to harmonisation – the ‘top-down’ approach of and the ‘bottom-up’ approach illustrated by the pollen harmonisation tables outlined in this paper and available from the Fig Share link (see Data Availability). The ‘top-down’ approach presented by is created at the global scale, It uses 2831 pollen sequences as the basis for harmonisation, consisting of 1075 sequences from Europe, 1032 from North America, 488 from Asia, 150 from Latin America, 54 from Africa, and 32 from the Indo-Pacific region. These 2831 sequences contain 10,110 pollen taxon names. These names are initially screened to remove synonyms prior to harmonisation. The harmonisation criteria applied are: (i) pollen of all woody taxa and of major herbaceous taxa (e.g., ) are harmonised to genus level, and (ii) pollen of other herbaceous taxa (e.g., Poaceae, Cyperaceae, Fabaceae, Apiaceae) are harmonised to family level (see for further details). As the starting point is 2831 sequences from six continents, it can be viewed as a top-down procedure that reduces 10,110 unharmonised taxon names to 1002 harmonised taxon names. After this harmonisation, use a consistent age-depth procedure to derive harmonised chronologies for these 2831 sequences. Many of the top-down harmonised pollen sequences form the basis for climate (2594 sequences) and biome (1887 sequences) reconstructions for the Northern Hemisphere ( ; ; ; - see also ). An alternative approach (‘bottom-up’) is achieved by first defining the likely regional pollen source-area and flora for the geographical area under study based on the present-day flora and plant geography and, if known, possible past history (harmonisation region, sensu ). Knowledge of the pollen morphology and nomenclature of all the plant taxa in the regional flora is required so that morphologically similar fossil pollen that have been identified and named in different sequences within the harmonisation region can be assigned to the finest pollen taxonomic level that is consistently distinguished within all the sequences in the region of study and then given an appropriate and unambiguous name. Taxon names at the species level (e.g., ) may have to be assigned to a coarser pollen taxonomical level (e.g., -type, or -type) as additional sequences and their taxa are added. This means losing some taxonomic detail and thus potential botanical and ecological insights. Monospecific taxa with distinctive pollen morphology (e.g., ) remain at the specific level, in contrast to the harmonisation in the top-down approach where is grouped with all other Poaceae. Different hierarchical levels can be distinguished in the bottom-up approach within a study area such as Europe. For example, discuss three harmonisation levels (H , H , H ). Level H represents the currently accepted most detailed level of pollen identification based on the relevant literature, extensive reference collections, and expert knowledge and contains about 1200 taxon names. Based on this H taxonomy, present two coarser hierarchical levels (H , H ) reflecting different levels of taxonomic precision. Level H (860 taxa) combines morphologically similar pollen types that can only be separated using extensive modern reference collections and requiring considerable palynological expertise. This often results in identification of pollen taxa at the species, species-group, genus, or sub-genus taxonomic level. Level H contains 310 pollen taxa with distinctive and readily identifiable features. These taxa mainly belong to plant genera, sub-genera, groups of genera, or families ( ). As examples of harmonisation tables, we present pollen harmonisation tables constructed using the bottom-up approach to harmonise fossil pollen records in the HOPE (Humans On Planet Earth) project ( ) ( and Fig Share archive; see Data availability). Initially, fossil pollen data were downloaded from the Neotoma Paleoecology Database ( ; ) or from the Pangaea archive (Data Publisher for Earth and Environmental Science, ), or were provided by individual researchers ( ). The world was divided into seven harmonisation regions (see the Shapefile delimiting these regions archived on Fig Share; see Data availability) covering all continents except Africa and Antarctica. Harmonisation tables were developed for each region using the bottom-up approach. Several data-quality or data-type criteria (e.g., depositional environment, chronological controls) were applied to these data following the guide to processing pollen data for broad-scale analysis (see fig. 1 in ). The main criteria applied were (i) only data-sets from certain sediment depositional environments were included (see Table S3 in ); (ii) only pollen of terrestrial tree, shrub, dwarf-shrub, herb, palm, succulent, and mangrove taxa were included, (iii) only data-sets with five or more levels were included; and (iv) only data-sets with more than two acceptable chronology control points were included (see Table S2 in ). The seven tables are summarised in and are archived on Fig Share (see Data availability). For each table, the number of pollen sequences used in the harmonisation, the number of unharmonised pollen taxa, the number of harmonised taxa, and the initials of the person/people responsible for the harmonisation for the particular region are given. In all the harmonisation tables archived on Fig Share except Europe, column 1 contains the unharmonised raw pollen taxon names in the original data from that geographical region, column 2 gives the raw taxon names modified into a form readable by the FOSSILPOL workflow ( ), and column 3 contains the harmonised taxon names for all regions except Europe. In the European table, column 3 contains the harmonised names (741 taxa) at the finer H taxonomic level (MHVar.1) of , and column 4 lists the H harmonisation (280 taxa) (MHVar.2) level of updated and slightly modified by VAF and HJBB. In all HOPE data analyses for Europe, the harmonisation from column 4 was always used. By looking at the raw unharmonised pollen taxon names and the harmonised pollen taxon names in the Fig Share tables, an interested reader can readily see what harmonisations were made and which and how many unharmonised taxon names were included in a particular harmonised taxon. Despite the inevitable loss of taxonomic detail and hence botanical and ecological information due to harmonisation, such harmonisation whether it be top-down or bottom-up is essential to achieve a critical and consistent pollen taxonomy and nomenclature for the comparison and analyses of the stratigraphical sequences used in synthetic studies. At a broad scale of analysis, this is adequate, even though some taxonomic and ecological detail at individual sites may be lost. Harmonisation is conventionally done for a specific region to address particular research questions (e.g., ; ; ; ; ; ; ; ; ). These questions define the spatial and temporal domains of study and thus the harmonisation region. Harmonisation based on the bottom-up approach described above is spatially specific and hence project-specific. It is also important to note that such a bottom-up harmonisation table constructed for a specific area and based on all appropriate pollen sequences in that area may not necessarily be identical to a harmonisation table for the same area but based on, say, only 50% of the sequences in the area. Bottom-up harmonisation tables are thus not only but are also . These harmonisation tables cannot thus be directly transferred to a different project but they may serve as a starting point for creating a table relevant to that project and the sequences in hand. emphasise that “it is advisable to work with an expert familiar with the modern and fossil pollen flora of the spatiotemporal domain of interest in order to create a reliable project-specific table of harmonized taxa”. It is therefore necessary to create new tables for different projects, each with their specific spatial and temporal domains as the bottom-up harmonisation procedure is based solely on which pollen sequences are included and their pollen composition to be harmonised which, in turn, depends on the research question(s) being addressed (e.g., comparing rates of palynological change in Scandinavia with rates of palynological change in the European Mediterranean). The locations of the sequences define the harmonisation region of the underlying regional pollen flora and the nomenclature of all the pollen taxa in the individual sequences selected from that geographical area provide the basic material for critical bottom-up taxonomic and nomenclatural harmonisation. The problem of sequence-specificity inherent in the bottom-up approach is largely avoided in the top-down harmonisation procedure of as it is based on harmonised pollen taxon names for 2831 sequences from six continents at the genus or family level. The harmonised names are ‘fixed’ in contrast to the bottom-up approach where the harmonised names depend solely on the sequences included for a particular study. Inevitably in the top-down approach using only genera and families for the taxonomic levels adopted, loss of taxonomic detail occurs, particularly in herbaceous taxa (e.g., Poaceae, Saxifragaceae, Gentianaceae, Plantaginaceae, Scrophulariaceae). In the HOPE bottom-up approach some harmonisations remain at the species level (e.g., ) in contrast to the top-down approach where these cereals are amalgamated with other Poaceae pollen. There may also be a loss of taxonomic detail for some major arboreal taxa (e.g., ) in the top-down approach. In the bottom-up approach, pollen of taxa consistently distinguished at the sub-genus level (e.g., sub-genus (diploxylon pines) and sub-genus (haploxylon pines); sub-genus (deciduous oaks) and sub-genus (evergreen oaks)) are harmonised below the genus level, whereas in the top-down approach they are automatically harmonised at the genus level. In harmonising the 2057 unharmonised pollen taxa in the European HOPE data, the bottom-up approach yields a total of 280 harmonised taxa ( ) whereas the top-down approach results in 208 harmonised taxa. This is a reduction of 26% compared to the bottom-up harmonisation. A reduction in the number of harmonised taxa (22%) similarly occurs when the North American HOPE data are harmonised using the top-down approach compared to the bottom-up harmonisation summarised in . The strengths and weaknesses of these two approaches are summarised in . It is important to emphasise that all harmonisation tables relate directly to pollen and spores and not to plants or vegetation ( ). To link pollen and plants, the plant data need to be converted into pollen taxa to create a ‘common currency’ (sensu ) or taxonomic equivalents (‘species equivalents’ sensu ) between pollen and plants that are required to permit comparison of the two. For example, compared modern surface-pollen data with the composition of contemporary vegetation from which the pollen samples were collected by grouping the taxa in the vegetation into their corresponding pollen taxa (see also ). In conclusion, careful and rigorous pollen taxonomic harmonisation is an essential step to ensure a consistent and unambiguous taxonomy and nomenclature of the fossil pollen recorded in all the stratigraphical sequences under study prior to any synthetic analyses of combined pollen data-sets. It can be achieved using either the top-down or bottom-up approach. Which to use depends on the research questions being asked, which in turn define the relevant taxonomic detail and spatial scale. The tables provided in Fig Share serve as a starting point for future palaeoecological or biogeographical studies, which may require a bottom-up harmonisation approach to ensure consistent comparisons between sequences at different locations.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Not Used\n",
      "* **AIモデルの判断:** Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 10 / DOI: 10.1016/j.resconrec.2023.107296  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Global Carbon Budget 2022\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Government resource allocation practices toward carbon neutrality in China: A hybrid system approach\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "The Chinese central government uses a top-down management but also the bottom-up style to ensure that frontline practices align with established policies and regulations to achieve carbon neutrality. Carbon neutrality is a balanced neutral system to achieve net-zero carbon dioxide emissions by including many activities within the scope of indirect emissions and applying science-based tools to reduce emissions as well as relying on offsets ( ; ; ). To ensure this top-down and bottom-up alignment, there is an urgent need to adopt dynamic system theory (DST) to better understand the complex activity relationships and interrelationships among the various aspects, including imports, exports, processing trade, industrial demand, consumption, and carbon emissions ( ; ; ). Through DST, it is possible to elucidate the complex natural system and recognize the interrelationships between the temporal dynamics and impacts of the system ( ; ; ). These discovered interrelationships among the dynamics can provide solid guidance to decision-makers to facilitate resource allocation effectiveness ( ; ; ). Moreover, resource allocation requires a clear picture of dynamic flows to guide provinces/cities to invest in critical dynamics; otherwise, mis-investment may result in resource waste and more carbon emissions. The Chinese central government has aggressively launched dual carbon goals to plan for a carbon peak in 2030 and thereafter carbon neutrality by 2060. Local provincial and city governments rely on experts to identify the driving dynamics to ensure alignment with these goals ( ; ; ). However, in the absence of an effective approach, provinces/cities not only face difficulties in facilitating accurate dynamics identification by combining public interests and other data but also fail to recognize their position regarding the resource allocation level to develop these dynamics ( ; ). Complex interrelated influences have still been excluded from these discussions, although studies have attempted to simulate a system to identify the specific dynamics driving carbon neutrality to guide provinces/cities ( ; ; ). These studies have all reiterated the importance of DST in assisting in pursuing carbon neutrality in terms of identifying the international trade effect on carbon reduction in terms of industrial demand, transportation, and dynamic control by modeling interrelationships, recognizing changing dynamics, and assisting decision-makers in formulating relevant policies ( ; ). The DST is an important instrument for identifying dynamic changes in long-term operations and confirming system stability ( ). attempted to determine the link between practices and the low-carbon economy from a DST perspective to assess the regional innovation system. analyzed the relationship between supply chain resilience and profitability by implementing DST and the structural dynamics control approach. used DST to develop a model for exploring long-term operation strategies for the solar photovoltaic supply chain. evaluated sustainable building materials from a dynamic system perspective to reduce the negative environmental impact and life cycle cost of buildings. established a supply chain model based on DST to generate resilience, sustainability, and flexibility and to guide decision-makers in formulating relevant strategies. The DST can accelerate the achievement of carbon neutrality is still omitted in these current discussions Although these studies presented that the DST has been more widely applied in different fields by distinguishing the potential dynamics to support decision making. Many studies have attempted to develop potential models based on DST to promote carbon neutrality. However, these studies have focused on applying empirical studies and qualitative analyses without integrating data sets to guide resource adjustment ( ; ). Accordingly, a hybrid method was proposed to address these shortcomings while allowing for the consideration of complex interrelationships. Therefore, exploratory factor analysis (EFA) and reliability test (RT) methods can be employed to discover potential aspects and strengthen the theoretical basis with validity and reliability verification ( ; ). The qualitative normalization method, social media transformation method, and fuzzy synthetic method (FS) can help convert various data into unit-free values, entropy weights, and crisp values, respectively, to provide visual analysis by integrating these various data types into the Decision-Making Trial and Evaluation Laboratory (DEAMTEL) ( ; ; ). Then, dynamic resource analysis (DRA) can be adopted to position provinces/cities and provide a precise direction to guide improvements. The objectives of this study are (1) to explore the critical dynamics based on DST in promoting the success of achieving carbon neutrality; (2) to propose a hybrid method to bridge theory and practice; and (3) to provide the precise improvement direction for provinces/cities. This study contributes to (1) constructing the potential aspects to strengthen the theoretical basis and understanding; (2) establishing a hybrid method to identify the complex interrelationships among potential dynamics by integrating different data types as well as identifying gaps among experts, the public and governments; and (3) providing visual analysis for positioning provinces/cities to determine the direction toward carbon neutrality by allocating resources effectively and efficiently. The remainder of this study is structured as follows: in the next section, a literature review is presented to provide background information on the theory, proposed methods, and collected measures. In , detailed computational principles for the hybrid method and the proposed analytical procedures are provided. In , the case background and analytical results are described by linking the proposed analytical procedures. In , a discussion of the obtained theoretical and managerial insights is presented. The conclusions, contributions, main findings, research limitations, and further research are outlined in the last section. A detailed DST background is provided to facilitate a better understanding. The proposed methods and measures are also presented. DST is a theoretical framework that views the resource allocation system as a complex, dynamic, and adaptive entity that interacts with environments over time rather than as static and isolated components ( ; ). Within this context, dynamics is the constantly changing nature of the system and its elements over time. This dynamic change is usually difficult to predict and exhibits unpredictable characteristics ( ). Then, the resource allocation system can be defined as a collection of interrelated and interdependent components or entities that result in a coherent and collaborative whole ( ). These interrelationships and interdependencies can result in composite and dynamic behaviors that cannot be predicted by simplifying the behavior of any single component alone ( ). The DST concept emphasizes the complexity and interdependence of multiple components and the continuous and evolving nature of these interactions over time. established a two-parallel model based on DST and compared the traditional supply chain without carbon emission reduction and the low-carbon supply chain to address the complexity of sales efforts and carbon emission reduction efforts. Then, investigated the feed-in tariff and tax rebate regulation for the solar photovoltaic supply chain by developing a decision model using this theory. adopted DST to develop a simulation model for foreign trade to guide short- and long-term decision-making processes to reduce the complexity of achieving system stability. formulated a multi-objective optimization problem based on DST to address distributed power resource allocation for a microgrid. used DST to model the stability of equilibrium queue length to assess reactive capacity allocation and customer satisfaction. Studies aimed to combine DST with carbon neutrality to guide practice. For instance, used the interaction relationship model and simulation experiments to identify the possible trends of carbon neutrality by promoting the understanding of the evaluation principle of regional low-carbon innovation systems, verifying the need for DST adoption. adopted the analytical hierarchical process by developing a dynamic system model under DST to select sustainable building materials to achieve carbon neutrality. combined DST with game theory to study specific solar regulation and investment issues to achieve carbon neutrality by developing a dynamic decision model. These studies have provided solid evidence for achieving carbon neutrality, which requires DST implementation to reveal the complex interrelationships and discover long-term strategies for achieving a stable or balanced state ( ; ). Carbon neutrality occurs when the human-caused carbon dioxide emissions associated with a given object are offset by human removal over a given period ( ). To achieve carbon neutrality, each nation must develop effective strategies to limit its associated emissions. Many nations plan to achieve net-zero carbon dioxide emissions by 2050 ( ; ). To ensure that this goal can be achieved, the Chinese government has set dual carbon goals, requiring not only that carbon emissions must peak by 2030 but also that carbon neutrality must be achieved by 2060 to meet the agenda of the United Nations Climate Summit ( ; ; ). emphasized that the pursuit of carbon neutrality is not an easy and solitary task and requires the efforts not only of countries and firms but also of the public. In carbon neutrality studies, Wang et al. (2023) found that trade openness can inhibit carbon emissions once structural breakpoints are crossed and can contribute to global carbon neutrality. In addition, and suggested that industrial demand must be considered; otherwise, carbon emissions may be underestimated and hinder the achievement of this goal. emphasized that transportation emissions also highly impact this pursuit; therefore, this process must rely on different region-specific strategies instead of adopting an overall approach. In addition, other studies have attempted to increase the likelihood of achieving carbon neutrality by addressing carbon intensity, which includes renewable energy, resource productivity, and economic globalization ( ; ). These studies have demonstrated the challenging characteristics of the pursuit of carbon neutrality, which cover multiple aspects and complex interrelationships. attempted to use the generalized method of moments associated with fully modified ordinary least squares and quantile regression estimators to develop a dynamic model to account for the variables of income, energy consumption, resource dependence, and trade openness. applied bibliometric analysis to determine the dynamic trend to guide the world in the pursuit of carbon neutrality. employed kernel density estimation and statistical analysis methods to reveal that China's carbon neutrality development is on the rise based on the dynamics of temporal evolution. Recent studies have provided guidance from an energy consumption perspective and bridged knowledge gaps with dynamic system considerations by generating qualitative discussions and involving social aspects ( ; ). The effects of complex interrelationships on these dynamics in achieving carbon neutrality have been neglected, although studies have implemented various approaches to identify the potential dynamics. In pursuing carbon neutrality, mitigation efforts for energy combustion sources are crucial to the development of neutrality dynamics ( ). Because of this consideration, coal, oil, and natural gas combustion processes are listed in China's emissions inventory. Accordingly, the carbon emission intensity of natural gas combustion (D1) was selected to represent residential demand, and the industrial demand for raw coal combustion (D2) and the industrial demand for coke combustion (D3) were selected to represent industrial demand ( ). However, studies have emphasized that dynamic emissions originating from the transportation sector must notably be considered in addressing neutrality issues ( ; ; ). Therefore, data on the combustion of other gases (refer to hydrogen, methane, etc.) in transportation (D4), carbon emissions resulting from gas and oil combustion in transportation (D5), and carbon emissions resulting from diesel combustion in transportation (D6) were collected as the concerned measures. In addition, pointed out that the total carbon intensity (D7) should be considered to reflect the gap between actual practices and planned goals. Nevertheless, several studies have emphasized the need to include other carbon emissions produced in the process (D8), carbon emissions originating from the combustion of cleaned coal (D9), carbon emissions originating from the combustion of other washed coal (D10), carbon emissions originating from the combustion of briquettes (D11), carbon emissions originating from the combustion of coke oven gas (D12), and carbon emissions originating from the combustion of other coking products (D13) to obtain the dynamic distribution characteristics, development patterns, and harmonization of carbon reduction ( ; ; ). In addition, carbon emissions originating from the combustion of crude oil (D14), carbon emissions originating from the combustion of kerosene (D15), carbon emissions originating from the combustion of fuel oil (D16), carbon emissions originating from the combustion of refinery gases (D17), carbon emissions originating from the combustion of other petroleum products (D18), and carbon emissions originating from the combustion of natural gas (D19) are related to industrial emissions and must be considered simultaneously ( ). Recent studies have examined dynamic changes from a trade perspective for achieving carbon neutrality, as different trade structures have distinct dynamic characteristics with unique patterns ( ; ; ). Accordingly, general trade imports (D20), total exports (D23), general trade exports (D24), total imports (D27), total general trade imports and exports (D28), and total imports and exports (D31) were selected to identify the trade structure associated with carbon emissions ( ). Certain dynamics may occur at the processing stage that can affect carbon neutrality; therefore, it is necessary to consider the incoming processing and assembly trade exports (D21), the import and export volumes of the processing trade (D22), the import volume of the incoming material processing and assembly trade (D25), the total imports and exports of the incoming material processing and assembly trade (D29), and the total imports and exports of the imported processing trade (D30) ( ; ). The detailed instructions for hybridization of these proposed methods are illustrated equation by equation in this section to guide the practitioner. This study collected data from four databases: the China Energy Statistical Yearbook, Global Real-time Carbon Database, Ministry of Commerce Statistical Yearbook, and Customs Statistics of China (as indicated in ). These collected data cover 30 Chinese provinces/cities from 2013 to 2018, including at least 5580 pieces of data to facilitate comprehensive analysis in dynamics identification. However, the collected data exhibit different units, which prevents direct analysis. Therefore, the following hybrid method must be used to solve this problem under validity and reliability confirmation. This study proposes a hybrid method by integrating the quantitative normalization, EFA&RT, social media transformation, FSDEMATEL, and DRA methods. In this regard, quantitative normalization is an effective approach to address different data set units ( ). This method can be employed to transform different unit values into unit-free values between 0 and 1 ( ). These unit-free values are the basis for further analysis. Then, EFA&RT can be used to structure relevant theoretical aspects and strengthen the foundation, thus promoting understanding ( ; ). Therefore, the EFA method can be adopted to confirm the validity of the overall obtained structure, and the RT method can be used to ensure reliability for verifying the internal consistency of criteria within grouped aspects ( ; ; ). The Baidu Index is an effective platform for collecting social media data because it is the largest search engine in China ( ). These collected social media are presented in accumulated frequencies to represent public search interests ( ). However, these accumulated frequencies, which exhibit qualitative characteristics, can be converted into entropy weights via the entropy weighting method ( ; ). Both and emphasized that this method allows weight calculation based on the degree of diversity or duality of the data, which makes it possible to remove bias due to subjective feelings. also reported that this approach can provide broader applications due to its reliability in extensive evaluations and simplification within operations. Subsequently, the obtained weights must be integrated by using the FSDEMATEL method. This method overcomes the weakness of the traditional DEMATEL method in solving hierarchical problems by integrating the FSM ( ; ; ). In addition, this technique can provide a simple visual diagram to enhance the understanding instead of complex numbers to guide practitioners. The DRA method is an extension of the performance and resource analysis method proposed by . Within performance and resource analysis, provinces/cities are positioned in a four-quadrant diagram to diagnose their current positions to implement correct adjustments. In this study, the DRA method is proposed according to this concept to detect the potential dynamic changes for provinces/cities to improve the resource allocation accuracy. However, the above collected quantitative data can hardly be analyzed because they possess different units. These different units cause analysis complexity, and it is difficult to make direct comparisons and calculations to obtain support in decision making. Therefore, in this study, min–max normalization is proposed to normalize these quantitative data into unit-free values ( ). This approach has been widely used in various fields and yields unit-free values that vary between 0 and 1 ( ). This transformation allows these unit-free values to be integrated with various mathematical approaches for decision support. Assuming that there are types of data for city during period , the collected data can be written as . For data with positive features, the normalization process can be expressed as follows ( ; ): The EFA method was used to determine the potential aspects for representing the precise dynamics by bridging the theory once the data were normalized. For this purpose, the principal component method was used to obtain the factor loadings, which must be greater than 0.5. Then, the Kaiser‒Meyer‒Olkin (KMO) Bartlett's test of sphericity was employed to confirm that the was less than 0.05 and that the KMO value was greater than 0.7 to ensure validity ( ). Once the validity has been confirmed, the RT method must be applied to ascertain the internal consistency of the structured aspects. These aspects must pass the RT standard, and Cronbach's , average variance extracted (AVE), and composite reliability (CR) must be evaluated to demonstrate reliability and should be greater than 0.7, 0.36, and 0.7, respectively ( ). This study collected search frequency data from the Baidu Index based on the aspects obtained via the EFA and RT methods to consider the expectations of the public. The Baidu Index accumulates the frequency of specific terms from the search engine to provide data accessibility based on China's largest search engine Baidu ( ; ). The accumulated frequency data can represent the public interest in specific areas to guide decision-makers for considered public interests to prevent subjectivity. However, these frequency data exhibit qualitative features and cannot be directly applied to the calculation. Therefore, the entropy weighting method was adopted in this study to overcome this deficiency. Suppose the accumulated frequencies of the acquired aspects can be denoted as , where is the acquired aspect and is the corresponding year. Then, the entropy weight can be calculated as follows ( ): The entropy can be generated as follows: . Then, the entropy weights for the various aspects can be calculated as follows: In the traditional DEMATEL method, problems with a hierarchical structure can hardly be resolved. Moreover, repeated requests for expert evaluations of the influences of the proposed criteria and aspects are needed in the traditional DEMATEL method. Therefore, extended the traditional DEMATEL method to solve three-level hierarchical problems by integrating the FSM and DEMATEL methods without repeated expert evaluations. Assume that aspects are obtained by grouping the proposed criteria via EFA. Then, these c criteria are evaluated by experts. These ratings are represented as by adopting a five-level linguistic scale including nonsignificant , lower , equal , higher , and substantial levels. Therefore, these assessments can be written as follows ( ): , , , , and indicate the accumulated frequency of each of the linguistic scale intervals for the expert assessments. The accumulated frequencies adhere to the following equations, including the membership function and crisp values . The obtained crisp values can be arranged into a self-matrix under each of the determined aspects. At the second level, the following equation can be employed for generating the measurement criticality to illustrate the interrelationships between the obtained criteria and aspects. Then, the factor weight can be determined to compute the membership function as follows: indicates that the grouped criteria belong to the structured aspects provided by EFA. After factor weight calculation, the membership function for each aspect can be obtained as follows: can be rewritten as . Then, the crisp values can be rearranged into the direct relation matrix as follows: is the direction matrix from the expert opinions, merges the social media results and expert opinions, integrates the quantitative data and expert opinions, and denotes the combination of the social media results, quantitative data and expert opinions. The total relation matrix for the determined aspects can be expressed as follows: is the unit matrix. This step can be repeated for , , and to obtain the driving and dependence power. Then, the driving and dependence power can be calculated as follows: With the use of the coordinates , the aspects can be mapped into a cause-and-effect diagram. Here, and denote the horizontal and vertical axes, which also reflect the importance and divide the aspects into cause and effect components. Subsequently, the threshold for interrelationship identification can be obtained as follows: For , there exists an interrelationship between the and aspects. Conversely, these aspects are not interrelated. To obtain a better understanding of the current situation in the considered provinces, dynamic positioning analysis was adopted to determine the dynamics under different considerations. Assuming normalized data, these data can be merged into one system as follows: Subsequently, a two-dimensional diagram was generated, in which the average value denotes the intersection of and , and the provinces/cities were mapped into the diagram. This section provides the case background and analysis results to provide an understanding of the application of the obtained analysis results to reveal significant findings. Under the Chinese government's dual carbon goals, each province and industry strive to reduce carbon emissions. The achieved progress is supported by the fact that China's carbon emissions increased at a rate of 3.5% from 2020 to 2021 and subsequently exhibited a negative growth rate of 0.9 % in 2022 ( ). However, this notable reduction did not impede the thriving international trade between China and the world. According to a relevant report (Digital and Sustainable Trade Facilitation: Global Report 2021), international trade still provided strong increasing dynamics to leverage the economic growth of relevant industries ( ). However, the tradeoff between economic growth and carbon emissions is a double-edged phenomenon. The booming domestic demand could affect transportation emissions and industrial production demand, which could increase carbon intensity to the detriment of carbon neutrality. To identify the complex interrelationships, DST was applied to sort the interrelationships and identify the critical dynamics in promoting the success of achieving carbon neutrality. Although all levels of the Chinese government have attempted to identify the critical dynamics in the pursuit of carbon neutrality, the diversity of data sets increases the identification complexity and uncertainty. Dynamic identification also relies on expert opinions and experience, and dynamics are difficult to account for in public interests. This could lead local governments in the wrong direction in their quest to realize a low-carbon province/city. In addition, the current resource support status in each province has not been determined due to the lack of an appropriate approach. Without a clear understanding of the resource support status, it is impossible to invest in the right direction, which prevents the generation of driving dynamics to achieve carbon neutrality effectively and efficiently. To address these shortcomings, this study proposes a hybrid method by integrating the quantitative normalization, EFA, RT, social media transformation, FSDEMATEL, and DRA techniques to fully consider different data types and provide a visual analysis to guide resource adjustment in each province/city toward carbon neutrality. This chapter separate five subsections to illustrate the significant findings from the analytical results. This study found that the relationship between DST and carbon neutrality can identified by four factors: including international trade dynamics (A1), industrial demand dynamics (A2), dynamic transportation emissions (A3), and emission intensity control dynamics (A4). This confirms that A1 has an important role to play in moving the industry toward carbon neutrality ( : ). These complex influences are shown in , where (A) and (B) show that the experts and the public may have the same thoughts regarding the pursuit of carbon neutrality. Both considered that A1 plays an important role in achieving carbon neutrality via a strong interrelationship with A4, a notable direct influence on A2, and a moderate direct influence on A3. In addition, the interrelationships with A2 may facilitate the influences of A2 and A3. Thus, international trade is the basis of the pursuit of carbon neutrality from the perspectives of the experts and the public. In practice, Chinese governments focus more on A4 by applying a top-down and bottom-up management combination style to ensure that the requirements of the central government can be satisfied ( ). Comparing government practices (C) and the combination of all types of data (D) shows that governments, experts, and the public think differently, although (A) and (B) show the experts and the public to have the same thoughts. Therefore, (C) and (D) provide solid evidence that A4 is the critical dynamic that generates a small change to affect the entire dynamic system in accordance with government practices. A4 exhibits a strong interrelationship with A3 and generates notable direct influences with A1 and A2. This suggests that Chinese governments prefer to centralize control in achieving carbon neutrality by utilizing the total amount of carbon intensity control rather than adopting international trade to reduce carbon emissions. Specific insights can be drawn from the geographical comparison based on these observed dynamics. For example, the coastal provinces have higher international trade dynamics than the inland provinces, while the southern provinces also have higher trade dynamics than the northern provinces, as shown in (A1). In (A2), HEB Province is the leading province requiring the highest carbon consumption to support its related industrial production. Moreover, HEB surrounds BJ, which could cause severe air pollution in BJ. (A3) shows that the inland provinces (including QH, HUB, HUN, GZ, and YN) rely on land transportation to deliver raw materials and goods. Accordingly, these provinces produce higher dynamic transportation emissions. Although the industrial production level and transportation emissions are low in IM and NX, these two provinces still suffer an out-of-control carbon intensity due to the presence of coal-fired power plants. In current provincial practices, shows a radar chart for each province/city. There are 13 provinces/cities that focus more on developing A1, including Beijing (BJ), Chongqing (CQ), Fujian (FJ), Guangdong (GD), Hainan (HN), Jiangsu (JS), Liaoning (LN), Shaanxi (SNX), Shandong (SD), Shanghai (SH), Sichuan (SC), Tianjin (TJ), and Zhejiang (ZJ). Notably, HEB is the only province focusing on A2 but achieves a low control capability for A4 (regarding A4, a low value suggests a high control capability; conversely, the control capability is low). Anhui (AN), Gansu (GS), Heilongjiang (HLJ), Henan (HEN), Hubei (HUB), Hunan (HUN), Jiangxi (JX), Jilin (JL), Qinghai (QH), and Yunnan (YN) rely on transportation and must therefore particularly consider A3. IM, NX, and SX exert less control over A4, almost to the point of exhibiting no control. There are several provinces that may face double dynamic problems in achieving carbon neutrality. For example, GX exhibits better dynamic development in A1, but this province must rely on A3, which could affect carbon neutrality. XJ is a critical province that must be urgently addressed by the government by lowering A3 and increasing the control of A4. Thus, SC has the potential to reach carbon neutrality due to its aggressive development of A1 with better control of A3. Although these radar charts reflect the current dynamics considered, each province/city must still diagnose the available resource support to guide the direction of achieving carbon neutrality. shows the analysis results, and the positions of the provinces/cities can be highlighted via a comparison to . Although BJ, FJ, JS, and SH exhibit similar attention levels in A1, as shown in , the positions of these provinces/cities are completely different when considering resource support. Therefore, BJ is in the urgent improvement center quadrant due to its lower overall dynamics and resource support level. SH is a dynamic startup center because it exhibits higher overall dynamics with a lower resource support level. FJ is a resource-rich center with lower dynamics but a higher resource support level. Thus, JS is one of the dynamically driven centers with strong dynamics and sufficient resource support to pursue carbon neutrality. This very large difference exposes the weakness of a single method that can create blind spots in decision making, leading to incorrect resource investment decisions. Only four provinces have a higher probability of achieving carbon neutrality, including GD, JS, SD, and LN. This occurs because these provinces have sufficient dynamics and resources to support government practices. In addition to SH, HEB, ZJ, SC, and HEN are dynamic startup centers. HEB is a unique province that surrounds BJ and attracts many manufacturers for settlement by relying on A2. The local government of HEB aims to balance A2 and A4 at the same time ( ). The DRA results provide a clear picture for the HEB government, which must increase resource support to drive the development of A1 toward a dynamically driven center. HN is recognized as a resource-rich center. However, its dynamics still rely on A1 development by aligning with Chinese President Xi's declaration plan to explore a free trade port with Chinese characteristics in 2018. This study applies the Sankey diagram method to determine the reinforcement effect of dynamic flows on dynamics in the pursuit of carbon neutrality. The left side shows that the electronics and telecommunications equipment industry is in the dynamically driven center quadrant. Moreover, the transportation industry; production and supply of electric power, steam and hot water industry; and smelting and pressing of ferrous metals industry exhibit high flows of dynamics into the four types of centers. In addition, the above four positioning centers generate contribute to the final dynamics at different strengths. For example, GD and JS, in the dynamically driven center quadrant, attain higher dynamics of processing trade and imports. Although SH and ZJ occur in the dynamic startup center quadrant, SH exhibits higher import dynamics than ZJ. In addition, HEB is the largest contributor to the total carbon intensity in the dynamic startup center quadrant, which may expose its weakness of lower control dynamics. Within the urgent improvement needed center quadrant, BJ relies on import dynamics, but CQ adopts even investments in dynamic development, including processing trade, imports, exports, liquefied petroleum gas, diesel, gasoline, total carbon intensity, gas consumption, coke, and raw coal. Although TJ and FJ are in the resource-rich center quadrant, their resource-rich conditions do not help these two centers generate the specific dynamics in pursuing carbon neutrality. The right side of clearly shows that the dynamics of the 30 provinces/cities flow to A1 by manufacturing trade, imports and exports. GD, JS, SH, and BJ are the main contributors to this dynamic, with A1 exhibiting the highest dynamics in the pursuit of carbon neutrality. A3 achieves the second highest dynamics, in which the dynamics of the provinces/cities encompass liquefied petroleum gas, diesel, and gasoline. Except for GD, which has slightly stronger dynamics, the remaining provinces/cities reach almost equal dynamics that flow into these three elements. A3 is ranked third in terms of dynamics, which shows that the introduced policies and regulations allow a certain impact control in the management of the total carbon intensity. This also provides solid evidence to explain why governments' practices focus on A4 instead of A1 ( (C)). Within these dynamics, JS, SD, HEB, and IM are the main contributors. This finding also reflects a phenomenon whereby the provinces/cities in the resource-rich center quadrant may have effective control ability to reduce the overall carbon intensity. Although most of the provinces/cities exhibit dynamics that flow into A2 via coke and raw coal, fortunately, these accumulated dynamic flows remain low because governments exert systematic control over these two elements. Thus, A2 is ranked last in terms of dynamics in the pursuit of carbon neutrality. In summary, the experts and the public considered A1 to be the driving force influencing carbon emissions to achieve carbon neutrality. However, the analysis results provided evidence that the government's considerations and practices facilitate achieving carbon neutrality by greatly focusing on A4 development. In addition, the analysis results indicated that to promote the accuracy of decision making, which cannot rely only on expert opinions, it is necessary to combine all kinds of data by using the proposed hybrid method to support decision making. GD, JS, SD, and LN are dynamically driven centers that can achieve a higher probability of realizing carbon neutrality because the electronics and telecommunications equipment, electric equipment and machinery, and raw chemical materials and chemical products industries provide high dynamics in these four provinces to pursue carbon neutrality. Although current practices can generate higher dynamics for A1, achieving carbon neutrality still relies on A4 to control the accumulated carbon intensity and reduce the consumption of liquefied petroleum gas, gasoline, and diesel to constrain A3. The obtained results provide a solid basis for bridging theory and practice and offer possible guidelines for different provinces/cities to achieve carbon neutrality. Several research limitations remain. The initially collected 31 criteria may not comprehensively cover the overall DST principle toward carbon neutrality. Therefore, as many potential criteria as possible must be selected in future research to conduct a comprehensive analysis. In addition, this study focused on only the Chinese case, which may provide insufficient external generalization. To overcome this weakness, the proposed analytical procedures should be adopted in future studies to identify the conditions in other countries or perform a cross-country comparison to increase the generalization level. Although we considered different types of data and integrated them to eliminate subjective bias, other methods should be added in future studies to maintain the originality of the secondary data without excessive transformation. This study also heavily relied on the time and effort of experts to make assessments. However, the level of consistency may be reduced by the loss of human concentration over time. To prevent the recurrence of this problem, a survey instrument should be developed to shorten the response time while maintaining consistency. In addition, the variation in the work experience of the experts may have a large impact on the analytical results. Future studies may need to standardize these work experiences and discuss whether they affect decision-making regarding carbon neutrality. This study makes theoretical, methodological, and managerial contributions. The theoretical contribution lies in identifying the four aspects of international trade dynamics, industrial demand dynamics, dynamic transportation emissions, and emission intensity control dynamics to strengthen the DST theoretical basis by confirming its validity and reliability as well as providing a greater understanding of the complex interrelationships. The methodological contribution lies in the proposed hybrid method that allows addressing different data types. Then, based on the DRA results, the position of the provinces/cities can be determined to provide a precise direction for implementing improvements. In addition, the generated diagrams facilitate a straightforward visual analysis to promote the understanding and assist the provinces/cities in adjusting their resource allocation toward carbon neutrality. DST emphasizes achieving a systematic tradeoff between the obtained dynamics. Carbon intensity control dynamics influence the other dynamics to promote the successful achievement of carbon neutrality, regardless of the opinions of experts, the public, and governments. The experts and the public consider international trade dynamics to constitute the priority in the pursuit of carbon neutrality. The results showed that the coastal provinces/cities depend more on the development of international trade dynamics than the inland provinces/cities. This locational advantage facilitates lower transportation emissions in the coastal provinces/cities. Moreover, HEB is the only province with a high level of industrial demand dynamics. The result also revealed a highly notable phenomenon between IM and NX in terms of carbon intensity control dynamics. These two provinces attained low levels of international trade dynamics, industrial demand dynamics, and transportation emissions, but they exhibited high levels of carbon intensity dynamics because the coal-fired power plants in these two provinces cause lower control dynamics. There were four provinces with a higher probability of achieving carbon neutrality: GD, JS, SD, and LN. However, the electronics and telecommunications equipment industry could support these four provinces. In current practices, governments must allocate more resources to develop carbon intensity control dynamics instead of heavily relying on international trade dynamics. Writing – original draft, Writing – review & editing. Writing – original draft, Writing – review & editing. Writing – original draft, Writing – review & editing. Writing – original draft, Writing – review & editing. Writing – original draft, Writing – review & editing.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Used\n",
      "* **AIモデルの判断:** Not Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 11 / DOI: 10.1016/j.solener.2025.113509  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Modeling and performance analysis dataset of a CIGS solar cell with ZnS buffer layer\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Predictive modeling and optimization of CIGS thin film solar cells: A machine learning approach\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "In the pursuit of sustainable energy solutions, thin-film solar cells have emerged as a promising technology, offering flexibility and cost-effectiveness compared to their traditional counterparts due to almost zero emission of carbon derivatives at the user end . The recent emergence of thin film solar cells has strongly impacted the global market due to their benefits over traditional solar cells . Along with energy harvesting, thin film solar cells offer novel functionalities like flexibility and portability for wearable electronics . Providing functional and efficient materials for the global market has attracted the attention of new research generation . According to a recent analysis of the efficiency of all types of solar cells, thin film solar cells are one of the exponentially grown branches in the world . Due to the advantages of thin film solar cells, researchers are focused on improving the solar performance parameters. Various materials such as Cadmium Telluride (CdTe) , Copper Indium Gallium Selenide (CIGS) , Copper Zinc Tin Sulfide (CZTS) , Silicon Germanium (SiGe) , CsPbI Br , Copper Zinc Tin Selenide (CZTSe) , Copper Indium Sulfide (CIS) , Copper Nickel Tin Sulfide (CNSS) are being studied for the application of TFSCs. Among these, Copper-Indium-Gallium-Selenide (CIGS) thin-film solar cells have garnered significant attention due to their highly efficient potential and compatibility with flexible substrates. In , the research articles published per year are graphically represented. This representation shows that CIGS-based solar cell research gained lots of attraction in recent years. CIGS thin film consists of semiconductor layers composed of copper, indium, gallium, and selenium deposited onto a substrate. The unique and quaternary composition of CIGS offers tunable bandgaps and high absorption coefficients, making it an ideal set of materials for photovoltaic applications . However, achieving the desired efficiency requires precise control over various material parameters such as composition, thickness, grain size, and defect density. Nevertheless, the materials involved are toxic and require great handling at the time of deposition. Moreover, the abundance of indium in the earth is very low and scarce. Thus, even though the efficiency of the CIGS films is very high, the synthesis of an optimal film and the optimization itself is challenging . However, achieving optimal performance in CIGS thin-film solar cells necessitates a thorough understanding of various material properties and fabrication parameters. At present, the power conversion efficiency (PCE) of both Cu( )Se (CIGS) and halide perovskite thin-film solar cells have achieved >23 %, which is comparable to the crystalline silicon solar cells (maximum PCE of 26.7 %) and perovskite solar cells . The performance enhancement in the CIGS solar cells can be achieved by optimizing the proper synthesis and experimental parameters. A good counter to this would be getting the results without actually carrying out the deposition and advancements in Machine Learning (ML) can make it possible without consuming much time. Using ML in material synthesis can adequately save time and the resources involved at the deposition level. As a result, fewer films need to be produced to obtain an optimized film. Traditional approaches to optimizing CIGS thin-film solar cells have often relied on empirical experimentation and numerical simulations, which can be time-consuming and computationally intensive. Also, conducting numerous experiments is very harsh to the pocket of the researchers and research institutes. In contrast, ML offers a data-driven methodology that leverages algorithms to extract intricate patterns from large datasets efficiently. By harnessing the power of ML, it can expedite the optimization process and uncover subtle relationships between input parameters and device performance (responses as called in ML) . ML algorithms have found widespread applications in real-life problems and material science. Materials science spans different scales, from atomic-level interactions to bulk properties. ML models integrate data across these scales, offering insights into how atomic-scale changes influence macroscopic properties. By analyzing vast datasets, significant reduction in time for experimental time is possible. ML algorithms help to optimize material properties by exploring the relationship between synthesis parameters and the desired properties. ML models predict experimental outcomes before actual trials, minimizing errors and reducing the number of failed experiments. Also, ML models guide researchers in designing experiments by predicting which experiments are most likely to yield valuable insights. Karade et al. have used an ML approach on 1300+ CZTSSe TFSC devices to identify key fabrication factors influencing optoelectronic properties. In their study, Random Forest (RF) outperformed ANN in predicting ability. Further, key factors like precursor composition, i-ZnO thickness, sulfo-selenization temperature, and AZO deposition temperature were identified. A PCE of ≥11.0 % is reported. This ML-based approach aided in optimizing TFSCs and other device types. Maoucha et al. have utilized a numerical simulator a Solar Cell Capacitance Simulator (SCAPS-1D) along with ML and explored the important design parameters of CIGS TFSCs. The study focused mainly on defects and appropriate charge transport mechanisms in different layers which was utilised to simulate current–voltage (I–V) characteristics. Moreover, by incorporating Deep Learning (DL) with ML, crucial variables to optimise solar cell performance were investigated. In another study, presented by Maoucha et al. , degradation effects in CIGS TFSCs are explored by varying device parameters using a SCAPS-ML approach. In this article, to model (I–V) characteristics, different mechanisms like volume and interfacial defects, and degradation of the transparent conductive oxide layer were considered. This approach extensively reduced the need of any physical simulation while enhanced understanding about solar cell reliability. Katubi et al. have designed 1000-plus organic semiconductors for organic solar cells using ML models. The dataset used for the ML model contained experimental and theoretical datasets which were previously collected. The author have reported that out of all the 22 ML models used, the random forest regressor and extra tree regressor showed better capability to predict the outcome. Rumman et al. have predicted optoelectronic properties of Sn-based perovskite solar cells i.e., open-circuit voltage (V ), short-circuit current (J ), fill factor (FF), and power conversion efficiency (PCE) using various ML algorithms. The R-squared (R ) score varied from 0.4341 (Ridge regressor) to 0.7061 (Random Forest) (RF) for the algorithms used. A dataset of 26,000 experimental records was used. However, the data was taken from 'The Perovskite Dataset Project'. The authors also have reported a 28.35 % increase in PCE from 12.24 % to 15.71 % for architecture optimization and a 24.6 % increase in PCE from 12.24 % to 15.25 % for deposition method optimization. Moreover, at various stages of solar cell fabrication i.e. material synthesis to device characterization, ML can aid in optimizing deposition temperature, identifying optimal compositional ratios required, and predicting device performance based on material properties. By integrating ML into the fabrication process, researchers can achieve higher efficiency while reducing the time needed and resource consumption. Previous studies have made significant strides in optimizing thin-film solar cells, but several limitations persist. The biggest being the shortage of data points considered, and the usage of simulated data points with little to no variation. Further, these include reliance on simplistic models, limited consideration of material variability, and neglection of interactions between multiple parameters. Moreover, the lack of a standardized methodology for data analysis and model validation hinders the reproducibility and comparison across studies. These are some of the most important aspects of modern research to be investigated. This work addresses the limitations of optimizing CIGS thin-film solar cells using ML algorithms by proposing a comprehensive framework, specifically RF and Decision tree (DT). The data on material properties and fabrication parameters was collected, followed by the development and training of ML models using Google Collaboratory (Python) and MATLAB . Through rigorous testing and optimization, we unveil insights into the underlying factors influencing CIGS thin-film solar cell efficiency. This approach not only enhances predictive accuracy but also provides valuable insights for guiding future research and development efforts in the field of photovoltaics. This study uses an experiment-oriented ML strategy to break saturated PCE for CIGS TFSCs. It consists of over 5000 data points which were manually extracted from the chosen research paper according to the needs and with the minimum requirement that the efficiency is ≥5 % to build efficient prediction models . The present work has the strong motivation of employing different ML algorithms and finding a suitable ML model that gives key governing factors (device fabrication steps). In addition to that, it makes use of validation of the proposed ML model by fabricating the CIGS TFSCs based on Decision tree (DT). Also, the work delves into the realm of optimizing CIGS thin-film solar cells using advanced ML algorithms, particularly focusing on ANN, RF, DT, and Classification and Regression Trees (CART). By comprehensively analyzing these parameters a robust model capable of predicting and optimizing the efficiency of CIGS thin-film solar cells is constructed. Initially, the collected data was sorted in the manner that ML algorithms namely ANN, DT, and RF can be employed efficiently. ANNs are inspired by the way a human brain works. They consist of layers of interconnected nodes aka neurons that work together to process information. The system learns by adjusting the connections between these nodes through a process called backpropagation. The process helps the model to improve over time. ANNs are an excellent tool at recognizing patterns in complex data, making them popular for tasks like forecasting trends, and identifying anomalies. DT is a powerful model which is used for both classification and regression tasks. They work by breaking down a decision-making process into a series of questions, each leading to a branch in the tree. At each step, the tree splits the data based on the most significant feature, making decisions that are easy to follow and understand. This structure makes DTs very interpretable. While DTs are easy to build and understand, they can be prone to overfitting, especially, when they become too deep or complex. On the other hand, RF is an algorithm which uses multiple models to make a decision. In this case, the forest is made up of many decision trees, each trained on a random subset of the data. After the trees have made their individual predictions, RF combines them by voting in case of classification tasks or averaging in case of regression tasks to produce a final result. RF can handle large and complex datasets, and is less prone to overfitting. It is more useful than DTs for complex problems in which the dataset is not homogenous. The algorithms put forward a unique relation between the regressors such as i-ZnO thickness, RTA temperature, and compositional ratios of the material involved and the optoelectrical properties of the solar cell. Using the trained models, a well-optimized and high efficient CIGS thin film solar cell (TFSC) can be synthesized. All the codes are generated in Google Collaboratory using Python 3 Google Compute Engine backend by using the following libraries − NumPy, Pandas, and sklearn.model_selection, sklearn.ensemble, sklearn.metrics, sklearn.neural_network, matplotlib, seaborn. NumPy, standing for numerical python, deals with array manipulation, mathematical operations, and numerical as well as statistical analysis. Pandas, being a go-to tool for exploratory data analysis, is used to read a file (usually in a.csv or.xlsx) into a data frame. When it is used along with NumPy it can clean, prepare, filter, sort, and analyze data. Scikit-learn aka sklearn is a ML library. Splitting datasets into training and testing sets, performing cross-validation, and tuning hyperparameters (e.g. limiting the depth of the decision tree) are done using sklearn.model_selection. sklearn.ensemble is used to implement ensemble learning methods such as random forest regressor and classifier. All the evaluation metrics for the confusion matrix, ANN, and RF ML algorithm such as accuracy, precision, recall, F1-score, mean squared error (MSE), RMSE, R , and Adj.R are calculated using sklearn.metrics while the ANN model is run and optimized using sklearnneural_network. Some scatter plots and bar graphs are created using Matplotlib. However, as Seaborn offered more informative and creative statistical graphics, the visualization of some of the plots i.e., scatter plots, bar plots, heatmaps, violin plots, swarm plots, and strip plots was done using Seaborn. A confusion matrix is an N × N matrix that evaluates the ML algorithm used by displaying the number of true positive (TP), true negative (TN), false positive (FP), and false negative (FN), where N is the number of the target class . The confusion matrix obtains a holistic view of the comparison of actual target values and predicted values. The diagonal elements represent the number of points for which the predicted values are equal to the actual values, whereas the off-diagonal elements are those that are mislabelled by the classifier. It means that a CM with a higher number of diagonal elements performs better. The evaluation of the confusion matrix is done by measuring some properties. One such property is the accuracy. As the name suggests, it provides the accuracy of the model by calculating the ratio of total accurate predictions to the total prediction. Despite being easy to calculate, understand, and interpret, accuracy doesn’t always give the best view of the model. The accuracy can be high even if the model performs poorly. Thus, other factors need to be evaluated along with it. The precision and the recall measure the proportion of true positive predictions among all positive predictions and the true positive predictions among all actual predictions, respectively. The precision indicates the ability of the model to avoid false positives. Thus, a high value of precision is preferred. On the other hand, the recall indicates the ability of the model to find all positive instances regardless of false positive values predicted by the model. The harmonic mean of precision and recall is called the F-1 score. It provides a balance between precision and recall, making it more useful for evaluating models because the harmonic mean gives more weight to lower values, so the F1 score is high only if both precision and recall are high. F-1 score works well on uneven class distribution or when both false positives and false negatives are important considerations . The first step of any ML-driven prediction was to collect datasets that lead to the regressor (aka criterion). The predictors that can be used to foretell outputs of TFSCs are annealing time, temperature, pressure, the thickness of the material deposited, highest occupied molecular orbital, lowest unoccupied molecular orbital, orbital levels, and band gap. However, in the present work, the efficiency of the CIGS TFSC was predicted using i-ZnO thickness, RTA temperature, and compositional ratios of Cu, In, Ga, and Se. The dataset includes i-ZnO thickness, RTA temperature, and compositional ratios of Cu, In, Ga, and Se as the input parameters and PCE, V , J , and FF as the output parameters. As mentioned earlier, the research articles with CIGS TFSCs’ efficiency ≥5 % were collected. The articles were thoroughly analyzed and required data was used to prepare a dataset. The data points were highly inhomogeneous which makes it difficult to analyze and derive any significant result from it manually. This s where the ML algorithms came into play, as these are built to handle large amounts of diverse and non-uniform data . In order to predict the optoelectronic properties of CIGS TFSCs, two of the most widely used ML algorithms were used. The first algorithm is ANN, inspired by the neurons in a human brain consisting of interconnected nodes, most popular to get desired results for complex problems of classification, regression, and clustering . The ANN model was implemented on the dataset in a 70 %-30 % split manner i.e., 70 % of the data is used to train the model, and the rest of the 30 % was used to test it (The test set analysis is given in in .) An R value is calculated using a line of best fit i.e., a linear fit. The R aka coefficient of determination interprets the goodness of fit of a model. It is a statistical measure of how well the regression line approximates the actual data. Its value ranges from 0 to 1. When R is 1, all the predicted values are accounted for by the actual values. If R is 0, none of the predicted values are accounted for by the actual values. As mentioned earlier, usually a value of ≥ 0.75 is considered useful however there can be certain applications for which even an R value of 1 can be inadequate . is the sum of squared residuals i.e. the sum of squared differences between the observed values and the predicted values by the model. is the total sum of squares i.e. the sum of squared differences between the observed values and the mean of the dependent variable. ANN resulted in R < 0.68 for all the output parameters i.e., PCE, V , FF, and J The scattered plots of the ANN model using training set are provided in The scattered plots obtained using testing set is provided in . An R value ≥ 0.75 is considered a success, otherwise a failure. The mean absolute errors of ANN models are 2.12, 33.57, 6.25, and 3.53 for PCE, V , FF, and J respectively for the training sets. The low R values suggested that the data is so multifarious and disparate, thus the prediction of the optoelectronic properties of CIGS TFSCs could not be done efficiently using the ANN ML algorithm. The residual plot using training set (shown in ) is a scatter plot of the residuals (the difference between the actual and predicted values) on the y-axis and the actual values on the x-axis. The evaluation of all the models is done based on R values. The residual plot using testing set is provided in . It is used to assess the quality of continuous target variables such as regression models. The residual plot shows that the model is doing a good job of predicting the output values only for a few of the data points. However, there were so many data points where the model is over-predicting or/and under-predicting the output values. Overall, the residual plot suggested that the model was not a good fit for the dataset. The ineffectiveness of the ANN model can also be verified using the confusion matrices obtained using training and testing set is shown in and , respectively. There are a lot of off-diagonal observations affecting the overall performance of the matrices. The accuracy of these confusion matrices came out to be 0.64 %, 0.56 %, 0.65 %, and 0.59 % using training set. As a result, the ANN ML algorithm failed to predict the optoelectronic properties of CIGS TFSCs. Random forests (RF), on the other hand, easily outperformed the ANN model. It turned out to be more accurate. The superior performance of RF over ANN in predicting CIGS thin-film solar cell efficiency can be attributed to the heterogeneity of the dataset and the algorithmic characteristics of the models. RF’s ensemble structure that aggregates predictions from multiple DTs demonstrated inherent robustness to outliers, and mixed data types with varying range. Even though ANNs are theoretically capable of modeling complex non-linear relationships ended up exhibiting sensitivity to data heterogeneity. In contrast, RF inherently mitigates overfitting due to bootstrap aggregation and randomization in features which ensured better generalization. Also, it was the black-box nature of ANN that limited physicochemical interpretability. These results suggest that for heterogeneous, moderate datasets, which are common in solar cell research and balance of accuracy, robustness, and interpretability of RF algorithm made it preferable to ANN unless exceptionally curated datasets were available. Similar findings are reported in the study carried out by Zhu et al. . In this work, ML algorithms were implied to explore key factors affecting the performance of CIGS solar cells. Of the various ML algorithms used including linear regression, neural networks, RF, and extreme gradient boosting, RF achieved the best accuracy – Root Mean Squared Error (RMSE) of 0.9 % and 1.8 % along with a Pearson r coefficient of 0.9 and 0.88 for validation and test sets. RF uses multiple decision trees and considers the average of all the DTs making it one of the best predictor models in ML algorithms. Moreover, each DT consists of different data which are subsets of a universal set (complete dataset in this case). It can be well understood using the example: If there is a dataset with f features and r rows, the various DTs in the RF include DTs with rows (r’, r’’ and so on) and features ((f’, f’’ and so on, where r’, r’’ < r and f’, f’’<f) . It also reduces the overfitting, and train-test time and increases the accuracy of the predictions. RF, being a supervised ML technique, required labelled targets . The adjusted R (Adj.R ) values for PCE, V , FF, and J are 0.98, 0.91, 0.97, and 0.96, respectively, using training set. Adjusted R , a modified form of R , removed unnecessary predictors from the regression model if those exist. By taking the number of predictors into account, it modified R and offers a more precise evaluation. where, is the regular R-squared value, is the number of observations (sample size), is the number of independent variables (predictors) in the model. The scatter plots along with the line of best fit, the Adj.R values, and the density plot for predicted and actual values for training and testing set are shown in , respectively. A satisfying Adj. R value of >0.87 was observed for all the optoelectronic properties of CIGS TFSCs. The density plot gave a proper look at the distribution of the model. The peaks in these plots indicate frequently predicted values the RF model makes . For all four output parameters, greater peaks appeared for high and very high values (>15 for PCE, > 650 for V , > 65 for FF, and >30 for J ). Also, the narrow distribution that occurred suggested that most of the predictions were clustered around a central value. The predictability of the model can be verified by comparing the density plots of the predicted values with the actual values. The predictive plots (opposite to the Y-axis) resembled the actual plots (opposite to the X-axis). These results suggested that the RF model can be used efficiently to predict the compositional ratios for efficiencies of the CIGS TFSCs. To further ensure the adequacy of the model, confusion matrices for the optoelectronic properties of CIGS TFSCs using the RF model were built . These confusion matrices for training set are shown in while the confusion matrices primes using testing set are shown in . The dataset was categorized into four classes: low, medium, high, and very high for all the regressands. shows the categorical ranges of PCE, V , J , and FF. The accuracy of all the confusion matrices is greater than 0.94 for training data and 0.88 for the testing data. Unlike the confusion matrices prepared for the ANN model, it was seen that most points were placed diagonally in the RF model. However, accuracy was not always a reliable metric for confusion matrices thus, other performances such as precision, recall, and F-1 score using training set are provided in Along with this, the similar predictive matrices were prepared for testing set whose metrics are provided in . The precision is a measure of the accuracy of positive predictions made by a classification model. Recall, aka sensitivity or true positive rate, is a measure of true positive from all actual positive instances in the dataset. The confusion matrices clearly showed that there exist imbalances between the classes. Thus, F-1 scores were calculated to obtain a more balanced evaluation of the performance of the model . It was seen that the RF model is more reliable than the ANN model as it provided relatively higher Adj.R values. In RF most of the data points were near or/and on the line and in the confusion matrix the data points were diagonally placed, making RF a better choice for the prediction of output parameters. Thus, the feature importance plot was built using the RF model. The RF model uses feature importance to quantify each feature's contribution to the model prediction performance. It's a useful tool for figuring out which factors contribute the most when predicting outcomes . Notably, RF provided interpretable feature importance rankings, enabling actionable insights for experimental optimization. The feature significance plot is shown in , which assists in achieving the objective by lowering the number of criteria that must be taken into account, therefore, saving a significant amount of time when making choices as well as reducing the cost consumption involved. It calculated a score for all the input parameters fed to the model. The scores here represented the importance of each feature in predicting the outcome i.e. optoelectronic properties of CIGS TFSCs. A higher score means that the specific feature will have a larger effect on the model that is being used to predict a certain variable . The features that scored higher than the average score (= 0.1667) showed more influence on the output than the remaining features. By looking at the plot, it was clear that the compositional ratios of precursor are more useful in predicting the efficiency of the solar cell than the rest of the features. Even though, the score for i-ZnO thickness was lower than the average score, it affected the model more than the RTA temperature. The i-ZnO thickness was followed by the Se/(Cu + In + Ga) ratio. Selenium in CIGS TFSC helps in absorbing broad-spectrum sun radiation, inevitably enhancing cell efficiency. The remaining compositional ratios viz. Cu/Ga, Cu/(In + Ga), and Ga/(In + Ga) were crucial for adjusting the band gap of the solar cell and to increase conductivity. The heterogeneous nature of these predictors and regressors can also be verified from the plots in . The study of compositional ratio to predict the bandgap of TFSCs can be a whole different topic and may be discussed in a separate paper. Using the results observed/derived from the feature importance, a new DT using only the compositional ratios was prepared. It is shown in . The analysis of the DT: first point denoted the feature and its value. The root node got splitted and gave the initial rule that if the ratio Ga/(In + Ga) is ≤0.261, the average efficiency of 13.95 % can be obtained. As there was a split, all the samples with Ga/(In + Ga) ratio ≤ 0.261 follow the left path. The samples that did not follow the rule, follow the right path. The next rule (towards the left) is Cu/(In + Ga) ≤ 0.52 with an average PCE = 10.872. Further, splitting towards the left results in lower PCEs thus, those are of no use. Hence, the splitting towards the left side can be ignored. However, avoiding these ratios ineluctably helped in increasing efficiency. The right split from the root node gave another split of the precursor ratio Ga/(In + Ga) ≤0.505 and the average PCE of 17.34 can be obtained. It was further splitted into (left-side) Cu/(In + Ga) ≤0.775 and (right-side) Cu/(In + Ga) ≤0.985 providing PCE 15.705 and 19.845 respectively. It suggested that a high amount of Cu in the precursor yields better efficiency. A high amount means a higher concentration of Cu w.r.t In and Ga combined. The DT can be read further in a similar way. The DT consisting of RTA Temperature, i-ZnO Thickness along with the compositional ratios of the precursors gave a more detailed condition for the fabrication of the film. This DT is provided in . When the RTA temperature was ≤485, an efficiency of 13.86 % was obtained. RTA temperature of about 425 °C gave an efficiency of around 12.98%. Further, a decrease in RTA temperature by 100 °C, decreased the PCE by 4.02%. The splits towards the right side provided the synthesis conditions for higher efficiencies. When the RTA temperature ≤485 °C and the i-ZnO thickness ≤55, PCE ≈ 19.63 can be obtained. The best conditions were obtained as RTA temperature of around 550 °C (≈ 500 °C) with an i-ZnO thickness of 46.5 nm, and a Cu/Ga ratio ≈ 1.3. It was also seen that when the RTA temperature is lower than 325 °C, poor efficiencies are obtained. This could be due to the incomplete crystallization process which resulted in crystal defects. Due to the formation of improper crystals, the number of charge carriers available to generate electric current reduced. Also, lower temperature during rapid thermal annealing led to poor interface between adjacent layers. The DT also specified that a film with an i-ZnO thickness of around 42 nm underperformed. This was probably due to the less confinement of light in the active layer and the increase in recombination due to the thin buffer layer . The hypothesis that a higher selenium composition enhances efficiency (as seen in 1 DT) was consistent with experimental observations. The ‘samples’ in the decision node denoted the percentage of samples following the particular decision . It was to be noted that as the DT developed towards the leaf nodes, the number of samples went on decreasing and as a result even though the efficiency of the TFSC was on the higher side, the probability of obtaining such high efficiency was lower. The squared error in the DT provided the quality of a split and told the amount of variation that may occur in the output. The splits were chosen in a way that led to a smaller squared error for the next split. The process was applied recursively and the DT was constructed with each split minimising the squared error at each node. A shallow decision tree was easier to understand as it contained fewer splits . On the contrary, a decision tree with greater depth contained, many splits which can be difficult to interpret and utilize experimentally. Thus, post-pruning of the DT was done and the number of splits and the depth of the tree was lowered. It inevitably lowered the number of variations that need to be considered while implementing the results in the actual experiment. The shows the comparision of present study with literature survey. A more detailed analysis of the interdependencies among input parameters revealed non-trivial correlations. For instance, a strong negative correlation between Cu/(In + Ga) ratio and FF suggested that higher Cu content beyond the optimal range might introduce shunting paths, reducing fill factor. Similarly, Se/(Cu + In + Ga) displayed a moderate positive correlation with J , indicating selenium's role in charge carrier collection efficiency. Further, a strong positive correlation between Ga/(In + Ga) and V , reinforcing the hypothesis that increased Ga content effectively widened the bandgap, enhancing open-circuit voltage. However, this increase also exhibited an inverse relationship with J , likely due to reduced photon absorption in higher bandgap materials. Conversely, i-ZnO thickness showed a more complex, non-linear relationship with both V and FF. The analysis suggested that maintaining an intermediate thickness—around 50 nm—yielded the best performance. A thinner layer might not provide adequate interface passivation, leading to charge recombination, while a thicker layer could introduce resistive losses that hinder overall efficiency. This highlighted the delicate balance needed in optimizing buffer layer properties to ensure both effective charge transport and minimal resistance. Using i-ZnO thickness, RTA temperature, and the compositional ratios of materials involved as input parameters for the RF and ANN models, optoelectronic properties of CIGS TFSCs were forecasted in the present work. The R value >0.87 was obtained for all the output parameters using RF while the R value for ANN ranged from 0.34 to 0.68. The higher R for RF indicated a good correlation between the input and output parameters. Furthermore, a confusion matrix allowed a comprehensive analysis of the classes defined for the four output parameters viz. PCE, V , FF, and J . The R values and the performances of the confusion matrix suggested that the RF model fitted finer than the ANN and had more predictability. A feature importance score using the RF algorithm was obtained to limit the number of parameters to be included in the fabrication of films. The complete DT allowed more insights due to the inclusion of device fabrication conditions. As a result, a PCE of 22.86 and 21 % was obtained using the complete DT and the DT whose parameters were decided based on the feature importance score respectively. Along with obtained results, some limitation can influence the performance parameters. The experimental data collected for CIGS TFSCs were collected from different laboratories, multiple fabrication processes, and distinct measurement setups which can lead to inconsistency in the practical implication of this work. The hindrance in the optical and electrical signals due to the encapsulation, spectral shifts, and environmental fluctuations are nearly impossible to be involved in the datasets, which limits the real-world applicability of the model. Also, the lab-based conditions usually lead to PCE predictions that may not translate to outdoor conditions. Yet, the biggest limitation is the black-box nature of the current-gen algorithms. Without proper interpretability, the model focuses on correlations rather than fundamental physics. A real-world performance prediction requires various parameters under a similar condition and usage of the Internet of Things (IoT) can provide long-term operational data and data for the degradation mechanism, thermal cycling, UV degradation, and seasonal variations in PCE. Another important point that needs to be noted is that there is no inclusion of morphological and other important factors that can affect the efficiency of solar cells such as surrounding temperature, intensity of the sun or solar simulator, and testing conditions. However, the incorporation of these factors can potentially increase the robustness of the model. Including morphological parameters (grain size, crystallinity, roughness) extracted via feature extraction techniques, allows the model to learn the relationship between microstructural properties and PCE, improving its ability to generalize across different fabrication conditions. However, to utilize morphological patterns in the model, DL algorithms, such as Convolutional Neural Networks (CNNs) or preferably Fully Convolutional Networks (FCNs) should be utilized instead of traditional ML approaches. But, the challenge lies in gathering enough labelled data which can result in overfitting. Even though these algorithms demand higher computational power, promising insights in the field of TFSCs can be gained. Writing – original draft, Software, Methodology, Investigation, Data curation, Conceptualization. Writing – original draft, Validation, Software, Methodology, Investigation, Data curation, Conceptualization. Formal analysis. Supervision, Project administration. Supervision, Formal analysis. Supervision, Resources, Project administration. No data was used for the research described in the article.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Used\n",
      "* **AIモデルの判断:** Not Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 19 / DOI: 10.1016/j.ces.2024.121101  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Dataset of wet desulphurization scrubbing in a column packed with Mellapak 250.X\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Experimental and modelling study of ammonia-based FGD scrubbers\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "The growing energy demand and the need for more sustainable processes make Flue-Gas Desulfurization (FGD) processes still a hot topic for chemical and environmental engineering. Nowadays, most of the power generation plants, industrial activities and the marine transportation sector still use fossil fuels for power generation ( ) and, even if SO emissions can be limited by using low-sulfur fossil-fuels, for several applications after-treatment processes are the only suitable options to comply with the increasingly stringent regulations ( ). These technologies include dry, semi-dry or wet scrubbing columns ( ), the latter option being the most effective and well-established. Sulfur dioxide is a very water-soluble gas, but it can be more efficiently removed from flue-gases using reactive-absorption processes in wet scrubbers. Meanwhile, European Community has set a new Directive (2016/2284/UE), which amends the 2003/35/UE Directive and repeals the 2001/81/UE. The Directive forces a drastic cut in global SO emissions with an average reduction for all Member States up to 70 %, to be achieved within 2030 ( ). For coal-fired power plants with a power generation capacity higher than 300 MW , the SO emission limits are between 70 and 130 mg/m and similar measures have been also established in other countries worldwide, e.g. for USA and China the limits for new plants installation are 135 and 35 mg/m , respectively ( ). To date, several traditional and non-traditional chemicals have been used ( ) to improve SO removal from flue-gas, including ammonia-based scrubbing, which is widely adopted for FGD in coal-fired power plants operating with high-sulfur content ( ). Further interest in this process is given by the formation of (NH ) SO that, when further oxidized in the aqueous-phase and purified operating with downstream chemical units, leads to the formation of ammonium sulfate (NH ) SO , which is an important precursor in nitrogen-based fertilizer manufacturing processes ( ). Despite the maturity of wet-FGD technologies and the progress achieved to date, an extensive study focused on the effects of the key parameters on the efficiency of traditional and non-traditional FGD processes must be carried out, so to improve the performances and simultaneously optimize the consumption of chemicals. In turn, this also allows limiting the water flow required and consequently reducing the footprint of plants. However, one of the most critical issues in the ammonia-FGD process is the ammonia slip ( ) and consequent formation of ammonium-based salts aerosol in the gas leaving the column ( ). The authors reported that from element composition analysis of fine particles in gas-phase after ammonia-based desulfurization scrubbing, sulfur, nitrogen and oxygen were found, suggesting predominant formation of ammonium sulfate particles because of the overall heterogeneous reactions between desorbed ammonia from the liquid and sulfur dioxide in gas-phase. This is mainly due to improper overdosing of the chemical: in fact, some fundamental aspects about the intrinsic kinetic of the process including kinetic data of the gas–liquid reactions would deserve a dedicated study for process design and optimization on large-scale plants, also considering that most scientific papers cited above refer to simulations data or experimental data without modelling interpretation. To this end, we investigated the wet desulfurization of a model flue-gas using aqueous solutions containing NH , operating according to the following reaction: ). This work aims to study the mass-transfer kinetics by determining the Enhancement Factors (E ) of the SO -NH absorption system, isolating the mass transfer rate given by the chemical reaction from the physical mass transfer rates given by the gas (k a ) and liquid-side (k a ) coefficients. To this aim, absorption experiments were performed in a lab-scale falling-film absorber to investigate SO mass-transfer rates in ammonia aqueous solutions, as it allows highly controlled mass-transfer conditions for a thorough assessment of the fundamental kinetic aspects of an absorption process with fast gas–liquid reaction ( ). This unit grants a precise knowledge of the gas–liquid specific contact surface (a [m /m ]); hence, it allows to accurately assess the mass-transfer rates. The SO mass transfer coefficients (k and k ) were known from a previous published work using the same experimental set-up ( ). To support the kinetic study, thermodynamic data of the reactive absorption were also retrieved by means of dedicated experiments in a fed-batch bubble absorber, aimed to estimate the solubility dataset of SO -NH system. In addition, these data were validated with a chemical process simulator ASPEN PLUS® (vers. 8.6) using a Thermodynamic Flash block. A predictive model to estimate the Enhancement factors for ammonia-based FGD process was also developed using the absorption kinetic equation valid for very fast gas–liquid reactions, also called as Danckwerts correlation ( ). This kinetic expression allows relating the E calculated from experiments in the falling-film absorber to the Hatta number (Ha), according to a kinetic expression based on a pseudo-m , n -order non-reversible reaction ( ) estimating the pre-exponential factor, the activation energy and two reaction orders. This correlation takes into account the effect of pressure and temperature of the process and the concentrations of both the absorbed gas and the reactant on the gas–liquid reaction kinetics. These kinetic parameters are independent on the reactor configuration and makes this kinetic model suitable for calculations in any other mass transfer contactors, e.g. different types of packed and tray towers, as long as the gas-side and liquid-side mass-transfer coefficients are known. The correlation of the Enhancement Factors for this specific reactive process, coupled with predictive equilibrium and mass transfer coefficient models, allows for a more accurate FGD scrubber design, able to correctly estimate the chemicals consumption and the volume of the liquid required to reach a specific target of desulfurization and consequently preventing or limiting the formation of by-products in the treated gas stream. To this end, experimental data on performance of flue-gas desulfurization with aqueous ammonia solutions performed in a lab-scale packed column as a function of key process parameters (temperature, gas and liquid flow rates, SO gas and ammonia concentrations and solution pH) were collected from the literature and compared with predictions of the ammonia-based FGD model developed in the present work to test its reliability. The overall resistance to mass transfer for SO gas absorption process can be written according to two-film theory ( ), as the sum of the gas-side (first term) and liquid-side (second term) mass transfer resistance: [kmol/m s] is the overall resistance to the SO mass transfer coefficient in gas-side, k and k [m/s] are the gas and liquid-side SO mass-transfer coefficients, ρ and ρ [kmol/m ] are the molar density of the gas and liquid, E is the Enhancement factor for the liquid phase representing the contribution to the enhanced liquid-side mass transfer given by the chemical reaction (E = 1 for a purely physical absorption, while E > 1 for a chemical absorption), and F represents the derivative of the equilibrium function that coincides with the gas–liquid interfacial conditions. It corresponds to the dimensionless Henry’s constant given by the ratio between K /P, i.e. the Henry’s constant for SO physical absorption in water (K [atm]), and the total pressure of the system (P [atm]). The prediction of the Enhancement factor for gas–liquid reaction in can be obtained using the Danckwerts kinetic expression ( ); it correlates E with the Hatta number (Ha) according to the following : ∼ Ha. This expression represents a pseudo-m , n -order non-reversible reaction type equation to account for both the concentrations of sulfur dioxide in the gas-phase and ammonia in the liquid-phase. In this equation, k [(m /kmol) s ] is the pre-exponential factor of the reaction, E [kcal/kmol] is the activation energy, m and n are the reaction orders (which are the model kinetic parameters), D is the diffusion coefficient of the SO into the liquid-phase, R is the universal gas constant equal to 1.987 kcal/kmol K, T [K] is the absolute temperature, C [kmol/m ] is the ammonia concentration in the liquid-bulk while C [kmol/m ] is the SO liquid concentration at the liquid-interface. This latter can be calculated according to the two-film theory referring to gas-side mass transfer ( ): [kmol/m s] is the absorption rate, given by the ratio between absorbed SO flow rate [kmol/s] and the volume of the gas–liquid contactor [m ], C [kmol/m ] is the SO concentration in the gas-bulk and H is the Henry’s constant for SO physical absorption expressed in atm·m /kmol, which can be obtained by the ratio between K and the molar liquid density (ρ ). The experimental assessment of Enhancement factor of the gas–liquid reaction in can be performed with dedicated absorption experiments using the differential mass balance equation, i.e. the classic absorber design equation for mass-transfer in gas–liquid systems ( ): is the Height of Transfer Unit representing the overall resistance to mass transfer referred to the gas-phase along the column, while NTU is the Number of Transfer Units, representing the driving force of the absorption along the column, written as a function of the SO gas mole fraction (y [mol/mol]). In , Z [m] is the height of the gas–liquid contactor, S [m ] is the cross-section area of the gas–liquid contactor, G [kmol/s] is the molar flow rate of the gas, y [mol/mol] is the inlet mole fraction of SO , y [mol/mol] is the outlet mole fraction of SO , y = C ·10 [mol/mol] is the mole fraction of SO in the gas-phase in equilibrium with total sulfur in the liquid-phase (x ), which represents the gas solubility in a given NH aqueous solution. High pressure cylinders of pure N at 5.0 technical grade and SO at 3000 ppm in N (Rivoira Gas, Italy) were used for simulated flue-gas production; NH aqueous solution at 30 % w/w and distilled water at pH = 6 (Sigma Aldrich, Italy) were used as absorbing liquid in the experiments. The experimental set-up consists of a dedicated gas–liquid contactor section, a model gas preparation and feeding section, a gas analysis section, and absorbing liquid preparation and feeding section. The experiments to evaluate the Enhancement factor dataset of the gas–liquid reaction of SO in ammonia aqueous solutions were performed using a falling-film absorber as gas–liquid contactor. This is a continuous liquid-film contactor with counter-current gas flow and consists of a Pyrex glass column (effective gas–liquid contact height, Z = 0.155 m) with 18 mm internal diameter, and 2 mm thickness. The absorber is also equipped with a Pyrex glass chamber at the top, used for feeding the liquid by overflowing it from the top to the bottom chamber, in which the spent liquid is collected. The gas–liquid interfacial area (a ) is 222 m /m , which corresponds to a specific surface area equal to geometric surface of the wetted column walls by the liquid-film during each experiment, assuming that the liquid-film thickness is negligible as compared with the column diameter ( ). shown the experimental set-up for kinetic tests. The model gas was prepared by mixing a pure nitrogen stream with a sulfur dioxide stream available at 3000 ppm in nitrogen and using two SCM digital flow meters (0–20 NL/min) to regulate the total flow rate (G [L/h]), so to obtain the desired SO inlet concentration (C [ppm ]). The experimental apparatus is also equipped with a gas heat exchanger with electrical resistance (maximum power 100 W) connected to a Proportional-Integral-Derivative (PID) controller to set gas temperature, and temperature/pressure meters (HOBO® four-channels digital thermometer and SMC digital manometers) to record gas temperature and pressure during the experiments at inlet and outlet of the absorber unit. At the outlet of the described absorber unit, the gas stream is sent to a Pyrex glass chamber (condenser) to collect the entrained liquid droplets and any other condensates before to be sent to the gas analysis system. The condenser is fed with water cooled to 2 °C in the outer jacket. As a further water guard, the gas is dried by a Portable Pollutek eGAS-200P-Dryer quenching unit prior to be fed to the analytical cell of the gas analyzer, so that concentration values are expressed on a dry-basis. The concentration of SO in the gas stream at the column outlet is checked over time using a gas analyzer Pollutek eGAS-200R UV-DOAS CEM (dual range 0–600/0–3000 ppm ; detection limit of 0.1 ppm ; and deviation 0.1 % of the full scale of the instrument). The absorbing reactive solution is prepared with the desired concentration of ammonia (C [kg/kg]) starting from a concentrated ammonia aqueous solution at 30 % w/w. The ammonia solution is stored in a vessel with a maximum capacity of 15 L and its temperature is controlled with HOBO® four-channels digital thermometer. An immersion pump with a maximum power of 30 W placed inside the tank feeds the liquid to the column with a liquid flow rate (L [L/h]) controlled by a SMC digital flow meter (0–4 L/min). For each test, the liquid temperature can be set to the desired value by a thermoregulating system with a PID controller consisting of a HAAKE DC 10 thermostatic bath (equipped with internal pump) connected to the external water recirculation jacket of the storage tank. The liquid flowing out the column is discharged into another tank with a maximum capacity of 20 L, and a sample of exhaust liquid can be taken to measure temperature and pH with a HOBO® digital pH-meter. Further details on falling film absorber units and all equipment (digital flow meters, digital thermometers, digital manometers, digital pH-meter, PID controllers, thermostatic bath and gas analyzer) are given in our former works ( ). The experiments to evaluate solubility dataset of SO in ammonia solutions were performed using the same general experimental set up with a fed-batch bubble absorber in place of the falling-film absorber, for equilibrium tests (see ). This unit is a dynamic liquid–gas contactor and consists of a Pyrex glass column (overall height 0.2 m; internal diameter 0.025 m; thickness 2 mm) where the gas is continuously fed while the liquid is injected in the column as batch phase when the test starts. The gas stream flows through a porous sintered glass gas diffuser (P2 porosity class) to obtain bubble sizes around 4 to 20 µm. This allows a significant improvement in gas–liquid contact so to reduce the time for reaching the equilibrium condition. In this case, the ammonia solution is stored in a 0.1 L glass flask, and only a small amount is used for each test. A finite volume of the absorbing solution (L [mL]) is injected into the unit, and it is not altered throughout the duration of the test (batch conditions). The injection of solution is made through the valve-syringe system, which is equipped with a luer-lock inlet system at the top of the column. The liquid is kept at the desired temperature for each test by the same thermoregulating system used above, connected to external water circulation column jacket (internal diameter 0.04 m and thickness 2 mm), to perform tests at constant temperature. At the end of the test, the temperature and pH of the exhaust liquid can be measured with a with a HOBO® digital pH-meter. Further details are given in our former works ( ). The first experimental activity consists in the kinetic absorption tests and was performed at constant temperature of T = 20 and 50 °C to determine the contribution of SO gas–liquid reaction kinetics (E ) during the chemical absorption with ammonia in distilled water at different concentrations. The ammonia concentrations used in these experiments were selected based on the investigated gas and liquid flow rates, and SO inlet concentrations, so to explore a broad range of the feed molar NH /SO ratio (r ), i.e. with NH strongly in defect or in excess, up to values close to the stoichiometric one (see ). The main process parameters in the kinetic experiments are summarized in . A kinetic experiment is considered completed when the SO outlet concentration (C ) reaches the steady-state condition. Accordingly, the fed-batch bubble absorber is used to estimate solubility dataset ( − ) at constant temperature of T = 20 and 50 °C and at the same ammonia solution concentrations ( [% w/w]) reported in (to support the kinetic modelling), investigating over a wide range of SO gas inlet concentrations ( ), from 120 to 2960 ppm . The main process parameters adopted in the equilibrium experiments are summarized in . In equilibrium experiments, the absorbing solution (batch) is able to absorb SO over time up reaching the saturation limit (corresponding to saturation time, t ), i.e. when C at t coincides with the inlet gas (C = C ). In this final condition, the amount of SO absorbed, including both the fraction physically solubilized (SO ) and its fraction converted to sulfites by chemical reaction in the liquid, corresponds to the total sulfur concentration (x ) in the liquid, which is in equilibrium with the concentration of SO in the gas in outlet (C = C = C ). In this case, it is worth noting that C can vary in the deviation range of the instrument (see above); consequently, at equilibrium condition this value can fluctuate in this narrow range. The experimental procedures adopted follow the same approach proposed by . The experimental data from falling-film unit (y = C ·10 mol/mol) were used to calculate the Enhancement factor (E ) of the reaction in according to the absorber design equation in modified in a simplified version, valid for dilute absorption at constant temperature and pressure: = G /SK a as the logarithmic mean (LM) value calculated from the top and bottom of the column ( ). In fact, in the case of chemical reaction, the liquid-side resistance may vary along the column due to the variation in the Enhancement factor, but although E can vary along the column, in case of fast chemical reaction, it is still possible to assume a constant value of HTU as the average between the column top/bottom because in most cases the liquid-side resistance can be considered as negligible ( ). The NTU can be calculated by the finite difference method using the experimental absorption results (y = C ·10 mol/mol) and the equilibrium dataset (y − x ) for SO chemical absorption in NH solutions, as suggested in several Unit Operations textbooks ( ). This dataset can be obtained with equilibrium experiments in the fed-batch bubble absorber. The mass transfer due to bare chemical reaction is concentrated in the Enhancement factor (E ), which can be calculated for each experiment from if the purely physical mass transfer coefficients in the falling-film absorber are known. reports the volumetric gas-side (k [m/s]) and liquid-side (k [m/s]) mass transfer coefficients calculated by using the same operating conditions and experimental set-up of the present work, and assuming an interfacial area a = 222 m /m . It is worth underlining that, since the experimental unit used in the present work does not allow gas and liquid sampling at intermediate column heights, it is assumed that the E calculated from represents an average value for each experiment, as for the previous assumption. The kinetic parameters of the Danckwerts model in are estimated from the best fitting of a non-linear regression of the experimental data of Enhancement factors obtained by . The experimental data obtained in the fed-batch bubble absorber over time ( (t)) were used to calculate the solubility dataset of SO in ammonia solution to support the kinetic modelling. The [mol/mol] is expressed as the ratio of total moles of SO absorbed from the gas to the total moles of the absorbing solution. To this end, a differential mass balance equation over time and up to the saturation time of the solution (t [s]) referred to the gas-phase as proposed by was used to calculate the total concentration of SO absorbed under thermodynamic equilibrium conditions: [mol/s] is the molar flow rate of SO fed at bubble column, MW [g/mol] is the molecular weight of the absorbing liquid solution and L is its mass (batch) [g]. To calculate solubility data of SO -NH system at different concentrations, temperatures and pressures not investigated in the present work, a thermodynamic equilibrium model was developed in ASPEN PLUS® simulator to predict SO solubility data (C −x ) based on an equation code that accounts for the entire absorption mechanism of SO in NH solutions with gas–liquid phase equilibria and chemical reaction equilibria in liquid-phase. The equilibrium equations and the values of the related constants, as a function of the temperature, are given in . It is worth noting that the overall absorption reaction reported in is given by the sum of , , twice and twice the inverse of . These equilibrium equations were implemented with the Elec-NRTL “Electrolyte Non-Random two-Liquid” ( ) activity coefficient model (to take into account the ions interaction in liquid-phase) in the Chemistry and Property Method sections of the software using a Flash block of ASPEN PLUS® 8.6 simulator. This block is a continuous unit working at constant pressure and temperature (i.e. an ideal equilibrium stage) fed by a gaseous stream (nitrogen and sulfur dioxide at the desired concentration) and a liquid stream (water and ammonia at the investigated concentration) and allows to solve simultaneously the mass, energy and charge balance equations. At the Flash block output, two streams (gas and liquid) at thermodynamic equilibrium are obtained. By using the sensitivity analysis tool of the process and varying e.g. the liquid flow rate fed to the Flash block, a complete equilibrium dataset can be obtained for each fixed NH concentration. The modelling results were used to validate the experimental data obtained with the fed-batch bubble absorber. The main target of the work is an experimental determination and analysis of the Enhancement Factors for the absorption of SO in ammonia aqueous solutions. To this aim, a former knowledge of the equilibrium data is mandatory for the modelling analysis with . In the next section the equilibrium tests, together with the corresponding model validation, are presented. shows the equilibrium dataset of SO in ammonia solutions, with x calculated from for each experiment performed in the fed-batch bubble absorber at 1 atm and 20 and 50 °C. Experimental data are compared with the simulation results deriving from the Thermodynamic Flash block of ASPEN PLUS®. In particular, and report the SO solubility dataset (Py −x ) at both 20 °C and 50 °C with ammonia concentration C = 0.004 % w/w and C = 0.016 % w/w, respectively. The solubility data were reported as a function of partial pressure of SO (Py [atm]). The solubility curves shown in assume the typical trend of chemical absorption given by the reactive network in . In particular, all the curves consist of a horizontal line with equilibrium gas-phase concentration of SO equal to zero that coincides with the x-axis due to the chemical reaction for the presence of ammonia and a vertical line, which starts when ammonia in the liquid is fully reacted (batch tests). This branch of curve has a slope dependent by the Henry constant of SO in water (K ), and follows the physical absorption trend for high gas concentrations of SO . It is also observed that the break point of the curve depends on ammonia loading and shifts to the right the higher its concentration in solution; however, it does not depend on the temperature at which absorption takes place. The solubility of SO is strongly affected by the increase of ammonia concentration in the absorbing solution with an x at the break point of about 8.5 times more when ammonia concentration is 0.016 % w/w with respect to 0.004 % w/w. Moreover, a more significant effect is observed at 20 °C, as compared with the experiments performed at 50 °C. also shows the solubility curves (black lines) related to the predictive thermodynamic equilibrium model in ASPEN PLUS®. The experimental data have been successfully validated by model simulations with coefficients of determination of R > 0.99, as a function of the main parameters, i.e. SO gas concentration, ammonia concentration and temperature. The average values of the experimental Enhancement factors (E ) as calculated by for each experiment are listed in as a function of main process parameters, i.e. pressure (P), temperature (T), gas flow rate (G ), liquid flow rate (L ), gas inlet concentration of SO (C ), and concentration of ammonia solution (C ). The Enhancement Factors showed a rising trend with molar NH /SO feed ratio (r ), as expected from other similar works ( ), i.e. when the liquid flow rate of ammonia solution or its concentration increases, while it decreases if the gas flow rate or its SO concentration increases. In addition, the Enhancement Factors increase with temperature at the same ammonia concentration in the solution due to the effect of increased liquid-phase reaction kinetics, which is consistent with similar data reported in the literature ( ): in particular, an increase in E up to 60 % is observed when absorption occurs at a temperature of 50 °C, as compared to 20 °C. Finally, it should be noted that most of the Enhancement Factor values are E > 3, confirming that the assumption of a very fast regime made for the reaction in was correct; consequently, the Hatta number can be expressed according to an absorption kinetic as in . Out of a dataset of 72 total points, only a few data showed a value less than 3, in correspondence of operating conditions of severe stoichiometry defect. These few experiments were not included in the non-linear regression procedure to find the kinetic parameters. The average values of the experimental Enhancement Factors (E ) as calculated by for each experiment are also shown in a parity-plot ( ) and compared with the values obtained from the Danckwerts model equation for a pseudo-m ,n -order non-reversible reaction (E ), whose parameters were determined by best-fitting of the experimental data of E (k = 0.60 (m /kmol) ·s ; E = −4171 kcal/kmol; m = −0.06; n = 1.38). For a further validation of the selected reactive regime for SO absorption in ammonia solutions, the same experimental data were also compared with the theoretical model for calculating Enhancement factor in case of instantaneous gas–liquid reaction (E ) in a parity-plot ( ). This model is a zero-parameter equation and reported in : representing the overall absorption reaction, while α is a correction factor which can be set to 0 for a general case (film model) or 0.5 for a plug flow (penetration model) ( ). The E calculated from Danckwerts model in showed a good agreement (R = 0.93) with the experimental E using the kinetic parameters obtained by best-fitting of the experimental data, recording a maximum deviation less than ±20 %. On the contrary, the results of E calculated with theoretical equation (according to film and penetration models) in fail the prediction with a severe overestimation of the data, confirming that SO absorption in ammonia solutions is not controlled by an instantaneous gas–liquid reaction. The assumption of a constant value of HTU in for each experiment and the repercussions on the accuracy of the model parameters in the kinetic expression ( ) was checked using the differential mass balance equation ( ) along the column for a rigorous gas absorption calculation ( ) using the following expression of the gas-side overall coefficient to mass transfer (K a [kmol/m s]): The SO gas concentration at different sections of the column can be calculated from , which can be solved by finite difference method by discretizing the falling-film absorber into z segments with a number of control volumes equal to z – 1. For our calculations, after some preliminary evaluations on the accuracy of the calculation, we selected = 11, which assured a difference <0.5 % when compared with the value calculated at = 10. The point values of K a can be calculated with , where C along the column was calculated according to , while the C is given by mass balance according to stoichiometry of the reaction in with respect to the SO absorbed. This rigorous approach allows predicting the SO concentration profile along the column for each experiment, but for the sake of simplicity, shows the parity plot comparison of the experimental values of SO concentration in the outlet gas (C [ppm ]) the concentration calculated (C [ppm ]) from the differential mass balance equation ( ) combined with . The modelling analysis provided a very good matching between the experimental and modelling SO outlet concentrations ( ) even when used in the rigorous design approach, with a maximum prediction error less than ±10 %. It is worth noting that this value is comparable with the prediction error made on model Enhancement factors (E ), confirming that the assumption adopted to calculate an average HTU value between top and bottom of the column for each experiment did not significantly affect the accuracy of the kinetic model. The predictive capability of the computational tool developed from the combination of the thermodynamic and kinetic models for the reactive absorption of SO in ammonia solutions, which will henceforth be called Ammonia-based FGD model, was tested on experimental data taken from the literature. Although the literature provides many works on ammonia FGD scrubbers ( ) only a limited number reports complete indications of the experimental conditions adopted and the performed dataset, so to allow to replicate the experiments. Among them, the work of reports a significant amount of data on the performance of a lab-scale packed-bed column, investigating the main operating parameters of the SO -NH reactive process, such as the liquid flow rates (L = 20–75 L/h), gas velocity (u 1–3.5 m/s), liquid-to-gas ratio (L/G = 2–4.5 L/m ), SO gas concentration (C = 380–1340 ppm ), solution pH (pH = 5–8) and temperature (T = 20–60 °C). The column was equipped with stainless steel Raschig rings packing (0.5 mm) with diameter of 0.05 m and packing height of 0.3 m, while the operating conditions of each experiment are shown in detail in , as well as the obtained performance in terms of removal efficiency (η ) and SO outlet gas concentration (C [ppm ]). shows the parity plot comparison between experimental SO outlet concentration in the gas-phase (C [ppm ]) reported in the concentration calculated (C [ppm ]) from the ammonia-based FGD model using the . For the latter, the gas-side overall coefficient to mass transfer (K a ) was calculated according to where the E along the column was estimated by Danckwerts model using the kinetic parameters previously found, and y was determined via the thermodynamic model in ASPEN PLUS by setting the proper temperature and concentration of ammonia solution. It should be noted that the volumetric mass transfer coefficients adopted by the Authors ( ) were evaluated using the Onda model ( ) that provides accurate estimates for this packing. The gas (k ) and liquid-side (k ) coefficients and interfacial area (a ) were calculated using MT Solver tool which is a free software developed for academic purposes ( ). Model results show good ability to predict the experimental data (R = 0.90) for all process parameters change, with a maximum deviation of ±25 %. Probably this wider prediction error range is due to the estimation of the mass transfer coefficients via predictive models, which although suitable for describing both mass transfer resistances are likely inaccurate for the estimation of the interfacial area and consequently for the single gas and liquid-side coefficient. This work reports an experimental and modelling study on reaction kinetics of SO absorption in aqueous solutions containing ammonia at 20 and 50 °C for a wide range of feed molar NH /SO ratios (r ), aimed at calculating the Enhancement factor (E ), which represents the contribution of the fast gas–liquid reaction. The Danckwerts kinetic equation (pseudo-m ,n -order non-reversible reaction type equation) valid for very fast gas–liquid reactions, was able to describe the experimental average Enhancement Factors (E ) with very good accuracy (R = 0.93 and error prediction of ±20 %) using the model kinetic parameters k , E , m and n determined from a best-fitting of the experimental data. The kinetic absorption model was also used in the rigorous mass-transfer equation to calculate the concentration profile along the column and SO outlet concentrations (C ), providing a ± 10 % estimation error of the experimental data. In addition, solubility dataset (y −x ) at 20 and 50 °C of chemical absorption of SO in ammonia solutions were evaluated from experiments in a fed-batch bubble column and used as supporting data for the kinetic modelling of the reactive absorption. These data are also validated using a thermodynamic model based on the equilibrium equations implemented in the ASPEN PLUS simulator finding an excellent fit with experimental data (R > 0.99). Finally, the overall model developed in the present work, including both thermodynamic and kinetic models for SO absorption in ammonia solutions, was tested on similar data collected from literature on a lab-scale column, as a function of the main process parameters. The results showed a good prediction capacity of experimental data (R = 0.90) with a maximum prediction error of ±25 %. This model could be proficiently applied in the design of ammonia-based FGD scrubbers on larger scale and for any contact unit to correctly estimate water and chemicals demand, aimed at a minimization of plant footprint and by-production in the gas-phase. In general, this approach based on extensive study on thermodynamic and kinetic aspects could make wet-desulfurization processes and gas purification less energy-intensive and costly and consequently more sustainable. Writing – original draft, Software, Methodology, Investigation, Formal analysis, Data curation, Conceptualization. Writing – review & editing, Validation, Supervision, Formal analysis. Visualization, Validation. Visualization, Validation. Visualization, Validation. Writing – review & editing, Visualization, Validation, Supervision. Writing – review & editing, Visualization, Validation, Supervision, Methodology, Funding acquisition, Conceptualization.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Not Used\n",
      "* **AIモデルの判断:** Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 25 / DOI: 10.1016/j.ref.2023.100490  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Developing reliable hourly electricity demand data through screening and imputation\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Performance and dynamics of California offshore wind alongside Western US onshore wind and solar power\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "The global emergency to swiftly transition to more sustainable ways to generate and deliver energy in our society is driving us to consider alternative and creative energy conversion and storage systems. The ocean represents a vast majority of the area on the Earth and an immense source of mostly untapped renewable energy. Studies such as the “Teal Deal” proposed in have reframed the importance of the ocean’s role in mitigating climate change and providing solutions to the climate crisis. While deployment of solar and onshore wind has grown substantially in the last decade, due to technological innovations, cost reduction and policies that support the adoption of both technologies, the role of offshore wind is burgeoning in the renewable energy sector. Offshore wind is a low-carbon, abundant and widely available energy resource technology that is rapidly maturing around the world . With costs expected to decrease significantly in the next decades , from between 35 % and 41 % by 2050 relative to 2014 , offshore wind is expected to attain a more significant presence globally as a formidable source of renewable energy production. The International Renewable Energy Agency (IRENA) predicts over 2000 GW of offshore wind will be installed by 2050 in its 1.5 C scenario . In 2020, the global offshore wind industry grew by over 5.5 gigawatts (GW) achieving a cumulative installed capacity of 35 GW globally . Although China and Europe accounted for most new installations in 2020, it is expected that Asia will emerge as the prominent continent for offshore wind : Japan and the Republic of Korea have the goal to deploy 45 GW by 2040 and 12 GW by 2030, respectively . In the United States, two operational projects, the Block Island Wind Farm and the Coastal Virginial Offshore Wind projects, accounted for a total offshore wind capacity of 35 MW in 2020 . The global offshore wind capacity potential is 126 terawatts (TW) with a potential global generation of 315 petawatt hours (PWh) per year (1 PWh = 10 GWh). Some of the advantages of offshore wind that have been consistently reported in the literature include (1) stronger and more consistent wind velocities , resulting in consistently higher capacity factors than solar and land-based wind; (2) less likely to interfere with land-use activities ; (3) minimal impact on the population and sea life ; and (4) it may potentially face less community resistance since turbines can be placed beyond the visible horizon from the shore . Additionally, offshore wind is less constrained by size and noise pollution , it is highly scalable , it shares important synergies with oil and gas sectors , and it has the potential to be coupled with high voltage direct current for efficient electric transmission for sites far from the shore (more than 50 km) . Existing offshore infrastructure, previously used for extraction, processing, and delivery of fossil fuels, could be repurposed for the delivery and transmission of offshore wind energy. Subcoastal powerplant transmission infrastructure, and depleted subterranean and subsea natural gas and oil reserves may be suitable for storage and delivery of offshore wind via hydrogen produced from wind power . Additionally, offshore wind may enable hydrogen to dramatically reduce emissions from sectors that are difficult to decarbonize such as iron, steel, and shipping . Lastly, offshore wind turbines have the potential to be co-located with other offshore resources such as ocean current energy converters , although currently, offshore wind is the only commercially mature ocean energy technology . Historically, offshore wind farms have been deployed mainly in the North Sea and the Baltic Sea which has been possible due to the convenient location of high resource potential offshore wind in shallow regions of the ocean near the shore. However, advancements in floating platform technology have opened the opportunity for installation of this resource in deep waters. Today, various countries including Germany, the United Kingdom, Denmark, Belgium, India, China, Chile, and others are seriously considering offshore wind an essential element of a viable renewable energy mix to achieve a 100 % renewable future. Europe and China have been leading the world in the development of offshore wind technology. Currently, over 70 % of the installed capacity is either in the North Sea or the Atlantic Ocean . The United Kingdom and China account for 28.9 % and 28.3 % of total installations, respectively . In 2020, China accounted for 50 % of new installations, making China the country with highest amount of new offshore wind installations. The improved technological efficiencies, reduced transmission constraints, and successful offshore wind farm sites in Europe have made offshore wind more attractive in North American and economically competitive . Offshore wind is a nascent technology in the United States, with a total gross offshore wind energy resource potential up to 10.8 TW and a gross annual energy potential of 44,378 TWh/yr . The vast majority of offshore wind energy resource potential in the United States has not been explored which represents an enormous energy resource . In the East Coast, offshore wind is a significant cornerstone of future clean energy portfolios in various states, where a total of 22 GW of new capacity will be installed by 2035 . Recently the Biden-Harris administration set a goal to install 30 MW of offshore wind by 2030 as part of their plan to achieve 100 % renewable electricity by 2035 in the United States . This includes 4.6 GW of offshore wind energy in the California Coast, where the potential for offshore wind energy is immense. The National Renewable Energy Laboratory (NREL) has identified six potential offshore wind farm sites in California based on proximity to infrastructure, wind resource quality, known existing site use, and physical site conditions . One challenge for deploying offshore wind in California is the near-shore precipitous drop of the continental shelf along the Pacific Coast which requires floating platforms; however, as noted above, this technology is rapidly advancing and has already been successfully demonstrated in the North Sea (see ). If offshore wind is deployed in California and connected to the electrical grid with renewable resources in the same electrical grid, it is important to understand the extent of its temporal variability and its coincidence with demand and complementarity with solar and onshore wind. Furthermore, offshore wind is still susceptible to the intermittency, volatility, and seasonal characteristics of renewable resources . The extent of coincidence and complementarity has important consequences for energy storage and back-up capacity as well as the operational requirements of a highly renewable electrical grid . This is especially important as more renewable generation resources are installed . While prior studies have investigated the relationship between onshore wind and solar (e.g., ), limited studies have considered offshore wind. Even fewer studies have completed statistical analyses of the coincidence and extent of correlation of offshore wind with the demand and other renewable resources. Wang et al. , reported that offshore wind possesses a significant advantage in the temporal alignment between peak power demand in the electrical grid and the diurnal and seasonal patterns of offshore wind. The daily offshore wind generation peaks in the evening hours, which coincides with the timing of the daily peak demand in the evening hours. During the winter months, offshore wind is two to four times more valuable than solar and land-based wind when considering demand-based values. The authors of the study found that between solar, land-based wind, and offshore wind, offshore wind demonstrates the best temporal alignment with demand and possesses the largest demand-based value when factoring in temporal correspondence between power and demand. One limitation of this study is that the authors only considered the spatiotemporal patterns of potential offshore wind production in the Central California coast. Additionally, since the authors only analyzed the value of the monthly averages, this eliminates the dynamic daily and weekly fluctuations in the profiles and does not allow determination of long-duration and seasonal storage requirements. While the results of Wang et al. suggest offshore wind is a promising source of renewable energy, there needs to be further analysis to determine whether the conclusions of this study can be expanded to other potential offshore wind farm sites. In , Energy and Environmental Economics (E3) used a resource planning tool, RESOLVE, to quantify the economic value of offshore wind in California by estimating the potential electric system savings if offshore wind was deployed to meet the state’s climate goals in 2030 and 2040. Although they predict that offshore wind will continue to have the highest capital costs in comparison to solar and onshore wind, it is one of the least-cost resource options that has the potential to save ratepayers approximately one to two billion US dollars. E3 estimates that between 7 and 9 GW of offshore wind will be the optimal amount to meet the state’s energy goals in the lowest cost manner. The literature has reported that offshore wind possesses various inherent advantages over land-based wind and solar energy. This warrants further investigation and consideration for deployment and development of offshore wind in California. Motivated by recent policies in California, we present a case study on offshore wind: We assess the advantages and challenges by examining the dynamics of different California offshore wind farm sites and evaluating the performance of different combinations of resources in the Western Interconnection (solar, onshore wind, and offshore wind). To achieve this goal, we use the Pearson Correlation, generation duration curves, a demand-based metric, and the Fast Fourier Transform to analyze hourly dynamics of the capacity factor over a five-year period. First, we present a statistical and quantitative analysis of the dynamics of offshore wind in relation to solar, onshore wind, and demand to investigate whether the advantages reported in the literature are consistently true for potential offshore wind sites. Then, we use the Fast Fourier Transform algorithm to investigate the periodicity and dynamics of offshore wind in relation to solar and onshore wind. Understanding the coincidence, variability, alignment, and predictability of offshore wind power production with electricity demand and onshore renewable resources has important implications for the grid system flexibility, energy storage, backup capacity, and power system reliability. Offshore winds can vary interannually, seasonally, sub hourly, and spatially which requires careful consideration of the spatiotemporal variability when estimating power production from offshore wind . For each of the six potential California offshore wind farm sites identified in , we estimated the hourly and spatially resolved offshore wind capacity factors using simulated hourly and spatially resolved wind speeds at an altitude of 100 meters from NRELs WIND Toolkit for the years 2007–2012 (see and ). The goal was to capture spatial variation in wind energy availability in each potential site. To calculate the wind power at each site, we assumed a representative turbine with a maximum capacity of 15 MW, with turbine specifications from Siemens Gamesa . To estimate the wind power curve, we used the following expression : is the power output at time (W), is the power coefficient, is the air density (kg/ m ), is the swept area of the turbine (m ), and is the wind speed (m/s) at time . A value of 0.35 was selected for the power coefficient, which is consistent with real-world power coefficient values . Consistent with prior studies , the cut-in speed was 3 m/s, and the cut-out speed was 25 m/s. We adjusted the wind data for an altitude of 149 m, which corresponds to the hub-height of a 15 MW turbine previously reported in the literature using the following relation: is the wind speed given in the data at time , is the hub height of the wind turbine (149 m), is the height at which the wind speed data are given (100 m), and is a friction coefficient (0.11 for offshore wind ). The onshore wind and solar capacity factors used in this study represent the hourly capacity factors in the Western Interconnection region, excluding Mexico and Canada, and we acquired them from a previously published study . We used the same range of data that we used for the offshore wind speeds. Additionally, the hourly electricity demand is representative of the total California electricity demand for a five-year period. The demand was acquired from a publicly available dataset record that spans from July 2, 2015 to July 1, 2020 . Ruggles et al. developed a data cleaning technique to predict erroneous data, such as outliers and missing data, in the EIA’s public electricity demand records. We used the entirety of the demand data with data from the renewable resources between July 2, 2007–July 1, 2012. , which shows the box plots of the hourly capacity factors for each resource and the normalized demand, reveals the monthly trends for the capacity factor of different resources and the normalized demand. The peak demand is 62,787 MW, which was used to normalize the demand dataset. The boxes show the quartiles of the dataset, with the bottom of the box showing the 25 percentile and the top of the box showing the 75 percentile. The black horizontal line represents the mean. The whiskers of the boxplots represent the 5 and 95 percentiles, while any points outside the whiskers are considered extreme values that lie outside of the 5 and 95 percentiles. For the remainder of the paper, the capacity factors and normalized demand data were used with a time step of one hour in each of the subsequent analyses. The solar and onshore wind hourly capacity factor values are directly taken from what has been previously reported in the literature for the Western Interconnection region in the United States , which follow the same profiles as reported elsewhere in the literature . The offshore wind hourly capacity factors in California we’ve calculated as described above. We used generation duration curves to compare the probability distribution of offshore wind with solar and onshore wind. The curves reflect the percentage of time each resource can operate at a particular output. We also used a metric for reliability, firm capacity, from to assess the reliability of power production. Firm capacity is defined as the capacity factor that can be expected at least 87.5 % of the time . Instead of using capacity factors for the generation duration curves, we assumed relative capacities for each resource to scale each resource by its potential capacity to have more practical results. We analyzed a total of four configurations based on current offshore wind targets and solar and onshore wind capacities in California (see ). The capacity of offshore wind is based on the Biden-Harris administration’s goal to deploy 4.6 GW of offshore wind in California with a focus on Morro Bay and Humboldt . Additionally, we assumed 13 GW of solar and 6 GW of onshore wind for each configuration. In the generation duration curves (shown in ), the hours were rearranged based on decreasing capacity factor value; this was plotted as a decreasing curve . Then, the capacity factors of each resource were multiplied by the resource’s power capacity from . In the generation duration curves, the y-axis represents the potential generation (GW), and the x-axis represents the percentage of hours in which that potential generation is available . The Pearson correlation is used to assess the degree of complementarity between the capacity factors of renewable energy resources and their coincidence with the normalized electricity demand. The Pearson correlation coefficient measures the linear dependence between two variables . Prior studies have used the Pearson correlation to analyze the coincidence between different resources . This methodology was applied between each of the renewable energy resources (onshore wind, solar, and offshore wind), between renewable energy resources and the normalized demand, and between various offshore wind sites. The Pearson correlation coefficient varies between -1 and 1: resources with values close to 1 are highly correlated in time and are coincident; resources with values close to -1 are anticorrelated in time and are complementary; and resources with values near 0 suggests no positive or negative correlation. Results are shown in . An additional Pearson correlation analysis was done on the configuration with the highest firm capacity from Section 2.2. Various studies in the literature have reported that one of the attributes of offshore wind is temporal alignment with peak electrical demand. In California, the peak demand occurs from 16:00 to 22:00 Pacific Standard Time (PST) . To investigate the value of offshore wind during peak demand hours, we calculated a demand-based value metric during peak demand and analyzed the metric in the summer and winter months. This analysis was based on the demand-based value metric developed by Wang et al., . We use a variation of the metric used by Wang et al. : is the capacity factor at each hour , and is the normalized demand, which is calculated as the demand at hour divided by the highest demand in the dataset. In this way, the metric is a fraction from 0 to 1 that represents the demand-based value of the power generated during a given hour. The higher the metric, the more valuable the resource is. We calculate the value of this metric for peak demand hours in the summer and winter seasons and present the resulting statistics in . To understand the seasonal components, predictability, and time series patterns in the data, we used the Fast Fourier Transform (FFT) algorithm in the SciPy library in Python to perform a discrete Fourier transform (FT) on the hourly capacity factors of the solar, onshore wind and each of the six offshore wind sites. The model to perform the FFT on the datasets is adapted from the model in . The goals of the FFT analysis were to (1) identify the fundamental frequencies of offshore wind and to understand how it compares with solar, and onshore wind and (2) to understand the level of predictability of offshore wind energy. Previously, the FFT has been used to detect the seasonal periods of discrete time series , however one challenge is that the FFT method is only suitable for stationary data, that is, time-series data with static properties . Time series data are nonlinear and non-stationary, which may require decomposition of the original data . While we cannot completely eliminate the non-stationary nature of our time series data, we divided the data into groups of meteorological seasons so that we can more appropriately assume the data are stationary. After the time-series data were transformed to the frequency domain, we used the frequencies and amplitudes to replicate the data using a series of Fourier terms and periodic functions (sines and cosines). The resulting terms were then used as inputs in a linear regression model from the Scikit-learn Python package . To assess the predictability of the data, we calculated the coefficient of determination, R , value of the original data with the modelled data. This is shown in . In this section, we present the generation duration curves in for each of the configurations listed in . The firm capacity is used as a metric to assess the reliability of different configurations of the resources and is defined as the amount of potential power generation 87.5 % of the time. Thus, less power than the firm capacity is expected to be generated for 12.5 % of the time. A summary of the firm capacities for each configuration is shown in . Since the capacities of solar and onshore wind do not change for any of the scenarios, the firm capacities of solar and onshore wind are constant values: 0 GW and 0.901 GW, respectively. Furthermore, potential generation of at least 0.426 GW and 2.15 GW can be expected 50 % of the time for solar and onshore wind, respectively. The generation duration curve for configuration one (13 GW solar, 6 GW onshore wind, and 0.767 GW of offshore wind deployed at each site), shows that the firm capacity of offshore wind alone is 1.51 GW. Therefore, the firm capacity of each resource corresponds to 0 % of the installed solar capacity, 15 % of the installed onshore wind capacity, and 17 % of the offshore wind capacity. Additionally, at least 2.71 GW of potential generation from offshore wind can be expected 50 % of the time, which is 59 % of the installed capacity. also shows that, while the firm capacity for these resources is moderate, when the resources are paired the firm capacity increases: 1.94 GW for the solar and offshore wind pairing, 1.92 GW for the solar and onshore wind, and 3.21 GW for the onshore wind and offshore wind. The most reliable case is when offshore wind, solar, and onshore wind are deployed at the same time, which results in a firm capacity of 4.32 GW, or 18 % of the total renewable capacity. If only solar and onshore wind are deployed, at least 4.08 GW of potential generation can be expected 50 % of the time. In contrast, if solar, onshore wind and offshore wind are all deployed under configuration one, 7.08 GW can be expected 50 % of the time. In configuration two (13 GW solar, 6 GW onshore wind, 2.6 GW in Morro Bay, 2.0 GW in Humboldt) the firm capacity of offshore wind is 1.51 GW. This is 33 % of the total offshore wind capacity installed. The most reliable combination of resources is when onshore wind, solar and offshore wind are combined, which results in a firm capacity of 4.36 GW. This represents 18.4 % of the installed capacity. The remaining results for configurations three and four show similar findings to configuration one and two. The combinations of resources with solar offer the largest potential capacity until approximately 40 % of the time. This is due to the large installed capacity of solar (13 GW) and its diurnal generation pattern. The combination of all resources provides the largest firm capacity, and the combination of offshore and onshore wind provides the second largest firm capacity. This emphasizes that interconnecting resources is advantageous for not only increasing the maximum potential capacity but also for improving the reliability of highly renewable energy systems. The results of a prior studies about the advantages of interconnecting multiple onshore wind farm sites are reflected in the results of this study with offshore wind farms: Interconnecting multiple wind farms results in an increase in the firm capacity. Deliberately interconnecting multiple offshore wind farm sites results in higher firm capacities such as in configuration one. Of the four configurations studied, the one with the highest firm capacity was configuration one with 0.767 GW of offshore wind deployed across six sites and connected to 13 GW solar and 6 GW onshore wind. Pairing existing solar and onshore wind resources with offshore wind will result in higher firm capacity values than when only solar and onshore wind resources are paired. This can be observed across each of the four configurations: For each configuration, the combination of offshore wind, solar and onshore wind resulted in the highest firm capacities. The Pearson Correlation analysis results shown in and suggest that onshore wind and solar are the most complementary resources over the 5-year period since these resources have a Pearson correlation coefficient value of −0.39. This has been well-reported in the literature previously (e.g., ). In , the demand is most strongly correlated with solar (0.19) and most strongly anticorrelated with onshore wind (−0.26). shows that the combination of offshore wind and solar from Configuration 1 increases the correlation with demand, as shown in the marginal increase in Pearson Coefficient from 0.19 with only solar to 0.21. Therefore, as other studies have found, solar and onshore wind are the most complementary resources, but solar and offshore wind generation are the resources most positively correlated with demand. The near zero value of the Pearson correlation coefficient between the offshore wind farm sites and the demand and solar in suggests there is no observable correlation. There is a weak positive correlation between onshore wind and Channel Islands North (0.17), Channel Islands South (0.14), and Morro Bay (0.12). Additionally, the analysis shows that the offshore wind sites are not consistently correlated with each other. Crescent City and Humboldt, which are in close proximity with each other, have a significant level of correlation, demonstrated with the correlation coefficient of 0.7. Bodega Bay has a medium positive correlation with Humboldt (0.54) and Morro Bay (0.43), and a weak correlation with Channel Islands North (0.22) and Channel Islands South (0.29). Morro Bay has a strong correlation with Channel Islands North and South with coefficient values of 0.68 and 0.75, respectively. The two Channel Islands locations are strongly correlated with each other (0.88). The correlation between offshore wind farm sites is heavily dependent upon location; the closer the sites are the higher the correlation, as expected. These results demonstrate that the offshore wind sites are not ubiquitously correlated or complementary with each other. Instead, offshore wind generation has large temporal and spatial variability. In the present analysis, we summarize the demand-based value metric results during peak demand hours (16:00–22:00) in the summer and winter ( ) for each renewable energy resource and for each of the six representative offshore wind sites. The boxes show the quartiles of the dataset, with the bottom of the box showing the 25th percentile and the top of the box showing the 75th percentile. The black horizontal line represents the mean. The whiskers of the boxplots represent the 5th and 95th percentiles, while any points outside the whiskers are considered extreme values that lie outside of the 5th and 95th percentiles. The metric captures how well power generation from these sites aligns with the demand during peak hours. When the metric value is 1, the alignment is perfect, and when the metric value is 0, the alignment is poor. In the summer, the median values are significantly higher for the offshore wind sites than for solar and onshore wind. This is also true for the median value of the offshore wind metric. There are multiple instances where the metric for offshore wind reaches values that are above 0.8. In the summer, the most valuable offshore wind sites are located Bodega Bay, Humboldt, and Crescent City. During peak demand hours in the summer, offshore wind is highly valuable and a desirable resource for the electric grid. In the winter, the median value for solar is zero, and the majority of the metric values range between 0 and 0.1. The metric for offshore wind is more variable in the winter than in the summer and the median values are lower. Additionally, the median value for the combined offshore wind metric (0.23) is nearly equivalent to the onshore wind median (0.22). For the Channel Islands North, Channel Islands South, Morro Bay, and Bodega Bay sites, the median metric tends to fall between 0.11 and 0.20. Of the offshore wind sites, Morro Bay represents the site with the lowest median metric, and it has significant variability. This suggests that Morro Bay may have the least coincident dynamics with the electrical demand. The two most valuable sites are Humboldt and Crescent City, which have a median metric value between 0.25 and 0.28 in the winter. These results further illustrate that the value of offshore wind is highly dependent upon both temporal and spatial factors. The value of offshore wind depends upon the location of the offshore wind site (e.g., Morro Bay vs Crescent City) and the time of year studied (e.g., summer vs winter). Interestingly, in both the summer and winter, the demand-based value metric for onshore wind experiences the least amount of variability. This is likely because the onshore wind data is the aggregate value of wind speeds over the Western Interconnection region in the United States. While the results from the Pearson correlation analysis showed that solar is most coincident with the demand, the demand-based value metric analysis results show that during the peak demand, offshore wind is significantly more valuable than solar because its capacity factors are much higher in this period. The profile of solar generation may be most like the demand, but solar is not highly available when the demand is at its highest. On the other hand, the capacity factors of offshore wind are consistently higher than those of solar or onshore wind during peak demand. To assess the predictability of the renewable resources, we used the FFT to identify the most important frequencies. The FFT produced frequencies with the highest magnitudes in the FFT results reveal the underlying trends and most important periodic components of the data. The most significant periodic trends are provided in the . The most significant period in the onshore wind data is 1-day. For onshore wind, the R value for comparison of the FFT results to the original data varies between 0.52 and 0.87 for the seasons studied, indicating a match that is not very good. On the other hand, the FFT performs very well for the solar data, revealing that the top three periods with the highest signal for solar are 1 day, 0.5 day, and 0.25 day. This was consistent across all the seasons studied. The high R values for solar, which are consistently greater than 0.9, indicate that solar is highly predictable and consistent across the seasons and locations studied. In contrast to solar and onshore wind, the FFT is not able to capture the most significant periods of the average offshore wind data. Unlike solar and onshore wind, the offshore wind data does not have any periods that can be consistently distinguished across the periods studied. presents a summary of the average R values obtained for the comparison of FFT results to the original data for solar, onshore wind, and the average offshore wind capacity factors. Note that in all the seasons analyzed in this four-year period, solar power is the most predictable resource with R values equal to or above 0.95, while onshore wind ranges between 0.52 and 0.87, and offshore wind ranges between 0.45 and 0.73. The R values for offshore wind may be lower than onshore wind because the onshore wind dataset is representative of the Western Interconnection region, whereas the offshore wind is the average of six sites. In this study, we use statistical methods to evaluate the coincidence, variability, alignment, and predictability of various offshore wind sites in California with demand and onshore renewable resources in the Western Interconnection. The goal of this study is to investigate the benefits and challenges of deploying offshore wind and to evaluate the performance of offshore wind. One of the major benefits of offshore wind is that it has the potential to increase the firm capacity of renewable energy resources when it is paired with solar and onshore wind. The generation duration curves revealed that the combination of solar and onshore wind with each of the offshore wind farm sites leads to a firm capacity that is higher than the highest firm capacity of an individual resource. Additionally, the demand-based value metric revealed that offshore wind is the most valuable resource during peak demand hours in the summer in California. Due to the high variability of offshore wind, its value and benefits fluctuate spatially and temporally, which may present challenges for grid operators. In the winter, offshore wind is significantly less valuable than in the summer during peak hours. The Pearson Correlation results further reflect the high spatial variability of offshore wind, which showed that offshore wind sites are not coincident with each other nor with the onshore resources or the demand. Thus, proper planning and storage will be essential for decision makers to deploy offshore wind in a way that result in the maximum benefits and reliability. Future work should consider optimization analyses that consider the role of transmission and storage in offshore wind deployment. Additional research should also investigate the coincidence of offshore wind with a projected electricity demand that accounts for the future load of electric vehicles and incorporates the techno-economics of an energy system with offshore wind.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Not Used\n",
      "* **AIモデルの判断:** Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 35 / DOI: 10.1016/j.sftr.2025.100578  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "The KOF Globalisation Index – revisited\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Towards a sustainable future: The interplay of trade globalization and regulatory quality on environmental outcomes in India\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "Global warming and climate change due to persistent rise in greenhouse gas (GHG) emissions stand out as humankind's most severe environmental challenges [ ]. Measuring the losses from these issues on current and future stewardship is arduous because the impacts are often interconnected and indirect [ ]. As per the global footprint network (GFN), environmental degradation caused by anthropogenic activities has already exceeded the earth's carrying capacity by 75 % [ ]. Further, 2023 estimates of National Footprint Accounts 2023 reveal that the world's total ecological footprints (EF) have continuously increased since 1961 at an annual growth rate of 2.1 %. These challenges threaten economies through natural disasters, destroyed infrastructure, biodiversity loss, floods, droughts, sea-level rises, increased social inequalities, displacement, and claims of thousands of deaths annually [ , ]. At the world level, many initiatives have been taken to control environmental risks, including the [ ]. The recently held 29th Conference of Parties (COP 29) in Baku, Azerbaijan, also emphasized the importance of persistent efforts for resource conservation, energy transition, and climate finance to mitigate climate impacts and build resilience. Notwithstanding all these initiatives, very little progress is observed in attaining sustainable development [ , ]. Therefore, to address these challenges effectively, it is important to examine the impact of various factors responsible for environmental harm [ , ]. The previous studies have used greenhouse gases, CO2, SO2, PM2.5, and PM5.0 as the proxy for environmental degradation [ , ]. But these measures do not account for the degradation of natural resources such as forests, croplands, and mines [ ]. Therefore, the need for a comprehensive measure of environmental degradation is strongly felt to assess the true impact of economic and trade policies on the environment. Ecological footprints (EF) is a comprehensive metric representing the wider range of environmental impacts such as water use, land degradation, resource depletion, and pollution. This broader approach offers a more holistic strategy in addressing sustainability issues and ensures that efforts to reduce carbon emissions don't come at the cost of worsening other environmental problems. Intuitively, economic development, globalization, trade expansion, industrialization, unplanned urbanization, and poor regulatory control are the primary drivers of the rise in GHG emissions causing environmental deterioration [ , ]. Unsustainable consumption and production practices, poor waste management, air pollution, water contamination, and soil degradation further deteriorate the environment [ ]. Nevertheless, it sometimes becomes very difficult, especially for emerging nations, to establish a balance between economic and environmental goals. The empirical outcome on the effect of globalization on environmental quality suggests that globalization is both the underlying cause and the assuaging factor for environmental deterioration [ ]. On one hand, it removes cross-border barriers, facilitates structural transformation, promotes technical collaborations, and fosters green innovation through trade advancement and financial integration, which improves environmental quality [ ]. On the other, it results in the over-exploitation of natural resources owing to increased production and consumption activities, leading to environmental deterioration [ , , ]. Additionally, the RQ of a country moderates the nexus between globalization and environmental sustainability [ ]. Poor RQ, characterized by rent-seeking, bribery, and clientelism culture, can result in environmental degradation [ ]. Relaxing regulations to foster a trade-conducive economy may also have detrimental effects on the environment [ , ]. Consequently, ecological outcomes are expected to differ between countries with strong and weak RQ [ ]. Hence, in regions with weak environmental regulations, globalization paves the way for the relocation of ‘unclean’ businesses, turning them into pollution havens [ ]. Our study considers India for exploring the growth-environment nexus. The past three decades have been transformative for India, as the Southeast-Asian economy embraced globalization, resulting in a major boost to international trade, technological advancements, and foreign investments and thus driving rapid economic growth (EG) [ , ]. At present, India is the third-largest and fastest-growing economy and is also home to the largest population in the world. The production and consumption patterns of such a large population exert enormous pressure on the ecology [ ]. It is also the third most polluting country after the United States and China in terms of greenhouse gas emissions [ ]. With an estimated annual cost of approximately USD 95 billion (or 7 lakh crores INR), air pollution places a significant financial burden on India [ ]. India is also highly vulnerable to climate change, evident from its fifth ranking in the Global Climate Risk Index of 2020. According to the World Bank estimate, “if not tamed, climate change may diminish India's GDP by nearly 3 % and unfavorably disturb the living standards of nearly half the country's population by 2050.” All this tends to decouple the course of nature and the environment at large, with the potential to impact the growth of the Indian economy adversely. India is also been a signatory to Kyoto since 2002 and a partner to the Climate and Air Coalition since 2019. Despite its commitment to sustainable development, India has been challenged by the adverse impacts of CO emissions. Furthermore, India's dismal position in Transparency International's 2023 Corruption Perception Index [ ] indicates a significant rise in corruption and a weakening of regulatory quality (RQ) in India. This decline in RQ is expected to deter the effective implementation of environmental regulations and corrode the capability of institutions to implement effective trade and ecological regulations [ ]. Thus, the need to investigate the globalization, regulation, and environmental nexus is relevant, as India has remained a developing economy struggling with environmental depletion despite its huge resource ability and economic sector reforms. Our study is expected to reveal the divergent causes and their expected impact on dysfunctional conditions and help policymakers integrate various economic policies to ensure growth and environmental balance [ , ]. In light of these considerations, the primary objective of this study is to analyze the globalization-regulation-environment nexus in India and determine the presence of the pollution haven hypothesis (PHH) or pollution halo effect hypothesis (PHEH) for the period from 1990 to 2022. The contributions of this study to the existing knowledge are manifold. Firstly, while the majority of existing studies focus on investigating the Environmental Kuznets Curve (EKC) theory, our study makes an initial comprehensive attempt to investigate the presence of PHH/PHEH in India. This exploration assumes more significance as with rising international trade, there is an increased possibility of industries moving from countries with stringent environmental regulations to countries with weak environmental controls, creating new pollution havens, thus providing a new perspective on the trade-regulation-environment relationship [ , ]. Second, this study uses EF as a proxy for environmental depletion, providing an all-inclusive measure that makes it easier to model this intricate relationship [ ]. Despite its advantages, EF has been selectively used in similar studies, and our study fills this conspicuous gap [ ]. Moreover, this work utilizes the KOF globalization index as a proxy for TG, which is a novel and effective measure of globalization than FDI and trade openness [ ]. Third, our study uses a host of control variables, economic growth (EG), urbanization (UB), and biocapacity (BC), thus reducing the possibility of omitted-variable bias. Fourth, this work also explores the interaction effect of TG-UB and RQ-BC to ascertain the synergy in policies, which has been given less importance in previous studies. Fifth, the study employs the QARDL model to reveal the potential nonlinear impacts of TG and RQ under different stages of environmental deterioration. The study also deals with potential endogeneity concerns by using the 2-stage least square (2SLS) regression technique, which increases the reliability of its findings. Finally, the study provides actionable policy implications for regulators to address the ecological concerns stemming from globalization and weak regulatory structures. The remainder of the paper is structured as follows: presents the theoretical framework and synthesis of the empirical literature. explains the data sources and the empirical methodology. reports the results, followed by a discussion in . Finally, provides the conclusion, policy recommendations, limitations of the study, and future research directions. The theoretical foundation connecting economic development and environmental sustainability is grounded in several theories. These theories seek to integrate ecological and economic systems for long-term human welfare within the bounds of Earth's natural resources. The negative repercussions of globalization on the environment are elucidated by the PHH, which argues that a relaxed regulatory framework attracts foreign trade and investments that bring about ‘dirty’ technology, increasing non-renewable energy demand and CO release [ ]. Conversely, the PHEH assumes that the environmental conditions of the host nations are better off with the transfer of ‘green’ techniques [ ]. Compact city theory (CCT), ecological modernization theory (EMT), and urban environmental transition theory (UETT) explain the UB-development-environment link [ ]. According to the EMT, planned UB minimizes environmental hazards. The UETT suggests that urban people are proactive in pollution reduction, while CCT finds UB environmentally depleting. Given the upward trend of TG in India, revisiting PHH and PHEH is substantial. A significant body of research has emerged in the quest to explore the drivers of environmental sustainability in different contexts, resulting in a hot debate among researchers, environmental economists, and policymakers. This section presents a critical review of the existing literature on the impact of globalization and governance quality on environmental sustainability, along with the studies explaining how urbanization and biocapacity moderate their effect on environmental quality. The globalization-environmental connection has been examined in a large number of studies in various geographical and socio-economic contexts using different methodologies. Prominent studies demystifying this linkage include Tiwari et al. [ ], Villanthenkodath and Pal [ ] in India; Soti et al. [ ] in QUAD; Azam et al. [ ] in SAARC nations; Kumar and Soti [ ] in Japan; Wang et al. [ ] in China; Agila et al. [ ] in South Korea; Figge et al. [ ] in 171 countries; Jahanger et al. [ ] in 73 developing countries; Hashmi et al. [ ] in BRICS-T panel; Nadiri et al. [ ] in EU countries; Leal et al. [ ] in 58 selected developing and developed countries; Rehman et al. [ ] in selected emerging countries; Güngör et al. [ ] in South Africa. These studies have used trade openness, trade intensity, foreign direct investment, economic globalization, financial globalization, and technology transfer as a proxy for globalization more frequently however, the specific effect of TG on EF has been inadequately studied [ ]. Despite the growing literature, the empirical evidence on the impact of globalization on environmental quality is conflicting, which primarily is due to the differences in the countries/panels for investigation, estimation techniques, data employed, the scope of the study, and temporal span, among other reasons [ ]. Figge et al. [ ] studied the impact of the Maastricht globalization index on EF in a panel of 171 countries using multivariate regression analysis. Their study reported that the globalization considerably mounts EF in sample countries. Similarly, Agila et al. [ ] utilized nonparametric causality and quantile-on-quantile approaches to study the impact of trade globalization on environmental quality in South Korea, and their empirical findings reported that trade globalization policies of South Korea have a plummeting impact on the environment. Also, using a dataset from 1990 to 2016, Dauda et al. [ ] investigated the impact of trade openness on carbon emissions for selected African nations, and their findings indicated that trade openness deteriorates environmental quality, therefore validating the PHH. Contradictorily, Villanthenkodath and Pal [ ] found that economic globalization helps reduce EF in India in the long-run. Supporting these findings, a recent study by Soti et al. [ ] investigated the complex relationship between TG, RQ, and EF in the QUAD nations (India, Japan, USA, and Australia) for the period 1990–2021 using ARDL methodology. The empirical results of their study demonstrated that TG improves environmental quality by reducing EF in QUAD countries except for Australia. Similarly, Nadiri et al. [ ] reported that trade and financial globalization, together with carbon taxation and renewable energy, contribute to a sustainable environment in EU member countries by reducing carbon emissions. Likewise, employing the ARDL bound test on time series data of China over the period from 1980 to 2017, Umar et al. [ ] studied the relationship between natural resources, globalization, and financial development, the empirical findings showed that economic, natural resources and financial development deteriorate the environmental quality while globalization helps in its sustainability through the promotion of clean technology. Similar results have been reported by Pata and Yilanci [ ] for G7 nations, where they find that globalization substantially plummets EF. Besides, Leal et al. [ ] examined the impact of globalization on CO emissions from 1995 to 2017 for a mixed panel of 58 developed and developing countries. The findings of the study revealed that while economic globalization reduces CO emissions in developed economies, it accelerates emissions in developing countries. Similarly, in 73 developing nations over the period from 1990 to 2016, Jahanger et al. [ ] found that globalization has mixed effects on environmental sustainability. While it supports sustainable development in Africa and Asia, it may lead to detrimental consequences in other contexts, especially when developing countries focus on pollution-intensive industries. Likewise, for BRICS-T nations, Hashmi et al. [ ] reported that environmental impacts vary across the different components of financial globalization. The empirical findings in their study showed that de facto financial globalization increases emissions, while de jure financial globalization had a favorable impact on environmental sustainability by reducing emissions. The ambiguity in the empirical findings makes it important to understand this nexus further in different contexts and regions to validate the findings [ ]. Many recent studies have investigated the impact of RQ on sustainable environment in different contexts [ , , , , , , , , ]. Empirical evidence demonstrates that stringent regulations promote sustainability by ensuring stiff compliance with environmental laws, fostering clean energy usage, and encouraging green innovation [ ]. In contrast, fragile RQ contributes to environmental deterioration by unchecked pollution, resource depletion, ineffective waste management, unplanned urbanization, and unsustainable economic policies [ , , , ]. For instance, Kashif et al. [ ] assessed the importance of RQ in controlling CO emissions in 37 OECD countries from 2001 to 2019 using panel regression estimators, finding that RQ significantly reduces CO emissions in these countries. Danish et al. [ ] reported that climate change mitigation in BRICS nations is largely driven by effective environmental regulations and economic development. Ibrahim & Ajide [ ] also observed that better regulatory systems lead to enhanced environmental outcomes in BRICS countries by fostering green technology, reducing non-renewable energy usage, and creating barriers to resource exploitation by polluting industries. Likewise, Handoyo and Anas [ ] found that effective RQ encourages environmentally sustainable policies and improves environmental quality. These findings are further corroborated by studies from Omri and Ben [ ] for Middle-East and North African nations as well as Uzar [ ] for emerging seven economies, who also support the link between RQ and a sustainable environment. Conversely, Adedoyin et al. [ ] for BRICS and Obobisa et al. [ ] for South Africa found that weak institutions hinder the enforcement of environmental regulations and promotion of sustainable practices, which results in increased pollution in these nations. In a recent study, Liu and Zhang [ ] found that poor governance quality, ineffective corruption control, and torpid RQ adversely impact green growth in BRICS nations, emphasizing the importance of strong regulations for promoting sustainability. Likewise, Mignamissi et al. [ ] for 50 African countries found that poor regulatory quality increases pollution in Africa and turns these resource-rich countries into pollution havens, attracting polluting industries from developed nations. On a similar line, Chhabra et al. [ ] found that corruption and weak environmental regulations significantly contribute to environmental deterioration in BRICS countries, undermining efforts toward sustainable development and cleaner energy practices. Although these findings are important, there are some conflicts and inconsistencies with the empirical outcomes. As such, stringent environmental regulations are not a guarantee of enhanced environmental quality because of various economic, social, and political factors. It is, therefore, important to revisit this nexus from a contemporary perspective. The empirical studies on the linkages between UB and environmental quality have produced mixed results [ , , ]. Danish et al. [ ] examined the impact of UB on EF in BRICS countries for the period 1992–2016, using panel data techniques, and found that UB contributed to environmental sustainability in BRICS countries by reducing EF. Similarly, Kızılgöl and Öndes [ ] investigated the antecedents of EF in OECD countries from 1995 to 2017. Their study found that in the long-run, UB exerts a favorable impact on environmental quality by reducing EF. The study also reported unidirectional causality flows from UB to EF. By contrast, Chatti and Majeed [ ], using a panel dataset comprising 60 developing and 34 developed countries from 1998 to 2016, found that UB negatively affects environmental quality. Similarly, Nathaniel and Khan [ ] investigated the association between EF and UB in the ASEAN region using data from 1990 to 2016. The authors revealed that UB significantly contributed to the rise in EF, leading to environmental mitigation in ASEAN countries. Analogous results regarding the adverse impact of unplanned UB on environmental sustainability have been observed in other studies [ , ]. Additionally, some studies have reported an insignificant impact of UB on environmental sustainability, further contributing to the mixed findings in the literature [ , ]. The interplay between trade globalization and urbanization significantly influences ecological footprints (EF), with their combined effects being context-dependent [ ]. While both these factors generally increase EF, their effects can be moderated by renewable energy adoption, effective urban management, and stringent environmental policies [ ]. Conversely, some argue that globalization and urbanization can lead to a \"race to the bottom\" in environmental standards, where countries may lower regulations to attract trade and investment, worsening EF. In light of contrasting and conflicting empirical findings, confirming the exact relationship between UB and environmental sustainability is important. Thus, deriving new knowledge is crucial, which is the main objective of the present study. BC refers to the capability of an ecosystem to regenerate resources and absorb waste [ ]. When EF exceeds the BC, it results in an ecological deficit, contributing to environmental harm [ ]. In emerging countries, poor RQ, rapid UB, industrialization, increase in resource consumption, and unbridled population growth further intensify this problem, resulting in a critical ecological deficit [ ]. Even though technological development offers some respite, the overall trend remains incommodious [ ]. Hassan et al. [ ] reported that BC operations increase pollution. On a similar line, Marti and Puertas [ ] used data envelopment analysis to assess the efficiency of African nations in maintaining sustainable growth, the empirical outcome of their study highlighted that most of the countries in the region are facing ecological deficits. Pandey et al. [ ] studied the effects of BC, globalization, urbanization, and life expectancy on environmental deterioration in Asian nations and found that BC, life expectancy, and urbanization increased environmental deterioration, while globalization mitigated it. The explicit findings in the extant studies further indicate that BC reduces EF in high-income countries but increases it in middle- and low-income countries [ ], revealing the asymmetry in its impact. Interestingly, the impact of BC on environmental deterioration is more profound in countries where RQ is poor. In the absence of strong regulations, resource overuse and pollution can quickly surpass biocapacity, leading to an ecological deficit. This results in a larger EF, as unsustainable consumption and waste outpace the Earth's ability to regenerate resources and absorb pollution. Finally, empirical studies stressed the pressing need for sustainable practices and policy interventions to integrate economic policies with environmental preservation. An insight into the literature helped us identify some notable gaps. Indeed, such voids in the literature obscure crucial information that limits the effectiveness of policies to enthrone sustainable development. Prior studies fail to account for the specific implications of TG and RQ on EF, as a significant number of earlier studies focus on carbon emissions while measuring environmental quality [ ]. Additionally, studies exploring the nexus between TG, RQ, and EF are primarily confined to developed countries and selected developing nations only, neglecting emerging economies with significant levels of EF. This spatial imbalance is an important policy concern that needs immediate attention. Moreover, the extant literature on this knowledge field has yielded conflicting findings, which weakens the clarity required for effective policymaking. Furthermore, the mixed impacts of UB on environmental quality highlight the need for more nuanced studies that address both planned and unplanned UB and its effects. Finally, many prior studies demonstrate methodological flaws and thus fail to account for the true impact of these variables on environmental sustainability. This study uses annual data from 1990 to 2022 in India to investigate the influence of economic and regulatory factors on environmental sustainability. The choice of the time period for the study can lead to variation in results due to changes in the economic and environmental policies over time [ , ]. Our work employs EF measured by global hectares per capita as the dependent variable, while TG and RQ are the main explanatory variables. Following the existing literature, we control for the effect of EG, UB, and BC [ , ]. The data for the study is retrieved from the Global Footprint Network (2019), the KOF Index of Dreher [ ], the World Development Indicator, and the Worldwide Governance Indicators database. Missing values have been estimated using linear interpolation following the works of [ ]. For econometric analysis, we have used the natural log values of all the variables, which helps in reducing the skewness and variances in the time series data [ ]. A detailed description of these variables is provided in . For a comprehensive insight, we have examined the impact of TG and RQ on EF both separately and jointly in three different models. Using three different models increases the flexibility, consistency, and soundness of our approach [ ]. The first model expresses EF as a function of TG, which helped us investigate the role of TG in environmental sustainability independent of RQ. In our second model, we replaced TG with RQ, which has been crucial in examining how environmental governance affects sustainability independent of TG. Finally, the third model jointly considers TG and RQ, which provides a more holistic view of the impact of globalization and governance quality on environmental sustainability. This segregation facilitates sensitivity analysis of both the individual and combined effects of TG and RQ on EF and also enhances the depth of our analysis. Empirically, this approach enables assessing which policy or combination is more effective in reducing EF, offering insights into their relative importance. This approach is also justified from a policy perspective as it enables policymakers to design more targeted interventions, such as prioritizing environmentally friendly TG policy or using stringent RQ measures for addressing environmental concerns in specific economic and regulatory settings. to specify the conventional forms of these models, respectively. For further explanation of equations, TG and RQ will be collectively termed as regressors denoted by (X), and EG, UB, and BC will be referred to as control variables denoted by (Z). The present study investigates the impact of globalization and regulatory control on environmental degradation in India by following a progressive econometric methodology divided into several stages. depicts the methodological framework of the study. The econometric methodology for the study begins with computing the descriptive statistics of the variables to gain insight into their distribution, followed by multicollinearity analysis. Assessing multicollinearity is crucial, as ignoring it can cause significant distortions in the results. The stationarity of the time series data is then examined using the augmented Dickey-Fuller (ADF), Phillips-Perron (PP), and Zivot-Andrews tests. Testing for stationarity is important as non-stationary variables can produce spurious results. In this study, the Autoregressive Distributed Lag (ARDL) bound testing approach advanced by Pesaran et al. [ ] is employed, which produces both short-run and long-run cointegration estimates. This technique has been extensively used in the literature and offers several advantages over other estimation methods. First, in contrast to traditional methods, it can be used where the variables are I(1) or I(0) or a combination of both [ ]. Second, it generates more reliable results for small sample sizes [ ]; therefore, it is more appropriate for our study with 33 observations (1990–2022). Third, the residuals in the ARDL model are not correlated, which makes the findings robust. Finally, the endogeneity issue does not arise if lag selection is appropriate [ ]. In using the ARDL method, the following error correction models have been estimated: Where denotes the first difference operator, p represents the lag order chosen based on Akaike Information Criteria (AIC), and is the white noise error term. The summation sign (∑) epitomizes the short-run section of the equation, and the short-run parameters of the equation are represented by , while are the long-run parameters. The joint significance of the long-run parameters is examined with the help of the F-test with the null hypothesis of H : , meaning there is no long-run level relationship. The first step in the ARDL cointegration method is the bounds test based on the F-statistic. The F-statistic computed is compared with the critical values to ascertain the presence of long-run association. With the rejection of the null hypothesis, the cointegration or long-run association among the variables is established. Our study also employs the QARDL technique of Cho et al. [ ] to investigate the nonlinear association among the variables by extending the ARDL model to ascertain quantile-dependent short-term and long-term relationships, especially with dependent variables exhibiting extreme tail distributions. The rationale for applying both ARDL and QARDL is based on the following arguments: First, even though ARDL is suitable for capturing both short- and long-run relationships for small samples, it assumes uniform effects of independent variables across the distribution of the dependent variable. Thus, it may not reveal the true impact of independent variables on dependent variable [ ]. Contrastingly, the QARDL approach considers locational asymmetries, non-linearities, and heterogeneous effects of independent variables across different quantiles of the dependent variable (EF). The literature also postulates that non-linearities are expected, as higher levels of EF might invite stringent regulatory control and environment-friendly trade policies [ ]. The QARDL approach, thus, is more apt to understand the dynamic and heterogeneous effects of globalization and regulations, ensuring more effective and inclusive policy directions. Thirdly, the QARDL model also scores over the nonlinear (NARDL) approach, where nonlinearly is delineated by setting the intensity at zero as against the QARDL model, where it is recognized based on the data-driven process. Finally, to devise a robust policy, the differential effects of the explanatory policy parameters must be examined across the entire band of the target policy variable, and this research objective is achieved by employing the QARDL approach [ ]. Thus, the application of QARDL for ascertaining nonlinear and asymmetric relationships among environmental deterioration, globalization, and institutional quality in India is justified. Following [ ], the ARDL model ( ) is expressed in QARDL form in , expressed below: In , the term , and p, m, and n are lag orders indicated by AIC. Furthermore, represents the quantile, 0 < < 1. To avoid the possibility of a serial correlation in the error term, can be expressed as per : The error correction model reparameterization form of is outlined in : The summative short-run impact of the previous EF value is measured using , while the same for regressors have been measured from in . The parameter ρ of lag values of EF in must be negative and statistically significant. The long-run cointegrating coefficients of all regressors have been computed using the following formulas presented in : The asymmetry in our results for short-run and long-run have been tested with the help of the Wald test. For this test, the null hypothesis for the parameter’ ’ is stated as : ρ*(0.10) = ρ*(0.20) = ρ*(0.30) = ρ*(0.40) = ρ*(0. 50) = ρ*(0.60) = ρ*(0.70) = ρ*(0.80) = ρ*(0.90) against the alternative hypothesis of : ρ*(i) ≠ ρ*(j) with i, j ∈{0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90} and i ≠ j. The same procedure has been followed for all other regressors as well. The study began with obtaining the summary statistics of all the variables to gain insight into their nature for choosing an appropriate estimation model. The descriptive statistics of all the variables (without log) are reported in . The average values for all variables are positive. Further, the value of the coefficient of variance is highest for EG (0.4354) and lowest for BC (0.0489), indicating that EF is the most volatile and BC is the most stable variable. Further, TG, RQ, and BC are negatively skewed, while EF, EG, and UB are positively skewed. The kurtosis measure value indicates the absence of extremities in the dataset. Further, all the variables are normal in their raw form. The results of correlation and multicollinearity, as reported in (Panel A and Panel B, respectively), do not highlight any multicollinearity issue as VIF values are less than the threshold value of 5 [ ]. The stationarity of the underlying variables has been tested using augmented Dickey-Fuller (ADF) and Phillips-Perron (PP) tests ( ). The results of unit root tests indicate that variables are a combination of I(0) and I(1). The study also employs the Zivot and Andrews [ ] unit root test, which helps detect the presence of a structural break in the time series data. As time-series data have the possibility of structural breaks, it is important to identify and amend the model for robust analysis. The impact of the structural break was observed by introducing a dummy variable since the results were not significant; therefore, these have not been reported in the study. The existing literature also suggests that the use of dummy variables for potential series breaks is overlooked when assessing the scale of environmental pollution [ ]. The results of the Zivot and Andrews test are presented in , which elucidates that the dependent variable (EF) had a structural break in 2003. This period corresponds to the phase of the economic boom of 2003–2008 in India when the Indian economy grew at a rate of nearly 9 % per year until the financial crisis of 2008, which also led to a rise in industrial and manufacturing activities, resulting in significant changes in the ecological level. This study has employed the ARDL bounds cointegration method to ascertain long-term association among the variables. The results of the ARDL bound test are sensitive to the choice of appropriate lag length; therefore, identifying the proper lag is imperative for a robust analysis. The appropriate lag length order is determined according to the AIC, which is considered superior for small sample data [ ]. This scientific lag selection ensures that our results are not adversely affected by the inappropriate choice of lag length. The results of the bounds test for the cointegration are reported in . The null hypothesis of no-level cointegration is sturdily rejected as the F-statistics lie above the upper bound at a 1 % significance level. Thus, the ARDL bounds test findings establish a long-term cointegration among the underlying for India for the chosen study period. The results of long-run estimates are presented in . Model 1 estimates the effect of globalization on environmental quality without accounting for the impact of the RQ. The findings reveal that TG has a statistically insignificant effect on EF in the long-run, suggesting that TG has not yet attained a threshold level at which it can affect environmental quality. The specific impact of the other covariates, EG and BC, on EF is positive, and a 1 % rise in both these variables results in an increase of 0.3176 % and 0.3011 %, respectively, in EF in the long-run. An increase in BC implies that abundant resources are required to fulfill human needs and absorb waste. This situation induces people to exploit the available resources, which negatively affects the environment. However, environmental deterioration can be negated if BC is increased using renewable energy resources [ ]. Consistent findings have been observed in Pakistan [ ]. The long-run impact of UB on EF is negative, suggesting that urban population growth in India helps to mitigate the levels of EF, thus contributing to environmental quality. Model 2 examines the impact of RQ on EF after dropping TG from the model. The findings of this model reveal that RQ in India significantly affects EF, indicating that regulations effectively control ecological degradation due to economic activities in the country. Specifically, a 1 % rise in RQ leads to a 0.0595 % decrease in EF. Similar to Model 1, both EG and BC are responsible for the surge in EF, while the impact of UB on EF is negative, and a 1 % rise in EG and BC increases EF by 0.39 % and 0.6206 %, respectively. Model 3 is a comprehensive model that collectively explores the effect of TG and RQ on EF in the presence of other covariates. The collective examination of TG and RQ assumes significance, as regulatory structure moderates the relationship between TG and ecological quality in India. According to the findings, the impact of both de facto TG and RQ is negative on EF. A 1 % rise in TG and RQ results in a fall of 0 .0473 % and 0.0472 %, respectively, in EF in the long-run. The results do not show any substantial change in the impact of TG on EF in models 1 and 3, which means that RQ does not affect the relationship between TG and the environment in the long-run. Even though RQ, such as environmental taxes and subsidies for using green technology, are causing desired outcomes in the form of a decrease in EF, the same is not instrumental in affecting the nexus between globalization and environmental quality. Additionally, the impact of EG and BC on EF is positive and statistically significant in this model. In statistical terms, an increase of 1 % in EG and BC causes an increase of 0.3894 % and 0.5157 %, respectively, in EF in the long-run. The impact of UB on EF is negative and statistically insignificant. A comparative analysis of all three models exhibits that the results are consistent in terms of the values of the coefficients and statistical significance for all the variables, demonstrating that the present study's findings are robust. also presents the results of the error correction model, which is the basis for the conclusions about the short-term relationship between the underlying variables. The short-run estimations results indicate that the impact of TG on EF is statistically significant and negative for Models 1 and 3, which are 0.0037 and 0.0968, respectively. More importantly, the impact is statistically more substantial in the presence of RQ and globalization variables. The effect of RQ on EF is negative and significant for Models 2 and 3. Similarly, the impact of all other covariates is consistent across all the models. The error correction term (ECT) coefficient for all three models is statistically significant and has values between 0 and −1. The value of ECT explains the speed of adjustment in the case of any deviation from long-run equilibrium values. The speed of this adjustment is 61.54 %, 58.73 %, and 73.56 %, respectively, for the three models. These findings in the short-run indicate that the PHEH holds in the case of India in the short-run as in the short-term period, both TG and RQ plummet EF significantly. This can be attributed to the fact that the economies start to rely more on environment-friendly technologies, thereby adapting to the changing environmental regulatory landscape. But in the long-run, these results become insignificant since TG offers immediate advantages in the short-term, like exposing people to greener technologies and procedures, but it is perhaps not supported by environmental regulations. Both these aspects are important as RQ supports and reinforces these advantages. As RQ advances over time, it might eventually eclipse globalization's contribution to lessening the ecological impact. A more reliable and straightforward method of lowering EF than globalization alone is through regulatory systems like emission limits, pollution control standards, and green certifications. In the long-run, the results of TG become insignificant, which may be a hint that the main factor influencing sustainability results is now RQ and the marginal impact of TG on lowering the footprint may seem statistically small once regulatory frameworks have stabilized and industries have adjusted. Similar results were reported in the case of ASEAN by [ ] and in the USA, by [ ]. However, these findings are in contrast with the study of [ ], according to which de facto globalization increases CO emissions in developing nations. Finally, the ARDL model's findings have been tested by applying suitable diagnostic tests for heteroskedasticity, serial correlation, correct functional specification, and stability. presents the results of these tests, which do not provide any evidence of structural anomalies in our estimations. The outcomes of 2SLS are reported in , demonstrating that the impact of TG and RQ aligns with the baseline models, highlighting their validity. The lagged values of independent variables are taken as instrumental variables, this choice is apt as lagged value exerts a significant positive impact on their respective main variable. Further, the findings of the endogeneity test to confirm if the endogenous variables are endogenous and require instrumentation are significant, implying variables are endogenous, hence the choice of the 2SLS technique is correct. Additionally, the test statistics for weak instruments test used are significant indicating that the choice of instruments is appropriate. Finally, the test statistics for over-identification indicate that there is no over-identification problem in our models and the results are valid. To derive more empirical insights, our study includes interaction terms of UB with TG and BC with RQ into the model. The results of the interaction effect are reported in . The results portray that the interaction of UB with TG helps in improving the impact of TG on EF as indicated by their joint coefficient (−0.0854 in the long-run and −0.1056 in the short-run), which is also statistically significant at a 1 % level. This implies that sustainable urbanization practices contribute to environment-friendly trade practices. This also suggests that TG in combination with UB is more effective in environmental sustainability. The interaction effect of BC with RQ, however, does not help control the environmental problems both in the long-run and short-run. The findings are consistent with an earlier study by [ ], who also provided a similar interaction effect of UB and TG in enhancing environment sustainability. The results of the QARDL test are presented in . The findings of the model are consistent with those of the ARDL model, confirming the reliability of the ARDL estimates. Furthermore, the similarities in the marginal effects for the different variables confirm the robustness of the ARDL model outcomes for all models. The values for the estimated adjustment parameter of the QARDL model are negative, and statistical significance is a prerequisite for the long-run equilibrium between TG, RQ, EG, UB, BC, and EF. The results are consistent across all three models. The results for the long-run parameters of all the variables except TG are statistically significant for all quantiles. The long-run impact of RQ and UB on EF is negative, whereas the effects of EG and BC on EF are positive. The long-run estimations are also uniform across all quantiles across the three models. The long-run impact of TG on EF is statistically significant only at the upper quantile of 0.8 in model 3. These findings indicate that RQ and UB are important for reducing the harmful effects of environmental deterioration in India. Conversely, EG and BC are responsible for the increase in EF. Whereas there are asymmetries in the impact of TG on EF. The short-run findings of the QARDL approach show that EF's lagged values significantly impact it at all quantiles for all three models. The short-run impact of TG on EF is positive and significant only for the extreme quantiles. However, at the quantile of 0.2, this impact is negative and significant. The short-term effect of RQ on EF is statistically significant and negative for Models 2 and 3 across all quantiles. Likewise, UB helps mitigate EF in the short-run for all quantiles except for the lower quantile (0.2). These results are consistent for all three models. Similar to the long-run effect, EG and BC are responsible for the increase in EF, but this impact is not significant for the lower quantiles. The results are consistent for all models employed. The intensity of the effect over quantiles suggests an asymmetric impact of the independent variables on EF. The linearity of the parameters of the QARDL approach has been assessed using the Wald test. The results of this test are presented in . These findings confirm the asymmetric effect of all the independent variables on EF in the short- and long-run. In the long-run, the null hypothesis of the parameter constancy of the speed of the adjustment parameter is rejected, signifying an asymmetric connection. In addition, the null linearity across the different tails of each quantile for the long-run parameters is rejected. This outcome may be due to structural changes in macroeconomic indicators in India during the study period. The last decade of the previous century and the first two decades of the present century are marked by economic reforms in India, which have been instrumental in shaping the structure of the Indian economy. Finally, to observe bidirectional causality between explanatory and explained variables, the study employs the Granger-causality in quantile test across the quantiles [ ]. The p-values of the test are presented in , which provides evidence of a bidirectional relationship between EF with RQ, EG, UB, and BC at all quantiles. This study explores the impact of TG and RQ on environmental quality in India, suggesting valuable insights into the nexus between economic activities, regulations, and environmental sustainability. In a developing economy, TG is expected to dilute environmental quality; therefore, it is imperative to derive meaningful implications. The empirical findings of our study do not suggest any impact of TG on EF in the long-run, but in the short-run, this impact is significant. This finding aligns with many previous studies that suggest that the impact of TG on environmental outcomes is contingent on the context, time, specific conditions, and level of economic development of a country [ ]. In the Indian context, [ ] found that trade practices in India have not reached a level where they can present a problem for environmental quality. For QUAD countries, [ ] reported that trade liberalization in panel nations, barring Australia, has robust regulations, which proved to be a solution to environmental problems. Conversely, our findings diverge from those of [ ], who reported that TG exacerbates environmental degradation in developing countries. Additionally, the role of RQ in promoting economic globalization and environmental quality has been conceptually and empirically proven, thereby confirming PHEH. The favorable impact of RQ on ecological sustainability means that sterner regulations are helpful for sustainable business, economic, and environmental development [ ]. Analogous results have been obtained for South Africa [ ], Pakistan [ ], African economies [ ], and India [ ], while conflicting results have been reported for BRICS nations [ ]. Further, the study reports a statistically significant inverse relationship between UB and EF. The positive impact of UB on the environment can be attributed to increased awareness, improved urban planning, robust regulations, effective waste management, and green initiatives, leading to a rise in green resource consumption. Our findings also validate the theories of EMT and UETT. These findings are in harmony with the study of [ ] which investigated the antecedents of EF in OECD countries from 1995 to 2017. Their study found that in the long-run, UB has a favorable impact on environmental sustainability by reducing EF. TG has a statistically significant adverse impact on EF at the upper quantile of 0.8 only for the long-run in model 3 while in the short-run, it has a significant impact on EF at all quantiles. The asymmetries in the results of TG on EF for the short-run and long-run indicate the possibility of advancing theoretical foundations that provide robust explanations for the variation in the impact of underlying variables in different regulatory and economic conditions. The short-run effects of TG and RQ on EF are the reflections of the policy interventions, changes in consumption patterns, production methods, and public awareness of environmental concerns [ ]. These impacts are more sensitive to changes in extraneous variables such as economic situation, environmental conditions, political regimes, and geo-political factors, which makes them more volatile [ ]. On the other hand, the long-run effects refer to persistent changes in environmental sustainability primarily due to structural transformations that are triggered by sustained mechanisms, such as technological innovation, policy implementation, institutional reforms, and permanent changes in consumption patterns [ ]. These effects demonstrate a change in the underlying system dynamics, which makes them more stable yet slow to happen. Short-run changes might happen quickly, but these effects can dissipate quickly if they are not supported by long-term structural changes. For example, the introduction of carbon taxes may help in emissions reduction in the immediate future, but without corresponding investments in green energy or energy technologies innovations, the reduction may not be sustainable eventually. Overall, the cumulative impact of continuous policy reinforcement, technological advancements, and changes in societal behavior led to more profound and lasting reductions in EF. These temporal distinctions of the impact of TG and RQ on EF are important for policymakers. In the short-run, more emphasis can be placed on the measures that produce immediate results, such as giving incentives for environment-friendly practices, penalizing emissions, and introducing short-term inhibitions. However, for sustainable gains, long-term measures such as investments in green technologies, environmental regulation, sustainable infrastructure, ongoing monitoring, adaptive policy adjustments, and long-term policy commitments for supporting enduring changes in industry and consumer behavior are necessary. The favorable effect of EG on EF is in line with the EKC hypothesis, which states that in the initial stage of development, EG results in environmental degradation before reaching a threshold level where marginal growth leads to improvements in environmental quality [ ]. The results of the present research indicate that India has yet to attain this threshold level, emphasizing the need to integrate economic efforts with sustainable development [ ]. The results of this study are consistent with the studies [ , ], revealing similar findings. India is an influential part of the Global South, so it is important to investigate the trade-regulation-environment situation and understand the agenda it puts forth for the entire world. This study explores the presence of PHH/PHEH in India, offering a new perspective on the trade-regulation-environment linkage for the period 1990–2022. Our study utilizes 3 different models for assessing the specific and joint impacts of TG and RQ on EF. Furthermore, it examines the interaction of UB-TG and BC-RQ to gauge their multiplying effect on EF, which has been given less attention in previous studies. This helped us ascertain how regulation moderates the relationship between TG and environmental quality. ARDL-bound cointegration test has been applied for assessing the cointegration among the variables. The short-and long-run impacts of the explanatory variables have been tested with ARDL and QARDL tests. To check the problems of multicollinearity, heteroskedasticity, and autocorrelation, VIF, ARCH, and LM diagnostic tests are applied. Additionally, the Wald test has been used for assessing the asymmetric effects of parameters of the QARDL approach. These diagnostic tests indicate that our model has no problem of multicollinearity, autocorrelation, and heteroscedasticity, and the findings are robust. Moreover, to address the endogeneity issue in the model, the 2SLS model is employed. Finally, the Granger-causality in quantile test is applied. The results from both the ARDL and QARDL models indicate that EG and BC contribute to environmental pollution in India. Conversely, TG (short-run), UB and RQ (short-run and long-run) exert a negative impact on EF, promoting environmental sustainability. The interaction effect of TG-UB also helps reduce environmental deterioration. These findings testify that PHEH holds in the case of India in the short-run, implying that stringent and green regulations boost ‘clean’ trade in the host nation. The empirical outcomes indicate that EG alone cannot ensure sustainable growth, therefore trade and environment-related policies should be framed in conjunction with each other to dissuade polluting industries and foster cleaner sectors. Further, TG helps in sustainable growth in the short-run, but in the long-run impact is not statistically significant. Therefore, the main policy directions coming from this observation states that India should focus on formulating a comprehensive trade policies stressing on green trade practices and gradual elimination of emission-intensive imports for achieving sustainable growth targets. The study shows a favorable impact of RQ on environment, therefore the policymakers can offset the negative impact of economic development by further improving the environmental regulations, clinging to global emission standards, and adopting novel mechanisms to reduce environmental deterioration. The findings also reveal that BC worsens environment quality in India therefore, the regulators should focus on energy-efficient production process, sustainable urbanization policies, and encourage people to adopt environmentally sustainable consumption habits. The efforts should be made to reduce the negative impact of urbanization due to population growth, industrialization, transportation and enhance the positive impacts by formulating effective waste management, resource conservation, planning sustainable smart cities, encouraging renewable sources of energy, targeting zero emissions, and transitioning towards circular economy. An impetus on green technology by injecting more money into the energy-efficient industries and renewable energy pojects. Although this study addresses environmental concerns from divergent perspectives, various dimensions remain unexplored due to time and resource constraints. The study acknowledges that the focus on one country (India) limits the generalizability of the empirical outcomes to other emerging nations given differences in regulations, trade policies, and focus on sustainable practices. However, the findings may be relevant to countries with similar economic and social structures. Nevertheless, future studies are necessary to examine the applicability of our findings in other contexts. This research considers EF as the only dependent variable, whereas other measures of environmental degradation, like CO emissions, biodiversity loss, and greenhouse gas emissions could also be employed in future studies. Moreover, upcoming studies can also investigate the relationship between governance quality and political, social, and cultural globalization. Additionally, the impact of other macroeconomic variables, like energy use, natural resource rent, and industrial development, on the environment deserves attention. The role of artificial intelligence in reducing environmental harm must also be explored in future research as AI-driven solutions have the potential to optimize energy use, enhance emission monitoring, and drive innovation in sustainable technologies [ , , ]. Finally, the EF-TG-RQ nexus in the light of the carbon trade market of developed and emerging nations is an area worth exploring. Writing – review & editing, Software, Conceptualization. Writing – review & editing, Validation, Methodology. Validation. Writing – review & editing, Software. Data curation, Visualization, Software.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Not Used\n",
      "* **AIモデルの判断:** Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 48 / DOI: 10.1016/j.gloplacha.2022.103785  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Lake Ohrid’s tephrochronological dataset reveals 1.36 Ma of Mediterranean explosive volcanic activity\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Linking the Mediterranean MIS 5 tephra markers to Campi Flegrei (southern Italy) 109–92 ka explosive activity and refining the chronology of MIS 5c-d millennial-scale climate variability\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "Near-vent volcanic successions provide fundamental information for reconstructing the eruption history and dynamics of volcanoes. Proximal exposures, however, often provide only fragmentary records of the past activity of a volcanic system, since deposits of older explosive events can be eroded, not preserved or, more commonly, covered by products of younger eruptions. In contrast, tephra layers preserved in sedimentary successions located far away from the volcanic source and characterised by a continuous sediment accumulation history, can provide detailed and undisturbed records of explosive eruptions for a given volcano, including events that are poorly represented or missing in near-vent sections (e.g., ; ). This also applies to the Neapolitan volcanic area (Campania, southern Italy), including Campi Flegrei, Ischia, Procida and Somma-Vesuvius ( b ), where the intense Late Pleistocene explosive activity (e.g., and reference therein) made the earliest pyroclastic products barely accessible in proximal settings. However, these activities are instead documented in distal sedimentary archives. Indeed, since the first distal marine discoveries of , several occurrences of widespread tephra layers with a Campanian geochemical signature embedded within Marine Isotope Stage 5 (MIS 5) sedimentary successions suggested the occurrence of major explosive activity, however this had never been documented in proximal sections of the Neapolitan volcanoes. Among them, the C-22 ( ), X-5 and X-6 ( ) tephra layers have been traced widely across the central Mediterranean area in several terrestrial (e.g., ; ; ; ; ; ; ; ; ; ) and marine (e.g., ; ; ; ; ) sedimentary archives. Moreover, at least two additional tephras, occurring between the C-22 and X-5 markers, with a similar Neapolitan geochemical signature, are also found in Mediterranean MIS 5 records ( ; ; ; ; ). Over the recent decades, these widespread tephra layers have been used as remarkable marker horizons for dating, synchronizing, and correlating MIS 5 Mediterranean sedimentary successions, the chronologies of which would have otherwise been poorly determined. With this regard, tephra markers from Neapolitan volcanoes arise as pivotal stratigraphic and chronological tools for paleoclimatic and archaeological investigations at the regional scale (e.g., ; ; ; ; ; ; ). Despite their great chronological importance, the lack of near-vent counterparts has left the specific volcanic source of these marker layers still undetermined, leading authors to ascribe them either to unspecified Campanian volcanism (e.g., ), or to an undefined Neapolitan volcanic area (e.g., ) or to the so-called “Campanian Volcanic Zone” (CVZ; ) (e.g., ). Furthermore, in terms of tephrochronological applications, precise and accurate radioisotopic ages are currently available only for two of these markers (i.e., X-5 and X-6), and their full geochemical characterization (i.e., major, trace elements, and Sr Nd composition) in near-vent outcrops is still pending. Such remaining uncertainties on their origins and incompleteness of their geochronological and geochemical characterization, prevent their use for any volcanological purposes and limit their tephrochronological potential. In order to fill the knowledge gaps surrounding these tephra markers and to exploit their full potential for both volcanological and tephrochronological perspectives, we acquired stratigraphic, geochemical, and geochronological data for five medial-distal (30–60 km from the vent) pyroclastic units, preceding the Campanian Ignimbrite (CI) eruption, outcropping around the eastern rim of the Campanian Plain ( b). Four of these units (Maddaloni, Montemaoro, Cancello and Santa Lucia; b) were previously described ( ), while a fifth one (i.e., Triflisco) is recognised as a distinct, younger event in this study ( b). The new chemical, isotopic and geochronological data acquired in this study allowe us to confidently correlate the five medial-distal fall units to the widespread X-6, X-5, TM-24b/POP-2a, TM-24a/POP2 and C-22 marker tephra ( ), attributing them to Campi Flegrei explosive activity between 109–92 ka. Our findings thus extend back in time the explosive history of the Campi Flegrei volcanic field and provide new precise dating for refining the chronology of the millennial-scale climatic oscillations of the MIS 5c-d in the Mediterranean area. The Neapolitan volcanoes comprise of Campi Flegrei, Ischia and Procida islands and Somma-Vesuvius ( ). The Campi Flegrei volcanic field was the site of the most intense activity among the four Neapolitan volcanoes, as well as in the whole Mediterranean area. Three main eruptions occurred in this volcanic area, i.e., the Campanian Ignimbrite (CI; 39.85 ± 0.14 ka; ), the Masseria del Monte Tuff (MdMT; 29.3 ± 0.7 ka; ), and the Neapolitan Yellow Tuff (NYT; 14.5 ± 0.4 ka; ; ) caldera-forming eruption. Overall, while there is quite satisfactory knowledge on the activity occurred in-between and after these three main events, especially that following the NYT (e.g., ), the eruptive history preceding the CI is still poorly resolved, being documented only by deposits sporadically exposed outside the caldera and dated back to ~80 ka (e.g., ; ). Far from the Campi Flegrei volcanic area, several pyroclastic units documenting explosive activity in the Campania region can be dated as back as 290 ka ( ; ). This older activity is however referred to the so-called Campanian Volcanic Zone ( ), i.e., a hypothesized diffuse, regional volcanism not related to the present Campi Flegrei source area. Volcanic activity at Ischia Island, off the Gulf of Naples ( b), is documented as back as 150 ka, which is the age of the oldest exposed deposits, and up to historical times (e.g., ). The activity of Ischia is subdivided in five stages (i.e., >150–75 ka; 75–55 ka; 55–33 ka; 28–12 ka; 12 ka-1302 CE), characterised by different eruptive styles and types of products (e.g., ; ). The third stage of activity (55–33 ka) included several explosive events, following the largest 55 ka Monte Epomeo Green Tuff eruption (MEGT; ), recognised in the Mediterranean region as the Y-7 tephra marker horizon (e.g., ), although this attribution has been recently questioned ( ). The Island of Procida, located between Ischia Island and Campi Flegrei ( b), was active over a period of ~60 kyr, between ~80 ka and 23,624 ± 330 cal yr BP (e.g., ; ). Its activity originated from five eruptive centres, i.e., Vivara, Terra Murata, Pozzo Vecchio, Fiumicello, and Solchiaro ( ) and is documented by pyroclastic deposits interbedded with Campi Flegrei and Ischia units, which act as stratigraphic, chronological markers (e.g., ). The Somma-Vesuvius stratovolcano, east of the Naples metropolitan area ( b), has completely grown on the products of the CI eruption (e.g., ), and thus it is younger than 40 ka. Its activity is subdivided in three main stages: (i) the pre-Mercato eruption stage (ca. 35–9 ka), (ii) the stage between Mercato and the famous 79 CE Pompeii eruption (ca. 9 ka-79 CE) and (iii) the stage following the Pompeii eruption until present (i.e., last historical eruption of 1944 CE). These three stages differ from one another in terms of either the frequency of the related inter-Plinian eruptive episodes (e.g., ) or the degree of silica undersaturation, both increasing over the time ( ). For the present study, we used samples collected from 4 eruptive units out of the 14 pre-CI eruption deposits recognised by in the Campanian Plain ( b). They are, from bottom to top, SC2-a (hereafter Maddaloni), SC2-b (hereafter Montemaoro), CA1-a (hereafter Cancello), and Santa Lucia ( ; c). However, new analysis for the Montemaoro unit was not possible due to unavailability of the sample previously collected by and the inaccessibility of the outcrop during the new field investigations. The list of the investigated units is integrated with the fall deposit outcropping near the Triflisco village, at the edge of the Campanian Plain, here labelled Triflisco ( b-c). Moreover, the isotopic characterization has been performed also on samples CIL1 and CIL2 from the Cilento Coast being representative for the X-5 and X-6 stratigraphic markers ( ). The samples selected for the major and trace glass composition ( c; ) were wet sieved with tap water through a series of sieves with decreasing mesh openings. All fractions were successively oven-dried at 100 °C until completely dry. For major element analysis, selected fractions of 60–250 μm were mounted on 29 × 49 mm glass slides, embedded in epoxy resin, progressively ground to a thickness of 60–100 μm and finally polished to be analysed with the electron microprobe. For trace element analysis, selected samples were embedded in epoxy resin and successively polished. Glass shards and (micro-)pumice fragments were analysed by single-shard major element chemical analysis using the electron probe micro analyser (EPMA). Analysis was first performed with a Jeol JXA-850F equipped with five wave dispersive spectrometers (WDS), installed at the Institute of Petrology and Structural Geology, Charles University of Prague (Prague, Czech Republic). The machine operated at 15 kV accelerating voltage, 10 nA beam current and 10 μm defocused beam to limit alkali loss. Element counting times were of 20 s for all elements, except for Na, K, and S, for which counting times of 10 s (Na and K) and 30 s (S) were employed respectively. For all measurements, the F content was always below the detection limit of the machine. Standards for calibration were quartz (Si), corundum (Al), rutile (Ti), magnetite (Fe), periclase (Mg), rhodonite (Mn), albite (Na), sanidine (K), diopside (Ca), apatite (P and F), tugtupite (Cl) and anhydrite (S). The secondary standards GOR128-G ( ) and CFA47 ( ) were analysed at the beginning of each microprobe session for a total of one point each to evaluate analysis accuracy. Further WDS analyses were carried out at the Dipartimento di Scienze della Terra, Università degli Studi di Firenze (Florence, Italy), with a Jeol Superprobe JXA-8230 equipped with five-WDS spectrometers. Operating conditions were set to 15 kV accelerating voltage, 10 nA beam current and 10 μm defocused beam diameter to limit Na mobilisation. Element counting times were 15 s for all elements except for Na (10s), F (20s), S (30s), Mn, P and Cl (40s). Albite (Si and Na), ilmenite (Ti and Fe), plagioclase (Al), bustamite (Mn), olivine (Mg), diopside (Ca), sanidine (K), apatite (P), fluorite (F), tugtupite (Cl) and celestine (S) were used as internal standards. The accuracy of the measurements was assessed using the glass secondary standards GOR128-G, ATHO-G and StHs6/80-G ( ), Lipari ID3506 ( ), Scapolite NMNH, and CFA47 ( ). For both analytical facilities, the ZAF method was used for matrix effect correction. We adopted 93 wt% as a threshold for the measured totals. All compositional data are shown as oxide weight percentages (wt%) in the TAS and bi-plots diagrams, with total iron measured as FeO, and normalized to 100% on a volatile-free basis for correlation purposes. Collected data and secondary standards measurements are all reported in Supplementary Materials-1. Trace element analyses were conducted on volcanic glasses from the Triflisco, Santa Lucia, Cancello, and Maddaloni units. The analyses were performed using an Agilent 8900 triple quadrupole ICP-MS (ICP-QQQ) coupled to a Resonetics 193 nm ArF excimer laser-ablation device at the Department of Earth Sciences, Royal Holloway, University of London. Full analytical procedures used for volcanic glass analysis follow those reported in . Crater sizes of 20, 25 and 34 μm were used depending on the sample vesicularity and/or size of glass surfaces available for analysis. The repetition rate was 5 Hz, with a count time of 40 s on the sample, and 40 s on the gas blank to allow the subtraction of the background signal. Typically, blocks of eight glass shards and one MPI-DING reference glass were bracketed by the NIST612 glass adopted as the calibration standard. The internal standard applied was Si (determined by EMP-WDS analysis). In addition, MPI-DING reference glasses were used to monitor analytical accuracy ( ). LA-ICP-MS data reduction was performed in Microsoft Excel, as outlined in . Accuracies of LA-ICP-MS analyses of the MPI-DING reference glasses, ATHO-G and StHs6/80-G, were typically ≤5% for the majority of elements measured. Tephra and standard measurements are all provided in Supplementary Materials-1. Data averages reported in the text are accompanied by a ± 2 standard deviation (2 s.d.), whilst error bars in the plots are typically smaller than the data symbols. Sr/ Sr and Nd/ Nd isotope ratios have been determined on two samples from the Santa Lucia and one from Maddaloni units (i.e., ZS97–10 and ZS97–28, and ZS97–286 respectively) previously investigated by , on two samples from the Triflisco unit (i.e., TRIF 1/3 and TRIF 2/3) and on the CIL1 and CIL2 units from the Cilento Coast ( ; c; ). Measurements have been performed either on the glasses (pumices) and/or crystals (pyroxene or feldspar). Nd/ Nd measurement were performed on the glass fraction of samples from Triflisco, Santa Lucia (ZS97–10) and Maddaloni (ZS97–286) units. The different fractions were handpicked under a binocular microscope. Among all the available glass shards/pumices the most homogeneous in colour, and visibly poorly affected by secondary alteration, were selected for isotope analyses. Feldspar and pyroxene crystals were handpicked avoiding those characterised by the presence of glass rinds attached on their surfaces. Before chemical dissolution, glass shards/pumices were acid leached three to five times to reduce as much as possible the alteration effects. The leaching procedure was prolonged until the acid solution became light-yellow in colour. Leaching was carried out each time by placing the beakers containing samples and high purity 6 N HCl on a hot plate for 10 min. During each leaching step and after the final leaching, samples were rinsed with Milli-Q® H O. Feldspar and pyroxene were cleaned with Milli-Q® H O for 10 min. in an ultrasonic bath. Dissolution was obtained with high-purity HF–HNO –HCl mixtures. Sr and Nd were separated from the matrix through conventional ion-exchange procedures. Sr and Nd isotopic compositions were determined in a static mode by thermal ionisation mass spectrometry (TIMS) using a Thermo Finnigan Triton TI® mass spectrometer equipped with one fixed and six adjustable Faraday cups. Average 2σ mean, i.e., the standard error with = 180, was better than ±9 × 10 for Sr, and better than ±7 × 10 for Nd measurements. The mean measured values of Sr/ Sr for the NIST-SRM 987 standard and Nd/ Nd for the La Jolla standard were 0.710261 ± 0.000021 (2σ, = 169) and 0.511845 ± 0.000010 (2σ, = 55), respectively; external reproducibility (2σ) during the period of measurements was calculated according to . Measured Sr/ Sr ratios were normalized for within-run isotopic fractionation to Sr/ Sr = 0.1194, and Nd/ Nd = 0.7219. The final, measured isotope ratio values were normalized to the recommended values of the NIST SRM 987 ( Sr/ Sr = 0.71025) and La Jolla ( Nd/ Nd = 0.51185) standards, respectively. Chemistry processing and isotope analyses were performed at the Radiogenic Isotope Laboratory (RIL) of the Istituto Nazionale di Geofisica e Vulcanologia, Osservatorio Vesuviano, and the full analytical dataset is reported in Supplementary Materials-2. The Ar/ Ar ages were obtained at the Laboratoire des Sciences du Climat et de l'Environnement (CEA, CNRS UMR 8212, Gif-sur-Yvette, France) dating facility. Fresh and transparent K-rich feldspars were extracted from Triflisco, Santa Lucia, Cancello and Maddaloni samples. After being washed in distilled water, transparent K-feldspars (500–630 μm) without any visible inclusions were handpicked under a binocular and used for dating these four pyroclastic units. Between 20 and 30 crystals for each sample were irradiated in the Cd-lined, in core CLICIT facility of the Oregon State University TRIGA reactor for 2 h (IRR. CO-007) for Triflisco and 1 h in the same reactor (IRR. CO-009) for Santa Lucia, Cancello, and Maddaloni. Interference corrections were based on the nucleogenic production ratios given in . After irradiation, individual crystals for each tephra layer were transferred into a copper 133 pits sample holder placed into a differential vacuum Teledyne Cetac window connected to a home designed compact extraction line. Minerals were fused one by one using a 100 W Teledyne Cetac CO laser during 15 s at 2.5 W. Before fusion, each crystal underwent a 10s long sweeping at 0.3 W to remove unwanted gas potentially trapped on the crystals surface and fractures. Extracted gases were firstly purified by a SAES GP 50 cold getter for 90s and then for 230 s by two hot SAES GP 50 getters. The five Argon isotopes (i.e., Ar, Ar, Ar, Ar and Ar) were measured using a multicollector NGX 600 mass spectrometer equipped with 9 ATONA® amplifiers array and an electron multiplier. More technical specifications regarding the NGX 600 ATONA detector array are presented in detail in . Ar, Ar, Ar, and Ar isotopes were collected simultaneously while the Ar was measured in a second time. In the first run, Ar, Ar and Ar were measured simultaneously on 3 ATONA® amplifiers and Ar on the electron multiplier. Following this first run the Ar was measured alone using the electron multiplier. Each isotope measurement corresponds to 15 cycles of 20-s integration time. Peak intensity data were reduced using ArArCALC V2.4 ( ). Neutron fluence J factor was calculated using co-irradiated Alder Creek sanidine standard ACs-2 associated to an age of 1.1891 Ma ( ) according to the K total decay constant of (λ = (0.5757 ± 0.016) × 10 yr and λ = (4.9548 ± 0.013) × 10 yr )). To determine the neutron flux for each sample we used at least 6 flux monitor crystals coming from pits framing the samples in each irradiation disk. J-values are of 0.00056080 ± 0.00000062 (Triflisco [Base]); 0.00028350 ± 0.00000023 (Santa Lucia [ZS97–10); 0.00028340 ± 0.00000028 (Cancello [AS96–400]); 0.00028340 ± 0.00000020 (Maddaloni [ZS97–286]). To verify the detectors linearity, mass discrimination was monitored by analysis of at least 60 air shots of various beam sizes ranging from 5.0 10 up to 2.0 10 V (1 to 4 air shots). About 15 air shots analyses are performed every day. These measurements are done automatically during the nights before and after the unknown measurements. Discrimination is calculated according the Ar/ Ar ratio of 298.56 ( ). Procedural blank measurements were achieved after every two to three unknowns. For typical 5 min time blank backgrounds are between 2.5 and 4.0 10 V for Ar and 60 to 90 cps for Ar (about 1.0–1.3 10 V equivalent). Full analytical data for each sample can be found in Supplementary Materials-3. Most samples investigated in this study refer to the pyroclastic units already described in , to which the reader is referred for the lithostratigraphic details. They generally consist in dm-thick fallout deposits made up of either pumice lapilli or coarse ash ( c). At the site “Schiava” (SC2 in b) all the four previously investigated units, i.e., Maddaloni, Montemaoro, Cancello and Santa Lucia, occur as distinct eruptive units separated by either paleosols, epiclastic deposits or unconformity bounding surfaces. The newly recognised Triflisco unit ( b), consists in an 80 cm-thick fallout deposit made up of moderately sorted, white-pinkish and well-vesicular pumice lapilli (max Φ 3 cm) with intervening coarse ash layers. Accidental lithics are scant ( c). The Triflisco unit overlies a paleosol and at the top, in turn, it is capped by a thick reddish paleosol on which lays a greyish pyroclastic flow deposit that we attribute to the CI ( c). Thus, our interpretation differs from previous ones that correlated the pumice fall at this locality to the CI Plinian fall exposed elsewhere ( ; ). All samples analysed in this study have a dominant composition overlapping the boundary between phonolite and trachyte fields ( a ) of the vs (TAS, ) classification diagram. Mean compositions are reported at 2 σ (2 standard deviation) error. It is made up by four sub-units ( c) with a relatively homogeneous composition. The majority of the data straddle the boundary between trachyte and phonolite fields (SiO content of 59.5 ± 0.9 wt%, and alkali sum of 12.8 ± 1.0 wt%), depicting a trend within the trachyte field with decreasing alkali negatively correlated with a small increase in silica; a-b). The CaO/FeO values are <1 (0.8 ± 0.1) and the Cl content is 0.6 ± 0.1 wt% for all sub-units ( c). Glasses display a High Alkali Ratio (HAR), with K O/Na O generally >2 and up to 3.08 ( d). There is no appreciable chemical variation from the lowermost (Triflisco Base) to the uppermost (Triflisco Top) sub-units. The glass of this unit, analysed in six samples ( c; ), is characterised by the most heterogeneous composition among those analysed, although mainly phonolitic, with a mean SiO content of 59.0 ± 2 wt%, and an alkali sum of 12.8 ± 1 wt% ( a-b). The CaO/FeO ratio is ≤ 1 (mean of 0.9 ± 0.1) and the Cl content is medium-high (0.6 ± 0.1 wt%; c). Santa Lucia glasses display a HAR typically ≥2, with a mean K O/Na O ratio of 2.1 ± 0.5 ( d). Glasses from this unit are phonolitic-trachytic in composition, with a SiO content of 60.9 ± 1.3 wt%, a mean alkali sum of 13.1 ± 0.9 wt% ( a-b) and a HAR of 2.2 ± 0.6 ( d). The CaO/FeO ratio ranges between 0.7 and 1.0 and the Cl content between 0.5 and 0.7 wt% ( c). As stated in the previous section, it was not possible to acquire new data for this unit. Thus, for the purpose of this study, we rely on the available glass-EDS data from . The Montemaoro unit is mainly trachytic in composition with some points straddling the boundary with the phonolite field, with silica and alkali sum content of c.a. 61 wt% and 13 wt%, respectively ( a-b). The glasses display a HAR, up to 2.5, with a CaO/FeO ratio of c.a. 0.7 ( d), while Cl content was not determined. The glass of this unit, collected from the former SC2 and SA3 sections ( ; ), is characterised by a homogeneous SiO content (mean 61.6 ± 0.5 wt%), with an alkali sum of 13.7 ± 0.6 wt% ( a-b). Respect to all the above-mentioned samples, Maddaloni glasses predominantly display a Low Alkali Ratio (LAR), with K O/Na O typically ≤1.5 (0.9 ± 0.1; d), due to an almost equal content of K O and Na O of ca. 7 wt%. In addition to a lower K O/Na O ratio, with respect to the other analysed units, the glass of the Maddaloni pumices has noticeably lower CaO/FeO ratios (0.5 ± 0.1), whilst the Cl content is appreciably higher, up to 1.1 wt% ( c). The Triflisco eruption unit contains HAR glasses that are fairly homogeneous in terms of their incompatible trace element contents (e.g., Th = 29.9 ± 2.9 ppm [ e-f]; Nb = 59.2 ± 4.0 ppm [ e]; Zr = 327 ± 31 ppm [ f]) and ratios of other High Field Strength elements (HFSE) to Th remaining constant (e.g., Nb/Th = 2.0 ± 0.1; Zr/Th = 10.9 ± 0.5). Light Rare Earth Elements (LREE) are enriched relatively to the Heavy Rare Earth Elements (HREE) with La/Yb = 27.9 ± 3.7. The HAR Santa Lucia eruption products in the SC2, CA1 and SA1 sections (see ) are compositionally consistent and fairly homogeneous (e.g., Th = 27.1 ± 3.1 ppm; Nb = 55.8 ± 4.5 ppm [ e]; Zr = 306 ± 29 ppm [ f]; Rb = 307 ± 25 ppm; La = 77.9 ± 6.0 ppm) with constant HFSE/Th ratios (Nb/Th = 2.1 ± 0.1; Zr/Th = 11.3 ± 0.5) and displaying LREE enrichment relatively to HREE (La/Yb = 27.7 ± 3.1). The Cancello unit displays HAR glasses that are fairly homogeneous in composition (e.g., Th = 26.7 ± 3.0 ppm; Nb = 56.1 ± 7.0 ppm [ e]; Zr = 316 ± 34 ppm [ f]; La = 81.4 ± 8.3 ppm), with minor variation relating to a single less enriched analysis. HFSE to Th ratios remain constant within the Cancello glasses (Nb/Th = 2.1 ± 0.2; Zr/Th = 11.8 ± 0.4), and LREE are enriched relative to the HREE where La/Yb = 27.6 ± 3.4. The LAR Maddaloni tephra shows variable incompatible trace element glasses compositions (e.g., Th = 84–114 ppm; Nb = 169–231 ppm [ e]; Zr = 1037–1319 ppm [ f]) and displays far greater levels of enrichment relative to the above mentioned HAR units (i.e., Triflisco, Santa Lucia and Cancello; e-f). HFSE/Th values remain constant within these glasses (Nb/Th = 2.1 ± 0.1; Zr/Th = 11.8 ± 0.4) and are entirely consistent with the HAR samples from Triflisco, Santa Lucia and Cancello deposits ( e-f). Whilst the Nd/ Nd isotopic ratios are homogeneous within the analytical error (c.a. 0.51250), the Sr/ Sr ratios ( a ) range from 0.70687 and 0.70780 (glass from sample ZS97–286). The highest value is possibly due to post-depositional alteration of the glass as suggested by the Sr isotope composition of the embedded feldspar. Samples from Triflisco and Santa Lucia display similar and lower Sr isotope composition (c.a. 0.7069) with respect to sample from Maddaloni unit. CIL1 and CIL2 are characterised by Sr isotope ratios (from ca. 0.7071 to 0.7072) similar to that of the Maddaloni feldspar (from c.a. 0.7071). displays the variations in terms of Sr Nd isotope ratios compared with literature data. All Ar/ Ar results for individual tephra layers are presented as probability diagrams ( ). Weighted mean age uncertainties are all reported at 2σ, including J uncertainty and were calculated using Isoplot 4.0 ( ). For each sample, inverse isochrones have an atmospheric Ar/ Ar initial intercept with uncertainties suggesting that dated crystals are without detectable excess argon. Full inverse isochrones dataset can be found in Supplementary Materials-3. 13 single crystals were individually dated. Eleven out of thirteen crystals analysed gave a similar age within uncertainties ( a). The two other older crystals, one sanidine (~104 ka, red bar in ) and one plagioclase (~404 ka, not shown in ; see Supplementary Materials-3) according to their Ca/K ratio, are interpreted as xenocrysts. The main population of crystal interpreted as juvenile allows to calculate a weighted mean age of 91.8 ± 1.2 ka (MSWD = 1.20, = 0.27). A total of 14 individual sanidine crystals were dated. The probability diagram is simple ( b) with one mode allowing to calculate a meaningful and precise weighted mean age of 101.2 ± 0.8 ka (MSWD = 0.59, = 0.87). The probability diagram displays one single mode with no xenocryst contamination ( c). These crystals are interpreted as juvenile ones (12 crystals), allowing to calculate a precise weighted mean age of 102.5 ± 0.8 ka (MSWD = 0.58, = 0.85). 14 crystals were dated individually. All gave within uncertainty the same age resulting in a gaussian probability diagram ( d). Using this very homogenous crystal population we calculated a weighted mean age of 109.3 ± 1.0 ka (MSWD = 0.43, = 0.96). While for Maddaloni we obtained the Ar/ Ar age of 109.3 ± 1.0 ka, the overlying Montemaoro unit, which was not resampled or re-analysed in this work, was not dated. All the investigated pyroclastic fall units occur in a range of 30–35 km to 55–60 km in the eastern quadrants from the Neapolitan volcanoes, including Campi Flegrei, Ischia, Procida and Somma-Vesuvius ( b). Somma-Vesuvius can be reasonably excluded as a potential source since its oldest known activity is younger than Campanian Ignimbrite (i.e., < 40 ka; ) and thus incompatible with the 109–92 ka chronology obtained here for the Campanian Plain units. Also, in terms of glass chemical composition, the Somma-Vesuvius products appear incompatible due to the higher alkali sum at similar SiO content ( a-b), and the significantly higher CaO/FeO at same Cl content and alkali ratio ( c-d). Procida island can also be excluded based on the Sr Nd isotope compositions, which are clearly different from those of the Campanian Plain units ( a-b). Among the two remaining potential sources for the investigated units (i.e., Campi Flegrei and Ischia), based on the lithostratigraphic and geochemical characteristics, as already argued by , the Campi Flegrei volcanic area is the most probable. In terms of major element glass composition, although Campi Flegrei and Ischia products partially overlap, each of the two volcanic sources shows distinctive features in terms of oxide concentrations and ratios ( a-d). This applies to four out of the five investigated units (i.e., Triflisco, Santa Lucia, Cancello, and Montemaoro), which unambiguously plot in the compositional field of the Campi Flegrei glass because of the higher K O/Na O and CaO/FeO values with respect to the Ischia products ( b-c). Trace elements glass compositions also support the attribution of the Triflisco, Santa Lucia and Cancello units to the Campi Flegrei, for instance all these units show enrichment in Zr that is diagnostic of the Campi Flegrei products and is slightly lower than that of the products typically erupted at Ischia ( f). Owing to its distinctive K O/Na O and CaO/FeO values, which are lower than those of the most common Campi Flegrei products ( c-d), the source attribution of the Maddaloni unit is not so straightforward. Indeed, CaO/FeO and K O/Na O ratios of Maddaloni unit partly overlap with those of Ischia ( b-c). However, using the CaO/FeO vs. Cl diagram, the glass composition of Maddaloni unit falls out of the Ischia field and within the Campi Flegrei one, though, sporadically, the Campi Flegrei compositions can overrun the typical Ischia one ( c). Indeed, such chemical characteristics, i.e., LAR trachyte-phonolite with relatively low CaO/FeO ratio, are also found in Campi Flegrei products (e.g., ), notably in the CI ( ) and some minor Campi Flegrei eruptions following the NYT caldera-forming eruption (e.g., Averno 2, Fondi di Baia, Monte Nuovo; ). Likewise, the Maddaloni unit can be attributed to the Campi Flegrei and ascribed to this less common, LHR trachyte-phonolite compositional group of this volcanic area. Incompatible trace element enrichment of the Maddaloni glasses exceeds that currently recognised in the known products of Campi Flegrei and Ischia ( e-f) making their use again less conclusive. However, the lower Zr/Th ratios observed in the Maddaloni glasses are seemingly more akin to those of Campi Flegrei, rather than to the higher values typically observed in the eruptive products of Ischia spanning a period of intense explosive volcanism at ~40–80 ka ( ). More convincing, and seemingly definitive, evidence to confirm Campi Flegrei as the source for the Maddaloni unit, is provided by isotope data. Indeed, the Sr- and Nd-isotope compositions for the Maddaloni samples are positioned well within the field of the Campi Flegrei ( a-b). In summary, the volcanological and sedimentological constraints, the acquired major and trace elements glass composition, the geochronological and Sr- and Nd-isotope data consistently indicate that Campi Flegrei is the most probable source for all the five investigated Campanian Plain units. This significantly extends back in time the known explosive activity of this volcanic field, previously documented only up to ca. 80 ka ( ), or as far back as 290 ka (CVZ; ; ). Regardless the precise vent location, our data point to a frequent activity that took place within the Campi Flegrei volcanic area. Specifically, we recognised five eruptions that, based on their lithological features in medial settings, can be likely considered of Plinian intensity and magnitude. These occurred across approximately a 17 kyr time-window, with recurrence times of a few thousands of years and in one case are barely more than 1 kyr (e.g., time elapsed between the 102.5 ± 0.8 ka Cancello and the 101.2 ± 0.8 ka Santa Lucia eruptions). Consequently, the period of 109–92 ka was characterised by a high frequency of moderate to large explosive eruptions, i.e., an eruptive behaviour that has not been recognised within the more recent (i.e., post-CI) activity of the Campi Flegrei volcano. To correlate the investigated units of the Campanian Plain with MIS 5 Mediterranean tephra markers, we refer to the Central Mediterranean tephrostratigraphic successions spanning this interval that (i) record in stratigraphic order most MIS 5 tephra markers, (ii) have a good geochemical characterization of all tephra, (iii) have a good radioisotopic or stratigraphic chronology, and (iv) have a good expression of the MIS 5 climate variability, which enable a reliable assessment of the tephra climato-stratigraphic position. These requisites are fully or partially met by (i) the rich tephrostratigraphic record of the Lago Grande di Monticchio, southern Italy ( a), located ca. 120 km east of the Neapolitan volcanoes – thus in an ideal position for recording their explosive activity ( ) – and (ii) the Popoli MIS 5 succession, in Sulmona Basin ( a), where MIS 5 tephra were dated by Ar/ Ar method ( ; ), allowing a direct, unambiguous comparison with the Ar/ Ar chronology here obtained for the investigated Campanian Plain units. Both Ar/ Ar chronology and major elements ( a ) glass composition of Triflisco unit (91.8 ± 1.4 ka) are fully consistent with those of the Sulmona tephra POP1 (92.1 ± 4.6 ka; ) that, in turn, was correlated to the Monticchio tephra TM-23-11 ( ; a), dated at 95.18 ± 4.76 ka ( ). POP1/TM-23-11 was also correlated to the widespread C-22 tephra marker ( ) of the Tyrrhenian Sea tephra series ( ). The correlation of Triflisco with POP1//TM-23-11/C-22 is supported also by incompatible trace element contents plotted against Th ( a ). Furthermore, Sr/ Sr isotope ratios of Triflisco perfectly match literature values for the POP1/TM-23-11/C-22 tephra layers/markers ( a), strengthening this correlation. The POP1/TM-23-11/C-22 was also identified in Fucino succession, as layer TF-10 ( ), and in San Gregorio Magno basin, as layer S14, ( ; ; a). The C-22 was also correlated to the 11 cm- thick tephra layer CET1–10/14 in the Tyrrhenian core CET-1 ( ). This marker was also identified in the marine core PRAD 1–2, in the Adriatic Sea, as layer PRAD-2517 ( ; ). Finally, in the peatland succession of Tenaghi Philippon, in Greece ( a, ), recent cryptotephra investigations by allowed the correlation of the C-22 marker with tephra layer TP05–25.195 ( a). Both major ( b) and trace ( a) element compositions, as well as the chronology of Santa Lucia unit, are compatible with TM-24a tephra of Monticchio, to which the Sulmona tephra POP2 was also correlated ( ; ). The Monticchio varve-supported age for TM-24a is 102.0 ± 5.7 ka ( ; Monticchio chronology MON-2014, Sabine Wulf, personal communication 2017), whereas the modelled age of POP2 is 102.0 ± 2.4 ka ( ). In Sulmona paleo-hydrological record, POP2 falls in the early stage of a period of increasing precipitation correlated to the Greenland Interstadial 23, which in reference records (e.g., Corchia Cave, ; ) starts about 102–103 ka, thus in agreement with the estimated age of POP2/TM-24a. Here Santa Lucia unit is precisely Ar/ Ar dated at 101.2 ± 0.8 ka ( b), supporting this correlation. Sr/ Sr isotope ratios determined on Santa Lucia samples show values similar to those obtained for Triflisco/C-22 tephra layer/marker ( a), but they can be discriminated based on the higher HFSE (e.g., Th, U) contents of the Triflisco/C-22 glasses than the Santa Lucia ones, thus preventing erroneous correlations ( a). At Fucino Basin ( a, ), tephra layer TF-11, located immediately below tephra layer TF-10/C-22, was correlated by to the Monticchio tephra POP2/TM-24a ( ). The same marker horizon was also recognised in Lake Ohrid (North Macedonia-Albania; Figs, 1a, 7) as layer OH-DP-0404 ( ). The crypto-tephra CET1 12–13-14 in the Tyrrhenian core CET1 ( a) also has major element composition compatible with TM-24a/POP2 ( ), thus representing the unique so far known occurrence of Santa Lucia unit in the marine realm. Finally, at San Gregorio Magno ( ; ), tephra layer S13, underlying tephra layer S14/C-22, would be stratigraphically well suited to be a potential candidate for the TM-24a/Santa Lucia unit ( ). However, EDS-glass chemical composition supports only partially this correlation ( b), and WDS-glass composition should be acquired on purpose. The Cancello pumice fall occurs below Santa Lucia unit = TM-24a/POP2 ( ; b), thus indicating POP2a ( ) and TM-24b ( ), which respectively underlie POP2 and TM-24a, as the best candidates for correlation to this unit ( ). Major element biplots confirm this correlation ( c), also supported by trace element comparison ( a). In particular, with respect to the Santa Lucia unit, the Cancello unit appears to extend to lower Th contents ( f, a), which allows discriminating these two tephra markers. At Monticchio, TM-24b is varve-dated at 103.1 ± 5.7 ka ( ; MON-2014), while in Sulmona Basin a modelled age of 103.3 ± 1.4 ka was obtained for POP2a ( ), in agreement with the more precise age of 102.5 ± 0.8 ka here measured for the Cancello unit ( c). In summary, the correlation of Cancello unit to POP2a/TM-24b is fully supported by all the geochemical and geochronological data. At San Gregorio Magno ( ; ), the tephra layer S12 is stratigraphically well suited for being a good correlation candidate for POP2a/TM-24b ( ). The geochemical correlation of S12 with Cancello unit, based on the available EDS-glass composition, is quite convincing ( c), but more compelling WDS data would be required. Finally, the crypto-tephra CET1–15 in the Tyrrhenian core CET1 ( a) is stratigraphically ( ) and compositionally consistent with TM-24b/POP2b ( ). At Masseria Montemaoro quarry (site Schiava - SC2; b and c), these two, stratigraphically superimposed units underlie the Cancello unit ( c). The Ar/ Ar age of 109.3 ± 1.0 ka determined for Maddaloni unit is virtually indistinguishable from that of 109.1 ± 0.8 ka, obtained for the Sulmona tephra layer POP4 ( ), which was correlated to the X-6 tephra marker ( ), corresponding to the Monticchio TM-27 ( ). Consistent with this chronological information, the major ( d) and trace ( b) elements composition of the Maddaloni unit well match the most evolved term of POP4/TM-27 tephra layer/marker and those of the other X-6 equivalent layers in distal archives through the central Mediterranean area ( a). Sr Nd isotope values obtained from the Maddaloni unit are consistent with those of CIL-2/X-6 tephra layer/marker from the Cilento coast and other X-6 occurrences ( a-b), thus further supporting its correlation with the X-6 marker. Moreover, major and trace elements glass compositions are extremely distinctive (e.g., different alkali ratio and Cl content, incompatible trace element patterns), and we thus highly recommend the employment of these geochemical tracers as a correlation tool for the X-6 tephra ( d, b; Supplementary Materials-1). All geochemical and chronological data thus corroborate Maddaloni as the most proximal equivalent of the X-6 tephra marker. The Ionian X-6 tephra marker ( ), with its other marine and terrestrial equivalents, is the most widespread MIS 5 Campi Flegrei tephra ( a, ), while considering the whole Campi Flegrei record, in terms of dispersal area it is second only to the CI (e.g., ). It has been recognised in a series of sedimentary successions in the central Mediterranean area ( a, ), including the Ionian Sea (I-9, ), Tyrrhenian Sea (C-31, ), Adriatic Sea (PRAD-2812, ), Fucino Basin (TF-13, ), Cilento Coast ( ; ), San Gregorio Magno Basin (S10, ), Valle del Crati (Tarsia, ), Grotta del Cavallo Palaeolithic site (Unit G, ), Lake Ohrid (OH-DP-0435, ), and Tenaghi Philippon (TP05–27.915, ). Regarding Montemaoro unit, its stratigraphic position between Maddaloni/X-6 and Cancello/TM-24b, which chronologically constrains it between ~109 ka and ~ 103 ka, makes it the best candidate for the terrestrial counterpart of the Ionian Sea marker X-5, which lays immediately above the X-6 ( ). The X-5 is equivalent to tephra layer TM-25 in the Lago Grande di Monticchio succession, dated at 105.6 ± 0.5 ka (recalculated with ACs at 1.1891 Ma; Tyrrhenian Sea), or POP3 Ar/ Ar dated at 105.8 ± 1.3 ka (Sulmona Basin; ). Although it was not possible to acquire new major (glass-WDS) and/or trace element data, the available glass-EDS composition for the Montemaoro unit supports this correlation ( d). Consequently, we suggest that the Montemaoro unit is the most likely proximal counterpart of the X-5, based on the chronological and stratigraphical constraints provided in this study and the existing chemical data. Future discovery of a new field exposure of Montemaoro would allow further verification of this correlation. Although less dispersed, the X-5 marker, like the X-6, was reported in several stratigraphic successions ( a, ), including the Fucino Basin (TF-12, ), Cilento Coast (CIL1, ; LeS1, ), San Gregorio Magno Basin (S11, ), and as the lowermost tephra layer in CET1 core (i.e., CET1–18), Ar/ Ar dated at 105.18 ± 0.5 ka (2σ analytical uncertainty). A potential correlative for either the X-6 or X-5 marker was also recognised at the remote site of marine core LC21 ( ), in the Aegean Sea ( a). Specifically, the crypto-tephra LC21–7.915, whose base was dated at 104.1 ± 2.2 ka, according to the LC21 age model ( ), presents two geochemical compositions indicating both Santorini and Campanian sources. The Campanian component is in turn represented by two glass HAR and LAR trachyte-phonolite populations, which are compatible with X-5 and X-6 compositions, respectively ( d). Moreover, in terms of trace element composition, the few available data appear quite consistent with both markers ( b). However, both the age and climatostratigraphic position of the crypto-tephra LC21–7.915 make the X-5 the most probable correlative. Indeed, LC21–7.915 precisely marks the onset of the sapropel S4 deposition, at the base of which in the Tyrrhenian Sea records the tephra X-5/C-27 is also found ( ; ; ). Therefore, we are inclined to consider the X-5 as the most likely correlative for the HAR component of the Campanian portion of LC21–7.915 crypto-tephra, though the co-presence of the LAR component would require a plausible explanation. Overall, the correlations of the investigated Campanian Plain units with the five distal tephra markers are well supported by several lines of consistent, independent evidence, including their stratigraphic order, geochronology, and geochemistry (major and trace elements, and the Sr Nd isotopes). However, in some cases, the geochemical variability of the investigated Campanian Plain units is less wide than the corresponding distal tephra. This is especially true for Cancello and Maddaloni units, whose composition covers only a part of the wider variability observed in distal settings ( c-d, b). This is not surprising, as the occurrence of the analysed medial-distal units is relatively scant with respect to the distal ones, and thus could be not representative of the complete eruptive sequence and geochemical variability. This would suggest that not all the eruptive phases or sub-units, e.g., pyroclastic flow or fall, of Cancello and Maddaloni units reached the distance of 30–40 km, at which the investigated sections are located ( b) or had dispersal axes not compatible with pumice deposition in these localities. The great relevance of some of the investigated tephra layers as fundamental chronological and stratigraphic markers for the Mediterranean MIS 5c-d high frequency climatic variability was widely acknowledged and discussed in previous papers (e.g., ; ). However, the acquisition of new high-precision Ar/ Ar ages for two previously undated tephra (Cancello/TM-24b and Santa Lucia/TM-24a tephra layers/markers), the substantial improvement in accuracy of the Triflisco/C-22 unit, as well as the acquisition of new high-resolution records containing these tephra markers (e.g., Tenaghi Philippon; ), give us the opportunity to discuss the implications of these new data from a palaeoclimatological perspective, in particular on the timing and spatial synchronicity of MIS 5c-d climatic variability. For this purpose, we consider the records endowed with suitable resolution and good expression of the millennial scale climate oscillations of the MIS 5d-c, which can be reasonably correlated to the succession of stadial and interstadial events documented in the reference record of the Greenland ice (e.g., ). These are (i) the Lago Grande di Monticchio pollen profile, Southern Italy (e.g., ; ), (ii) the isotope series of Sulmona Basin, Central Italy ( ), (iii) the Tenaghi Philippon pollen record, in Greece ( ), and (iv) the pollen record of Lake Ohrid, North Macedonia-Albania ( ; a, ). For the sapropel stratigraphy, we also consider the Tyrrhenian Sea record of the core KET 8004 ( ), which contains three out of the five markers ( ), and the Aegean Sea core LC21, likely containing the X-5 layer ( ; ). So far, Monticchio and Sulmona are the only Mediterranean records containing all the five MIS 5d-c tephra from Campi Flegrei ( ), whereas Tenaghi Philippon and Ohrid contain only two markers, i.e., Maddaloni/X-6 and Triflisco/C-22, and Maddaloni/X-6 and Santa Lucia/TM-24a, respectively ( ; ). The Maddaloni/X-6 unit, the most common tephra in the considered records ( ), occurs at the very end of a short interstadial pulsation, likely corresponding to Greenland Interstadial (GI) 25 . It precedes the onset of the first marked stadial oscillation of the MIS 5 period, the Greenland Stadial (GS) 25 , corresponding to the North Atlantic cold event C24 (e.g., ; ). The temporal offset, i.e., the difference between the radioisotopic Ar/ Ar age of Maddaloni/X-6 tephra and the age reported in the various paleoclimatic records (Δt in ), is small, reaching the maximum value of ca. 1 kyr in the Monticchio record ( ; ). However, assuming that the inferred position of Maddaloni/X-6 tephra layer/marker in the Greenland isotope record is correct, then, the age of the end of the GI-25 , 110.6 ka, according to GICC05 ( ), should be approximately 1.3 kyr younger ( ). On the contrary, Sardinian stalagmite evidence suggests instead that the GI-25 ended at 110.5 ka ( ), which is fully consistent with the GICC05 chronology. The Montemaoro/X-5 tephra occurs in the middle of an interstadial oscillation correlated to the GI-24 (e.g., ). More precisely, Montemaoro/X-5 tephra occurs close to a very brief stadial pulsation within the GI-24 that is quite evident in all the considered records and that likely corresponds to the short GS-24.2 ( ). The Δt, relative to the Montemaoro/X-5 tephra, is of ca. 1 kyr in all records, except Monticchio, in which it is negligible ( ; ). In the Tyrrhenian Sea, and likely in the Aegean Sea, the Montemaoro/X-5 also represents an excellent marker for the Sapropel S4, which is in turn correlated to the GI-24.1 ( ; ). The Cancello and Santa Lucia tephra layers form an interesting couplet of temporally closely related tephra, which mark the onset of an interstadial and the ensuing stadial phase, likely corresponding to the GI-23.2 and the GS-23.2, respectively ( ). In all the considered records, the negligible Δt relative to this couple of tephra evidences a good agreement between the Ar/ Ar chronology and the age models of the records ( ). Finally, the Triflisco/C-22 tephra occurs at the beginning of a stadial event that interrupts a relatively long interstadial period, likely corresponding to the GS-23.1, which occurs at the end of the long-term cooling period featuring the GI-23.1 ( ). Noteworthy, the Δt relative to this tephra is quite long for the Monticchio and Tenaghi Philippon pollen records, reaching the considerable value of ca. 4 kyr ( ). The Δt is instead quite negligible for the NorthGRIP record (~1.2 kyr), provided that the inferred climatostratigraphic position of Triflisco/C-22 in the Greenland record is correct. In summary, the Δt is relatively little for most of the climatic events, for which the Campi Flegrei MIS 5 tephra act as fundamental markers, and generally does not exceed the uncertainty associated to both Ar/ Ar ages and the age models of the respective paleoclimatic records ( ). However, the event associated with the Triflisco/C-22 represents a notable exception, for which the Δt can exceed the uncertainty associated to the Ar/ Ar dating of Triflisco/C-22 and of the paleoclimatic record age models ( ; ). With this regard, we emphasize that such a wide Δt cannot be explained invoking an uncertainty of the tephra position within the records, as, at least for Monticchio and Tenaghi Philippon, where the Δt is −3.6 ± 4.0 kyr and − 4.0 ± 3.0 kyr ( ), it is not relayed on an inference, this being based on undisputable stratigraphic evidence ( ). Overall, the investigated tephra can be considered good stratigraphic markers of some of the stadial-interstadial events, as well as of the very short sub-stadial and sub-interstadial oscillations that punctuated the MIS 5c-d climatic variability ( ), whose chronology can greatly benefit from the high precision Ar/ Ar dating of the Campi Flegrei eruption products. In this study, we presented an extensive dataset (i.e., stratigraphy, major, minor, and trace elements, Sr Nd isotopic composition and Ar/ Ar ages) required for a full characterization of four pumice fall deposits, named Triflisco, Santa Lucia, Cancello, and Maddaloni, occurring in the Campanian Plain, 30 to 60 km east of the Campi Flegrei volcanic field, and stratigraphically positioned below the CI (~40 ka). Based on these data, these units are here attributed to a previously unknown 109–92 ka explosive activity at Campi Flegrei volcano and correlated with the widespread C-22, TM-24a/POP-2, TM-24b/POP-2a and X-6 tephra markers, respectively. Furthermore, the chronological and stratigraphic constraints provided in this study, and a review of previous EDS data, allow us also to propose the correlation of a fifth unit (i.e., Montemaoro) with the X-5 marine tephra marker as well. Our data confidently allow us to trace the volcanic source of these fundamental Mediterranean marker horizons, so far only hypothesized. This extends the activity history of the Campi Flegrei volcano back ~110 ka at least, setting the groundwork for a reassessment of the volcanic history, and related hazards, and confirming the Campi Flegrei volcano as one of the Europe's most productive sources of widespread and disruptive ash fall events. The Maddaloni/X-6, given its wide dispersal area, clearly arises from one of the largest explosive events through the whole Campi Flegrei eruptive history and demands further volcanological investigations. The new high-precision Ar/ Ar dating of the investigated units provide new fundamental temporal constraints for refining and consolidating the chronology of MIS 5d-c period, characterised by a marked millennial- to sub-millennial scale climatic variability, well documented in Mediterranean archives. Specifically, the ages obtained for the Cancello and Santa Lucia tephra markers provide two new chronological constraints for the climatic oscillations likely corresponding to the Greenland interstadial GI-23.2 and Greenland stadial GS-23.2. Furthermore, the improved precision of the age for Triflisco would imply a substantial extension of the duration of the interstadial period corresponding to the GI-23.1 up to 92 ka, i.e., ~4 kyr later than the ~96 ka age reported for the GI-23.1 in several reference Mediterranean records. ( ). A reappraisal of the related chronologies is thus required. Future research development on the eruptive history and long-term hazard assessment at Campi Flegrei, as well as paleoclimatic and archaeological investigations in central Mediterranean, will greatly benefit from the geochemical and geochronological dataset provided in this contribution. Given the strong geochemical similarities of the HAR Triflisco/C-22, Santa Lucia/TM-24a, and Cancello/TM-24b units, we encourage caution to avoid potentially misleading correlations based solely on major elements. Therefore, especially when a specific tephra layer occurrence lacks bracketing tephra units, we recommend integrating major element data with trace element and Sr Nd isotope analysis, as they proved to be the best discriminating tool for these tephra deposits.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Used\n",
      "* **AIモデルの判断:** Not Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 59 / DOI: 10.1016/j.str.2024.10.016  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "A catalogue of 863 Rett-syndrome-causing MECP2 mutations and lessons learned from data integration\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "LEDGF interacts with the NID domain of MeCP2 and modulates MeCP2 condensates\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "The methyl-CpG-binding protein 2 (MeCP2) is a ubiquitously expressed nuclear protein that is involved in transcriptional regulation and chromatin remodeling. Differential splicing results in two isoforms, MeCP2 E1 and MeCP2 E2 that differ in function and expression profile ( A and A). MeCP2 E1 is encoded by exon 1, 3, and 4, while MeCP2 E2 is encoded by exon 2, 3, and 4, resulting in a 21 amino acid difference at the start of the protein. Both isoforms share the same functional domains, yet they differ in their N-terminal domain (NTD). The NTD is followed by a methyl-binding domain (MBD) that binds 5-methyl cytosine (5mC) and 5-hydroxymethyl cytosines (5hmC), an intervening domain (ID), and a transcriptional repression domain (TRD). Within the TRD, at the C terminus, the NCoR interaction domain (NID) functions as a recruitment platform for MeCP2’s multiple binding partners. Additionally, MeCP2 contains three AT-hook domains, in the ID, TRD, and C-terminal domain (CTD) that can simultaneously bind DNA independently of its methylation status. While the two isoforms are abundantly expressed in the central nervous system, different expression levels and distributions are found in developing and post-natal mouse brains. MeCP2 E1 is the major protein isoform, with a relatively uniform expression across the brain. MeCP2 E2, on the other hand, displays a later expression onset during development and shows differential enrichment across different brain regions. When looking at the amino acid sequence, the two isoforms only differ in the NTD ( A), which determines the turn-over rates of MeCP2 as well as the ability of the MBD to interact with DNA. The spatial and temporal difference in MeCP2 isoform expression suggests distinct roles for the two isoforms. Proper functioning of MeCP2 is essential for normal development and function of the nervous system. MeCP2 is an epigenetic reader for DNA methylation marks and binding of MeCP2 to methylated DNA either silences or promotes gene expression, depending on its interaction partners. MeCP2 is also involved in the modification of chromatin structure through the binding of methylated histone H3K9 and H3K27 marks. By recruiting chromatin-modifying proteins, also known as “writers”, such as histone deacetylases and methyltransferases, binding of MeCP2 to nucleosomes induces compaction of chromatin. Loss-of-function mutations in the MeCP2 gene are the main cause of Rett syndrome (RTT), a neurodevelopmental disorder that affects 1 in 10,000–15,000 females. MeCP2 duplication syndrome (MDS) is the mirror disease of RTT. MDS is caused by a duplication of the MeCP2 gene, leading to an overexpression of MeCP2. Most MDS patients are boys, affecting 1 in 150,000 births. Currently, there is no cure for RTT or MDS, and treatment consists of symptomatic therapies to slow down the progression of the disease and improve the life quality of the patients. A study of the interactome of MeCP2 in the mouse brain revealed that MeCP2 directly interacts with lens epithelium derived growth factor (LEDGF), another important regulator of gene transcription. This finding confirmed an earlier study where a protein-protein interaction was identified between MeCP2 and LEDGF. The latter study showed an interaction between MeCP2 and the N-terminal region of LEDGF that is shared between the two LEDGF isoforms. LEDGF is a ubiquitously expressed DNA-binding protein and is also, like MeCP2, expressed in the developing and adult human brain, more specifically in neurons. LEDGF functions as a transcriptional co-activator by interacting with the RNA polymerase II complex. The protein exists in two isoforms, LEDGF/p75 and LEDGF/p52, which are generated through alternative splicing but share the N-terminal part ( B and B). Both isoforms contain a proline-tryptophan-tryptophan-proline (PWWP) domain that allows LEDGF to interact with DNA regions that contain specific histone modifications. In particular, LEDGF interacts with di- and trimethylated H3K36 marks, tethering regulatory complexes to actively transcribed genes, and contributing to the definition of chromatin states and regulating gene expression. This binding to chromatin is supported by three charged regions (CR1, CR2, and CR3) and two AT-hooks C-terminally of the PWWP domain. The longer isoform LEDGF/p75 contains an additional integrase binding domain (IBD) which serves as an interaction site for cellular binding partners such as MLL1, PogZ, JPO2, and IWS1. In this study, we determined the molecular and functional interaction between MeCP2 and LEDGF. We characterized the interaction domains and identified the ID-TRD of MeCP2 as the important interaction domain of MeCP2, while on the side of LEDGF, the PWWP-CR1 region appears to be the main interaction site. Additionally, we studied the effect of the MeCP2-LEDGF complex on MeCP2 condensation. We observed that LEDGF disrupts MeCP2 oligomerization and that LEDGF reduces MeCP2 condensation in a cell model, potentially influencing chromatin organization. LEDGF and MeCP2 each exist in two distinct isoforms, LEDGF/p75 and LEDGF/p52, and MeCP2 E1 and MeCP2 E2, respectively ( A and 1B). The direct interaction between MeCP2 and the PWWP-CR1 region of LEDGF has been shown before. We now investigated the interactions of the respective isoforms using co-immunoprecipitation (coIP). In lysates of HEK293T cells overexpressing FLAG-MeCP2 E1 or FLAG-MeCP2 E2 both LEDGF isoforms were immunoprecipitated by FLAG-MeCP2, confirming their interaction ( A). No difference was observed between MeCP2 E1 and MeCP2 E2 in precipitating LEDGF. Since both isoforms of LEDGF formed a complex with MeCP2, the domain of LEDGF that is responsible for the interaction with MeCP2 is shared by LEDGF/p75 and LEDGF/p52. Interestingly, the relative abundance of LEDGF/p75 and LEDGF/52 isoforms altered upon precipitation. Whereas LEDGF/p75 is on average 5-fold more abundant in HEK293T cells than LEDGF/p52, in the precipitate this ratio was reduced to a ∼1.5-fold difference ( B). The quantification suggests a higher affinity of MeCP2 for LEDGF/p52 than for LEDGF/p75. We further validated the MeCP2-LEDGF interaction by performing a pull-down assay showing the interaction between recombinant, -expressed, FLAG-LEDGF/p75 or FLAG-LEDGF/p52 proteins, and endogenous MeCP2 ( C). Additionally, a reverse coIP in HEK293T cells overexpressing HA-LEDGF/p75 confirmed the interaction between LEDGF/p75 and endogenous MeCP2 ( A). To better understand the direct interaction between LEDGF and MeCP2, we performed a series of AlphaScreen experiments. FLAG-MeCP2 E1 and FLAG-MeCP2 E2 were purified from HEK293T cells. A direct interaction with GST-LEDGF/p52 was demonstrated, with similar apparent affinities for both MeCP2 isoforms ( D). His -MeCP2 E2 was also expressed in and purified from to determine whether post-translational modifications (PTM) of MeCP2 are required for the binding to LEDGF. Eukaryotic FLAG-MeCP2 E2 and bacterial His -MeCP2 E2 showed an identical binding profile to GST-LEDGF/p52, excluding the requirement of PTMs in MeCP2 for its interaction with LEDGF ( B). Indirect complex formation between MeCP2 and LEDGF may occur due to the presence of DNA as MeCP2 and LEDGF are both DNA-binding proteins. To remove potential DNA contaminants from protein productions, we performed AlphaScreens in the presence of micrococcal nuclease (MNase). Although MNase treatment clearly reduced the observed interaction between His -MeCP2 E2 and GST-LEDGF/p52, a residual DNA-independent interaction was evidenced ( E). To determine the interaction domain of LEDGF, a recombinant GST-LEDGF/p52 lacking the PWWP-CR1 domain (amino acids 1–143) was expressed, indicated as the interface by Leoh et al. In line with previously obtained results, the interaction with His -MeCP2 significantly decreased upon deletion of the PWWP-CR1, indicating the importance of this region for the direct interaction with MeCP2 ( F). This result was confirmed with FLAG-MeCP2 E2 ( C). GST-LEDGF/p52 ΔPWWP-CR1, lacking amino acids 1–143, bound less to FLAG-MeCP2 in comparison with WT GST-LEDGF/p52 ( C). Residual binding of the ΔPWWP-CR1 mutant to MeCP2 may result from indirect complex formation with DNA. MeCP2 consists of five domains: the NTD, the MBD, the ID, the TRD, and the CTD. To gain a better understanding of the MeCP2-LEDGF interface, we determined which domains of MeCP2 interact with LEDGF by using MeCP2 deletion mutants in a series of coIPs ( A). Results showed that the amount of precipitated LEDGF/p75 and LEDGF/p52 was reduced upon deletion of the ID or TRD of FLAG-MeCP2, irrespective of the MeCP2 isoform ( B and 3C). However, despite the observed reduction in precipitated LEDGF, deletion of the individual ID or TRD did not completely disrupt the interaction. An additional MeCP2 deletion construct was made, removing both the ID and TRD. CoIPs showed that FLAG-MeCP2 ΔID-TRD no longer binds to endogenous LEDGF ( D and 3E). Further AlphaScreens were performed to validate the interaction domain of MeCP2. Recombinant His -MeCP2 fragments were made to study the minimal domain of MeCP2 required for the binding to LEDGF. A His -MBD-ID-TRD construct was made containing only the functional domains of MeCP2 that are shared between the two isoforms ( A). Removing the NTD and CTD did not change the binding of His -MeCP2 to GST-LEDGF/p52 ( D). Additionally, we produced fragments corresponding to an individual MeCP2 domain (His -MBD, His -ID) or two consecutive domains (His -MBD-ID and His -ID-TRD) and compared them to the His -MBD-ID-TRD construct. All fragments showed binding to GST-LEDGF/p52 in the presence of DNA ( B), suggesting DNA-dependence of the observed MeCP2-LEDGF interaction. Therefore, we tested the interaction of the MeCP2 fragments with LEDGF after MNase treatment. When comparing the distinct His -MeCP2 fragments, we observed that the ID-TRD fragment was the only fragment that still binds LEDGF after treatment with MNase ( B–4H). In fact, the DNA bridging by the ID-TRD domain for binding to GST-LEDGF/p52 was minimal ( H). This result suggests that the ID-TRD of MeCP2 is responsible for directly binding LEDGF. We reasoned that residual binding of the ΔPWWP-CR1 mutant to His -MeCP2 E2 ( F) resulted from indirect complex formation with DNA. Therefore, we tested His -ID-TRD against GST-LEDGF/p52 WT or ΔPWWP-CR1 in the presence of MNase. Deletion of the PWWP-CR1 of GST-LEDGF/p52 significantly decreased the binding to His -ID-TRD ( I). Both MeCP2 and LEDGF contain highly charged positive and/or negative regions, as observed in the net charge per region analysis (NCPR) ( A–S3D; ). To determine which amino acids within MeCP2 are important for binding LEDGF, three sets of alanine mutants were made to alter three positively charged regions in the TRD to neutral amino acids ( A and E). Mutant one contained four point mutations (K210A, KR211A, K215A, and K219A; mut1), mutant two contained five point mutations (K266A, K267A, R268A, R270A, and K271A; mut2), and mutant three contained five point mutations in the NCoR interaction domain (NID) of the TRD (K304A, K305A, R306A, K307A, and R309A; mut3). Additionally, the R306C mutation was made as it is the most common clinical RTT mutation in the NID of MeCP2 ( A and 5B). An NCPR analysis of the TRD mutants can be found in the ( ). We observed a clear reduction in the binding of His -ID-TRD mut2 and mut3 to GST-LEDGF/p52 in the presence of MNase ( C). The reduction in binding to GST-LEDGF/p52 was less pronounced for His -ID-TRD mut1 that was mutated in the N-terminal region of the TRD ( C). A combination of mut2 and mut3 in the His -ID-TRD construct resulted in a clear decrease in binding to GST-LEDGF/p52 compared to ID-TRD WT ( D). In the presence of MNase, the clinical single amino acid R306C RTT mutant bound less to GST-LEDGF/p52 ( E). In the presence of DNA, all mutants were still able to bind GST-LEDGF/p52 ( E). Additionally, an AlphaScreen experiment was conducted to test the MeCP2-LEDGF interaction under varying salt concentrations ( F). The interaction between the ID-TRD domain of MeCP2 and LEDGF/p52 showed an optimum at 100 mM NaCl. Taken together, our results indicate that the NID of MeCP2 is the interface for the electrostatic interaction with the PWWP-CR1 region of LEDGF. MeCP2 is known to form electrostatic self-interactions which are essential for heterochromatin formation. Considering that MeCP2 forms multimers and that MeCP2 interacts with LEDGF, we studied the effect of LEDGF on MeCP2 multimerization. We confirmed that FLAG-MeCP2 self-associates using AlphaScreens and that this interaction can be disrupted by outcompetition with His -ID-TRD ( A, 6B , and G). No significant difference in outcompetition with His -ID-TRD WT was seen for the His -ID-TRD R306C RTT mutant ( C). However, Flag-MeCP2 E2 multimers were disrupted by increasing concentrations of GST-LEDGF/p52 ( D). Previous studies showed that LEDGF also forms dimers based on electrostatic interactions. We performed an AlphaScreen cross titration with GST-LEDGF/p52 and FLAG-LEDGF/p75, revealing the known interaction between both LEDGF isoforms ( E). A fixed concentration of 50 nM GST-LEDGF/p52 and 50 nM FLAG-LEDGF/p75 was used to form LEDGF heterodimers. These were outcompeted by using increasing concentrations of His -MeCP2 E2 ( F). In conclusion, these results indicate that MeCP2 multimers can be disrupted by LEDGF and vice versa. MeCP2 condensates are functionally important for the MeCP2-chromatin interaction and the compaction of chromatin. In NIH3T3cells this phenotype is clearly visible as MeCP2 is enriched within large foci at the heavily methylated pericentric domains. Therefore, NIH3T3 cells are used as reference model in the field for studying localization of MeCP2. We stably depleted endogenous mouse MeCP2 in NIH3T3 cells with a shRNA and rescued with human MeCP2-eGFP E1 or E2. The human MeCP2-eGFP displayed the typical speckled pattern as well in NIH3T3 cells, while this pattern was not observed for the GFP control ( A and ). Subsequently, both or individual isoforms of LEDGF were depleted in the MeCP2-eGFP E1 or E2 NIH3T3cells ( B). In order to assess the effect of LEDGF depletion on MeCP2 and DNA foci formation, the cells were synchronized using 10 μg/mL aphidicolin and MeCP2 and DNA foci were imaged at different time points using the Operetta CLS High Content imager. The area of MeCP2-eGFP foci was significantly larger when both LEDGF/p75 and LEDGF/p52 were depleted in the cells ( C and 7D; ). This increase was observed for both MeCP2-eGFP E1 ( C) and for MeCP2-eGFP E2 ( D) with 5% and 8% larger foci compared to WT, respectively. A 37% increase in the number of MeCP2-eGFP foci was seen for MeCP2 E2 ( F), although this phenotype was not observed for MeCP2-eGFP E1 ( E; ). We also observed an impact on the area and size of MeCP2-eGFP foci when either LEDGF/p75 or LEDGF/p52 was depleted but to a lesser extent ( A–S5D). Taken together, depletion of LEDGF led to more crowding of MeCP2, resulting in larger and/or more MeCP2 condensates in the cell. As MeCP2 foci in NIH3T3 cells colocalize with DNA foci, visualized with Hoechst staining ( A), we next assessed the effect of LEDGF depletion on DNA foci. LEDGF KD resulted in significantly larger DNA foci ( G and 7H; ). For MeCP2-eGFP E1 the DNA foci area increased with 9% and for MeCP2-eGFP E2 with 8%. An effect of LEDGF depletion on the number of DNA foci was not clearly observed as only a small reduction was observed in MeCP2-eGFP E1 NIH3T3 cells ( I and 7J; ). In analogy to the observations with MeCP2-eGFP foci, the effect of LEDGF depletion on the size and area of DNA foci was less pronounced when only one LEDGF isoform was depleted ( G and S5H). In summary, a strong correlation on the effect of LEDGF depletion on either MeCP2-eGFP or DNA foci was evidenced. LEDGF depletion increased both MeCP2 condensation and the area of DNA foci. MeCP2 is known to act both as transcriptional activator and repressor. To assess the effect of MeCP2 on transcriptional regulation we performed transcriptome analysis on MeCP2-depleted HEK293T cells. Depletion of MeCP2 resulted in a very limited number of significant changes in gene expression in RNA sequencing (RNA-seq) analysis. Only 20 upregulated and 9 downregulated genes reached the threshold of a 2-fold change and adjusted value of 0.05 ( A). The majority of the top 50 differentially expressed genes (DEG) in MeCP2 KD cells did not reach a 2-fold change in expression versus the control ( B). Gene ontology (GO) analysis of the few significant differentially expressed genes (DEGs) confirmed that MeCP2 even in HEK293T cells regulates genes associated with the differentiation of various cell populations in the brain ( C). The findings presented in this study contribute to a deeper understanding of the molecular interaction between MeCP2 and LEDGF, shedding light on the biological relevance. The interaction between MeCP2 and LEDGF had been explored in previous studies, but the domains responsible for this interaction remained unknown. This study aimed to elucidate the specific interaction domains of MeCP2 and LEDGF, as well as to investigate the functional implications of MeCP2-LEDGF complex formation in the cell. The interaction between MeCP2 and LEDGF was confirmed through a series of coIP and AlphaScreen experiments with recombinant proteins ( ). Our results corroborate the earlier studies on the interaction between MeCP2 and LEDGF. By using deletion constructs of LEDGF, we confirmed that the interacting domain in LEDGF is located in the N-terminal region that is shared between the two isoforms ( F, I, and C). More specifically, the PWWP-CR1 region of LEDGF is responsible for the interaction with MeCP2 as it was earlier suggested by Leoh et al. One key finding in our study is that both isoforms of LEDGF interacted with both isoforms of MeCP2, yet LEDGF/p52 showed an apparent higher affinity for MeCP2 than LEDGF/p75 in co-immunoprecipitation experiments (coIP; A and 2B). In MeCP2, on the other hand, the ID-TRD domain proved important for the interaction with LEDGF ( and ). This result was expected as the ID-TRD of MeCP2 is a known interaction site for many MeCP2 interaction partners. MeCP2 is an intrinsically disordered protein. Characteristically, these proteins lack a stable three-dimensional structure and interact electrostatically with DNA and other proteins. In MeCP2 only the MBD is structured, while the ID and TRD are structurally disordered but form a secondary structure upon binding interaction partners and/or DNA. We postulated an electrostatic interaction between the ID-TRD and LEDGF which is often the case for structurally disordered proteins. Three positively charged regions are present in the TRD, and we mutated the four or five positively charged residues to alanines which are neutral amino acids ( A and E). Using this approach, we could narrow down the interaction site in MeCP2 to the C-terminal residues of the TRD. More specifically, the positively charged amino acids 266–309 appeared to be important for the interaction ( C and 5D). Our NCPR analysis indicates that the CR1 region of LEDGF contains many negatively charged residues. Based on the results of our presented study, we suggest that electrostatic interactions are responsible for the MeCP2-LEDGF interaction, and therefore it is highly likely that salt bridges are formed between the negatively charged CR1 region of LEDGF and the positively charged regions in the TRD of MeCP2. By lowering the salt concentrations in the AlphaScreen experiments, we observed an enhanced interaction between MeCP2 and LEDGF. Lowering salt concentrations decreases the screening effect of ions in the solution, allowing stronger electrostatic interactions between the proteins and discriminating specific protein-protein interactions from non-specific interactions. The C-terminal region of the TRD, also known as the NID, is the interaction site of another important MeCP2 binding partner: the NCoR complex. The positively charged cluster 302–306 of conserved amino acids represents a recruitment surface for the NCoR complex. R306C is a known clinical RTT mutant and is the most common missense mutation in the NID of MeCP2 ( A and 5B). We observed a clear reduction in the binding affinity of the R306C MeCP2 mutant for LEDGF ( E). Although R306C is also defective for interaction with NCoR, a role for LEDGF in the pathophysiology of RTT patients with the R306C mutation may be plausible. DNA methyltransferase 3A (DNMT3A) is also known to interact with MeCP2 through the TRD. While DNMT3A is known to directly bind MeCP2 residues 214–228, mutations in this region of MeCP2 only showed a minimal reduction in LEDGF binding, indicating that DNMT3A and LEDGF do not compete for the same residues. However, it remains possible that a competition exists due to steric hindrance or that these residues support the proper folding of MeCP2 upon interaction with LEDGF. Interestingly, while DMNT3A also contains a PWWP domain, the chromatin interaction ADD domain of DNMT3a and not the PWWP domain is responsible for the interaction with MeCP2. The interaction between MeCP2 and LEDGF showed to be strongly DNA dependent ( E). Since both are DNA-binding proteins, indirect complex formation may occur due to DNA bridging. Additionally, the presence of DNA may also affect the protein folding of unstructured domains in MeCP2 and LEDGF. As the ID-TRD of MeCP2 is prone to non-specific DNA interactions, we had to exclude that the interaction with LEDGF occurs also in the absence of DNA ( C–4H). We observed a residual interaction after MNase treatment, indicating a direct MeCP2-LEDGF interaction independent from DNA. The disordered structure of proteins like MeCP2 supports their functional versatility. Interactions are context-dependent which enables MeCP2 to participate in various cellular processes. MeCP2 is known to form electrostatic self-interactions through the ID-TRD domain. These self-interactions were shown to occur in the absence of DNA. We confirmed that MeCP2 forms electrostatic self-interactions through its ID-TRD domain ( A, 6B, and G). Interestingly, the R306C mutation did not affect MeCP2 multimerization ( C). This result suggests that MeCP2 and LEDGF do not compete for exactly the same amino acid residues in the ID-TRD domain of MeCP2. However, steric hindrance may cause competition for binding. MeCP2 is a very versatile protein. One of the most described functions of MeCP2 is transcriptional repression and activation. However, the molecular mechanism by which MeCP2 regulates gene expression remains poorly understood. We performed RNA-seq on MeCP2 KD HEK293T cells and found only few significantly differentially expressed genes. This is in accordance with a number of studies on transcriptional profiling of RNA in mice lacking functional MeCP2; none revealed significant changes in gene expression. Changes in MeCP2 expression may cause only subtle alterations, comprising both increases and decreases in gene expression. The majority of changes in our dataset and in other studies had a magnitude less than 50%, thus making it difficult to distinguish between biological changes and background. The genome-wide distribution of MeCP2 is one of the important aspects to consider in these observations. When a transcriptional regulator affects a small number of genes, significant expression changes are expected when the transcriptional regulator is deficient. However, if a transcriptional regulator such as MeCP2 regulates a large number of genes, it is more difficult to predict how deficiency will impact gene expression as the supply of transcriptional machinery is limited in the cell. It has been hypothesized before that the MeCP2-LEDGF interaction may play a role in modulating transcriptional activity. Considering that MeCP2 is globally distributed in the nucleus and that both MeCP2 and LEDGF interact with the RNA polymerase II complex, it remains possible that the MeCP2-LEDGF interaction is involved in transcriptional regulation. Changes in MeCP2 expression cause large-scale chromatin reorganization and MeCP2 homo-interactions prove essential for heterochromatin organization. However, a recent publication questions the functional importance of MeCP2 in heterochromatin formation as MeCP2 foci in NIH3T3 cells persist after disruption of heterochromatin. In contrast, MeCP2 localization was highly dependent on the presence of methylated DNA that forms DNA foci in mouse cells. Results from our imaging assay show that LEDGF depletion enlarges these MeCP2 foci in NIH3T3 cells ( and ). Additionally, we show that LEDGF can disrupt MeCP2 multimers ( D) and that LEDGF depletion enlarges DNA foci ( C–7F). Our results thus reveal a previously unknown role of LEDGF in chromatin compaction by modulating MeCP2 oligomerization. Previously, a model has been proposed in which a single MeCP2 simultaneously binds two nucleosomes to form a “sandwich” that forms chromatin loops and compacts DNA. We propose instead that MeCP2 forms a dimer or multimer through its ID-TRD domain, allowing each MBD domain to bind methylated DNA in order to form chromatin loops and compact DNA. Methods to determine genome-wide chromatin status, such as ATAC-seq, will provide valuable information to support this hypothesis. While LEDGF modulates MeCP2 condensation, MeCP2 by itself can disrupt LEDGF dimers ( F). Even though the function of LEDGF dimers is not fully understood, disruption of LEDGF dimers may impact the various functions of LEDGF in the cell, such as transcriptional regulation and DNA damage repair. Considering that both MeCP2 and LEDGF are known to be involved in transcriptional regulation, MeCP2 may conversely affect LEDGF-dependent gene regulation. It has been previously hypothesized that MeCP2 dampens transcriptional noise throughout the genome. The binding of MeCP2 to LEDGF may have a dampening effect on LEDGF-dependent gene regulation, thereby affecting transcriptional activation by LEDGF. Trans-activation assays will have to show whether MeCP2 can influence LEDGF-dependent gene regulation to support this hypothesis. As both isoforms of LEDGF are capable of binding MeCP2 ( A–2C), the functional difference between LEDGF/p75 and LEDGF/p52 may also affect their respective function when bound to MeCP2. LEDGF/p75 is known to tether via its IBD-domain, which is not present in LEDGF/p52, several proteins to the chromatin. LEDGF/p75 is known to bind transcriptional regulators such as JPO2, PogZ, MED1, IWS1, and MLL and is also involved in DNA-damage repair by recruiting proteins from the homologous recombination repair pathway. In contrast, not much is known about the specific function of LEDGF/p52. One study suggested a specific role for LEDGF/p52, and not LEDGF/p75, in modulating splicing due to its colocalization and interaction with mRNA processing proteins. The differential roles of LEDGF/p52 and LEDGF/p75 raise the possibility that the biological role of the MeCP2-LEDGF/p52 complex differs from that of the MeCP2-LEDGF/p75 complex. LEDGF is known to bind di- and trimethylated H3K36 marks, while MeCP2 is known to bind methylated histone H3K9 and H3K27 marks. A study by Lee et al. showed a correlation between the impact of MeCP2 on transcription and binding of MeCP2 to H3K27 histone marks. Regardless of the fact that they do not bind the same histone modifications, steric hindrance for nucleosome binding may cause competition between MeCP2 and LEDGF. While competition for the nucleosomes would not necessarily involve a direct MeCP2-LEDGF interaction, it could have functional implications on transcriptional regulation. MeCP2 is highly expressed in the brain and is essential for the development and maintenance of neurons. Previous studies have shown that MeCP2 expression levels are tightly controlled to maintain proper neuronal function and overall cellular homeostasis. The occurrence of RTT and MDS, two neurodevelopmental disorders which are characterized by too little or too much functional MeCP2, respectively, also support this idea. The results of our study suggest that MeCP2 and LEDGF may contribute to a finely tuned regulatory system in the cell in which the balance between MeCP2 and LEDGF is the key. Our study provides novel insights in the function of MeCP2 through its interaction with LEDGF with a potential relevance for RTT and MDS. Requests for further information and resources should be directed to and will be fulfilled by the lead contact, Zeger Debyser ( ). Plasmids generated in this study are available upon request and will be shared by the . Financial support was received from ( ) and ( ). Y.H. received a personal fellowship from ( ). We thank the Genomics Core Leuven (KU Leuven/UZ Leuven) for performing sample preparation and NGS. We thank Cecilia Iglesias Herrero for writing an R script for gene ontology analysis of RNA-seq data. We thank the Cell and Tissue Imaging Core (KU Leuven) and the VIB Bioimaging Core (VIB-KU Leuven) for the use of their core facilities. All MeCP2 sequences were cloned from the pcDNA-FLAG-MeCP2 plasmid, a kind gift from Prof. Dr. Adrian Bird (University of Edinburgh, Edinburgh, UK). The authors declare no competing interests. HEK293T [ATCC; CRL-11268] and NIH3T3 [ATCC; CRL-1658] cells were grown in a humified atmosphere containing 5% CO at 37°C. Cells were cultured in Dulbecco’s modified Eagle’s medium (DMEM) with GlutaMAX [Gibco] supplemented with 5% fetal bovine calf serum (FCS; [Gibco]) and 50 μg/mL gentamicin [Gibco]. Cultured cells were routinely checked for mycoplasma using the PlasmoTest Mycoplasma Detection kit [Invivogen]. BL21 Star (DE3) [Invitrogen; C602003] were used for protein purifications. Bacterial cultures were grown in LB medium medium and growing conditions are described specifically for each protein purification in the . For co-immunoprecipitation assays (co-IP) 7 x 10 HEK293T cells were plated in 10 cm dishes in DMEM with GlutaMAX [Gibco] supplemented with 2% (v/v) FCS [Gibco] and 50 μg/mL gentamicin [Gibco]. After 24 hours cells were transfected with 20 μg of plasmid DNA (pCHMWS_3xFlag-MeCP2_E1 or pCHMWS_3xFlag-MeCP2_E2) using 10 μM branched PEI [Sigma-Aldrich]. Cells were harvested 24 hours after transfection and lysed in RIPA buffer (150 mM Tris-HCl [Sigma-Aldrich], 150 mM NaCl [Sigma-Aldrich], 0.5% (w/v) sodium deoxycholate [Merck Life Science BV], 0.1% (w/v) sodium dodecyl sulphate (SDS; [Acros Organics], 1% (v/v) IGEPAL [Sigma-Aldrich], pH 8) supplemented with protease inhibitor [cOmplete, EDTA-free, Roche]. The samples were incubated on ice for 30 minutes followed by centrifugation at 21,000 for 10 minutes. In case of Flag-tagged IP, the supernatant was incubated with anti-Flag-beads [Sigma-Aldrich; A2220] and 3 U/mL DNase [Merck Life Science BV] overnight on a turning wheel at 4°C. The beads were washed using TBS (50 mM Tris-HCl [Sigma-Aldrich], 150 mM NaCl [Sigma-Aldrich], pH 7.4) and collected using centrifugation at 6800 . Immunoprecipitated proteins were detected by western blotting. Input samples and precipitated proteins were separated on a 4-15% Tris-glycine gel [Bio-Rad Laboratories] and electroblotted on Amersham™ Protran Nitrocellulose membranes [VWR]. Membranes were blocked in PBS with 0.1% (v/v) Tergitol [Acros Organics] and 5% (w/v) milk. Subsequently membranes were incubated with primary antibodies (1:500 rabbit anti-LEDGF-PWWP [Abcam; ab177159], 1:1000 rabbit anti-MeCP2 [Cell Signaling Technology; 3456S], 1:1000 rabbit anti-GAPDH [Abcam; Ab9485], 1:400 rabbit anti-Flag [Sigma-Aldrich; F7425]). Detection was performed using secondary horseradish peroxidase-conjugated goat anti-rabbit [Agilent] and chemiluminescent substrate Clarity ECL or Clarity Max ECL [Bio-Rad Laboratories]. Imaging was done with the Amersham™ ImageQuant 800 Western blot imaging system [Cytvia]. Quantification was performed in the Image Lab software [Bio-Rad Laboratories] and statistical analysis was performed using GraphPad Prism 10.0 software package. Alternatively, cells were transfected with 20 μg of pCHMWS_HA-LEDGF/p75 using 10 μM branched PEI [Sigma-Aldrich]. Magnetic anti-HA-beads [MedChemExpress] were used for precipitation of HA-LEDGF/p75. The beads were washed using TBS-T (50 mM Tris-HCl [Sigma-Aldrich], 150 mM NaCl [Sigma-Aldrich], 0.5% Tween-20 [AppliChem], pH 7.4) and collected using a magnetic holder. Precipitated proteins were eluted in SDS buffer (2% (w/v) SDS [Acros Organics], 160 mM Tris-HCl [Sigma-Aldrich], pH 6.8) and analogously detected by western blotting. Primary antibodies used were: 1:1000 rabbit anti-LEDGF-PWWP [Abcam; ab177159], 1:500 rabbit anti-MeCP2 [Cell Signaling Technology; 3456S], 1:1000 rabbit anti-GAPDH [Abcam; Ab9485], rabbit anti-Flag [Sigma-Aldrich; F7425]). GST-LEDGF/p52 or GST-LEDGF/p52_ΔPWWP-CR1 were expressed from pGEX_GST-LEDGF/p52 or pGEX_GST-LEDGF/p52_dPWWP-CR1 in BL21pLysS competent bacteria and grown in LB medium medium supplemented with 100 μg/mL ampicillin [Sigma-Aldrich]. Bacterial cultures were grown at 37°C until an OD of 0.6 before protein expression was induced by adding 1 mM IPTG [Sigma- Aldrich]. After 4 hours at 37°C, cultures were harvested by centrifugation for 10 minutes at 2100 . Pellets were resuspended in 20 mL cold STE buffer (10 mM Tris-HCl [Sigma-Aldrich], pH 7.3, 100 mM NaCl [Sigma-Aldrich], 0.1 mM ethylenediaminetetraacetic acid (EDTA; [Sigma-Aldrich]), and stored at −20°C. The pellet was lysed using lysis buffer (50 mM Tris-HCl [Sigma-Aldrich], pH 7.5, 250 mM NaCl [Sigma-Aldrich], 1 mM dithiotreitol (DTT; [VWR Chemicals])) supplemented with protease inhibitor [cOmplete, EDTA-free, Roche] and lysed further by sonication [SFX250 Sonifier, Branson]. After sonication, 0.1 μg/mL DNase [Roche] was added and the lysate was incubated for 20 minutes on ice. The lysate was cleared by a 30-minute centrifugation at 27,000 . GST-tagged proteins were purified by affinity chromatography on Glutathione Sepharose-4 Fast Flow [GE Healthcare]. The resin was equilibrated with wash buffer (50 mM Tris-HCl [Sigma-Aldrich], pH 7.5, 250 mM NaCl [Sigma-Aldrich], 1 mM DTT [VWR Chemicals]) and bound proteins were eluted in wash buffer supplemented with 20 mM glutathione. The fractions were analyzed by SDS-PAGE and a Coomassie stain [Coomassie Brilliant Blue G250, Merck Life Science BV]. Peak fractions were pooled and dialyzed against 50 mM Tris-HCl, pH 7.5, 250 mM NaCl [Sigma-Aldrich] and 10% (v/v) glycerol [VWR chemicals]. Flag-LEDGF/p52 and Flag-LEDGF/p75 were expressed from pCPnat_3xFlag-LEDGF/p52 or pCPnat_3xFlag-LEDGF/p75 in BL21 Star (DE3) competent bacteria and grown in LB medium supplemented with 100 μg/mL ampicillin [Sigma-Aldrich]. Bacterial cultures were grown at 37°C until an OD of 0.6 before protein expression was induced by adding 0.5 mM IPTG [Sigma-Aldrich]. After 4 hours at 29°C, cultures were harvested by centrifugation for 10 minutes at 2100 . Pellets were resuspended in 20 mL cold STE buffer (10 mM Tris-HCl [Sigma-Aldrich], pH 7.3, 100 mM NaCl [Sigma-Aldrich], 0.1 mM EDTA [Sigma-Aldrich]), and stored at −20°C. The pellet was lysed using lysis buffer (50 mM Tris-HCl [Sigma-Aldrich], pH 7.5, 250 mM NaCl [Sigma-Aldrich], 1 mM DTT [VWR Chemicals]) supplemented with protease inhibitor [cOmplete, EDTA-free, Roche] and lysed further by sonication [SFX250 Sonifier, Branson]. After sonication, 0.1 μg/mL DNase [Roche] was added, and the lysate was incubated for 20 minutes on ice. The lysate was cleared by a 30-minute centrifugation at 27,000 . The supernatant was filtered through a 0.22 μm Millex-GS Syringe Filter Unit [Merck Life Science BV] before purification over a 5 mL HiTrap Heparin HP Column [Cytiva], equilibrated in 150 mM NaCl [Sigma-Aldrich], 30 mM Tris-HCl [Sigma-Aldrich] pH 7.0 and 1 mM DTT [VWR Chemicals]. The protein was eluted by increasing the salt concentration from 150 mM up to 2 M NaCl [Sigma-Aldrich] using the AKTA purifier [GE Healthcare] and Unicorn v5 software. Peak fractions were analyzed by SDS-PAGE and a Coomassie stain [Coomassie Brilliant Blue G250, Merck Life Science BV]. Fractions containing Flag-LEDGF were pooled and loaded on a superposeTM 6 10/300 GL size exclusion column [GE Healthcare] to purify further. The size exclusion column was equilibrated in 150 mM NaCl [Sigma-Aldrich], 30 mM Tris-HCl [Sigma-Aldrich], pH 7.4 and 1 mM DTT [VWR Chemicals]. Peak fractions were again analyzed on SDS-PAGE followed by a Coomassie stain [Coomassie Brilliant Blue G250, Merck Life Science BV]. The fractions containing Flag-LEDGF were supplemented with 10% (v/v) glycerol [VWR chemicals] and stored at -80°C. Full length Flag-MeCP2 E1 and E2 were produced in HEK293T cells. 7 x 10 cells were plated in 10 cm dishes in DMEM with GlutaMAX [Gibco] supplemented with 2% FCS [Gibco] and 50 μg/mL gentamicin [Gibco]. After 24 hours cells were transfected with 20 μg of plasmid DNA (pCHMWS_3xFlag-MeCP2_E1 or pCHMWS_3xFlag-MeCP2_E2) using 13 μM linear PEI [Polysciences]. Cells were lysed 24h after transfection in lysis buffer (75 mM Tris-HCl [Sigma-Aldrich], 400 mM NaCl [Sigma-Aldrich], 1 mM DTT [VWR Chemicals], pH 8) supplemented with protease inhibitor [cOmplete, EDTA-free, Roche] followed by sonication [SFX250 Sonifier, Branson]. After sonication, 0.1 μg/mL DNase [Roche] was added, and the lysate was incubated for 30 minutes on ice. The lysate was cleared by a 30-minute centrifugation at 27,000 . The supernatant was filtered through a 0.22 μm Millex-GS Syringe Filter Unit [Merck Life Science BV] before purification over a 5 mL HiTrap SP HP Column [Cytiva], equilibrated in 30 mM Tris-Base, 200 mM NaCl [Sigma-Aldrich], 1 mM DTT [VWR Chemicals], pH 8. The protein was eluted by increasing the salt concentration from 200 mM up to 2 M NaCl [Sigma-Aldrich] using the AKTA purifier [GE Healthcare] and Unicorn v5 software. Peak fractions were analyzed by SDS-PAGE and a Coomassie stain [Coomassie Brilliant Blue G250, Merck Life Science BV]. The fractions showing the expected molecular weight and purity were pooled and loaded on a superposeTM 6 10/300 GL size exclusion column [GE Healthcare] to purify further. The size exclusion column was equilibrated in 30 mM Tris-Base, 2 M NaCl [Sigma-Aldrich], 1 mM DTT [VWR Chemicals], pH 8. The fractions containing Flag-MeCP2 E1 or Flag-MeCP2 E2 were supplemented with 10% (v/v) glycerol [VWR chemicals] and stored at -80°C. His -MeCP2 E2 and His -MeCP2 fragments were cloned in pET constructs (pET_His -MeCP2_E2, pET_His -MBD-ID-TRD, pET_His -MBD-ID, pET_His -ID-TRD, pET_His -MBD, pET_His -ID, pET_His -ID-TRD_K210A-KR211A-K215A-K219A, pET_His -ID-TRD_K266A-K267A-R268A-R270A-K271A, pET_His -ID-TRD_K304A-K305A-R306A-K307A-R309A, pET_His -ID-TRD_K266A-K267A-R268A-R270A-K271A- K304A-K305A-R306A-K307A-R309A, pET_His -ID-TRD_R306C). Numbering of the amino acid sequences was based on the MeCP2 E2 isoform ( A). His -MeCP2 E2 and His -MeCP2 fragments were expressed in E. BL21 competent bacteria. Bacterial cultures were grown at 37°C until an OD of 0.8 in LB medium supplemented 0.5% (v/v) glycerol [VWR chemicals] and 100 μg/mL ampicillin [Sigma-Aldrich]. Protein expression was induced by adding 0.25 mM isopropyl β-D-1-thiogalactopyranoside (IPTG; [Sigma-Aldrich]) and bacterial cultures were grown at 18°C for 20 hours. Cultures were harvested by centrifugation for 10 minutes at 2100 . Pellets were resuspended in 20 mL cold STE buffer (100 mM NaCl [Sigma-Aldrich] [Sigma-Aldrich], 10 mM Tris-HCl [Sigma-Aldrich] [Sigma-Aldrich] pH 7.4 and 0.1 mM EDTA [Sigma-Aldrich]). Bacteria were lysed using lysis buffer (25 mM Tris-HCl [Sigma-Aldrich], 1 M NaCl [Sigma-Aldrich], 10 μM EDTA [Sigma-Aldrich], 1 mM DTT [VWR Chemicals]) supplemented with protease inhibitor [cOmplete, EDTA-free, Roche] and lysed further by sonication [SFX250 Sonifier, Branson]. After sonication, 0.1 μg/mL DNase [Roche] was added, and the lysate was incubated for 20 minutes on ice. The lysate was cleared by a 30-minute centrifugation at 27,000 . The fusion His -tagged proteins were captured using His-Select Nickel affinity gel [Sigma-Aldrich; P6611] beads and eluted with STE buffer supplemented with 250 mM imidazole. The eluate was fractionated and analyzed by SDS-PAGE with a Coomassie stain [Coomassie Brilliant Blue G250, Merck Life Science BV]. Fractions with high protein content were pooled and concentrated using Amicon filters [Merck Life Science BV; UFC500324]. The concentrated protein was loaded on a Superose 6 10/300 GL size exclusion column [GE Healthcare], attached to AKTA pure [Cytiva]. The column was equilibrated in 30 mM Tris-HCl [Sigma-Aldrich], 400 mM NaCl [Sigma-Aldrich] and 1mM DTT [VWR Chemicals]. The fractions with absorbance peaks were analyzed by SDS-PAGE with a Coomassie stain [Coomassie Brilliant Blue G250, Merck Life Science BV]. Fractions containing the protein of interest were pooled and stored in 10% (v/v) glycerol [VWR chemicals] at -80°C. For pull-down assays 7.0 x 10 HEK293T cells were plated in 10 cm dishes. After 48 hours the cells were lysed in RIPA buffer (150 mM Tris-HCl [Sigma-Aldrich], 150 mM NaCl [Sigma-Aldrich], 0.5% (w/v) sodium deoxycholate, 0.1% (w/v) SDS [Acros Organics], 1% (v/v) IGEPAL [Sigma-Aldrich], pH 8) supplemented with protease inhibitor [cOmplete, EDTA-free, Roche]. Samples were incubated on ice for 30 minutes followed by centrifugation at 21,000 for 10 minutes. The supernatant was incubated with 8 μg of recombinant Flag-LEDGF/p75 or Flag-LEDGF/p52, anti-Flag-beads [Sigma-Aldrich; A2220] and 3 U/mL DNase [Merck Life Science BV] overnight on a turning wheel at 4°C. The beads were washed using TBS (50 mM Tris-HCl [Sigma-Aldrich], 150 mM NaCl [Sigma-Aldrich], pH 7.4) and collected using centrifugation at 6800 . Immunoprecipitated proteins were detected on western blot. Input samples and precipitated proteins were separated on a 4-15% Tris-glycine gel [Bio-Rad Laboratories] and electroblotted on Amersham™ Protran Nitrocellulose membranes [VWR]. Membranes were blocked in PBS with 0.1% (v/v) Tergitol [Acros Organics] and 5% (w/v) milk. Subsequently membranes were incubated with primary antibodies (rabbit anti-LEDGF-PWWP [Abcam; ab177159], rabbit anti-MeCP2 [Cell Signaling Technology; 3456S], rabbit anti-GAPDH [Abcam; Ab9485]). Detection was performed using secondary horseradish peroxidase-conjugated goat anti-rabbit [Agilent] and chemiluminescent substrate Clarity ECL or Clarity Max ECL [Bio-Rad Laboratories]. Imaging was done with the Amersham™ ImageQuant 800 Western blot imaging system [Cytvia]. AlphaScreen assays were performed according to the manufacturer’s protocol [Perkin Elmer]. Briefly, reactions were performed in 25 μl final volume in a 384-well OptiPlate microtiter plates. The reaction buffer contained 25 mM Tris-HCl [Sigma-Aldrich] pH 7.4, 150 mM NaCl [Sigma-Aldrich], 1 mM MgCl [Sigma-Aldrich], 0.1% (v/v) Tween-20 [AppliChem], 1 mM DTT [VWR Chemicals], 0.1% (v/v) BSA [Merck Life Science BV] with or without 20 U/mL MNase. In case MNase was added, 2 mM CaCl [Sigma-Aldrich] was added to the reaction buffer. Varying concentrations of protein were incubated in 15 μL reaction volume at 4°C for 1 hour. Subsequently, 10 μg/mL of the donor and acceptor beads were added. After incubation for 1 hour in the dark at room temperature, light emission was measured in the EnVision Xcite Multilabel Reader [Perkin Elmer]. A non-linear regression – sigmoidal curve fit with 1/Y weighting was fitted to the data. shRNAs to create MeCP2 or LEDGF KD were generated by annealing sense and antisense oligonucleotide sequences ( ). The annealed oligos were cloned into the pGAE_SFFV plasmid backbone. For lentiviral vector (LV) production, transfer plasmids encoding the shRNA were cotransfected with a pVSV-G envelope plasmid and a pSIV3+ packaging plasmid as previously described. Briefly, LV vectors were produced by triple transfection of HEK293T cells using 10 μM branched PEI [Sigma-Aldrich]. Medium was replaced 24 hours posttransfection and supernatant was collected after 48 and 72 hours by filtration through a 0.45 μm pore-size filter [Merck Life Science BV]. The vectors were concentrated by ultracentrifugation [Amicon Ultra-15 centrifugal filter unit, 50 kDa, Merck Life Science BV]. To rescue MeCP2 depletion, Human MeCP2-eGFP E1 or E2 was cloned into the pCHMWS plasmid backbone for LV production using BamHI and BwiWI restriction enzymes. LV production was performed as described above, using triple transfection of the transfer plasmid, pVSV-G envelope plasmid and p8.91 packaging plasmid. First, stable MeCP2 KD NIH3T3 cells were generated by lentiviral (LV) transduction and subsequent selection with 5 μg/mL blasticidin [Invivogen]. MeCP2 KD NIH3T3 cells were subsequently transduced with LV vectors encoding shRNA-resistant human MeCP2-eGFP E1 or E2, or an eGFP control, and selected with 1 μg/mL puromycin [Invivogen]. These cells were again transduced with LV vectors encoding shRNA’s against LEDGF ( ) and selected with 100 μg/mL zeocin [Invivogen]. After transduction cells were continuously kept under selection. MeCP2 KD and control HEK293T cells were generated by lentiviral (LV) transduction. The LV vectors carried a shRNA against MeCP2 ( ) or a non-targeting control. The cells were subsequently selected with 5 μg/mL blasticidin [Invivogen]. Western blot was performed to determine protein levels in stably transduced NIH3T3 cell lines. Cells were washed twice with PBS and lysed in RIPA buffer (150 mM Tris-HCl [Sigma-Aldrich], 150 mM NaCl [Sigma-Aldrich], 0.5% (w/v) sodium deoxycholate, 0.1% (w/v) SDS [Acros Organics], 1% (v/v) IGEPAL [Sigma-Aldrich], pH 8) supplemented with protease inhibitor [cOmplete, EDTA-free, Roche]. Protein concentrations of whole cell extracts were determined using the BCA protein assay [ThermoFisher Scientific]. Cell extracts containing 30 μg of total protein was separated on a 4-15% Tris-glycine gel [Bio-Rad Laboratories] and electroblotted on Amersham™ Protran Nitrocellulose membranes [VWR]. Membranes were blocked in PBS with 0.1% (v/v) Tergitol [Acros Organics] and 5% (w/v) milk. Subsequently membranes were incubated with primary antibodies (1:1000 rabbit anti-LEDGF-PWWP [Abcam; ab177159], 1:1000 rabbit anti-GAPDH [Abcam; Ab9485]. Detection was performed using secondary horseradish peroxidase-conjugated goat anti-rabbit [Agilent] and chemiluminescent substrate Clarity ECL or Clarity Max ECL [Bio-Rad Laboratories]. Imaging was done with the Amersham™ ImageQuant 800 Western blot imaging system [Cytvia]. Total RNA from MeCP2 KD and control HEK293T cells was isolated using the Aurum Total RNA Mini Kit [Bio-Rad Laboratories]. Further sample preparation and differential gene expression analysis was performed by the Genomics Core Leuven. Sample quality was assessed using the Bioanalyzer [Agilent Technologies] with the RNA Nano Kit [Agilent Technologies]. Library preparation was performed with the TruSeq Stranded mRNA Preparation Kit [Illumina]. Denaturation of RNA was performed at 65°C in a thermocycler and cooled down to 4°C. Samples were indexed to allow for multiplexing. Sequencing libraries were quantified using the Qubit fluorometer [ThermoFisher Scientific]. Each library was diluted to a final concentration of 2 nM and pooled. Quantification of the samples was determined using the KAPA SYBR FAST qPCR kit [Roche]. Samples were then sequenced on the NovaSeq6000 generating 150 bp paired-end reads. A minimum of 20 x 10 reads per sample were produced. For bio-informatic analysis quality control of raw reads was performed with FastQC v0.11.7. Adapters were filtered using Trimmomatic v0.39. Splice-aware alignment was carried out using Hisat2 against the reference genome using the default parameters. Quantification of reads per gene was conducted using FeatureCounts from the Subread package. Count-based differential expression analysis was performed using the R-based Bioconductor package DESeq2. Reported p-values underwent adjustment for multiple testing using the Benjamini-Hochberg procedure to control the false discovery rate (FDR). Graphs were made in R using the gage package for gene ontology analysis, the ggplot2 package for creating the volcano plot, and heatmap.2 for representation of the top 50 differentially expressed genes. 2.5 x 10 NIH3T3 cells were seeded in an 8-well chamber slide [Ibidi] in DMEM with GlutaMAX [Gibco] supplemented with 5% FCS [Gibco] and 50 μg/mL gentamicin [Gibco]. After 24 hours, cells were fixed with 4% (v/v) paraformaldehyde PFA [Sigma-Aldrich] for 20 minutes. Cells were permeabilized using 0.1% (v/v) Triton X-100 [Acros Organics] in PBS followed by 30 minutes incubation in blocking buffer (0.5% (w/v) BSA [Merck Life Science BV] and 0.1% (v/v) Tween-20 [Acros Organics]) at room temperature. Cells were incubated with 1:500 primary rabbit anti-MeCP2 antibody [Cell Signaling Technology; 3456S] overnight at 4°C. Cells were incubated for 1 hour at room temperature with the secondary antibody, 1:1000 donkey anti-rabbit Alexa fluor 488, and 5 μg/mL Hoechst [ThermoFisher Scientific]. Images were acquired with the Zeiss LSM 880 confocal microscope at the Cell and Tissue Imaging Core at KU Leuven. 96-well black PhenoPlates [Perkin Elmer] were coated using poly-D-lysine (PDL; [Sigma-Aldrich]) and washed twice with PBS before plating 8 x 10 NIH3T3 cells from each cell line in DMEM with GlutaMAX [Gibco] supplemented with 5% FCS [Gibco] and 50 μg/mL gentamicin [Gibco]. 24 hours after plating, cells were synchronized with medium containing 10 μg/mL aphidicolin [ThermoFisher Scientific]. At time intervals, the synchronization medium was replaced with culture medium to measure cells in different stages of the cell cycle. After 30 hours, all cells were fixed with 4% (v/v) PFA [Sigma-Aldrich] and stained with 5 μg/mL Hoechst [ThermoFisher Scientific] for 20 minutes. The plates were imaged using the Operetta CLS High Content Analysis System [Perkin Elmer] at the Bioimaging Core Leuven (VIB-KU Leuven) (channel 1 – wide-field 386-23: Hoechst 333-42, channel 2 – wide-field 488-20: GFP; 20x objective; 4 images per well). The images were analyzed using the Harmony software. First, the nucleus of each cell was detected in the Hoechst channel. The mean area of MeCP2-eGFP foci and DNA foci inside the nucleus was measured in number of pixels. Results are expressed as mean ± standard deviation (SD). Statistical analysis was done using two-way ANOVA followed by Dunnett’s multiple comparison test vs. WT using the GraphPad Prism 8.0 software package. Quantification of western blot images was performed in the Image Lab software [Bio-Rad Laboratories] and statistical analysis was performed using GraphPad Prism 10.0 software package. Details of the experiments, including statistical test, number of biological repeats (n), and p-values, can be found in the figure legends. Error bars presented in the figures represent the standard deviation (SD) around the normalized mean. A non-linear regression – sigmoidal curve fit with 1/Y weighting was fitted to the AlphaScreen data. Error bars presented in the figures represent the standard deviation (SD) around the normalized mean. The number of biological repeats (n) can be found in the figure legends. Differential expression analysis of the RNA-seq data was performed using the R-based Bioconductor package DESeq2. Reported p-values underwent adjustment for multiple testing using the Benjamini-Hochberg procedure to control the false discovery rate (FDR). More details on the bioinformatic analysis can be found in the . For the high content imaging results are expressed as mean ± standard deviation (SD). Statistical analysis was done using two-way ANOVA followed by Dunnett’s multiple comparison test vs. WT using the GraphPad Prism 8.0 software package. The number of biological replicates within the experiment (n) can be found in the table legends.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Used\n",
      "* **AIモデルの判断:** Not Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 61 / DOI: 10.1016/j.uclim.2025.102409  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "The automatic weather stations NOANN network of the National Observatory of Athens: operation and database\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Government-involved urban meteorological networks (UMNs): A global review\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "Meteorological observation networks play a crucial role in monitoring climatic processes in cities, providing observational data for urban climate research. Understanding urban climate is essential for mankind as the global population residing in urban areas is continuously growing, being projected to reach 68 % by 2050 ( ). However, forecasting urban climate has become more challenging under the complex coupling between climate change and urbanization. The ascending frequency of extreme temperature events is visible under global warming ( ; ; ), as well as compound extremes of temperature and humidity in urban areas ( ; ). Additionally, changes in rainfall patterns add stress to urban water supply and drainage systems, increasing the exposure of urban inhabitants to both drought and flooding ( ; ). Meanwhile, heterogeneous urban environments further complicate microclimatic processes, which are influenced by various dynamic and thermal effects from the artificial landscape. For example, a high-density urban environment reduces advective ventilation from peripheral areas, retaining excess heat in city centers ( ; ; ). In light of the accelerating climate risks in cities, there is an urgent need to improve the understanding of meteorological network configurations specific to the unique characteristics and scales of urban areas. Conventional meteorological observation networks are initially designed for synoptic and regional weather observation. These networks, hereafter referred to as regional meteorological networks (RMNs), are built systematically following the World Meteorological Organization (WMO) Document No.8: the Guide to Instruments and Methods of Observation (WMO No.8). This guideline provides detailed instructions and clear explanations for setting up an automatic weather observing system (AWOS) or automated surface observing system (ASOS) with optimized instrument performance ( ). However, automatic weather stations (AWSs) in RMNs are rather scattered, and may not adequately represent the diverse characteristics within urban landscapes, as highlighted by and . The number of meteorological stations is often insufficient in high-density areas, and thus the geospatial differences in microclimate within the built environment cannot be captured. Recently, meteorological observation networks specifically set up for urban areas, hereafter referred to as urban meteorological networks (UMNs), have been introduced and established in some cities worldwide. In general, UMNs are equipped with simple weather stations across a city, focusing on specifically one or several atmospheric parameters. Some are set up by universities or institutions for research purposes (e.g. ; ), some make use of amateur weather stations or smartphones to harness the potential of big data for real-time urban weather monitoring and forecast (e.g. ; ), while some also involve the management and application by local governments. The prime advantage of UMNs is the higher density of meteorological stations within urban areas, providing data with finer resolutions compared to those recorded by conventional RMNs. However, unlike RMNs which are standardized globally by established technical guidelines, UMN configurations vary greatly for different cities as each location has its unique urban geometry. Although the Chapter 9 on Urban Observations has been introduced in the WMO No.8 ( ) since 2008, outlining the guiding principles and key points to note when establishing an urban station, there are no universal standards tailored for UMNs. As a result, observations from different UMNs may be incompatible with each other, impeding inter-city scientific comparisons or technical knowledge exchanges for future UMN development. The last comprehensive review of urban meteorological networks traces back to almost ten years ago by . Developing from the basis of this review, it is timely to investigate the improvement in techniques and set ups used in the latest UMNs across the globe. This paper reviews and synthesizes the current state of government-involved UMN projects worldwide. Our objective is to delineate the strengths of UMNs and highlight challenges still faced by the urban observation community, so as to offer insights for their future development. In recent years, the importance of UMN studies and climate policies has grown significantly across different disciplines that address climate change issues. Therefore, there is a need to establish a more comprehensive understanding of the current state of the government-involved UMN projects. This review could serve as a reference for government entities, researchers, and urban practitioners, facilitating the continual development and enhancement of UMN techniques and standards in the future. To clarify the scope of this research, we need to define a few important terms. Firstly, a meteorological observation network is considered an UMN if they are specifically set up at various urban settings in addition to conventional AWOS and are spaced at a city or neighbourhood scale (around 10 km or finer; ). Secondly, in order to formulate more relevant and focused recommendations for the way forward, this review targets at ‘government-involved UMN projects’, which must meet at least one of the following criteria: It is difficult to conduct a systematic review as different countries have various terms, definitions, or overall presentations of their UMNs. Therefore, as many as possible UMN projects are included to the best of the authors' effort and knowledge. With the timeframe of the literature search limited up to end-2023, the first step of the review process was a global search with generic key phrases on the scientific search engine ‘Web of Science’ to identify potentially relevant publications in the urban climate field. After trial-and-error, the optimized input phrases were ‘Urban scale meteorological observation network’ and ‘Urban scale citizen weather stations/crowdsourced network’. The search was repeated with combinations of these phrases with different country names (Appendix A). A second step to filter UMN projects fitting into the scope of this paper was then conducted. For preliminary results with accessible full papers written in English, they were manually filtered based on the three criteria stated above, and their abstracts were skimmed through to check for keywords such as ‘urban-scale’, ‘meteorological observation’, ‘crowdsourced’ (Appendix A). Some publications ( ; ; ) have summarized past UMN projects in tables – qualified government-involved projects from them were also identified and included for review. Furthermore, a last step to look for open-access documentations on non-academic websites was performed on ‘Google’ in attempt to encompass more relevant projects. Finally, 33 government-involved UMN projects (a total of 36 publications) were selected for review in this paper (Appendix B). This section presents an analysis on the essential components of implementing an UMN gathered from the reviewed projects. These components include the project metadata, UMN configuration, network management, overall outcomes and impacts. A summary table can be found in Appendix B. The objectives of an UMN project are pivotal as they define the UMN positioning in existing observation systems and guide their future development directions. In general, UMN projects are launched to enhance the current observation dataset from synoptic-scale networks by overcoming the spatial limitation of conventional RMNs, and to utilize the urban observations in socioeconomic and public applications. Reviewed UMN projects often include at least one of following four specific research purposes: (1) Developing quality assurance (QA) and/or quality control (QC) methods to assure the data quality (e.g. ; ; ); (2) Developing new datasets for research and daily operational use (e.g. ; ; ); (3) Refining or downscaling conventional RMN data using novel UMN observations (e.g. ; ; ); (4) Evaluating the potentials and robustness of UMN during operations (e.g. ; ; ). An overview of the number of government-involved UMN projects and their geographic distribution is presented in . A majority of the UMNs reviewed in this paper were implemented by European governments. The European countries contribute about two-thirds of the reviewed UMN projects (20, 61 %). Within the region, the Netherlands is the leading country when it comes to government-involved UMN research (5, 15 %), followed by Germany (4, 12 %), Belgium (2, 6 %) and France (2, 6 %). The remaining countries have a touch regarding this topic with one publication for reference ( ). Although not as active as in Europe, some governments in Asia (9, 27 %), North America (3, 9 %), and Oceania (1, 3 %) have also conducted UMN projects. The uneven project distribution may be attributed to more open access documentations in Europe. It is also possible that there are more meteorological collaborations between European countries, an example being the European Meteorological Network (EUMETNET), which provided initiatives for data exchange and experience sharing in urban climate research. On the other hand, the vast landmass of North American countries could present challenges for their governments to set up and manage UMNs, while priorities would probably not be given to urban climate observations in developing nations where there are less funding and resources. In this review, the project durations are classified into three types: 1) Short-term, for projects lasting only for several days to at most 6 months; 2) Medium-term, for projects lasting from 6 months to 3 years; 3) Long-term, for projects that have been running more than 3 years. Most UMN are developed for long-term operations, but a shorter data period may be extracted from the long timeframe for specific studies, depending on the project type and objective. If the project ending date is not stated in the publications, we assume the project is still ongoing in 2023. Reviewed studies are mainly long-term projects (18, 55 %), followed by medium-term projects (8, 24 %) and lastly, short-term projects (7, 21 %). The project duration is highly dependent on the project objective, and is also related to the UMN type (see ). For instance, the short-term projects are mainly for investigating one seasonal climate phenomenon or testing new equipment setup (e.g. ; ; ), whereas the medium-term projects provide a foundation for trend analysis of specific climate events and developing robust data quality control methods (e.g. ; ; ). The long-term projects usually do not solely focus on the scientific aspect, but also explore opportunities to incorporate urban weather observation data into different social applications (e.g. ). visualizes the data period of each project in the form of a timeline, showing an increasing popularity in UMN studies from the early 2010s. Since the earliest version of the WMO No.8 including the Urban Observations chapter was available in 2008, it may have may contributed to the growth of UMN studies. UMNs can be classified into single-sourced UMNs or crowdsourced UMNs. A larger proportion of the reviewed government-involved UMNs belong to single-sourced networks (19, 58 %). Their weather stations are erected by the government. Official bodies have the right to amend original configurations and have full access to the initial data collected by these stations. The centralized management of such UMNs often allow them to be run for a longer duration of time. The remaining UMNs in this review can be classified as crowdsourced networks (14, 42 %), in which the stations are mounted individually by citizens or private bodies. This opportunistic sensing approach is becoming more popular for acquiring low-cost, high-density urban observations, but data from such crowdsourced UMNs may require additional in QC and maintenance as will be further discussed in . It is a common practice for the government to outsource the data acquisition process and make requests for initial data from third parties who manage crowdsourced networks (e.g. ; ; ; ). Privately owned stations serve as a prevalent data source in current fine-scale urban climate research because there are fewer restrictions than conventional meteorological observation networks. They are more convenient in practice due to the flexible choice of sensors, which will be elaborated in the . Referring to the spatial extent classification in , conventional WMO-standard RMNs are typically of a mesoscale or a regional scale. In other words, their station separations are around 10–1000 km. Existing UMNs are mostly in a city or neighbourhood scale, where the station separations are below 10 km and much smaller than in RMNs. Specifically, neighbourhood-scale (10 –10 m) networks account for around two-thirds (22, 67 %) of the reviewed UMNs (e.g. ; ; ), while networks with city-scale (10 –10 m) coverage only make a small proportion (5, 15 %). As an indication of the spatial resolution in weather data typically achieved by UMNs, the most common distances of separation between urban weather sensors and stations in previous research are 2.5–3 km and 5–10 km, respectively ( ; ; ). For crowdsourced networks, the distances between stations are highly variable as they depend on the spatial distribution of individual station owners. Yet, less restriction in spatial siting allows the density of some crowdsourced networks to be even higher, and as a result falling into the neighbourhood scale (8, 24 %) with distances between stations down to only a few hundred metres ( ). By narrowing the separation distances between stations, UMNs can increase the spatial resolution of weather data and enhance the data richness within urban areas. Nevertheless, it should be noted that most of the reviewed UMNs currently do not provide coverage for the entire city, but cover only selected areas of interest. A conventional AWS should be placed in a flat, open space larger than , assuring the measurements are not interfered by any obstacles in surroundings ( , Volume I Chapter 1.3.3 Siting and Exposure). Noting that it is impossible for urban stations to conform to the same standards for site selection and instrument exposure of conventional AWSs due to the complexity of urban environments, the WMO has in fact laid out principles on the ideal site selection adapted for urban meteorological observations in Chapter 9 of the WMO No.8 Volume III ( ). It emphasizes on choosing a representative site that could reflect the typical conditions for the surrounding urban terrain. The sensor siting criteria should also be adjusted based on the UMN objectives and installation environment. Three typical approaches for weather station siting are identified from previous single-sourced networks. The deployment location of sensors will impact the quality and accuracy of data, which is crucial to subsequent meteorological research and application. The first approach is based on the land use type in the city. As shown in a , covering as many as possible of the different land covers present, such as buildings, parks, roadside, etc., is the simplest method to ensure spatial diversity in the collected data (e.g. ; ). Local climate zone (LCZ), a climatic-based landscape classification method on the local-scale (i.e., hundreds of meters to several kilometres; ( ), is a more systematic way to distribute the weather stations according to the zoning classifications. b shows an example of such approach by . This scheme facilitates inter-comparison of the observation data representative of each LCZ worldwide. The second approach considers siting density when deciding the weather station locations. There are two strategies for distributing sensors: homogeneous distribution in the urban areas or radiative distribution from the city centre to the suburban region. The homogeneous distribution method attempts to scatter the stations evenly within the site of investigation as illustrated in a (e.g. ; ; ). As for the radiative distribution, the weather sensors are unevenly spaced as shown in b, with density being the highest at the city centre and gradually decreasing with distance from the city centre (e.g. ; ; ). The choice of siting density distribution depends on the project objectives, for example whether the UMN is targeted for obtaining higher data resolution in the city centre or for investigating the transition in microclimate between urban and rural areas. Weather sensors can also be distributed depending on weather parameters. These weather stations are strategically placed at locations exhibiting the most representative data characteristics for designated parameters, such as temperature and humidity (e.g. ; ). For example, in urban heat island (UHI) studies, weather stations are often placed at a location where it is the most suitable for capturing the urban temperature variability in built-up regions against greenery areas (e.g. ; ; ). There are also recommendations on measurement approaches for UHI studies by the WMO ( ). This approach is typically applied in UMNs implemented with specific research objectives, thereby accommodating the data collection and analysis processes. There are some additional considerations for the station siting. The selected site should have a stable power source and communication network to ensure sensors collect and upload data promptly in real-time (e.g. ; ; ). Meanwhile, the locations should be easily accessible to technicians and volunteers (if any) for the convenience of maintenance (e.g. ; ). Furthermore, the urban weather stations should, as far as possible, be placed in relatively open spaces and at a distance from tall buildings. Commonly selected locations include the rooftops of schools, mounted on traffic lights, and inside urban parks. It is crucial to ensure that there is enough exposure for the sensor and it is not directly obstructed by neighbouring artificial structures or influenced by anthropogenic emissions. Reference can be made to the WMO siting classification scheme (e.g. ; ; ). It is worth mentioning that for crowdsourced networks, systematic or strategic planning of station siting is difficult as each station is operated by individual owners. The layout and distribution of crowdsourced UMN weather stations therefore cannot be determined at the early network design stage. Overall, the siting of urban weather stations within the city depends on the complex physical environment and needs to be fit for purpose. It is common for the two introduced network types to adopt different types of stations. In single-sourced UMNs, weather stations are typically more multifunctional and are maintained by government meteorological services or volunteers recruited by officials. The stations can be categorized as all-in-one stations and self-assembly stations. An all-in-one station is a professional weather station that has been calibrated and can be used instantly after purchase to measure multiple variables, such as temperature, humidity, wind, radiation, etc.. Contrarily, a self-assembly station is assembled using the same/different brand(s) of sensors and data loggers. It is recommended that these sensors are calibrated against reference stations before being deployed into the observation network. In crowdsourced UMNs, weather stations are often plug-and-play weather stations. They are simple to use, compact in design, and do not have fixed initial physical configurations nor specific software settings. This kind of amateur weather station is popular among citizens as they are available at affordable prices. Installing the stations in dwellers will concurrently form a dense sensor network in a city. In this context, governments can receive local scale observation data via various data sources by accessing companies' online databases or inviting the public to upload their data to government-managed crowdsourcing platforms. Among the reviewed projects involving crowdsourced data provided by different brands of personal weather stations, Netatmo appears to be the most widely adopted in urban climate research. (e.g. ; ; ; ). Heights of 2–3.5 m above ground and on building rooftops are the two most prevalent vertical positions adopted in the reviewed single-sourced UMNs. These positions prevent the data from being disturbed by human activities near the surface. Most of the UMNs set the sensor height at 2–3.5 m above ground ( ; ; ; ; ; ). Some sensors installed on lampposts are placed between 3.5 m and 10 m ( ; ). As for sensors located on rooftops, their approximate height above ground is 12–16 m for a building with four to five floors and assuming each floor is 3 m in height ( ; ; ). The crowdsourced networks do not provide information about the vertical siting height because the sensor height is subject to the users' preference. According to the , the screen height of urban-based stations can be 0.5–1.5 m greater than their suggestion, which is normally 1.25–2 m above ground. This adjustment is made considering the practical aspects of the network execution, such as security concerns, mounting prerequisites, and internet access. The mentions that the observation height should preferably not have a significant deviation from standard height observations of ASOS. It also pointed out caveats of placing stations on rooftops as they often create a unique microclimate from that within the urban canopy due to modified airflows and different construction materials. Nevertheless, studies by and indicate that sensors on the rooftop are also reliable in obtaining representative data of the urban atmospheric processes. Therefore, the vertical siting height of sensors in most of the reviewed projects are generally in the acceptable range for obtaining representative data. An UMN typically records several weather variables, and air temperature is the most analysed variable for research application and network validation after the measurement. More than half of reviewed projects measure 4–6 weather variables (20, 61 %), and some even more than 7 (5, 15 %). Self-assembly stations are usually simpler and designed to observe only 1–3 specific variables (8, 24 %). Most reviewed projects configured networks capable of measuring various weather variables, but most projects did not utilize all available variables in the network validation stage and urban climate analysis. The most studied variable from the reviewed UMNs is temperature (31, 94 %), followed by wind speed (13, 39 %) and wind direction (9, 27 %). Other variables that have been measured but less commonly utilized in follow-up analyses are humidity (8, 24 %), pressure (8, 24 %), precipitation (6, 18 %) and solar radiation (4, 12 %). This result indicates that current urban climate research heavily focuses on fine-scale temperature analyses, such as the UHI phenomenon. Other emerging areas of interest are on wind variability, street canyon ventilation and dispersion studies (e.g., ; ; ). By making use of all the available weather variables from UMNs, the research topics could be further diversified with the urban microclimate more comprehensively understood. Data measured by UMN stations must be processed appropriately before they can be used in relevant studies and applications. First, it is necessary to review the raw data time interval as the data timestamp is important to future data manipulation and analysis. There are two ways to adjust the time interval of the raw data before uploading them to the server – time aggregation and direct transmission. Time aggregation means that the data are temporally averaged to a time interval larger than the one actually measured before being uploaded and saved to the database. A few of the reviewed networks aggregate the raw data time interval (4, 12 %). For example, with the original measurement interval being <20 s, data are aggregated to 1–10 min intervals for storage ( ; ; ). The advantages of this method are saving up storage and filtering out noise from high-resolution data; however, some temporal details in the measurements may be lost and it requires an additional setup in the initial upload settings. Direct transmission refers to unprocessed data being directly uploaded to the server after measurement. In other words, the data time interval equals the sensor timestamp. This method is more straightforward and easier to implement. A significant proportion of networks apply the direct transmission method at the data collection stage (23, 70 %). The data time interval of this data transmission method is usually between 1 and 10 min (e.g. ; ; ). The goal of raw data time interval adjustment is to ensure the data are representative as the temporal resolution significantly impacts the quality control procedures, research methodologies, and future data analyses. Typically, the collected data are then passed through a network data validation process to ensure data quality. Quality assurance (QA) is executed before the station deployment, while quality control (QC) is conducted after the raw data acquisition. The general procedures of QA and QC are shown in . Two-thirds of the projects conduct QC procedures to detect missing and erroneous data (22, 67 %), while QA is a less popular practice in the reviewed projects (12, 36 %). QA focuses on hardware calibration with standardized equipment before the start of a project. It is often a time-consuming process. Most projects, especially the ones employing plug-and-play stations, adopted the sensor calibration range from manufacturers. It is easier for single-sourced UMNs to perform QA procedures, such as calibration with a standardized RMN station or laboratory testing (e.g. ) before installation. On the other hand, the data quality of crowdsourced UMNs with stations independently installed by different owners relies heavily on the QC procedures after the observation. Reviewed projects either develop their own QC method (e.g. ; ) or perform QC methodology scripted by previous researchers as open-sourced codes (e.g. ; ). Given the importance of data representativeness and quality for subsequent analyses, it would be more ideal to include both QA and QC in UMN projects. However, only a minority of projects include both QA and QC stages (8, 24 %). There is thus still room for improvement in data quality enhancement in the future. The station maintenance routine is occasionally covered in the documentations of previous projects. Around one-third of the reviewed projects briefly describe their station maintenance routines (10, 30 %). Station maintenance is essential as it ensures the proper functioning of the network and extends its time of service. The basic maintenance procedure involves regular site visits for sensor inspection and calibration. The assessment period depends on the type of weather station. All-in-one stations are approximately checked every 1–2 years (e.g. ; ), while self-assembly stations require more frequent checking, varying from every 10 days to 3 months, mainly for battery replacement and preventing them from malfunctioning due to network disruption etc. (e.g. ; ). Systematic, top-down station maintenance is often not applicable to crowd-sourced networks because the stations managed individually by the station owners and may be set up on private land. From the experience of the reviewed projects, the maintenance of UMNs requires substantial resources, including staff to visit the stations one by one regularly. Thus, the denser the UMN, the more time is required for the maintenance routine. With the involvement of the government, it seems more resources can be leveraged for maintaining such UMNs. The fine scale data collected by the reviewed UMN projects provide a solid foundation and useful indication for policy implementation and climate change mitigations. As mentioned in , the intensification of extreme weather and growing imperatives in recent decades have prompted nations to react to the adverse impacts brought by climate change, particularly in cities where a majority of the global population resides. The deployed UMNs, therefore, become crucial reference data sources (e.g. ; ; ; ). The data collected help to quantify current urban climate phenomena and extremes within urban areas, such as the UHI, making it easier for policymakers and stakeholders to grasp the intensity of events. The climatological information could also be combined with different environmental and health data for a coupled analysis of their impacts on the society and ecosystem. Most of the reviewed government-involved UMNs were launched in effort to provide a scientific basis for supporting future policy and legal implementation. The role of urban-scale observation data must therefore be acknowledged in future climate change adaptation planning. In addition to facilitating the policy planning process, UMNs also foster smart city evolvement as the physical component of the network can be integrated into the intelligent systems of a smart city (e.g. ). Though UMNs are initially designed for urban meteorological observation, the spectrum of data collected can be expanded by installing additional equipment, such as an air pollutant sensor and noise level meter, at the same locations. Real-time urban information can be applied in different disciplines when the network coverage is sufficient and the integrated systems become mature. There are only several reviewed projects that have considered extending the UMN to smart cities applications thus far ( ; ). Governments are encouraged to consider extending the applications of the deployed UMNs towards smart city development in the future. Besides the tangible benefits in data collection and urban climate research, an UMN can act as a pioneer in exposing citizens to different scientific fields. Nowadays, more projects try to combine citizen science concepts in their implementation. Citizen science is not only an effective way of public education, but also a channel to engage layman participants in science. More importantly, these participation programs can engage stakeholders in science and policy planning, bridging citizens with the government and researchers. In the context of UMNs, most projects with citizen science elements have proven that crowdsourced data can be a reliable data source (e.g., ; ; ; ). These projects not only encourage the progress of research and public data sharing, but could also narrow the gap between the general public and official authorities, and stimulate public interest in research and science. With proper public education and active support by the general public, the deployment of UMNs would be more widely accepted and the number of reliable observation points could increase significantly. In a nutshell, UMNs provide valuable real-time meteorological observations in the urban areas. They also yield unprecedented effects outside the science field, providing a concrete and solid foundation for social, technological, and policy development in the city. In this discussion, we first summarize the strengths and challenges of UMNs with respect to RMNs drawn upon the extensive review of UMN projects. For our regional interest, experience from UMN projects in Hong Kong are highlighted and briefly discussed. Finally, some suggestions for the way forward of global UMNs are provided. Hong Kong is a subtropical coastal metropolis located over the eastern part of the Pearl River Delta, China. With a total area of just above 1000 km , of which only a quarter is classified as built-up ( ), it is home to about 7.5 million people. Its diverse landscape and complex urban environment make it a perfect testbed for urban climate studies. Experiences from three government-involved UMNs implemented in Hong Kong are briefly discussed in this section. Hong Kong Observatory (HKO), the governmental body responsible for providing the city's meteorological services, launched a collaborative project with local universities and schools since 2007 called the Community Weather Information Network (Co-WIN). The primary objective of Co-WIN is to promote weather and climate education among participating schools. A crowdsourced UMN equipped with low-cost ‘do-it-yourself’ (or self-assembly) modular sensors at over 100 locations was thus formed ( ; ; ). The weather data, including temperature, humidity, rainfall, wind, solar radiation, mean sea level pressure, gathered are publicly shared online in real-time ( ), and can supplement the official weather observations made by the AWOS of HKO. However, the locations of these stations, mostly on the rooftops of school buildings, are subject to environmental constraints and the maintenance of stations depend heavily on resources and support available at individual schools, leading to high data variability and unstable availability of observations. Therefore, much work has to be done on data QC before application in scientific research ( ). In addition to its well-established network of over 100 AWSs in operation, the HKO initiated a microclimate observation project in 2017 using ‘i-button’ temperature sensors set up over Kowloon, an urban centre of Hong Kong ( ). The project faced a major challenge of sustainable human resources, as the sensors powered by internal batteries required manual data retrieval regularly. With government technology funding support in 2018, the network was enhanced and expanded with solar-powered microelectromechanical system (MEMS) sensors ( ) which allowed real-time automatic data transfer via the 4G or LoRaWAN network. These stations measured additional weather elements, including relative humidity, air pressure, and UV-index. The HKO also patented a bollard-style housing for the MEMS sensors such that they can blend in better to urban landscapes. As of the end of 2023, around 30 microclimate stations (MCS) have been established over urban and suburban areas of Hong Kong and covering various environments such as dense urban districts, urban greening, coastal areas ( ). A third government-involved UMN in Hong Kong was implemented as part of the Multi-functional Smart Lampposts (SLP) pilot scheme ( ), aiming to promote smart city development by collecting real-time city data, such as air quality and traffic flow, and supporting a city-wide 5G mobile network. This project missed the search radar of the review as it was not packaged as an ‘urban climate’ project, nor was it documented by any academic publications. Over the past few years, around 70 SLPs equipped with off-the-shelf compact meteorological sensors that measure air temperature and relative humidity, as well as wind speed and direction for roughly half of them, were progressively installed along selected roads. The sensors are placed at two levels, 3–5 m or 10–12 m off the ground, depending on the height of the lamppost. The data collected are released to the public as open data, but their usage in scientific research and applications are still currently limited, probably due to the relatively short data period and the highly concentrated yet uneven sensor distribution within the city for comprehensive studies. The locations of operational AWS and microclimate stations with active measurements (as of 2024) from the three UMNs mentioned above within the urban areas of Kowloon and Hong Kong Island are plotted in . It is clear that such UMNs can increase the spatial resolution of weather observation data and help to delineate neighbourhood-scale weather conditions more precisely. However, observations made by microclimate stations are more susceptible to environmental influences, including surrounding landscapes, surface properties of artificial materials, and human activities. Besides, compared to RMNs which have pretty standard AWS setup with meteorological sensors housed inside Stevenson Screen, the configuration of UMNs vary in equipment setup and sensor types. In particular, UMNs often adopt more light-weight and compact protection covers for more flexible deployment ( ; ). During a fine spell in mid- to late July 2022, the mean temperature recorded by different equipment setup at the same location was found to differ up to 1.5 °C during the day ( ) and the maximum temperature difference for a certain timestamp could even reach 3 °C (not shown). This example illustrates the potential challenge in integration and comparison of data between UMNs and the importance in QA and QC procedures before conducting urban climate analyses. Nevertheless, observations from UMNs in Hong Kong have been applied to better understand the temporal and spatial temperature variations in the city under different wind regimes ( ; ) and as a result of different land surface characteristics ( ; ). The HKO has also attempted to develop urban temperature forecasts by incorporating measurements from UMNs into statistical downscaling models ( ) and post-processing algorithms ( ), so to enhance public weather services and allow citizens to gain better access to weather changes within urban areas when planning activities or assessing health risks especially related to urban high temperatures. Moreover, the real-time temperature data collected by a microclimate station next to the district cooling system in east Kowloon was utilized to adjust chiller plant outputs, thereby saving electricity ( ). Therefore, although challenges remain in standardizing and managing UMNs, there is much potential in their contributions to urban climate investigations, local weather services, and even inter-disciplinary applications in the energy or health sectors as illustrated by the lessons learnt from Hong Kong. The WMO No.8 serves as an essential reference for development of weather observation networks globally. Inclusion of the general guidance on urban observations has likely encouraged more meteorological agencies and researchers to develop UMNs since 2008. Positive response from the urban climate research community may have then fostered subsequent revisions of the WMO No.8 Chapter 9, forming a positive feedback cycle. On the other hand, standard synoptic weather stations set up in compliance with the WMO No.8 standards were often used to crosscheck and calibrate observations from UMNs (e.g. ; ; ; ). However, previous research noted that it was difficult to follow the ideal principles set out in WMO No.8 and thus many preferred to adopt self-defined siting and instrumentation approaches, especially for crowdsourced networks (e.g. ; ; ; ; ; ). With less comprehensive and restrictive guidelines adopted globally for UMNs, much inconsistency in station siting, network configurations, equipment setup, and data processing exists among different countries or even within the same city, e.g. the three different UMNs in Hong Kong (see ). Urban environmental constraints and other factors such as station management and available resources also lead to differences in UMN configurations. These factors impede comparative studies or project collaborations as the so-called standards may not be compatible between networks. Moreover, though the LCZ scheme has been introduced by , there is currently no universally accepted scheme on urban classification specific for the purpose of microclimate monitoring. Being a representative scientific organization, the WMO could further enhance the current guidelines on urban weather observations ( ) by providing more comprehensive standards and concrete requirements for UMN setup that could ensure similar data representativeness and quality across cities, yet adaptable to the wide spectrum of urban forms. Examples of UMNs in demonstration cities could be provided such that others can learn from their success. Practical suggestions on choice of sensors and guidance on network management are also recommended for sustaining the data quality and improving the network's durability. To facilitate future UMN development and allow intercity comparative studies, it is crucial that governments work together to adopt similar standards when setting up their UMNs. The FAIR principle for scientific data could also be promoted to UMN managers so to further improve the usability of collected weather data among interested stakeholders. However, such standardization process would take time and more challenges are foreseen when it comes to crowdsourced networks which governments often has less control over. Nevertheless, implementation of city-specific and non-standardized UMNs are still highly encouraged to enrich the database in urban observations and for citizens and researchers to gain more experience in the field of urban climate. The reasons for a government to initiate an urban weather observation project can be twofold. First, the government can provide stable financial support for the UMN, which requires notable implementation and maintenance costs. This will contribute to prolonging the lifespan of an UMN and ensuring sustainable management and operation. It is also more convenient to deploy standardized setup within a top-down, single-sourced UMN. Secondly, the government can utilize data collected by the UMN to improve official weather services and benefit from the fine-resolution weather data by applying them to other disciplines, such as energy saving. Besides, with evidence from microclimate observations, the government can react more promptly in the review of policies relating to urban heat-health or environmental quality issues. There would be less misunderstandings and a more effective communication process between science and policy stakeholders when discussing climate policies and mitigation strategies. With reference to the Hong Kong case study, HKO has implemented various types of UMN in the past decade in collaboration with local universities and community stakeholders to test which UMN is the best-fit network in the urban settings of Hong Kong. This example demonstrates the importance of government involvement in scientific research and foresight in data implementation for different social disciplines. Hence, to launch a durable, cost-effective and community-accepted UMN, the active involvement, or leadership, of a government is indispensable, especially in the provision of reliable financial resources and bringing various sectors in society and science together. Global governments have visibly put more emphasis on urban climate research in recent decades. However, notable time is still required before reliable and persuasive results can be obtained and widely applied. Thus, there is more work for the full potential of UMNs to be unleashed. In the short term, governments could aim to continue developing and promoting permanent UMN networks, both single-sourced and crowd-sourced. In the long term, governments should share experiences and work towards establishing a set of comprehensive and standardized guidelines under the framework of WMO for future UMNs. Under climate change and urban population growth, UMN projects serve as crucial drivers in connecting science and different stakeholders regarding climate change issues and mitigation policies. In this paper, we reviewed 33 worldwide government-involved projects with UMNs that are dedicated to urban climate observations. The network nature, strengths and constraints of the projects can be summarized as follows: The main purpose of this study is to give an overview of the current state-of-the-art of the government-involved UMN projects worldwide. This review will hopefully serve as a useful reference for future UMN development or the implementation of more comprehensive standard guidelines. Governments play a crucial role in the development of sustainable cities. Their involvement in UMNs benefit research and applications in various physical and social science topics, such as human thermal comfort, urban greening, liveable city designs, climate mitigation etc.. All these studies are key drivers for achieving Goal 3 (Good health and well-being), Goal 11 (Sustainable cities and communities) and Goal 13 (Climate action) laid out in the 2030 Agenda for Sustainable Development ( ). Hence, the fine-scale, high-quality observations of microclimate processes within cities by UMNs can be a pivotal game changer in climate mitigation efforts and sustainable city development, bringing synergistic effects to urban social studies and climate science in the foreseeable future. Lastly, it needs to be acknowledged that the list of UMN projects reviewed in this paper is by no means exhaustive. A major limitation of this review comes from the difficulty in identifying all relevant UMN projects using a systematic screening process. The lack of a standard definition of the term ‘government-involved’, as well as variations in project naming and documentation in different countries also make it difficult to ensure no existing projects are left out, especially those not documented in scientific publications that are accessible online and written in English. The somewhat manual shortlisting method detailed in may have overlooked some potential research but we have tried our best to include as many UMN projects as possible. Writing – original draft, Methodology, Formal analysis, Data curation, Conceptualization. Writing – review & editing, Writing – original draft, Formal analysis. Writing – review & editing, Methodology. Supervision, Conceptualization.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Not Used\n",
      "* **AIモデルの判断:** Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 62 / DOI: 10.1016/j.imed.2024.12.001  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Building a knowledge graph to enable precision medicine\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Osteosarcoma knowledge graph question answering system: deep learning-based knowledge graph and large language model fusion\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "Osteosarcoma is one of the most common bone tumors in children and adolescents [ ]. Osteosarcomas exhibit a high degree of histological heterogeneity, including mature bone tissue, fibrous tissue, and tumor cells [ ]. Currently, treatment for osteosarcoma primarily involves surgical resection of tumor tissue, with chemotherapy employed to mitigate the risk of recurrence. However, the efficacy of this approach is suboptimal, and the treatment of osteosarcoma remains a significant clinical challenge [ ]. Therefore, a more comprehensive understanding of the underlying mechanisms of osteosarcoma is essential for providing key insights into the treatment process and for informing the design of precise medical treatments. Significant advances have recently been made in the study of the molecular mechanisms underlying the pathogenesis of osteosarcoma. These advances include the identification of numerous gene mutations, such as those affecting tumor protein p53 (p53) and RB transcriptional corepressor 1 (RB1), as well as abnormalities in signaling pathways, including the Phosphoinositide 3-Kinase (PI3K)/Protein Kinase B (AKT) and Mitogen-Activated Protein Kinase (MAPK) pathways. These findings have shed light on the complex relationship between these molecular alterations and the initiation and progression of osteosarcoma [ ]. Moreover, in recent years, numerous studies have identified several potential therapeutic targets. These include molecules such as Receptor Activator of Nuclear Factor Kappa-B Ligand (RANKL), Insulin-Like Growth Factor 1 Receptor (IGF-1R), and Cyclin-Dependent Kinase 4 and 6 (CDK4/6). Clinical trials have been conducted in this area; however, further studies are necessary to verify the overall efficacy of these treatments [ ]. Several authoritative organizations have successively issued guidelines/expert consensus statements on the diagnosis and treatment of osteosarcoma. These include the National Comprehensive Cancer Network guidelines and the European Society for Medical Oncology clinical practice guidelines. Nevertheless, given the continuous emergence of new evidence, it is necessary to ensure that these guidelines are continually updated and adjusted to reflect the latest developments. Integration of the various datasets will facilitate researchers in obtaining a more comprehensive and accurate understanding of the subject matter, enabling a deeper insight to be gained and providing a means of discovering links between different genes and targets. The knowledge graph (KG) is a knowledge representation and management technique that was proposed by Google in 2012 [ ]. KG is a structured semantic knowledge base that is constructed by organizing knowledge in the form of “entity-relationship-entity” triples, as well as entities and their associated attribute-value pairs. These entities are then linked to each other through relationships to form a mesh-like knowledge structure [ ]. In the context of the advent of the medical big data era, the ability to extract meaningful insights from vast quantities of data has become a critical aspect of medical big data analysis. The investigation of this technology's applications in the medical field will prove instrumental in reconciling the discrepancy between the inadequate supply of premium medical resources and the relentless growth in the demand for medical services [ ]. Nevertheless, KGs typically employ the ternary form (entity-relationship-entity) as a means of representing knowledge. This representation is constrained in its expressive power, as it is unable to fully express complex relationships and concepts, which could result in a reduction in their expressive potential in certain areas [ ]. Moreover, the management and querying of KGs may become increasingly challenging as they grow in size because of increased complexity [ ]. In recent years, large language models (LLMs), such as ChatGPT-3.5, ChatGPT-4, and others, have made significant advancements in natural language processing (NLP) capabilities, exhibiting impressive abilities to comprehend and generate text. These large models are adept at learning from vast quantities of data, demonstrating the capacity to perform a range of tasks and providing efficient question-and-answer and intelligent assistance services for humans [ ]. In the field of healthcare, the application of artificial intelligence (AI) technology has also become increasingly prevalent, encompassing a diverse range of applications including image analysis, clinical decision making, and drug development [ ]. For example, DeepDR-LLM represents a pioneering multimodal integrated intelligent system for diabetes diagnosis and treatment with a visual-big language model [ ]. As a significant subcategory of AI, LLMs are capable of efficiently comprehending and generating medical natural language content, thereby providing substantial assistance in various clinical contexts, including the design of treatment plans. Despite the potential for large models to offer significant benefits in various fields, including medicine, their application is occasionally impeded by a lack of guidance from specialized domain knowledge, resulting in models that are either factually incorrect or lack sufficiently detailed bases. This can lead to difficulty in fully satisfying the high demand for intellectual rigor in medical research and other areas [ ]. As a result, researchers have turned to combining KGs with LLMs to solve such problems. Specifically, LLMs leverage the KG to demonstrate reasoning and interpretation capabilities, enhancing the interpretability and transparency of the response results rather than simply repeating knowledge [ ]. For example, the ReLMKG method proposed by Cao and Liu [ ] integrates pretrained language models with KGs, utilizing a joint reasoning model to bridge the gap between questions and the KG. This approach leverages explicit and implicit knowledge for complex question-answering reasoning, yielding strong performance across multiple datasets. Wu et al [ ] introduced an LLM-based framework that employs a three-step process involving subgraph retrieval, KG-to-Text conversion, and answer generation, combining KGs and language models for reasoning. This framework significantly enhances question-answering performance across various datasets. Given the above considerations, we employed KG technology in conjunction with an LLM to construct the world's first KG in the context of osteosarcoma and developed an interactive question-and-answer (Q&A) system based on it. Specifically, we constructed the first osteosarcoma knowledge graph (OSKG) based on literature data from the PubMed database. Combined with an AI dialog system, this KG is designed to provide researchers with an efficient information retrieval platform for rapid access to knowledge related to the biology of osteosarcoma. This technological breakthrough fills the gap in the field of osteosarcoma regarding knowledge collection and AI research, advancing the development of related studies. Furthermore, the integration of the KG and ChatGLM proved to significantly improve the system's ability to retrieve scientifically accurate answers. The system demonstrated increased accuracy in querying complex biological relationships and increased relevance of answers than traditional methods, ensuring users receive not only a more intuitive query process but also reliable and scientifically validated information. Integrating KGs with LLM in this approach not only exploits the complementary advantages of the 2 but also fully develops their the data-driven and knowledge-driven capabilities. A comprehensive search of the PubMed database was performed using the keyword “osteosarcoma” from 2003 to 2023, a period selected to ensure the inclusion of up-to-date and relevant research. This approach allowed for the creation of a comprehensive and time-sensitive KG. The search was conducted using titles and abstracts from thousands of academic journals and research institutions in all areas related to “osteosarcoma.” After screening, we obtained a substantial corpus of articles comprising 25,415 entries. This comprehensive dataset served as a valuable reference and data source for the construction of KG in the field of osteosarcoma. During the data preprocessing phase, a series of procedures was conducted on the 25,415 obtained articles to guarantee the quality and accessibility of the data. The initial step in the process was data cleaning, which involves the removal of invalid retracted data and data that is not directly related to the keyword “osteosarcoma,” such as data on Ewing's sarcoma and canine osteosarcoma. After this step, 18,270 articles were retained. The text data were standardized by removing special characters (<sup>, <i>, <b>, etc.) and processing long sentences, including longer content in brackets and multiply hyphenated content, in the process of sentence deconstruction. If parentheses or hyphens are encountered within a sentence, the entire parenthetical expression and the entirety of the hyphenated content were treated as a single word and merged with the subsequent sentence. The aforementioned processing methodology circumvented the issue of incomplete entity identification, thereby facilitating the purification of entity content to prevent the extraction of invalid entity information. An additional exploratory analysis was conducted on the processed dataset, resulting in the extraction of more representative articles that were subsequently annotated (detailed in the Supplementary Note). Given the considerable number of documents, it would be an inefficient use of resources to annotate them all manually. Accordingly, approximately 400 papers were selected for manual annotation using the Doccano tool, resulting in the construction of a dictionary of entities and relationships. After annotation, the text was exported to JSON format, which was then used as input for the pretrained text model. This enabled further learning and the generation of a language model that could be automatically annotated. Ultimately, the model processed the entire document data set. In the construction of the entity dictionary, existing ontologies and knowledge bases (e.g., Gene Ontology [ ], GeneCards [ ], KEGG [ ], DrugBank [ ], etc.) were employed to identify pivotal entity types and criteria, thereby ensuring the accuracy and coherence of the resulting corpus. Domain experts reviewed the process and also contributed to formulating a unified specification to analyze and comprehend the text more fully. Manual annotation represents a pivotal stage in the process of developing accurate training samples for subsequent machine learning or NLP tasks. During the annotation process, it was revealed that some entities had synonyms or subcategorized names for the disease. For example, we encountered numerous synonyms during the annotation process, such as “OS,” “osteosarcoma,” and “osteosarcomas,” all of which can represent osteosarcoma. To address this issue, a classification dictionary was constructed to facilitate the categorization of disease entities during the annotation process and enable the differentiation between various names for the same type of entity. Entity recognition is a pivotal challenge within the domain of NLP. The objective of this task is to classify and subsequently identify specific types of entities derived from text [ ]. In the field of biomedicine, entity recognition is a subfield of NLP that is particularly concerned with identifying entities that are relevant to biomedicine. These entities may include, but are not limited to, genes, proteins, drugs, symptoms, diseases, complications, and others. The BioBERT-base-cased-v1.2 pretrained model was chosen as the base model. The BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining) model can be considered a kind of bidirectional encoder representations from transformers (BERT) model as a pretrained language model for use in the biomedical domain [ ]. A research team at the Laboratory for Medical Informatics, Seoul National University, Korea, developed the tool to pretrain biomedical text data to address the shortcomings of traditional BERT models when used in the biomedical domain. A prepared entity dictionary was initially employed to perform a dictionary-based begin, inside, outside (BIO) annotation of the data. The BIO annotation system is a widely utilized markup scheme in the field of sequence annotation, primarily to mark up entities within text [ ]. The BIO annotation method comprises 3 distinct types of tokens: the B token, representing the beginning of an entity, is followed by the I token, which denotes the middle or endpoint of the entity in question. Finally, the O token signifies the absence of belonging to the entity in question, as opposed to its inclusion. In the process of BIO annotation, it is important to recognize that each token represents a distinct entity. In the case of the B prefix, this entity is represented by a single token. In contrast, the pattern B-I is used to denote a series of consecutive tokens, whereas the token representing no entity is represented by the letter “O” [ ]. The text is subdivided into units of meaning—including words, lexemes, phrases, syntactic structures, and deep structures—using automated methods. This facilitates the understanding of text content by the machine learning system. The BioBERT-based v1.2 model was then trained using a labeled entity dataset that included 16 categories of entities, such as treatment modality, disease, classification, feature, drug, illness, gene, protein, and ribonucleic acid. Since each category of entities appears with varying probabilities within the corpus, the relative frequencies of each entity type in the training data also differ. The dataset was split into training, validation, and test sets with a ratio of 18:4:3, ensuring that the model could be properly trained, fine-tuned, and evaluated. The labeled entity data was then fed into the model, and during the training process, the model learned to map the input text to the corresponding entity labels, thereby performing the entity recognition task. The process of relational model training entails the application of machine learning methods to facilitate the capture of relationships between entities within the data [ ]. Such models are typically constructed on the foundation of deep learning techniques in particular neural networks with the objective of discerning patterns and regularities within the provided data. This subsequently enables the anticipation of new relationships between the various entities. Upon conversion of the labeled JSON relationship data into the model input format, 17 distinct types of inter-entity relationships were uncovered. These included facilitating, inhibiting, predicting, causal, and synergistic relationships. The probability of occurrence for each relationship type, along with the number of training relationships, also plays a role in this process, leading to slight variations in the number of relationships across different training sessions. The relationship dataset was divided into 2 subsets: a training set and a test set, with a ratio of 7:3. These subsets were then input into the BioBERT-base-cased v1.2 pretrained model. The model parameters were adjusted based on the input text features and the corresponding entity-relationship labels to optimize performance on the relationship recognition task. As a leading graph database, Neo4j offers significant advantages in the storage and querying of relational data [ ]. The entity-relationship structure in the KGs is managed efficiently. KG is a graphical data structure for representing knowledge, which includes the entities, attributes, and relationships between them [ ]. It presents the associations between different entities in a graphical format, akin to a network of concepts and relationships in the physical world. The purpose of a KG is to capture and organize complex and voluminous knowledge and to make it both computable and querriable [ ]. In KGs, entities are typically represented as nodes in the graph, with relationships between entities represented by edges connecting the nodes. Each node and edge can carry information about the entities and their relationships, including attributes of entities, types of relationships, etc. [ ]. The collected data was input into the previously trained model to construct the KG of osteosarcoma in the Neo4j database. In the process of constructing the gene regulatory network, all other nodes in the KG, except the gene and osteosarcoma nodes and their associated relationships, were first removed. Then, the directionality of edges was eliminated, and duplicate relationship pairs were merged, resulting in 2,244 unique relationship pairs. Based on these relationship pairs, an undirected gene regulatory network containing 1,277 nodes was constructed (Supplementary Table S1). To identify gene modules that may drive cancer development, we employed a restarted random walk algorithm to model the behavior of genes in regulating cancer progression [ ]. Specifically, it first randomly selects a gene from the network as a starting point and then performs a random walk to calculate the centrality scores of all the remaining nodes in the network. Subsequently, the node with the highest score is selected as the next target node and is used as a new starting point along with the previous node. The random walk continues to identify the next highest-scoring node. This process is repeated until the highest-scoring node is identified as an osteosarcoma. This process establishes a pathway that reveals the gene-regulated disease process and defines the set of genes in this pathway as osteosarcoma driver modules. Ultimately, we identified 22 driver modules associated with osteosarcoma. To further explore the functional properties of these modules, we performed enrichment analysis on each module to identify the biological pathways that are most similar to their module. ChatGLM is a novel approach to conversation pretraining developed by a collaboration between Wisdom Spectrum AI and KEG Lab at Tsinghua University [ ]. ChatGLM3–6B is a publicly available model within the ChatGLM3 series [ ]. ChatGLM3–6B maintains seamless communication and has minimal deployment requirements, offering enhanced capabilities and greater accessibility to open-source sequences. We developed an osteosarcoma agent that facilitates interaction between an LLM and KG using 3 distinct tools, enabling users to input the agent in natural language and subsequently extract keywords for querying in the Neo4j database through the 3 aforementioned tools, ultimately returning a natural language reply. : LangChain is a language model-based application development framework that supports the development of several types of agents [ ]. In this study, we employed a conversational agent to facilitate the interaction between the user and ChatGLM3–6B. The user initiates the interaction by posing a query to the ChatGLM3–6B agent, which is then transmitted along with the agent's prompts. The LLM then makes a judgment call based on the instructions to call the appropriate tools to obtain further information and constructs the final answer based on the information retrieved by the tools and returns it to the user [ ]. : A LLMCypherGraphChain is abstracted, and a natural language response to a natural language question is generated. Nevertheless, the system internally formulates a Cypher (the query language utilized by Neo4j) query to address the issue in question, employs an LLM to process the information, and then uses the result to generate a natural language response through the LLM. To produce the final response, several prompts must be constructed. These prompts should include instructions, relevant data from the vector index, relevant information from the graph database, and the user's query [ ]. The ChatGLM3–6B model is then presented with the aforementioned prompt, which it processes to generate a meaningful and accurate response. This is contingent upon the input information provided. The LangChain agent is permitted to retrieve information from the Neo4j database by constructing Cypher statements. Upon inputting their inquiry, the user transmits it to the LLM, which in turn initiates an agent prompt. In this instance, the LLM indicates the necessity for its utilization of the Cypher search tool. This tool, through the construction of a Cypher statement, is employed in a query of the Neo4j database. Subsequently, the query results are transmitted back to the agent. Thereafter, the agent transmits another request with a novel context to the LLM, which generates the ultimate response and directs the agent to convey it to the user, given that the context encompasses the data essential for formulating the response. : Existing implementations of KG indexing can support keyword searches. KG indexing requires the LLM to extract pertinent entities from the question and search the graph for any triples comprising these entities [ ]. Therefore, it is possible to implement a keyword search based on LLM within Neo4j. Nevertheless, while it is possible to search for entities using a simple MATCH statement, subsequent experimentation has demonstrated that it is preferable to utilize Neo4j's full-text index. Once the pertinent entities have been identified through the use of the full-text index, the ternary is returned, and the relevant information is then made available to answer the question. However, if there are multiple entities referenced in the question, it is necessary to construct an appropriate set of Lucene query parameters [ ]. Lucene is a high-performance full-text search engine library that is not a standalone application; rather, it is a component that can be embedded into a variety of applications [ ]. The Lucene search engine provides numerous query parameters that can be utilized to enhance the relevancy of search results. As the full-text index is based on the Lucene search engine, it is possible to return entity-related triples simply by querying the full-text index. The Lucene search engine provides numerous query parameters that can be utilized to enhance the relevancy of search results. Since the full-text index is based on Lucene, the ability to retrieve entity-related triples is simply a matter of querying the full-text index. Finally, the 5 most pertinent entities identified in the full-text index are utilized to generate triples via traversal to neighboring entities. The LLM is instructed to utilize a keyword search tool, whereas the agent is instructed to provide a list of entities that are relevant to the keyword search and then to query Neo4j using Lucene parameters. This methodology permits a more comprehensive examination of the database for information about the triples associated with the entities extracted from the user's input question. Vector similarity search is a novel and rapidly emerging information retrieval technique that has recently attracted significant attention. The core concept of this approach is to represent both the target problem and the data documents in vector format, enabling a determination of the degree of relevance based on the similarity between these vectors [ ]. Software such as LangChain offers a multitude of tools for vector databases, providing users with the capacity to conduct vector similarity searches [ ]. In version 5.11, Neo4j introduced vector indexing [ ]. We employed the vector indexing functionality of Neo4j to implement a vector similarity search for the KG Q&A system. Although traditional vector searches typically yield relevant document text, the data in question is graph-structured. Consequently, the search results are returned in the form of “entity-relationship-entity” triples, analogous to the outcomes of the keyword search mode. However, in contrast to the full-text indexing employed in the latter, vector indexing is employed in the former to identify relevant nodes. For the LLM to utilize the vector search tool, the agent is first obliged to call upon the embedding model to generate a vector representation of the problem at hand. This vector representation is then combined with Neo4j's vector indexing function to retrieve the relevant ternary information and subsequently returns the results to the LLM. It is at this point that the LLM is capable of providing a natural language response to said results. After fine-tuning the BioBERT model, the model exhibited excellent performance in identifying various types of osteosarcoma-related entities and relationships, with a precision of 98.09 %. This demonstrates the model's strong classification ability in osteosarcoma-related domains. Furthermore, the recall rate reached 91.5 %, indicating that the model is capable of effectively capturing relevant entities and relationships associated with osteosarcoma. Based on this KG, we extracted the entity composition subnetworks of genes and disease types and conducted an analysis. This analysis revealed that the genes situated at the center of these subnetworks were the key driver genes of osteosarcoma, thereby validating the rationality and validity of the KG. By calculating and comparing the cosine similarity between the gold-standard answers and the answers generated by different Q&A systems, the Q&A system generated by fusion with ChatGLM3 was found to significantly outperform other mainstream LLMs in terms of answer accuracy. The system assists researchers in rapidly acquiring precise knowledge related to osteosarcoma, improving the efficiency of clinical decision support, and providing a powerful tool for basic research to explore the pathogenic mechanisms and potential therapeutic targets of osteosarcoma. This method entails the construction of a professional osteosarcoma-related Q&A system through the integration of a substantial corpus of existing osteosarcoma literature, complemented by the utilization of a KG and an LLM. The complete process is depicted in . Specifically, we collected literature abstracts on osteosarcoma from the PubMed database over the past 2 decades and used them as the background to construct the first osteosarcoma single-disease KG using the BioBERT model. Subsequently, we interacted with the Neo4j database through the LLM using 3 tools, namely Cypher search, keyword search, and vector search, with the objective of enhancing the intuitive and user-friendly application of the KG while addressing the “phantasmagoria” problem prevalent among LLMs. The integration of the OSKG and the LLM led to the development of an intelligent question-answering system, Knowledge Graph Question Answering (KGQA). The system is centered on the OSKG, which represents a single disease. The system optimizes knowledge acquisition in the field of osteosarcoma by providing a comprehensive and efficient information retrieval platform for researchers. It facilitates the efficient and accurate retrieval of relevant literature, thereby significantly reducing the time and cost normally associated with literature retrieval. The construction of the entire premodel of the atlas was predicated on the BioBERT-base-cased-v1.2 pretraining model, and 16 different entity classes and 17 different relationship classes were used as inputs for prediction ( A and B). The final overall prediction accuracy of the entities reached 98.09 % ( A), including the overall correctness of the relationship prediction, which reached 86.46 % ( B). A total of 18,270 documents were incorporated into the construction of the OSKG, resulting in the identification of 6,633 related entities and 96,035 inter-entity relationships. Furthermore, we compared different KGs, including the Precision Medicine Knowledge Graph (PrimeKG) [ ], the Rare Disease Knowledge Graph (RDKG) [ ], and the OSKG to evaluate their respective network structures (Supplementary Table S2). Our KG exhibited notable superiority in terms of the number of entities and relationships related to osteosarcoma. Furthermore, regarding the consideration of relationship types, the other KGs contained only fuzzy relationships and lacked more specific relationship definitions, such as “promote” and “inhibit.” To evaluate the quality of the KG, we extracted the nodes of the type “gene” that promote the node “osteosarcoma” from the KG and selected a subset for display. The analysis demonstrated that our KG identified a substantial number of genes associated with osteosarcoma ( ). For example, YBX1 was identified as a splicing factor that up-regulates VEGF and downregulates VEGF , thereby promoting the progression of osteosarcoma [ ]. Additionally, another study demonstrated that HOXB6 regulates C-C motif (CC) and C-X-C motif (CXC) chemokines in the cytokine-cytokine receptor interaction signaling pathway, which may contribute to osteosarcoma development and maintain tumor stem cell properties [ ]. To ascertain the dependability and utility of this KG, we undertook the identification of osteosarcoma driver modules by the KG. Specifically, all nodes and their relationships between genes and osteosarcoma were extracted from the obtained KG, and a graph-based gene regulatory network related to osteosarcoma was constructed (see Methods). The objective was to identify the pathways of gene regulation associated with osteosarcoma within the gene regulation network ( ). The set of genes contained in these pathways were then considered as potential disease driver gene modules. Subsequently, to gain insight into the function of these modules, we conducted an enrichment analysis to identify related pathways. This analysis revealed 5 pathways that are significantly associated with osteosarcoma (Supplementary Table S3). Among the identified pathways, the ATP-binding cassette (ABC) transporter superfamily comprises a diverse array of proteins with multifaceted functions. Substrates of ABC transporters are commonly utilized as different backbone agents in first-line chemotherapy or salvage chemotherapy for osteosarcoma [ ]. The pentose phosphate pathway is also involved in the progression of osteosarcoma, regulating the growth and metastasis of osteosarcoma, which is closely related to this disease [ ]. Furthermore, NOD receptors are important components of the innate immune system and are promising targets for cancer immunotherapy [ ]. To enhance the applicability of this KG for osteosarcoma, we developed a KGQA system capable of responding to user queries about osteosarcoma, utilizing entities and relationships in the database in conjunction with the LLM. Specifically, the system can establish a good connection with the Neo4j database using cypher statement queries, keyword queries, and vector queries, which then return the correct results to the LLM. This ultimately outputs the final natural language reply, thereby realizing the osteosarcoma-based KG for natural language Q&A. To validate the accuracy of this method, 100 questions were formulated on the subject of osteosarcoma, comprising 25 questions on each of the 4 clinical, genetic, cellular, and signaling pathways. The correct answers to these questions were sourced from the relevant literature. These questions were then input into KGQA, ChatGPT 3.5, and ChatGPT 4. The responses from ChatGLM3–6B were also evaluated without connecting to the KG database. The responses were then aggregated and compared to the correct answer to calculate the cosine similarity. The large number theorem [ ] was employed to determine that when the number of questions is sufficiently large, the median of the similarity of all answers can be taken to divide the different answers into half correct answers and half incorrect answers. The accuracy scores for ChatGPT 3.5, ChatGPT 4.0, ChatGLM3–6B, and KGQA were calculated for each type based on the delineated cutoff values for correct and incorrect answers ( ). The final results demonstrate that the accuracy scores of KGQA responses were significantly higher than those of the other 3 groups of LLM in terms of clinical, genetic, cellular, and signaling pathways. When comparing ChatGLM3–6B and KGQA, the performance of the Q&A model after KG fusion was notably enhanced. This demonstrates that the KGQA system for osteosarcoma has a distinct advantage in scientific aspects and is capable of providing more accurate assistance and guidance for medical and nursing researchers, and KGQA can answer a question posed in more concise words than the other 3 models can ( ). Furthermore, this project illustrates the potential for the integration of fused KGs and LLMs in highly specialized and rigorous research fields such as medicine. It can provide more accurate assistance and guidance for medical researchers. In addition, this project illustrates that the fusion of KGs and LLMs has significant potential for application in the field of medicine and other specialized and rigorous research domains. A KG of osteosarcoma was constructed based on PubMed literature, comprising 6,633 entities and 96,035 relationships. The BioBERT model demonstrated an accuracy of 98.09 % and 86.46 % in entity identification and relationship prediction, respectively. The gene regulatory network constructed by this atlas identified key driver gene modules for osteosarcoma, and 5 important pathways closely related to osteosarcoma were identified by enrichment analysis. Based on this KG, we developed a smart KGQA that combines cypher, keyword, and vector search methods. When integrated with an LLM, it demonstrated significant accuracy advantages in answering questions in the areas of clinics, genes, cells, and signaling pathways, outperforming mainstream LLMs such as ChatGPT. This study illustrates the potential of KG and LLM fusion to enhance information retrieval and research efficiency in specialized domains such as medicine. Integrating a KG with ChatGLM offers advantages that extend beyond the resolution of hallucinations. By combining structured, updatable knowledge with contextual language understanding, this approach provides a robust foundation for more accurate and reliable responses [ ]. The KG is a dependable repository that facilitates real-time updates, which are vital in fields such as osteosarcoma. Its explicit entity relationships enhance the accuracy of responses, as the LLM draws on validated knowledge rather than relying solely on statistical correlations. This integration improves precision and ensures that new findings are seamlessly incorporated, increasing the relevance and reliability of responses. This study identified several limitations that may affect the accuracy and comprehensiveness of BioBERT-based models. First, the distribution of entities with varying frequencies during the training process resulted in a reduction in the accuracy of predictions for certain entities with low frequencies. Despite the best efforts of the annotators, the results remained unsatisfactory due to the limited data available. Techniques such as data augmentation may be employed to augment their representativeness to enhance the prediction of low-frequency entities. Second, as research trends evolve and the focus shifts to prominent topics, some pivotal entities may be overlooked or even excluded from the analysis. Regular updating of the dataset and balancing of the inclusion ratio of key entities help to maintain the comprehensiveness and consistency of the KG. Furthermore, regarding the fusion of KGs and models, the rapid advancement of LLM technology necessitates the integration of the KG with more advanced LLMs to ensure that model performance is always aligned with the latest technological advances. By taking these factors into account and implementing targeted improvements, the robustness of the KG can be enhanced, thereby making it a more reliable resource in osteosarcoma research. Regarding the present status of research in this field, integrating KGs with LLMs presents challenges such as data quality issues and complex model training, which hinder performance [ ]. Future research should prioritize optimizing data processing through automated cleaning and review to enhance KG accuracy [ ]. Additionally, employing alternative training algorithms and distributed computing can improve efficiency and reduce resource costs in healthcare. It is vital to keep KGs updated and address model bias [ ]. Despite current limitations, we are optimistic that further research will lead to a more robust support system for medical research and clinical practice. In conclusion, this study established the world's first single-disease KG specialized for osteosarcoma and successfully implemented and validated the integration of this KG with an LLM-driven Q&A system. This system is a notable enhancement in the retrieval and accuracy of information within the medical field. Despite some limitations, such as uneven entity distribution and changing research trends, the system is designed to provide a powerful and accurate tool for organizing and accessing complex biomedical information. This integration offers a promising template for the advancement of intelligent questioning in medical knowledge management and professional domains. Writing – original draft, Software, Methodology, Investigation, Data curation, Conceptualization. Validation, Investigation. Software, Formal analysis. Supervision, Project administration. Software, Funding acquisition. Formal analysis. Software. Writing – review & editing, Validation, Supervision, Methodology, Funding acquisition, Formal analysis, Conceptualization.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Not Used\n",
      "* **AIモデルの判断:** Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 67 / DOI: 10.1016/j.nut.2023.112182  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Multiple Indicator Cluster Surveys: Delivering Robust Data on Children and Women across the Globe\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Trends in socioeconomic inequalities in malnutrition among children under 5 in the Democratic Republic of the Congo from 2001 to 2018\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "Malnutrition is a global concern that retards physical growth, increases the risk for disease, and contributes to childhood mortality, especially among children under 5 y of age in low- and middle-income countries [ , ]. Despite collective efforts from the international community and governments to combat global malnutrition through the guidance of sustainable development goal (SDG) targets, the current global annual average rate of reduction (AARRs) is insufficient to achieve the target set for 2030. In most regions, the AARRs required from 2022 to 2030 are substantially higher than those of the past decade. Among these regions, Central Africa requires the most significant increase in acceleration rates, at 13% in the next decade. Its current speed needs to catch up, with a rate of only 0.04% from 2012 to 2022 . The Democratic Republic of Congo (DRC) is one of the largest countries in Central Africa. Almost half of the children under 5 died of severe malnutrition . As a result, malnutrition in the DRC ranked the fourth highest in the under-5 mortality rate . This high rate could be attributed to historical reasons, such as the Ebola outbreaks , civil conflicts , and chronic poverty experienced in the DRC over the past years. These events affected children's health and further weakened the already fragile health care system. Consequently, the vicious cycle between poverty and ill health was established, exacerbating existing challenges. Despite the assistance provided by international donors to mitigate child mortality rates caused by malnutrition, significant disparities persist among different groups of children, and overall progress in improving health and nutrition inequality remains limited. Socioeconomic factors, particularly wealth status and place of residence, significantly contribute to these deeply entrenched health inequalities in child malnutrition [ , ]. One study shows that malnutrition is spatially structured, with rates in rural areas significantly higher than in urban areas . Additionally, another study indicates that children from lower wealth quintiles have higher odds of stunting . Thus, a nuanced and stratified analysis to adequately understand and address these pervasive disparities in child malnutrition is needed. Previous studies on malnutrition in the DRC have focused on the associations of wealth status and place of residence with inequality in malnutrition at a specific time rather than changes in long-term trends in malnutrition inequality . However, examining the long-lasting trends in malnutrition prevalence and disparity among different socioeconomic groups is necessary when tackling the problem requires sustained and long-term efforts. Only through this comprehensive approach can enable us to deepen the understanding of underlying socioeconomic factors that affected malnutrition and identify critical areas for improvement. Based on this, more invaluable evidence supporting the development of effective long-term intervention measures and policies can be found . To better fill the gap, the present study aimed to examine the multilevel trends between socioeconomic status and the risk for malnutrition among children under 5 in the DRC. Using three rounds of Multiple Indicator Cluster Surveys (MICS) conducted in 2001, 2010, and 2017–2018, we compared the prevalence of malnutrition and inequality among children under 5 by their wealth index quintiles and place of residence. We used the slope index of inequality (SII) and relative index of inequality (RII) to measure the magnitude of inequality and assess changes in socioeconomic inequality trends over time. This insight can expedite the achievement of SDGs in the DRC, bringing the country closer to achieving its development targets. This study used data from the 2001, 2010, and 2017–2018 rounds of the MICS conducted in the DRC. The measurement and availability of variables for the three rounds of MICS data were generally consistent. The MICS is a nationally representative household survey that covers children 0 to 5 y of age, females 15 to 49, and males 15 to 59. The surveys were conducted by the National Institute of Statistics of the Ministry of Planning of the DRC in collaboration with UNICEF and the US Agency for International Development/Centers for Disease Control and Prevention. The data deidentification process removed any personal identifiers and protect respondents . The MICS has generated >150 key indicators, providing disaggregated data on the status and living conditions of children and adults in areas such as health, malnutrition, economic status, educational attainment, violence, and hygiene. This cross-sectional survey employed a multistage stratified cluster-sampling approach to cultivate representative household samples at national and subnational levels. This process commenced with a rigorous evaluation of the sample frame to guarantee its high quality, ensuring there were no duplicates in the up-to-date information. The sample frame also contained well-defined area units, geographic codes, measurements of size, and auxiliary information for stratification. In the first stage of sampling, census enumeration areas were selected, taking into account their size. In the second stage, households were listed within these areas to update the size measurements. Households were selected from this list through random systematic sampling to form survey clusters, each comprising 20 to 25 households. Only participants chosen within these households were included in the final stage. This sampling approach efficiently minimized the sampling bias and ensured the representativeness of the target population . Using a two-stage selection method, this study identified urban and rural areas as the primary sampling strata. The MICS included complete information on 10 254 children under 5 in 2001, 11 245 children in 2010, and 21 477 children in 2017–2018, resulting in 42 976 children under 5 being included in the final analyses. Using annually measured statistics from the National Center for Health Statistics (NCHS) on stunting, underweight, and wasting, we examined the trends of the socioeconomic inequalities in malnutrition among children under 5 in the DRC over time. Following the recommendations of World Health Organization growth charts , children with a height-for-age ratio of >2 standard deviations (SD) below the median of the reference population were categorized as stunted. Similarly, children with a weight-for-age ratio of >2 SD below the median of the reference population were classified as underweight. In contrast, children with a weight-for-height ratio of >2 SD below the median of the reference population were categorized as wasting. A single principal component analysis (PCA) was applied to estimate household economic status, measured by the wealth index, based on household assets across the entire sample. The wealth index is constructed by performing PCAs using the information on household characteristics related to wealth, such as ownership of consumer goods, dwelling characteristics, water and sanitation availability, and other characteristics, to generate weights or factor scores for each item included in the index . The resulting wealth index categorized households into five wealth quintiles, ranging from the poorest to the richest. The present study evaluated the trends of socioeconomic inequalities in stunting, wasting, and underweight among children using two measures of inequality: the SII for quantifying absolute inequalities and the RII for assessing the magnitude of relative inequalities [ , ]. We used generalized linear regression models (log-binominal regression model) with the logarithmic link function to calculate RII and the identity link function for SII. A relative wealth position indicator (value between 0 and 1), obtained through a Ridit score, was used as an independent variable to cumulatively rank individuals based on their wealth status such that 0 represented the richest level and 1 represented the poorest level . Outcome variables, including stunting, wasting, and underweight (coded 0 or 1), were analyzed using the RII (rate ratios) and SII (rate differences) with a binomial distribution. SII was interpreted as the difference between prevalence, whereas RII was interpreted as the ratio of prevalence between individuals in the lowest and the highest wealth quintiles. Furthermore, to adjust for demographic characteristics changes over time in the same study population and to account for the different age and sex distributions, all analyses for calculations of prevalence, SII, and RII were adjusted for age and sex in this study. Additionally, stratified analysis was conducted based on place of residence and wealth index quintiles to examine differences between different subsample groups. All analyses were performed with sampling weights and accounted for the clustering of children within the MICS data set. All statistical analyses were conducted by using Stata version 15 (StataCorp LLC, College Station, TX, USA). presents the participants’ sociodemographic characteristics from 2001 to 2018. All selected characteristics showed comparable proportions in each round of the survey. presents the prevalence of malnutrition (stunting, underweight, and wasting) across the three rounds of surveys conducted between 2001 and 2018, disaggregated by national, urban, and rural areas. Overall, the prevalence of malnutrition was consistently higher in rural than urban areas in all three surveys. The national-level prevalence of underweight and wasting significantly decreased from 31% (30.04–32.07) to 26% (25.20–27.17) and from 13% (12.69–14.17) to 6% (5.18–6.19), respectively, across the three rounds of surveys conducted from 2001 to 2018 ( < 0.001). Moreover, there was a significant reduction in the prevalence of being underweight and wasting in both rural and urban areas. The prevalence of stunting in urban areas decreased significantly from 29% (27.31–30.55) to 24% (22.80–26.15; < 0.001), although no significant changes were observed in the national prevalence of stunting over the three rounds of the survey ( = 0.105). In contrast, the prevalence of stunting in rural areas showed a slight but statistically significant increase from 43% (41.29–43.99) to 44% (43.03–45.82) during the study ( = 0.022). The past 2 decades have seen a widening nutritional disparity between children born into low-income families and those born into high-income families ( ). For each surveyed year, the results indicated a distinct social gradient, with a higher prevalence of malnutrition observed in households with lower levels of wealth. As shown in , the national prevalence of stunting experienced fluctuations but finally increased in the lower two wealth quintiles, whereas it declined in families in the higher three wealth quintiles over the past two decades, contributing to greater wealth inequalities on both absolute and relative scales (SIIs = –0.20 in 2001, –0.20 in 2010, –0.40 in 2018, < 0.001 and RIIs = 0.61 in 2001, 0.62 in 2010 and 0.367 in 2018, < 0.001). Similarly, for participants who were underweight, both absolute and relative inequalities increased significantly over time (SIIs = –0.21 in 2001, –0.21 in 2010, –0.28 in 2018, = 0.002 and RIIs = 0.53 in 2001, 0.52 in 2010, and 0.35 in 2018, < 0.001). However, no significant changes were observed in the time trend of inequalities for participants who were wasting from 2001 to 2018. Interestingly, in all three outcome variables, the higher two wealth groups demonstrated the most dramatic decline. In , the analysis of inequality in stunting, underweight, and wasting revealed that urban areas generally experienced greater inequality than rural areas. Specifically, there were no significant trends observed in being underweight by place of residence. In terms of stunting, a continuously increasing trend in relative inequality was observed in urban areas (RIIs = 0.41 in 2001, 0.33 in 2010, 0.25 in 2018, = 0.001), although no significant trend was observed in rural areas. For participants of wasting, a marginal statistically significant increasing trend in relative inequality was observed in rural areas (RIIs = 0.97 in 2001, 0.64 in 2010, 0.79 in 2018, = 0.052), but there was a slight improvement observed from 2010 to 2018. In contrast, the urban areas showed no significant trend over the reported time. These findings suggested that malnutrition remains a significant public health concern in the DRC, particularly in rural areas. Moreover, there has been a widening nutritional disparity between low- and high-income families, with urban areas generally experiencing greater inequality than rural areas. Although information regarding the risk factors associated with under-5 stunting, wasting, and underweight in the DRC regions is available , few investigations have holistically examined both the national trends and socioeconomic inequalities in child malnutrition under 5 y of age over time. Previous studies have been limited to a single year, lacking comparisons between trends over time and interactions among diverse wealth statuses and areas of residence. To address this research gap, we leveraged the MICS data set conducted in 2001, 2010, and 2017–2018 to provide an expansive view of how trends and socioeconomic inequalities in malnutrition among children under 5 in the DRC, aiming to present a more comprehensive understanding of under-5 malnutrition within the DRC. Overall, the present analysis indicated a decreasing trend of national malnutrition prevalence in the DRC during three rounds of investigation, suggesting a notable improvement that could be attributed to concerted national efforts in relevant nutritional programs. This declining trend aligns with previous studies and reports documenting the active interventions of the DRC and international agencies [ , ]. The 2018 Scaling Up Nutrition program reported a series of actions initiated by the DRC government, including the establishment of a multistakeholder platform, enhancement of maternity protection, and strengthening of the right to food legislation. Notably, there was a significant increase in the deworming treatment coverage for children ages 12 to 59 mo to 61%, facilitated by assistance from the United Nations . Additionally, USAID, serving as the most significant bilateral donor in the DRC's health sector, further bolstered nutrition initiatives and implemented health investment programs . This collective effort underlines the success of mitigating malnutrition, although the fight against this pressing issue is far from over. Although the overall prevalence of malnutrition showed a decline in the DRC, we found that the socioeconomic disparities were indicative of potentially widening during the period. After 2010, the prevalence of stunting and underweight in children under 5 residing in DRC's rural areas, particularly those within lower income brackets, exhibited fluctuations from 2010 to 2018. This inconsistency could be traced back to the Ebola epidemic, which predominantly affected the more rural areas already battling extreme poverty and hunger, with intermittent outbreaks between 2012 and 2018 . Compounding this situation is the national civil conflict that erupted in 2016 . The dual crisis not only interrupted local agricultural production and malnutrition screening programs but also aggravated socioeconomic inequalities. This created a scenario wherein those of lower economic status, already compromised by their vulnerability to malnutrition during the 2013–2016 period, found themselves further disadvantaged [ , ]. Thus, although overall trends point toward improvement, these findings underscore a nuanced and complex reality that warrants continued attention and intervention. Narrowing inequalities that do not accompany questions regarding the reductions in the prevalence of malnutrition are of growing interest . The present study further articulated this issue by finding a rapid decrease in the prevalence of malnutrition in urban areas but a more severe inequality in urban areas compared with rural areas, as indicated by their RII and SII values. We hypothesized that during the Eboa epidemic and civil conflict in Kaisai [ , ], more national and international assistance would be prioritized to poorer individuals living in the rural areas lacking access to health utilization due to their vulnerability under the certain circumstances where concerning conflicts were adjacent to rural areas in the DRC . This may have contributed to closing the internal gaps for socioeconomic and malnutritional disparities between each wealth group in rural areas compared with urban areas. Additionally, more displaced people from rural areas may have immigrated to urban areas for easier access to health care and security, but these migrants often face financial challenges that affect food security and purchasing power, leading to a substantial number of undernourished children in urban areas [ , ]. These factors may have contributed to expanding socioeconomic inequality in urban places . An alternative hypothesis is that the majority of DRC families in rural areas consumed similar types of agricultural products regardless of their wealth group, given the fact that 92% of rural households relied on agriculture, which contributed to approximately 64% of their food intake . Although we used nationally representative data on the population of the DRC, the study had several limitations when interpreting other results. First, certain variables were not available in the survey for all years, so key exposure data, such as the degree of crises, diseases, health care facilities, and governmental policies that may influence changes in inequalities among different socioeconomic groups, were not included in the present analysis. Second, the loss of granularity in data prevented us from using individual variations in malnutrition; instead, we explored group-level inequalities in malnutrition by urbanity and wealth group as alternatives, which warrants further investigation. Finally, limitations in the available exposure data prevented us from using a direct measurement such as monthly income to categorize wealth status. In this national survey, household economic status was measured based on household assets given the developing context of the DRC, which could limit the comprehensiveness of the findings to some extent. The present study provided additional evidence and underlined the critical need for targeted assistance to vulnerable groups, particularly impoverished and malnourished children, regardless of their geographic location. Moving forward, these data will have value for further exploration and intervention with respect to urgent needs for government intervention . For instance, more efforts should be made to provide emergency food assistance programs that offer critical nutritional support to vulnerable groups at risk for acute malnutrition due to conflict or natural disasters [ , ]. The present study shed light on the persistently high prevalence of malnutrition among children under 5 y of age in the DRC, with expanding socioeconomic inequalities, particularly in urban areas. The findings emphasized that although there have been slight improvements in malnutrition prevalence over the past 2 decades, there remains an urgent need for comprehensive efforts to address the persistent rise of socioeconomic disparities in child malnutrition. Policymakers should prioritize targeting nutrition-specific programs for economically disadvantaged households and address the inequality patterns between rural and urban areas in the DRC. It is imperative to ensure that the benefits of economic development and health care infrastructure are distributed equitably across the country so that all children have access to sufficient nutrition regardless of their location or socioeconomic status. These efforts necessitate collaborative and multisectoral approaches to tackle the complex and multifaceted causes of malnutrition among DRC children in the future.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Not Used\n",
      "* **AIモデルの判断:** Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 74 / DOI: 10.1016/j.pestbp.2025.106399  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Chromosome-level genome assembly of bean flower thrips Megalurothrips usitatus (Thysanoptera: Thripidae)\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Cythochrome P450-mediated dinotefuran resistance in onion thrips, Thrips tabaci\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "Insecticide resistance has been observed in numerous serious insect pests that attack a wide range of agricultural crops, making their effective control difficult in many countries, including Japan ( ). Thrips are one such pests that cause substantial economic losses by feeding on a myriad of vegetables, fruits, and flowers and acting as vectors for plant viruses ( ). Onion thrips, Lindeman (Thysanoptera: Thripidae), distributed worldwide, is one of the major agricultural pests in Japan ( ; ). has two reproductive forms: thelytoky and arrhenotoky. Arrhenotokous , which is believed to come from overseas, has developed resistance to many insecticides, including pyrethroids and neonicotinoids ( ; ; ). The molecular mechanism of resistance to pyrethroids in has been intensively studied, and amino acid mutations in the target-site gene, a voltage-gated sodium channel gene ( ), are considered to be the main factors responsible for resistance ( ; ; ; ). However, the molecular mechanism of resistance to neonicotinoids in is yet to be elucidated. This is also true for most other thrips species, including the western flower thrips ( ) and the melon thrips ( ), which are also major pests that have developed resistance to many insecticides, including neonicotinoids, in Japan ( ). Recently, (similar to ) in the bean flower thrips ( ) was reported to be involved in resistance to acetamiprid ( ). To the best of our knowledge, currently, this is the first and only report on the molecular mechanism of resistance to neonicotinoids in thrips species. Two mechanisms, target-site mutations and enhanced metabolic detoxification by cytochrome P450 monooxygenase (CYP), have been reported in several insect pests that are resistant to neonicotinoids. However, in field populations, resistance to neonicotinoids by target-site mutations has only been reported in three hemipteran species: the green peach aphid ( ) ( ), the cotton aphid ( ) ( ), and the white fly ( ) ( ). The involvement of enhanced metabolic detoxification by CYP in neonicotinoid resistance has been reported in the same hemipteran species as follows: resistance to imidacloprid by in ( ), resistance to seven neonicotinoids by and in ( ), resistance to dinotefuran by , , and in ( ), and resistance to imidacloprid and dinotefuran by in ( ; ; ; ). CYPs involved in resistance to imidacloprid have also been reported in other insect pest species: in the brown planthopper ( ) ( ; ), or in the Colorado potato beetle ( ) ( ; ), and and in the palm weevil ( ) ( ). Decreased sensitivity to -cyanoamidine neonicotinoids, such as thiacloprid metabolized by the CYP9Q subfamily, has also been reported in beneficial insects: in the honeybee ( ) and in the bumble bee ( ) ( ). Most of the CYPs involved in resistance to neonicotinoids belong to the CYP3 clan, including the CYP6, CYP9, and CYP345 families. Therefore, in thrips, CYP3 clan CYPs are also likely to be involved in the resistance to neonicotinoids. To identify the CYPs involved in insecticide resistance, it is important to construct a comprehensive and accurate CYP gene set that can be used for narrowing down and characterizing candidate CYP genes. In recent years, whole-genome assemblies of several thrip species, including ( ), ( ), tobacco thrips ( ) ( ), ( ), and rice thrips ( ) ( ) have been reported. However, the classification of the CYP genes was insufficient in these thrip species, except for in , whose CYP genes were classified by Prof. David Nelson ( ). In this study, to identify the CYP genes involved in resistance to dinotefuran, an important neonicotinoid used for controlling in Japan, the genome assembly of was constructed, and the CYP genes predicted in the genome assembly were manually curated. Classification of the curated CYP genes was conducted based on phylogenetic analysis that included the CYP genes of and . Expression analysis via RNA sequencing (RNA-seq) was conducted in strains that are susceptible or resistant to dinotefuran, and a candidate CYP gene involved in dinotefuran resistance was obtained. Mutation analyses of the target genes of neonicotinoids and commonly upregulated CYP genes in the dinotefuran-resistant strains were also performed to identify if mutations in the genes are involved in the resistance to dinotefuran. Finally, to validate the candidate CYP gene, an in vitro CYP metabolism assay was conducted for several CYP genes as well as the candidate CYP gene. The thelytokous strain (ANO strain) was established from a single adult female collected from a field in the Institute of Vegetable and Floriculture Science, NARO (Mie Prefecture, Japan) in 2016, and reared with green onions ( ) in laboratory condition. ANO strain was used only for constructing the CYPome based on the genome assembly of in this study. One field-collected dinotefuran-resistant strain (TKO, arrhenotokous) from Ibaraki Prefecture, two field-collected dinotefuran-susceptible strains (IDG2014, thelytokous; IKM2014, arrhenotokous) from Ibaraki Prefecture, three field-collected dinotefuran-susceptible thelytokous strains (OHBO2014, OHBE2014, and OKW2014) from Osaka Prefecture, and one laboratory dinotefuran-susceptible strain (KOC2442, arrhenotokous) were used for bioassay using dinotefuran (100 mg/L) and RNA-seq analysis for identifying candidate genes related to dinotefuran-resistance. Two laboratory-reared dinotefuran-resistant arrhenotokous strains (TKO-SPRR and KTF-SPRR) and one laboratory-reared dinotefuran-susceptible thelytokous strain (HKD3) were used for more detailed bioassay (LC evaluation) using different concentrations of dinotefuran. All the laboratory strains (except for ANO strain) were further used for validating the expression level of the candidate CYP gene ( ) by qRT-PCR analysis. The laboratory strains were reared with garlic ( ). Insects were maintained at 23 °C–25 °C under a long photoperiod (16 h of light and 8 h of darkness). The two laboratory dinotefuran-resistant strains (TKO-SPRR and KTF-SPRR) were collected from the field in Tsukuba City, Ibaraki Prefecture in 2018 and in Kyoto City, Kyoto Prefecture in 2019, respectively, and then they have been reared in the laboratory condition without exposure to neonicotinoids. The strains used in this study are summarized in Table S1. Commercial-grade dinotefuran (20.0 % SG, Mitsui Chemicals Crop & Life Solutions, Inc., Tokyo, Japan), Tween-20 (MP Biomedicals, CA, USA), and Gramin (Mitsui Chemicals Crop & Life Solutions, Inc.) were used for the bioassays in this study. The technical-grade dinotefuran and sulfoxaflor (purity, 99 %) used for the analysis of CYP metabolism was purchased from Fujifilm Wako Pure Chemical Co. (Osaka, Japan). Piperonyl butoxide (PBO) was purchased from Wako Pure Chemical Industries, Ltd. (Osaka, Japan) for the synergistic assay. The adult females of all the field-collected strains listed in Table S1 and one laboratory KOC2442 strain were treated with 100 mg/L of dinotefuran (agriculturally recommended concentration for controlling thrips in Japan) with spreading agent (0.01 % Tween-20 or 0.02 % Garmin) via the leaf-dipping method ( ) using kidney bean leaves. Mortality was assessed after 24 h (strains in Ibaraki Prefecture) or 48 h (strains in Osaka Prefecture and KOC2442 strain) of exposure at 25 °C. Three or more replicates (10–15 adult females per replicate), including the control (treated with water or water with the above spreading agent), were performed for each bioassay. The LC values of the three laboratory strains (HKD3, TKO-SPRR, and KTF-SPRR) were calculated by probit analysis with bioassays similarly conducted using different concentrations of dinotefuran (2, 4, 5, 10, 20, and 100 mg/L for the HKD3 strain; 25, 50, 100, 200, 400, and 800 mg/L for the TKO-SPRR strain; 100, 125, 200, 250, 400, and 800 mg/L for the KTF-SPRR strain) for 48 h. For the synergistic assay, the LC value of the TKO-SPRR strain treated with mixture of dinotefuran, 0.01 % Tween-20, and 0.295 mM PBO in 0.1 % acetone was also calculated similarly (control was treated with 0.01 % Tween-20, 0.295 mM PBO in 0.1 % acetone). The treatment and concentration of PBO were similar to the previous study ( ). An R script ( ) developed by Dr. Shigenobu Aoki of Gunma University, Japan, was used for the probit analysis and Pearson's chi-square goodness-of-fit test. The corrected mortality rate of thrips treated with dinotefuran was calculated using Abbott's formula ( ). The pooled genomic DNA used for genome sequencing via the NGS long reads was extracted from approximately 800 adult females of the ANO strain, as described in the Supplementary Methods (M1). The pooled genomic DNA used for genome sequencing via the NGS short reads was extracted from approximately 100 and 300 adult females of the ANO strain for the paired-end sequencing libraries (350 bp and 550 bp) and mate-pair sequencing libraries (3 kb and 8 kb), respectively, using the method described previously ( ). Library construction and long-read genome sequencing were conducted by Novogene Japan K.K. (Tokyo, Japan) using the PacBio Sequel System. Library construction and short-read paired-end (insert size: 350 bp and 550 bp) and mate-pair (insert size: 3 kb and 8 kb) genome sequencing were conducted by Macrogen Japan Corp. (Tokyo, Japan) using the Illumina HiSeq System. The details of the library construction and sequencing are described in the Supplementary Methods (M2). The total RNA was extracted from adult females of the dinotefuran-resistant TKO strain in three biological replicates (35 individuals for each replicate) by the method described previously ( ). Library construction and short-read RNA-seq of the TKO strains were conducted by Macrogen Japan Corp. (Tokyo, Japan) using the Illumina HiSeq 2000 System. A TruSeq RNA Sample Prep Kit v2 was used to construct the libraries, and the libraries were sequenced using the Illumina HiSeq 2000 System (101 bp paired-end). The RNA-seq data of six dinotefuran-susceptible strains (IDG2014, OHBO2014, OHBE2014, OKW2014, IKM2014, and KOC2442) that were previously generated ( ) were also used for further analysis. The raw subheads of the PacBio Sequel long reads were corrected and trimmed using the “canu” command in Canu ver. 2.1.1 ( ). Contig sequences were constructed by the “canu” command using the trimmed long reads and then polished using cleaned Illumina HiSeq X short reads (insert size: 350 bp) by GCpp ver. 1.9.0-SL-release-8.0.0+1-37-gd7b188d ( ) and FreeBayes ver. 1.3.0 ( ), as described in the Supplementary Methods (M3). Haplotig purging was then conducted for the polished contigs using purge_haplotigs ver. 1.1.1 ( ), as described in the Supplementary Methods (M4). Bacterial scaffolds, including predicted genes (described later) annotated with proteins of or species, were then removed. One scaffold (scaffold1397) with a cyclic structure was identified as a mitochondrial genome. Overlapping redundant regions due to misassembly of the cyclic structure in scaffold1397 were manually fixed by comparing the scaffold with the mitochondrial genome of the closely related species (GenBank: ). The genome size was estimated by GenomeScope 2.0 ( ) using k-mer count histograms (K = 21, 25, 31, 35, 41, 45, and 47) calculated by Jellyfish 2.2.10 ( ) with the cleaned short reads (insert size: 350 bp). The completeness of the haploid genome assembly was evaluated by BUSCO ver. 5.4.4 ( ) using the insecta_odb10 dataset (2020-09-10). De novo repeat library construction using the genome assembly was conducted using RepeatModeler ver. 2.0.1 ( ) with the option “-engine rmblast -LTRStruct”. The genome assembly was repeat-masked by RepeatMasker ver. 4.1.0 ( ) using the generated repeat library with the option “-xsmall”. Predicted gene sets in the repeat-masked genome assembly were constructed by BRAKER ver. 2.1.6 ( ) and TSEBRA ver. 1.0.3 ( ) using the RNA-seq data from the four strains (IKM2014, KOC2442, IDG2014, and TKO) and Arthropod OrthoDB proteins (v10) ( ) as hint data, as described in the Supplementary Methods (M5). The predicted genes (protein sequences) were annotated by the following combinations of software and databases: (1) BLASTP ver. 2.12.0+ ( ) with an e-value threshold of <1e-03 using NCBI-nr (retrieved in 2023-10-23), (2) HMMER ver. 3.1b2 ( ) with an e-value threshold of <1e-03 using Pfam R35 ( ), and (3) InterProScan ver. 5.55–88.0 ( ) with the options “-iprlookup -goterms -dp -pa.” using InterPro 88.0 ( ). The predicted genes belonging to the detoxification-related gene families (CYP, carboxylesterase (COE), glutathion S-transferase (GST), UDP-glucuronosyltransferase (UGT), ABC transporter) were identified based on the conserved domains of each gene family defined in the Pfam database (CYP: p450(PF00067), COE: COesterase(PF00135), GST: GST_N (PF02798 or PF13417) and GST_C (PF00043, PF13410, PF14497, or PF17171), UPDGT: UDPGT(PF00201), and ABC: ABC_tran(PF00005)). The predicted genes identified as CYPs were further manually checked based on the alignment results of BLASTP/NCBI-nr and TBLASTN (top-hit protein vs. the genome assembly), and the exon–intron structures of some CYP genes were manually fixed. Furthermore, some pairs of partially predicted CYP genes were merged, and some predicted CYP genes that were considered to be possible pseudo genes (due to short length and/or insertion of premature stop codon and/or duplication due to possible haplotigs in the genome assembly) were not counted as CYP genes. The CYP protein sequences of (manually annotated 127 genes), (96 genes retrieved from NCBI Annotation Release 100), (84 genes retrieved from Additional File 6 in ( ), (49 genes in Table 3 in ( )), (49 genes retrieved from Supplementary File S1 in ( )) and (83 genes retrieved from KAIKObase ) were used to reconstruct a maximum likelihood (ML) tree using ETE3 ver. 3.1.3 ( ) using the predefined workflow “standard_raxml_bootstrap” with 100 bootstrap replications. The reconstructed ML tree was visualized using iToL v7 ( ). The number of CYP genes in each CYP clan (CYP2, CYP3, CYP4, and Mito) was determined based on the CYP clusters identified in the ML tree. The expression level of each gene in the seven strains (TKO, IDG2024, IKM2014, KOC2442, OHBO2014, OHBE2014, and OKW2014) was calculated using STAR ver. 2.7.10b ( ) and RSEM ver. 1.3.3 ( ) using cleaned RNA-seq reads, as described in the Supplementary Methods (M6). Principal component analysis (PCA) of the calculated expression data was performed to evaluate the correlations of the biological replicates of each strain using plotPCA in DESeq2 ver. 1.26.0 ( ), where the raw count data was normalized among samples using a variance stabilizing transformation (VST) in DESeq2. The normalized expression data were also used to visualize a heatmap of the CYP gene expression level (both the absolute expression level (log2 normalized count) and the relative expression level among the seven strains ( -score)) in ComplexHeatmap ver. 2.2.0 ( ). The DEGs between each dinotefuran-susceptible strain and dinotefuran-resistant strain were calculated using the iDEGES/edgeR method in TCC ver. 1.26 ( ) using the expression data (raw counts). The threshold values for the DEG identification were a false discovery rate of <0.05 and a fold change of >2. The expression levels of the (gene ID: ) gene in two dinotefuran-susceptible strains (KOC2442 and HKD3) and two dinotefuran-resistant strains (TKO-SPRR and KTF-SPRR) were verified by qRT-PCR analysis. Total RNA was extracted from five adults from each strain for each replicate using an RNeasy Mini Kit (QIAGEN). Construction of the template cDNAs and qRT-PCR using the template were conducted as described in a previous study ( ). The list of primers used is shown in Table S2. The expression level of was calculated by the standard curve method using the ribosomal protein L32 (TtRPL32) gene (gene ID: ) as an internal control. To identify if resistance-related mutations in dinotefuran target-site genes and/or highly expressed CYP genes are involved in dinotefuran resistance in , mutation analysis of three nicotinic acetylcholine receptors (nAChRs) genes, four commonly upregulated CYP genes ( , , , and ) and three highly expressed CYP genes ( , , and ) in the three dinotefuran-resistant strains and four dinotefuran-susceptible strains was conducted (Table S3). Neonicotinoids resistance-related mutations in three nAChR genes ( , , and α3) have been reported previously in two species of aphid (R81T mutation in in and ) ( ; ), in (Y151S in and α3) ( ), and in (A58T and R79E mutations in ) ( ). The mapped whole-genome resequencing data of the three dinotefuran-resistant strains (TKO, TKO-SPRR, and KTF-SPRR) and two dinotefuran-susceptible strains (KOC2442 and HKD3) generated in the previous study ( ) and the mapped RNA-seq data of the two susceptible strains (IDG2014 and IKM2014) generated in this study were used to identify if there were amino acid mutations specific to the resistant strains in the target genes. Cloning of the six CYP3 clan genes ( , , , , , and ) was conducted as described in the Supplementary Methods (M7). Dinotefuran metabolism of the six CYP3 clan proteins were analyzed by in vitro recombinant CYP metabolism assays using insect cells expressing the cloned CYPs. The assays for TtCYP3652A1 and TtCYP6QS2 were conducted by the heterologous expression of the CYPs in S2 cells and subsequent metabolic analysis as described in a previous study ( ). The short summary of the method is as follows. The cells were transfected with CYP DNA and NADPH-cytochrome P450 reductase using X-tremeGENE HP DNA Transfection Reagent (Roche Diagnostics K.K., Tokyo, Japan). The medium was exchanged with CYP expression medium after the transfection, and then test compounds were added for metabolic studies with 72 h incubation time. The samples were processed and analyzed using HPLC with specific gradient elution conditions to evaluate the metabolic activity. The percentage of metabolism was calculated using the amount of compound present in samples (evaluated with the UV peak are) after 72 h incubation time and the amount of compound evaluated in the control sample (0 h incubation time). The assays for the remaining four CYP proteins were conducted in almost the same manner. However, since the medium (Insectagro DS2 medium) used in the previous study became difficult to obtain, we were forced to change the medium for the remaining CYP proteins. Accordingly, we modified several experimental conditions after re-confirming the CYP expression and compound degradation by TtCYP3652A1. The modification was as follows: (1) Insect-XPRESS™ medium (Lonza K. K., Tokyo, Japan), which includes -glutamine, was used instead of Insectagro DS2 medium supplemented with 4 mM Ala-Glu, (2) the amount of CYP expression medium was changed from 2 mL to 1 mL, (3) the amount of test compound was changed from 35 μL to 25 μL (final concentration was changed from 7 ppm to 10 ppm), (4) the incubation time of CYP expression medium with test compound was 96 h. As the negative control, metabolism rates of test compounds using sulfoxaflor (for TtCYP3652A1 and TtCYP6QS2) or untransfected insect cells (for remaining four CYPs) were confirmed to be close to 0 %. Dunnett's multiple comparison test was conducted between the metabolism rate of the negative control and each CYP protein. A corrected mortality rate of one field-collected susceptible strain (IDG2014 in Ibaraki Prefecture) treated with 100 mg/L of dinotefuran was 100 % ( ). Corrected mortality rates of other four field-collected susceptible strains (IKM2014 in Ibaraki Prefecture; OHBE2014, OHBO2014, and OKW2014 in Osaka Prefecture) were slightly lower (92.9 %, 82.5 %, 88.8 %, and 84.6 %, respectively) ( ). A corrected mortality rate of one field-collected resistant strain (TKO in Ibaraki Prefecture) was 38.9 %, which was significantly lower than those of the above field-collected susceptible strains. Similarly, a 100 % collected mortality rate and slightly lower one (96.3 %) were observed in the two laboratory susceptible strains (KOC2442 and HKD3), respectively. Significantly lower corrected mortality rates (14.8 % and 14.5 %) were also observed in the two laboratory resistant strains (TKO-SPRR, and KTF-SPRR), respectively. The LC values of dinotefuran for the laboratory susceptible strain (HKD3) and two laboratory resistant strains (TKO-SPRR and KTF-SPRR) were 8.121 mg/L, 287.384 mg/L (147.143 mg/L with PBO), and 189.756 mg/L, respectively. Resistance ratios (RRs) of the TKO-SPRR and KTF-SPRR strains compared to the HKD3 strain were 35.39 and 23.37, respectively ( ). The synergistic ratio (SR) of PBO in the TKO-SPRR strain was 1.95. These results demonstrate that TKO, TKO-SPRR, and KTF-SPRR strains were resistant to dinotefuran. The number of sequenced long and short reads used for the de novo genome assembly is summarized in Table S6. The depth of coverage of the long reads and paired-end and mate-pair reads were approximately 50×, 379×, and 118×, respectively. The estimated genome size according to GenomeScope 2.0 was 286.76–308.77 Mb (K = 21–47) (Table S7 and Figure S1). The size of the haploid genome assembly (scaffold level) was 329.74 Mb with 1472 scaffolds (no bacterial scaffolds), which is close to the estimated genome size (Table S8). The N50 was 348.3 Kb and the size of the gapped region was 930 Kb (0.28 % of the genome assembly) (Table S8). Among the 1367 BUSCO5 genes (insecta_odb10), 98.3 % of the genes (single-copy: 93.8 %; duplicated: 4.5 %) were completely identified, 0.7 % of the genes were fragmented, and 1.0 % was missing in the genome assembly, which indicates good completeness of the genome assembly. One scaffold (scaaffold1397) with a cyclic structure was identified as a mitochondrial genome. The size of the mitochondrial genome was 15,491 bp, which is close to that of (15,333 bp). The repeat-masked region in the genome assembly identified by RepeatMasker was 125.35 Mb (703,238 repeat elements), which is 38.01 % of the genome assembly (Table S9) and approximately eight times larger than that of (15.34 Mb repeat regions in 237.85 Mb genome assembly; 6.45 %) ( ). Overall, 18,965 predicted protein-coding genes with 20,384 transcripts were identified (Table S10). In the functional annotation process, all predicted genes (445 genes) in the 43 scaffolds were annotated with bacterial proteins ( or species) in the NCBI-nr database (top-hit of blastp search), and the 43 bacterial scaffolds with the 445 predicted genes were removed in the genome assembly. Approximately, 90.2 % of the predicted genes were annotated with one or more databases (Table S10). The number of five detoxification-related gene families (CYP, COE, GST, UGT, and ABC) in the predicted genes are summarized in . The number of genes in each detoxification-related gene family was largest in among the three thrip species, where the difference was relatively small except for the CYP family. possesses a substantially larger number of CYP genes (127 genes) compared with (96 genes), (84 genes), (49 genes), (49 genes), and (83 genes). The three thrip species possess a larger number of COE (47–66 genes) and GST genes (26–33 genes) compared with (18 COE and 9 GST genes) and (24 COE and 13 GST genes). However, the number of COE and UGT genes (15–23 genes) are lower compared with (87 COE and 42 UGT genes) and (40 UGT genes). In contrast, the number of ABC genes were relatively similar among these insects. We manually curated 156 predicted genes with a conserved Pfam domain of the CYP gene (PF00067). Among these, three pairs of partial CYP genes (the IDs of the predicted genes were / , / , and / ) were merged into three CYP genes ( ; CYP name: ), ( ), and ( ) and 27 genes, which were considered to be possible pseudo genes, were not counted as CYP genes. In addition, one CYP gene, ( ), which was not predicted by BRAKER, was identified in the TBLASTN search of CYP proteins against the genome assembly. Thus, 127 manually curated CYP genes were constructed (Table S11). The average length of the curated CYP genes was 518.7 amino acids. The ML phylogenetic tree of the CYP genes of , , , , and was reconstructed ( ). Four large clades corresponding to the CYP2, CYP3, CYP4, and Mito clans were identified in the ML tree (the ML tree of each clan is shown in (CYP3 clan) and Figure S3–S5 (CYP2 clan, CYP4 clan, and Mito clan). The number of CYP genes in each clan is summarized in . possesses the highest number of CYP3 and Mito genes among the six insect species. The number of CYP3 clan genes in was 39, which is nearly double that found in (20 genes), and many genes annotated as CYP6 subfamily genes were identified in the CYP3 clan (Table S11–S13). As numerous CYP3 clan genes related to insecticide or phytochemical resistance have been reported in many insects, these results may indicate the relatively high potential of insecticide or phytochemical resistance in . The CYP3 clan genes can be roughly classified into three types: (1) the CYP6 family clades specific to thrips, , , and , respectively, (2) the CYP9 family clade ( and ), and (3) the remaining clades ( ). There are three thrips-specific subfamilies (CYP3652A, CYP3653A, and CYP3654A) in the remaining clades. Of these, CYP3652A and CYP3653A subfamily genes formed a small clade with CYP9 family genes which are known to play an important role in developing resistance to many insecticides ( ). However, no clear relationship between the three subfamilies and CYP9 family was observed due to poor bootstrap support of the clade (0.09) ( ). Thrips CYP6BD subfamily genes, which were not included in the thrips CYP6 family-specific clade, formed a small clade with the gene with strong bootstrap support (1) ( ). RNA-seq data used for the expression analysis are summarized in Table S14. The results of the PCA analysis for the calculated expression data of each strain are illustrated in Figure S2. The three biological replicates of each strain formed a clear cluster, indicating that the correlation among the biological replicates was sufficiently high for each strain. The expression levels of the 127 manually curated CYP genes in the seven strains were analyzed by two types of heatmap, one was colored based on the absolute expression level (log2 normalized count) and the other was colored based on the relative expression level ( -score) among the strains ( A and B ). The 127 CYP genes were classified into five clusters in the absolute expression-based heatmap ( A). Among these, four CYP genes in cluster 5 were very highly expressed in the dinotefuran-resistant TKO strain and several susceptible strains: one CYP4 subfamily gene in the CYP4 clan ( ), two CYP6 subfamily genes in the CYP3 clan ( and ), and one CYP3652A subfamily gene in the CYP3 clan ( ) ( A). Eleven CYP genes in cluster 5 (five CYP6 subfamily genes, three Mito clan genes, two CYP4 subfamily genes and one CYP2 clan gene) were highly expressed in most of the strains ( A). in cluster 5 was highly expressed only in the thelytokous strains. The expression levels of CYP genes in the remaining four clusters were medium (20 genes in cluster 4), somewhat low (22 genes in cluster 3), low (34 genes in cluster 2), and very low (35 genes in cluster 1). Similarly, the 127 CYP genes were classified into five clusters in the relative expression-based heatmap ( B). Among them, the expression of 18 CYP genes in clusters 3, 4, and 5 was relatively highly and specifically expressed in the TKO strain. Specifically, three CYP3 clan genes ( , , and ) that exhibited very high absolute expression level ( A) were included in cluster 3 or 4. Similarly, one CYP4 subfamily gene ( ) and two Mito clan genes ( and ), which showed high absolute expression levels, were included in cluster 3 or 5. As most of the previously reported CYP genes involved in resistance to neonicotinoids belong to the CYP3 clan (CYP6 and CYP9 subfamilies), the three CYP3 clan genes relatively and absolutely highly expressed in the TKO strain were considered candidate genes of dinotefuran resistance CYP in . To identify the highly expressed candidate CYP genes involved in dinotefuran resistance in the TKO strain and to identify if genes in the other four detoxification-related gene families (COE, GST, UGT, and ABC) were highly and specifically expressed in the TKO strain, DEG analysis between the TKO strain and each dinotefuran-susceptible strain was conducted. The number of DEGs in each gene family in each comparison and the common DEGs among all the comparisons are summarized in . No commonly statistically upregulated or downregulated DEGs were found in the other four gene families in the TKO strain. However, four CYP genes, including three CYP3 clan genes ( , , and ) and one Mito clan gene ( ), were commonly statistically upregulated in the TKO strain. The absolute expression level (normalized count) of the four CYP genes among the seven strains and the three CYP genes ( , , and ) that were very highly expressed in the TKO strain are summarized in . Among these, as evaluated in the heatmaps, the expression level of one CYP3652A subfamily gene ( ) was significantly higher in the TKO strain than in all the susceptible strains in the absolute and relative levels (normalized count: 10,064, average fold change: 7.81). Although the two CYP6 subfamily genes ( and ) and one CYP3118E subfamily gene ( ) were statistically upregulated in the TKO strain, the absolute and/or relative expression levels were significantly lower (normalized count: 163–2547, average fold change: 3.01–4.00) than that of . The expression levels of , , and in the TKO strain were very high (normalized counts: 3775, 9383, and 20,038, respectively) and higher than those in the other susceptible strains, however, these genes did not meet the criteria of up-DEG when compared with one or more susceptible strains. Relative expression level of the gene in both the dinotefuran-resistant laboratory strains (TKO-SPRR and KTF-SPRR) was statistically significantly upregulated (fold change: 8.62 and 10.14 in TKO-SPRR, 8.79 and 10.33 in KTF-SPRR) compared with the two susceptible laboratory strains (KOC2442 and HKD3), as observed in the RNA-seq analysis of the field-collected susceptible and resistant strains ( ). No mutations specific to the dinotefuran-resistant strain were identified in the three nAChR genes and seven CYP genes (Table S3) in the three dinotefuran-resistant strains. Since up/downregulation in nAChR genes was also not observed in the DEG analysis, it is likely that the dinotefuran resistance in the three strains was not conferred by either a target-site mutation, differential expression of the target genes, or qualitative modification of the CYP genes. The percent metabolism of dinotefuran for six CYP proteins (TtCYP3652A1, TtCYP6QS2, TtCYP6EC3, TtCYP6QR1, TtCYP6EB2, and TtCYP6BD1) were 58.9 %, 5.9 %, 3.0 %, 0 %, 0 %, and 0 %, respectively ( ). Dinotefuran was clearly metabolized only by the protein of the gene, which was very highly expressed and upregulated in the resistant strains ( ). Dinotefuran was slightly metabolized by the protein of the gene, which was very highly expressed in the resistant (TKO) and several susceptible strains, where the expression level was highest in the TKO strain and the second highest in the IKM2014 strain ( ). The protein of the gene, another very highly expressed CYP gene ( ), was not able to metabolize dinotefuran at all. The two proteins of the and genes, which were highly expressed ( A and S6) orthologs of the and genes in , previously reported as pyrethroid resistance-related CYPs, were also not able to metabolize dinotefuran. , which was recently reported as a highly expressed CYP6 gene involved in resistance to neonicotinoid in , was an ortholog of (62.15 % identity by blastp) and (66.53 % identity by blastp) in . The protein of the gene, which was not classified in the thrips-specific CYP6 clade in the phylogenetic analysis ( ) and was moderately expressed among all the strains ( and S6), was also not able to metabolize dinotefuran. The results indicate that TtCYP3652A plays an important role in the development of dinotefuran resistance in . Development of insecticide resistance in thrips such as , , and has become a serious issue in many countries, including Japan. CYPs are believed to be involved in insecticide resistance in thrips ( ; ; ; ; ; ). However, only a few CYPs, such and in acrinathrin (pyrethroid) resistance in ( ) and (similar to ) in acetamiprid (neonicotinoid) resistance in ( ), have been reported as candidates of insecticide resistance in thrips, and only one CYP ( ) has been functionally validated to be involved in insecticide resistance. Herein, to construct a genomic and transcriptomic resource of for identifying CYPs involved in the resistance to insecticides including dinotefuran, we constructed its whole-genome assembly with good completeness and 18,965 predicted gene sets of . Detoxification-related genes, such as CYP, COE, GST, UGT, and ABC family genes were comprehensively identified in the predicted gene set. Among these, the CYP genes were manually curated and 127 CYP genes were identified ( and S11). The number of CYP genes in was clearly higher than the numbers in the other two thrip species ( ) and one lepidopteran insect, (basically susceptible to insecticides). Specifically, the number of CYPs in the CYP3 clan, including the CYP6 family, was much larger than that in the other two thrips ( ). As described above, most of the previously reported CYP genes involved in resistance to neonicotinoids are CYP6 or CYP9 family genes in the CYP3 clan. In this study, we observed that most of the CYP3 clan genes in were CYP6 family genes (36 of 39 genes in the CYP3 clan), including highly expanded (i.e., “bloom” of CYP genes ( )) CYP6 subfamilies (e.g., 10 genes in the CYP6KF subfamily; ), whereas the remaining three families (CYP3652, CYP3653, and CYP3654) included only one gene (no CYP bloom), which suggests that the CYP6 family gene is likely to be involved in resistance to neonicotinoids in . However, we observed that another CYP3 clan gene, , is likely to be a main factor of resistance to dinotefuran in based on the results of the transcriptome analyses (expression analysis and DEG analysis), mutation analysis, and metabolism analysis. The expression patterns of the CYP genes in the seven strains (one dinotefuran-resistant strain and six dinotefuran-susceptible strains) showed that three CYP3 clan genes ( , , and ) were absolutely and relatively highly expressed in the resistant strain ( ). The results of the DEG analysis further showed that only was highly and specifically upregulated in the field-collected resistant strain (TKO) compared with the field-collected susceptible strains ( ), which was confirmed via qRT-PCR analysis for the laboratory resistant and susceptible strains ( ). The DEG analysis also revealed that no other detoxification family genes (COE, GST, UGT, and ABC) and nAChR genes (target of neonicotinoids) were commonly up/downregulated in the resistant strain ( ). These results suggested that is a primary candidate gene involved in dinotefuran resistance. No mutations specific to the resistant strain were found in the nAChR and CYP genes (very highly expressed and/or upregulated in the resistant strain) by the mutation analysis, which indicated that target-site mutations and qualitative modification in the CYP genes were not involved in dinotefuran resistance in . Finally, we confirmed that only TtCYP3652A1 significantly metabolized dinotefuran by the metabolism analysis using recombinant CYP proteins expressed in insect cells ( ). Thus, the above results indicate that the gene in is a major factor in the resistance to dinotefuran. On the other hand, although the synergistic effect of PBO was observed in the laboratory resistant strain (TKO-SPRR), the synergism was moderate (SR was 1.95). This may indicate the possibility of additional resistance mechanisms such as reduced insecticide penetration. However, differentially expressed cuticular genes were not observed in the DEG analysis (data not shown). Other possible explanations for the moderate synergism are that (1) PBO may be a poor inhibitor for TtCYP3652A1 due to its higher affinity for other CYPs in as suggested in previous studies ( ; ), and/or (2) the inhibitory effect of PBO on CYPs in might be limited when treated via the leaf-dipping method. Most of the CYPs known to be involved in resistance to neonicotinoids belong to the CYP3 clan including the well-known CYP6 and CYP9 families. As expected, one CYP3 clan gene ( ) was suggested as a major factor involved in resistance to dinotefuran in this study. However, to the best of our knowledge, the CYP3652A subfamily has not been previously reported to be associated with insecticide resistance. The CYP3652A subfamily, named by Prof. David Nelson ( ), is a thrips-specific CYP subfamily in the CYP3 clan. The top-hit CYP gene of by blastp search was (31.53 % identity) when compared with CYP genes while those of and were (30.18 % identity) and (29.88 % identity), respectively. In the phylogenetic tree, the CYP3652A subfamily genes formed a small clade with CYP9 family genes of and , however, no clear relationship between the CYP3652A subfamily and CYP9 family was observed due to the poor bootstrap support (0.09 in ). These results suggested that the relationship between the CYP3652A subfamily and the two CYP families (CYP6 and CYP9), which are known to play an important role in developing resistance to many insecticides including neonicotinoids, is unclear from the point of view of sequence similarity. The same is true for the CYP3653A subfamily which is another thrips-specific CYP subfamily in the CYP3 clan. Among the CYP6 genes in the three thrips species, CYP6BD subfamily genes are the only CYP6 genes not classified in the thrips-specific CYP6 clade ( ). Interestingly, among the CYP genes in , showed the highest similarity to the CYP9 family in (38.04 % identity with ), however, clear relationship between the CYP6BD subfamily and CYP9 family was also not observed in the phylogenetic tree ( ). formed a small clade with (51.50 % identity) and (42.19 % identity) with strong bootstrap support (1 in ). Of these, (the only CYP6 gene in not classified in the specific CYP6 clade in ) was reported involved in resistance to two neonicotinoids (thiamethoxam and imidacloprid) in ( ). However, no upregulation (Figure S6) and no metabolism of dinotefuran ( ) were observed in . Therefore, it is likely that this gene is not involved in dinotefuran resistance. Resistance to neonicotinoids by several CYP6 family genes such as , , , , and have been observed in hemipteran aphids ( ; ; ). Of these, a large expansion of 19 CYP6CY subfamily genes (CYP bloom) was observed in the specific CYP6 clade ( ). Similar CYP blooms have been often observed in CYP9A subfamily genes in lepidopteran insecticide-resistant pests, including the fall armyworm ( ), the common cutworm ( ), the cotton bollworm ( ), the codling moth ( ), and the smaller tea tortrix ( ) ( ; ; ; ; ; ). However, to the best of our knowledge, no CYP9 genes in lepidopteran insects have been functionally validated to be involved in resistance to neonicotinoids. Decreased sensitivity to thiacloprid (neonicotinoid) by CYP9 family genes ( and ) has been observed in bees ( and ) ( ). In contrast to the CYP6CY genes in , a small or no bloom has been observed in the CYP9Q genes in the bees (three genes in , two genes in ) ( ). Similarly, although no bloom was observed in the thrips-specific CYP3652A subfamily (one gene in and , two genes in in ), was considered a major factor of the resistance to dinotefuran in this study. These cases might indicate that the CYP bloom in the subfamilies of CYP3 clan is not associated with the development of resistance to neonicotinoids. Therefore, other insects with small bloom or no bloom in the CYP3 clan might possess the potential for developing resistance to neonicotinoids. Although no CYP6 genes clearly involved in resistance to dinotefuran were observed in this study, the number of highly expressed CYP genes was the largest in the CYP6 family ( ). Furthermore, seven CYP6 subfamilies of thrips formed the thrips-specific CYP6 clade, where CYP bloom was observed in four subfamilies (CYP6KF, CYP6QS, CYP6QQ, and CYP6EC), including the highly expressed CYP genes in ( B). These results suggest that the CYP6 genes in play important roles in metabolizing a wide range of xenobiotics, including insecticides and phytochemicals, as observed in other insect pests ( ; ). Among the four subfamilies with CYP bloom, the number of CYP genes in the CYP6QS subfamily (5 genes) and the CYP6KF subfamily (10 genes) in were much higher (3–5 times) than those in the other two thrips species. Although only TtCYP3652A1 significantly metabolized dinotefuran, in the CYP6QS subfamily was very highly expressed ( ), and its recombinant protein slightly (5.9 %) metabolized dinotefuran ( ), which might suggest that contributes slightly to the resistance to dinotefuran. CYP6KF subfamily genes are likely to be the most intensively and recently duplicated in because short-length branches were intensively observed in the clade with all CYP6KF genes except for ( ). However, , which is not included in the intensively duplicated clade, showed the highest expression in the susceptible and resistant strains among the CYP6KF genes ( A). Although was not considered to be involved in the resistance to dinotefuran, because the gene was not upregulated compared with several susceptible strains, may play an important role in the detoxification of other xenobiotics. The same may be true for in the CYP6QR subfamily (no CYP bloom observed), which demonstrated the highest expression among the CYP3 clan genes in the TKO strain ( B), and dinotefuran was not metabolized at all by the recombinant protein of this gene. Although was considered a main factor in the resistance to dinotefuran, DNA markers, which can be used in genetic diagnostic methods (e.g., multiplex PCR-based methods) for efficiently detecting resistant individuals (i.e., resistance monitoring) in field populations, are not currently available. Mutations specific to dinotefuran-resistant strains (i.e., candidates for DNA markers) were not found in the locus of the gene (including the upstream and downstream regions of the gene between the adjacent genes) in the mutation analysis in this study. As no commonly upregulated transcription factors (trans-elements) were observed in the resistant strain compared with the three susceptible strains (data not shown), there may be mutations in trans-elements or in distant-acting enhancers (cis-elements) that confer the significant upregulation of in the resistant strain. The genomic and transcriptomic resources constructed in this study will be useful in the search for such mutations and for elucidating the regulation mechanisms of in future works. In this study, we constructed a whole-genome assembly of with satisfactory completeness. The phylogenetic features and expression profiles of the 127 manually curated CYP genes in indicated the importance of the highly expanded CYP6 family as well as the nonexpanded family, such as CYP3652 in the CYP3 clan, for the detoxification of xenobiotics such as insecticides and phytochemicals. was significantly upregulated in the dinotefuran-resistant strain of and was able to significantly metabolize dinotefuran. We concluded that should be considered a major factor in the dinotefuran resistance in . Writing – original draft, Visualization, Methodology, Investigation, Formal analysis, Data curation, Conceptualization. Writing – review & editing, Methodology, Investigation. Writing – original draft, Validation, Methodology, Investigation. Writing – review & editing, Methodology, Investigation. Methodology, Investigation. Resources, Methodology, Investigation. Resources, Methodology, Investigation. Resources, Methodology, Investigation. Resources, Methodology, Investigation. Resources, Methodology, Investigation. Resources, Methodology, Investigation, Conceptualization.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Not Used\n",
      "* **AIモデルの判断:** Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 93 / DOI: 10.1016/j.jhydrol.2024.132393  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "The global forest above-ground biomass pool for 2010 estimated from high-resolution satellite observations\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Evaluating SWAT-3PG simulation of hydrologic and water quality processes in a forested watershed: A case study in the St. Croix River Basin\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "Forests are an integral component of terrestrial ecosystems as they cover about 4.06 billion hectares of land, or about 31 % of the global land area ( ), and have a significant influence on the global hydrologic cycle ( ). Forests exert influence on terrestrial hydrology through their high evapotranspiration, including transpiration, snow and rainfall interception rates, and evaporation ( ). They also influence infiltration rate and capacity, soil moisture, and soil hydraulic conductivity ( ). As such, changes to forest land use and management can significantly alter water yield ( ), streamflow, and peak flow ( ). Alteration to forest ecosystems through natural and/or human-induced disturbances also impacts terrestrial and aquatic nutrient dynamics as sediment and nutrient losses are heavily related to water transport ( ) and forests accounts for 28 % and 38 % of ecosystem nitrogen and phosphorus storage, respectively ( ). Forests also play a vital role in the global carbon cycle ( ) as they serve as an important source of carbon stock as well as sink ( ) due to their high carbon sequestration potential. As such, forests have been prioritized as one of the important nature-based solution (NBS) for increasing carbon sequestration ( ) and mitigating increase in greenhouse gases (GHG) emissions to combat climate change. Quantifying forest aboveground biomass (AGB) and carbon sequestration potential is vital for contributing to climate mitigation policies ( ). Forest AGB is considered one of the essential variables for studying climate change impacts ( ). The increasing variability in climate along with the increase in climate extremes, including severe droughts, extreme precipitation, and heat waves due to the increase in global mean temperature ( ), has led to increasing water conflict between different sectors and geographic regions ( ), water quality degradation ( ), and loss of ecological function and diversity ( ), making planning for sustainable water resource management, water quality and ecological protection, and addressing climate change an immediate priority. This is especially critical for forests as they constitute a major land use type in most watersheds ( ), are impacted by increasing climate extremes such as droughts, wildfires, etc., and play a vital role in ecological sustainability ( ), water supply ( ), carbon storage ( ), and minimizing nutrient loss. Managing forests to improve their resilience to the impacts of climate change while contributing to ecological and environmental sustainability, however, requires understanding how the land–atmosphere-aquatic exchange of water, nutrients, and carbon in forested systems are altered due to changes in climate, management, and disturbances. Process-based hydrologic and water quality models are important tools that can run “what if” scenarios and help understand the impacts of land use/land management changes, climate change, and weather extremes on hydrology, water quality, and carbon fluxes in forested watersheds. This, however, requires the model to accurately represent all the critical biophysical processes within a watershed and simulate the hydrologic, nutrient, and carbon dynamics accurately not just at the watershed outlet but across the watershed. The Soil and Water Assessment Tool (SWAT) is a process-based, continuous time, semi-distributed model developed by the U.S. Department of Agriculture (USDA) – Agriculture Research Service (ARS) ( ) that has been widely applied to evaluate the impacts of land use/land management and climate changes on hydrology, water quality, and agricultural productivity ( ). Although the model has been applied widely with a focus on agricultural land use/land cover (LULC), it has seen limited applicability in forested systems due to its limitation in the simulation of forest carbon, water, and nutrient fluxes ( ). used the SWAT model to evaluate the impact of weather patterns and LULC changes including forests on watershed hydrology. evaluated changes in hydrologic responses with LULC, and evaluated the impacts of planting strategies as a measure for mitigating the impacts of climate change and increasing streamflow. The model has also been utilized to evaluate the impact of forest fires on post-fire hydrology ( ). The application of SWAT in forested watersheds has often been focused only on LULC change, while the evaluation of the forested systems itself including simulation of forest biomass, carbon sequestration, and the influence of different management practices on these variables has been limited due to the limitations of the SWAT model in accurately simulating important forest processes. These include using leaf area index (LAI) as a surrogate for forest structure without physically linking LAI to forest biomass assimilated, inaccurate representation of forest biomass loss from different forest types, biomass partitioning, and inability to appropriately initialize forest age, among others ( ). Most previous attempts to modify the SWAT model for improved forest representation were often limited to modifying how LAI was simulated ( ) or improving parameterization ( ), and they failed to address the key issues (listed above) associated with forest simulation in the SWAT model. ) developed a modified version of the SWAT model, called SWAT-3PG, with a new forestry module based on the 3-PG (Physiological Process in Predicting Growth) forest growth model ( ) that tried to address many of the limitations in the default SWAT forestry module and tested it for forest biomass assimilation and losses for evergreen, deciduous, and mixed forest types using field-scale models. SWAT-3PG, in that study, was, however, tested only for its capability in replicating net primary productivity (NPP), forest biomass partitioning and losses, simulation of LAI and evapotranspiration (ET), and the sensitivity of the model in biomass assimilation to stresses ( ). SWAT-3PG needs additional assessment to evaluate how the model performs for simulating NPP and forest biomass at the watershed scale as well as the impacts of the new forestry module on hydrology and water quality at the field and watershed scales. To address these questions, this study evaluated SWAT-3PG by performing a case study in the St. Croix River Basin of north-central U.S. Specifically, this study evaluated SWAT-3PG for its ability to 1) capture the spatial and temporal variability in carbon sequestration as well as assimilated forest biomass and 2) simulate landscape as well as riverine hydrology and water quality to see if the new module can help improve the hydrologic and water quality dynamics in forested systems. This section provides a brief overview of the new forestry module in SWAT that is based on 3-PG. Readers are referred to ) for additional detail. The new forestry module in SWAT-3PG has separate sub-routines for evergreen, deciduous, and mixed forest types. Biomass assimilation for each forest type is calculated using optimal canopy quantum efficiency (α ) based on equations used in 3-PG so that already calibrated α values for different tree species for 3-PG can be directly incorporated into SWAT-3PG. A fraction of assimilated biomass for each day is first assigned to roots based on environmental conditions, forest age, and user-defined maximum and minimum NPP fraction for roots for specific forest types, after which the remaining assimilated biomass is partitioned and assigned to stem and foliage biomass. The deciduous and mixed forest types, however, have an exception for biomass partitioning at the beginning of the growing season (end of dormancy). All assimilated biomass at the beginning of each growing season for deciduous and mixed forest types is assigned to foliage until the total foliage biomass lost at the onset of dormancy in the previous year is recovered. As a result, LAI in SWAT-3PG is a function of foliage biomass. It is important to note that forest biomass in SWAT is partitioned only into AGB and roots, and LAI is solely a function of accumulated heat units for each year ( ). The dormancy module has also been modified and differs between the forest types based on loss of foliage. Evergreen forests lose a certain amount of foliage biomass each day as litterfall based on age, while deciduous forests, in addition to the daily losses, lose all foliage at the onset of dormancy. Mixed forest types lose a user-defined fraction of foliage at the onset of dormancy as litterfall based on the deciduous fraction cover in the mixed forest system. Tree mortality, which is not simulated in SWAT, is simulated by SWAT-3PG based on age (density-independent) and self-thinning (density-dependent). SWAT-3PG also simulates root turnover which is not simulated in SWAT. As SWAT-3PG has additional algorithms for the simulation of forest processes not incorporated in SWAT, it requires additional input files. Each hydrologic response unit (HRU) needs a new input file that provides the initial values for forest biomass and age as well as forest ID to identify evergreen, deciduous, or mixed forest types. The initialized forest biomass is partitioned into stems, roots, and foliage based on . SWAT-3PG also requires a new forest database that provides 3-PG-related parameters for specific forest types being simulated in the model. As 3-PG runs at a monthly scale and all its time-related parameters are in the units of month , it is necessary to convert all these parameters to units of day for use in SWAT-3PG. Evaluation of SWAT-3PG for the simulation of forest physiological processes at the landscape level including biomass assimilation and water and nutrient transport along with its impact on riverine hydrology and water quality was performed using a case study in the St. Croix River Basin ( ) in north-central U.S. The river basin is part of the Upper Mississippi River Basin, is rich in forests, and covers an area of about 20,000 km , draining parts of eastern Minnesota and western Wisconsin ( ). The St. Croix River, which originates in the watershed and flows about 254 km south from the headwater to its confluence with the Mississippi River, is a designated National Scenic Riverway by the National Park Service (NPS) ( ). Forest is the predominant land cover type covering about 55 % of the watershed, of which close to 49 % is deciduous and the remaining 6 % is evergreen. The forest land cover is dominant in the northern part of the watershed. Agriculture covers about 16 % of the watershed and is dominant in the southern part of the watershed with corn, soybean, and alfalfa as the major row crop types. Wetlands and waterbodies make up about 11 % and 4 % of the watershed, respectively, while urban area accounts for only about 5 % of the watershed. Most of the watershed (about 75 %) consists of well-drained soil (<10 % clay) while about 24 % of the watershed consists of soil with moderately high runoff potential (10–20 % clay), and the remaining 1 % of the watershed is covered by soils with high runoff potential (>40 % clay). The watershed consists of a typical continental climate that is characterized by significant annual variation in temperature with cold dry winters and warm humid summers. The mean daily temperature in the watershed ranges from about −12.9 °C in December to about 20.6 °C in July. Annual precipitation in the watershed averages about 857 mm based on climate normals from 1981 to 2020 ( ) of which 42 % falls in the summer months of Jun-Aug while only 10 % of rainfall is received in the winter months from Dec-Feb. An existing SWAT model for the St. Croix River Basin ( ) was used as the base model and modified for running SWAT-3PG in the study watershed. This was possible because only the files associated with forest HRUs (.mgt file) need to be modified and additional files specific to SWAT-3PG need to be added to the project folder for running SWAT-3PG in existing SWAT models with the default forestry module. Topographic information for the base model was provided using a 30 m × 30 m Digital Elevation Model (DEM) data acquired from the United States Geological Survey (USGS) ( ) while State Soil Geographic (STATSGO) ( ) was used as the soil dataset. The watershed includes many waterbodies of which the largest 39 were simulated as lakes while the rest were simulated as ponds. Base land use for the model was incorporated using a Cropland Data Layer (CDL) LULC data product for the year 2007 provided by the USDA – National Agricultural Statistics Service (NASS) ( ). A threshold of 5 %–10 %–5 % was used for land use, soils, and slope, respectively, to define HRUs that resulted in a model with 419 subbasins and 3,010 HRUs, of which 1,003 were forest HRUs. Threholds for land use, soils, and slopes were used to limit the number of HRUs and improve the computational efficiency of the developed SWAT model ( ). Major crop rotations in the watershed included a 2-year corn/soybean rotation and a 6-year corn-grain/corn-silage/alfalfa for 4 years rotation and were also incorporated into the model. The model also included 48 different point sources for which annual averages of loads were provided. Additional detail on the model development is provided on . Climate forcing to the model was modified from the existing SWAT model and provided using the North American Land Data Assimilation Phase-2 (NLDAS-2) climate dataset which provides data at a spatial resolution of 1/8th degree at 1-hr interval ( ). The model was simulated from 2000 to 2020 with a 2-year warm-up period. The default SWAT model developed for the St. Croix River Basin did not have forest initialized for age and initial biomass although forests make more than half of the LULC in the watershed. Modification of the default SWAT model into SWAT-3PG required the identification of major forest types as well as the initialization of forest biomass and age for each subbasin in the model. Aspen (Populus ) and Jack Pine (Pinus ) were identified as the major deciduous and evergreen forest types in the watershed, respectively ( ), and were incorporated and modeled with SWAT-3PG. Initial forest biomass and forest age for forest initialization for the year 2000 were acquired from and , respectively. The range of initial forest biomass and age for model simulation is provided in . The SWAT-3PG model was first constrained for terrestrial processes by calibrating the model simulated NPP-Carbon (NPP-C) and LAI against remote sensed MODerate resolution Imaging Spectroradiometer (MODIS) NPP dataset (MOD17A3HGF) ( ) and LAI dataset (MCD15A3H) ( ), both of which are available at 500 m spatial resolution and have been validated at regional and local scales against observed datasets ( ) This was followed by the calibration of riverine hydrology and water quality. The model was calibrated for streamflow at 5 USGS stations ( ). A sequential approach to streamflow calibration was utilized in which flows at the headwater USGS stations were calibrated first followed by the downstream stations. Such an approach can help capture the spatial heterogeneity in the hydrologic response of the watershed due to differences in topography, soil, and land use ( ). Calibration for water quality was performed for sediment, total phosphorus (TP), and total nitrogen (TN) and was performed at the USGS station located below the region dominated by forests (USGS 5340500; ). As the availability of the datasets used for calibrating SWAT-3PG varied, presents the calibration and validation period for the different hydrologic and water quality variables. Model performance evaluation for the simulation of terrestrial processes was performed by comparing SWAT-3PG simulated variables against remotely-sensed estimated data products using time series graphs, while streamflow and water quality simulation were evaluated using graphical as well as statistical measures. Statistical measures for the simulation of streamflow and water quality included the coefficient of determination (R ), RMSE-observations standard deviation ratio (RSR), Nash-Sutcliffe Efficiency (NSE), and Percent Bias (PBIAS). R assesses the goodness of fit between the simulated and observed variables and ranges between 0 and 1 with 1 indicating a perfect model fit. RSR is the ratio of Root Mean Square Error (RMSE) to standard deviation of observed data and ranges from 0 to ∞, with values closer to 0 indicating better model performance. NSE ranges from −∞ to 1 with a value of 1 indicating a perfect fit between observed and simulated values. PBIAS evaluates the average tendency of the simulated values to be higher or lower than the observed values. All four measures have been widely used for the evaluation of hydrologic and water quality models ( ). Initial forest parameters for the evergreen and deciduous forest types for SWAT-3PG were acquired from ). Sensitivity analysis as well as the calibration of parameters for forest growth, hydrology, and water quality were performed using SWAT-Calibration and Uncertainty Programs (SWAT-CUP), a software developed for the calibration of the SWAT model ( ). It should be noted that SWAT-CUP is not compatible for the calibration of parameter files associated with the 3-PG in SWAT-3PG. Hence, additional modification was made to the SWAT source code, and parameters within the SWAT model that were not utilized in the project (related to the simulation of bacteria in basins.bsn) were borrowed to calibrate the 3-PG-related parameters in SWAT-3PG. This approach was utilized rather than a model-independent calibration tool such as PEST ( ) because of the advantages and wide adoption of SWAT-CUP in the calibration of parameters related to streamflow and water quality ( ). An important goal of incorporating the new forestry module in SWAT based on 3-PG is to improve the simulation of forest biomass assimilation and partitioning and make forest biomass assimilation more dynamic and responsive while also improving the surface and sub-surface hydrologic and water quality representation of forested watersheds. To evaluate SWAT-3PG for biomass assimilation, forest AGB simulated by SWAT-3PG for the watershed after model calibration and validation was compared against available multiple forest AGB data products for different time periods ( ) to evaluate the capability of SWAT-3PG to accurately replicate forest biomass. Two of the four forest AGB datasets ( ) had forest AGB estimated as a carbon fraction of forest live biomass and hence, were converted to total forest AGB by using a constant carbon fraction estimate in forest live biomass based on literature ( ). As the existing SWAT model for the study watershed with the default forest module was already calibrated in the previous study for hydrology (streamflow) and water quality (sediment, TN, TP) ( ), comparison of landscape level (HRU level) simulation of hydrologic and water quality variables along with forest biomass, streamflow, and riverine water quality were also performed to evaluate how the incorporation on the new forestry module altered the representation of hydrologic and water quality processes. summarizes the datasets used for model development, calibration/validation, and evaluation. Calibration and evaluation of the terrestrial processes of biomass assimilation were focused on the forest-dominated subbasins as only the forest-related model files were modified for SWAT-3PG and the other land uses were considered already calibrated in the model from the previous calibration effort. Hence, evaluation of NPP-C and LAI were performed only for subbasins that had >70 % of forest land use cover ( ). The calibrated SWAT-3PG parameters for the simulation of Aspen (deciduous) and Jack Pine (evergreen) forests are presented in . Comparison of SWAT-3PG simulated NPP-C against observed, derived from MODIS, shows that the model was able to adequately replicate the observed NPP-C in forest-dominated subbasins of the watershed over the simulation period ( ). The model was able to replicate the annual temporal variability of increase and reduction in biomass assimilation over the watershed well. SWAT-3PG was also able to capture the magnitude of annual NPP-C assimilated in forest-dominated subbasins although the model tended to slightly underestimate the assimilated NPP-C in the latter years of the simulation period ( ). Monthly simulation of NPP-C was also compared against observed for each forest-dominated subbasin and calculation of R , NSE, and PBIAS for the total simulation period and evaluation using a boxplot showed a median R > 0.65, NSE > 0.75, PBIAS < 15 % ( ), indicating that the model adequately captured the spatial and temporal variability in NPP-C in forest-dominated subbasins in the watershed. LAI in SWAT-3PG is a function of total foliage biomass, the partition to which from total assimilated biomass varies based on age, time of the year, and forest type. This provides a more robust mechanism for LAI simulation rather than the LAI simulation in the default SWAT model which is only dependent on the accumulation of heat units. Evaluation of SWAT-3PG-simulated average LAI across all forest-dominated subbasins against MODIS-derived LAI shows that the model adequately simulated LAI throughout the simulation period ( a), although the model was not able to replicate the reduction in LAI to almost 0 in the dormant period. It should be noted that the basin average simulated LAI included both deciduous and evergreen forests which could have led to the slight overestimation of LAI in the dormant period when evaluating for all forest types. Evaluation of simulated LAI for subbasins dominated only by deciduous forests ( b), which is the major forest type in the watershed, shows that SWAT-3PG was able to capture the seasonal variability of high as well as low LAI, with the simulated LAI at 0 during the dormant period. Simulation of LAI was also assessed for each forest-dominated subbasin and evaluation of R , NSE, and PBIAS using a boxplot showed that the model was also able to capture the spatial and temporal variability in the simulation of LAI for the forested regions of the watershed with a median R and NSE greater than 0.65 and PBIAS less than 10 % ( ). After the calibration and evaluation of NPP-C and LAI, SWAT-3PG simulated AGB was compared against forest AGB datasets available for the years 2003, 2005, 2010, and 2020 from different data products ( ; ) which showed that SWAT-3PG simulated forest AGB compared well against for 2003, ) for 2005, and for 2010. SWAT-3PG simulated forest AGB, was, however, higher when compared against for the year 2020. Comparison amongst the different forest AGB data products shows that the basin average forest AGB estimated by for 2020 is lower than other forest AGB estimated for all previous years (2003, 2005, and 2010). It is important to note that forest AGB data products provide only a snapshot for a specific time period without considering the influences of past events. They are also estimated using different methods and data sources ( ), and even individual data products carry uncertainty ( ) making the comparison of data products difficult. Along with that, forest disturbance, which can often lead to considerable changes in forest AGB, was not incorporated in SWAT-3PG, which could have potentially attributed to the differences in the forest AGB between SWAT-3PG and for the year 2020. Graphical comparison between SWAT-3PG-simulated and observed streamflow at the 5 UGSS stations after calibration and validation showed that SWAT-3PG adequately replicated observed streamflows at all USGS stations, although the model tended to slightly under-simulate peak flows during high flow events ( ). This was observed across all USGS stations but was more prominent at the most upstream USGS station ( ; USGS 5336770). Evaluation of model performance metrics also showed that SWAT-3PG was able to adequately replicate observed streamflows at all USGS stations, with RSR < 0.7, NSE > 0.5, and PBIAS less than 22 % ( ), except for the most upstream headwater station. Model performance for the most upstream station was slightly worse with RSR > 0.8 and NSE < 0.3. Evaluation of streamflow simulation at the USGS station below the forest-dominated region (USGS 5340500) also indicated a satisfactory model performance ( ). The difficulty of SWAT in replicating peak flows is potentially due to its daily time step, which has been observed in prior studies ( ). This could also help explain the reduced model performance in the headwater USGS station (5336700) where the streamflow response to a precipitation event can be sub-daily, impacting the SWAT model performance. Model evaluation for the simulation of sediment, TN, and TP at the USGS station below the forest-dominated region in the watershed (USGS 5340500) indicated a satisfactory performance with RSR < 0.7, NSE > 0.5, and PBIAS < 33 % for all three water quality variables during the calibration period ( ). Graphical comparison showed that the model generally performed well but failed to replicate the high loading of nutrients, especially for sediment and TP during the validation period, which resulted in the NSE < 0.5 and RSR > 0.7 ( ). Sediment and TP transport are highly correlated with streamflow and evaluation of simulated streamflow at USGS 5340500 for the period during which the model was calibrated and validated for water quality revealed multiple observed high flow events that were under-stimulated by SWAT-3PG, which could have led to the reduction in model performance for the simulation of sediment and TP during the validation period. Comparison of simulated basin average ET for forest-dominated subbasins between the calibrated SWAT model with default forestry module, hereinafter referred to as SWAT-default, and SWAT-3PG showed that the simulated ET between the two models were very similar for the simulation period ( ). Forest AGB, however, was very different with the SWAT-default model having an unrealistic representation of forest AGB over the watershed with forest biomass reducing to 0 at the end of each year and forest AGB never increasing over 14 tons/ha ( ). Forest AGB in SWAT-3PG demonstrates a realistic trend which included a realistic increase in annual forest AGB and the intra-annual variability in forest AGB due to the loss of foliage biomass in the deciduous forest dominant watershed ( ). Streamflow simulation between the two models were also similar for both the calibration and validation period ( vs ; vs ). Evaluation of performance metrics at each USGS station showed that SWAT-default performed slightly better in the most upstream USGS station (USGS 5336700) while SWAT-3PG performed better at USGS station 5338500, the station downstream of USGS 5336700. The metrics at the remaining three USGS stations including the station below the forest-dominated region (USGS 5340500) had comparable RSR, NSE, and PBIAS ( and ). Estimation of ET in SWAT is highly dependent on the simulation of LAI ( ) but because LAI in SWAT-default is only a function of heat units accumulated ( ), SWAT-default can realistically replicate LAI in forest HRUs irrespective of forest biomass assimilation and how forest AGB is estimated. This can lead to an accurate simulation of ET as well as streamflow although there could be many inaccurate representations of landscape-level processes in forest HRUs with SWAT-default. As LAI in SWAT-3PG is calculated based on foliage biomass, which is dependent on forest biomass assimilation, SWAT-3PG is better constrained for the simultaneous simulation of forest biomass assimilation and LAI, and hence, ET. Evaluation of surface and lateral runoff, groundwater recharge, and soil moisture showed important differences between the two calibrated models ( ), although the models had similar streamflow and ET. A monthly average surface runoff of 2.8 mm from forest HRUs accounted for about 4 % of average monthly precipitation (69.3 mm) in SWAT-default while surface runoff from forest HRUs in SWAT-3PG was almost non-existent for the simulation period (0.01 mm) with a maximum of 0.7 mm. SWAT-3PG had a higher monthly lateral runoff (4 mm; 5 % of precipitation) than SWAT-default (2.9 mm; 4 % of precipitation) and a graphical evaluation showed a less episodic lateral runoff generated by SWAT-3PG when compared to SWAT-default ( ). Groundwater recharge followed similar trends between SWAT-3PG and SWAT-default but average monthly recharge in SWAT-3PG was higher by about 8.9 mm ( ). Comparison of soil moisture showed that the average soil moisture in forest HRUs in SWAT-3PG was considerably higher (150 mm) compared to SWAT-default (82 mm). Improved representation of forest biomass assimilation, partitioning, and losses in forest HRUs also had a positive impact on the watershed scale simulation of water quality with SWAT-3PG. Comparison of performance metrics for sediment and TP simulation at the USGS station below the forest-dominated region (USGS 5340500) showed considerable improvement for SWAT-3PG with RSR improving by close to 0.2 and PBIAS improving by more than 30 % for both sediment and TP during both calibration and validation period when compared to the calibrated SWAT-default model ( and . The results for TN were slightly lower for SWAT-3PG but RSR < 0.7, NSE > 0.55, and PBIAS < 32 % still indicated a good model performance for TN simulation with SWAT-3PG. As the dominant pathways through which water moves through a landscape can have considerable influence on the transport of nutrients and sediments, the differences in landscape-level hydrology between the two models also influenced the mechanisms through which nutrients and sediments were transported through the forest HRUs. As surface runoff is a major driver for sediment loss, minimal surface runoff generated from forested HRUs in SWAT-3PG led to very little sediment yield from forested HRUs when compared to SWAT-default ( ). Evaluation of sediment simulation at the USGS station below the forest-dominated region (USGS 5340500) showed a considerable overestimation (PBIAS of 90.8 % and 66.3 % during calibration and validation, respectively) with SWAT-default but was simulated well with SWAT-3PG. This could potentially help explain the influence of SWAT-default’s difficulty in accurately representing sediment transport in forested HRUs leading to the overestimation of sediment. Similar results were also observed for soluble phosphorus and organic phosphorus yield with almost no P yield from forest HRUs in SWAT-3PG compared to SWAT-default ( ). This could be expected as the transport of organic phosphorus from land to aquatic environment is correlated with sediment loss which was minimal under SWAT-3PG. Similarly, plant uptake of phosphorus, which happens in soluble form, was much higher for SWAT-3PG when compared to SWAT-default ( ) leading to minimal soluble phosphorus loss from forest HRUs in SWAT-3PG. It is also important to note that the evaluation of individual forest HRUs in SWAT-3PG showed a loss of soluble phosphorus from some HRUs but was close to 0 when evaluated at the watershed scale. Evaluation of nitrate loss from forest HRUs for both models showed that nitrate leaching through soil profile was the dominant mechanism through which nitrate was lost from forest HRUs, and also followed similar temporal trends ( ) owing to the similarity in groundwater recharge simulation in both models ( ). Nirate loss as leaching through the soil profile as the dominant mechanism matched the trend in hydrology which had groundwater recharge as the dominant pathway for water loss when compared against surface and lateral flow. However, average monthly nitrate loss due to leaching from the soil profile was higher with SWAT-default (2.83 kg/ha) than SWAT-3PG (1.75 kg/ha) ( ) even though SWAT-3PG had a higher monthly average groundwater recharge by 8.9 mm ( ). Nitrate loss in surface runoff was almost non-existent due to little surface runoff generation in SWAT-3PG while there was some nitrate lost through surface runoff with SWAT-default, albeit minimal ( ). Evaluation of nitrate loss through lateral runoff showed distinct trends between the two models similar to that observed in lateral runoff generated with SWAT-3PG showing less temporal variability while it was more episodic in SWAT-default. It was again interesting to note that monthly average nitrate loss through lateral flow was lower in SWAT-3PG (0.08 kg/ha) than SWAT-default (0.22 kg/ha) even though SWAT-3PG had a higher monthly average lateral runoff. Accurate representation of forest physiology and its influence on hydrology, water quality, and carbon is vital in hydrologic models for sustainable and integrated catchment management in forest-dominated watersheds as models can provide a platform for decision support by providing an outlook on the impacts of multiple and complex management and climate scenarios. Evaluation of SWAT-3PG, a recently developed modified version of SWAT with a new forestry module, at watershed scale by comparing to observed data as well as results from a SWAT model with the default forest module showed that the new model can improve on the limitations of the default SWAT model and can be an important tool for decision-making in forested watersheds. LAI has a strong influence on ET ( ) as well as forest productivity ( ). As LAI in the default SWAT model is a function only of heat units and not associated with forest biomass assimilation and partitioning, the model can fail to accurately represent the interlink between the different forest components as evidenced by the accurate representation of LAI but poor representation of forest biomass with the default SWAT model. This limitation is addressed with SWAT-3PG as LAI in SWAT-3PG is dependent on the foliage biomass pool of forest AGB. The tight coupling of LAI to forest biomass is vital to realistically simulate the influence of climate and nutrient constraints on forest biomass assimilation and subsequently hydrology and water quality. This, along with the improvement in the process representation of litterfall in different forest types (see ) can help improve SWAT tracking of forest biomass as well as biomass associated with litterfall and mortality due to age and thinning. As SWAT-3PG allows for the initialization of forest age and biomass, which can significantly influence the carbon sequestration potential and ET, it also allows for a more accurate estimation of forest water use (ET) and carbon sequestration under different management practices which can be valuable information for making management decisions. Improved representation of forest processes with SWAT-3PG was also beneficial in improving the hydrology and water quality associated with forested landscapes. Although the comparison of streamflow simulation at the USGS station below the forest-dominated region in the study watershed between the two models showed comparable results, evaluation of the different hydrologic components at the HRU (landscape) level showed critical differences that indicate an improvement in the landscape level hydrologic representation with SWAT-3PG. It is vital to accurately simulate the hydrologic and water quality processes at the HRU level as it can, otherwise, lead to errors and uncertainties when evaluating the impacts of future climate or management scenarios. High soil porosity and infiltration rates in forested watersheds ( ) means that the subsurface is the dominant pathway through which water flows through a forested system ( ). Comparison between the two models showed that lateral flow was the dominant pathway for water loss with SWAT-3PG while surface runoff was the dominant pathway with SWAT-default ( ). Although SWAT-3PG had higher lateral runoff, it was observed to be less episodic than SWAT-default, which can be expected in forested systems as forests help moderate hydrologic response and flood generation to precipitation events. The improved representation of biomass assimilation and subsequent losses in forests with SWAT-3PG also led to an increase in soil moisture when compared to SWAT-default allowing SWAT-3PG to replicate the impacts of a thick layer of humus often present in forest floor ( ). This was obtained through increase in surface roughness, reduction in curve number, and increase in the available water holding capacity of the soil during model calibration. Inaccurate representation of forest biomass losses and hydrology can misinform sediment, nutrients, and carbon transport from forested watersheds leading to inaccurate simulation of riverine water quality and also misunderstanding the impacts of changes to LULC and management or climate-driven disturbances. SWAT-3PG was able to better replicate the observed variability in riverine sediment and TP below the forest-dominated region of the study watershed while performing comparably for TN, indicating an improvement in the simulation of riverine water quality with SWAT-3PG. Estimation of forest biomass as well as understanding the changes to forest NPP and carbon sequestration potential under varying climate and management practices for different species is a vital need for current forest managers and policymakers, especially in the face of climate change ( ). Comparison of SWAT-3PG simulated forest NPP-C against remotely sensed data as well as estimated forest AGB against other available data products showed that SWAT-3PG can replicate the observed spatial and temporal variability in forest NPP-C as well as forest AGB. This demonstrates the capability of SWAT-3PG as a useful tool to estimate the potential for carbon sequestration as well as forest AGB. This model also has added advantages over other statistical and remote-sensing methods in that it can provide a data product that is continuous over time and space ( ) while the other methods are limited to simulating past and present conditions and often only provide a snapshot of forest AGB. Initial forest stand age as well as management practices such as thinning, planting density, and rotation period can have a significant influence on carbon sequestration potential ( ) which can be incorporated in SWAT-3PG but is not possible with SWAT-default. These added capabilities, along with the potential of SWAT-3PG to leverage forest parameters for different forest species from 3-PG studies and run the model under past or future climate make SWAT-3PG a vital tool to forest managers and policymakers to evaluate the impacts of forest types, climate, and management practices in forest carbon sequestration. Although SWAT-3PG provides many advantages for the simulation of forested watersheds, there are important limitations that still exist in the model that need to be considered when evaluating model results and making management decisions, while also trying to address those limitations in the future. Comparison of SWAT-3PG simulated forest AGB against available AGB data products had shown that the SWAT-3PG estimated forest AGB for 2020 was higher than estimated by while the AGB was comparable for the other three years, albeit against other data products that were different in methods. The difference in forest biomass between the two products could be potentially attributed to the lack of incorporation of forest disturbances into SWAT-3PG, especially given forest AGB was comparable for the years 2003, 2005, and 2010. Human or climate-driven forest disturbances can significantly alter forest AGB ( ) and the post-disturbance management actions can influence the sequestration potential which needs to be incorporated, especially in a continuous-time, process-based model such as SWAT-3PG for the estimation of forest AGB. The lack of incorporation of forest disturbance and subsequent management activity to the disturbed land can be a significant source of uncertainty in the estimates of forest AGB and carbon sequestration as well as watershed scale hydrology and water quality processes. It should also be noted that SWAT-3PG uses a constant NPP/Gross Primary Productivity (GPP) ratio throughout the lifetime of the forest. As the NPP/GPP ratio is known to reduce for forests over time ( ), the constant ratio could lead to an overestimation of forest NPP as the forest stand age matures and needs to be further evaluated. Another important limitation of SWAT-3PG is the simulation of forest succession based on natural events. Forest succession in SWAT-3PG can be simulated by modifying the forest management file (.mgt) but needs to be incorporated into the model by the user. As forests are considered as NBS to improve carbon sequestration and help negate the impacts of increasing GHGs into the atmosphere, understanding the land–atmosphere-aquatic flux of forest carbon is a critical need. The current version of SWAT-3PG can be improved for better representation of carbon processes in forested systems by coupling it with SWAT-C ( ), a modified version of SWAT with improved soil organic carbon processes, for improved applicability in climate change research. As 3-PG has additional forest parameters that were incorporated into SWAT-3PG, this leads to additional computational and calibration burden when running SWAT-3PG, especially for a large watershed with multiple dominant forest types. The availability of remote sensing data products such as NPP, LAI, ET along with the availability of initial 3-PG parameters for different forest types can be vital resources for constraining the simulation of different forest types, but the lack of field-scale data on the amount of foliage biomass, root turnover rates, etc. can make constraining all the forest physiological processes in SWAT-3PG difficult and can introduce additional uncertainty and needs to be considered. There is also the potential to include additional management practices in SWAT-3PG focused on forest management such as raking, burning, etc., which can expand the applicability of SWAT-3PG. This study evaluated SWAT-3PG, a modified SWAT model with a new forestry module based on 3-PG, for its ability to simulate NPP and forest biomass at a watershed scale. The study also evaluated the impacts of the new forestry module in the simulation of landscape (HRU) level water, sediment, and nutrient transport along with riverine hydrology and water quality. Comparison against remote sensing-derived NPP-C and multiple forest AGB data products showed that SWAT-3PG can adequately simulate NPP-C and forest biomass at the watershed scale. The ability of the SWAT-3PG model to accurately simulate carbon sequestration in forests along with its advantages in incorporating forest management practices, multiple forest types, and future climate makes it a useful tool that can contribute to identifying appropriate management decisions and policies for tackling climate change. SWAT-3PG was also able to demonstrate improvement in the water, sediment, and nutrient transport at the landscape (HRU) level along with improvement in the simulation of riverine sediment and TP when compared to the results of the SWAT model with a default forestry module. This shows that the incorporation of the new forestry module in the SWAT model can help reduce the uncertainty associated with the simulation of hydrology and water quality in forested landscapes with the default SWAT model. Writing – review & editing, Writing – original draft, Visualization, Software, Methodology, Formal analysis, Conceptualization. Writing – review & editing, Software, Resources, Project administration, Methodology, Funding acquisition, Conceptualization. Writing – review & editing, Project administration, Funding acquisition, Conceptualization. Writing – review & editing, Resources.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Not Used\n",
      "* **AIモデルの判断:** Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 98 / DOI: 10.1016/j.foodres.2025.116829  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Health risk assessment of heavy metals (Hg, Pb, Cd, Cr and As) via consumption of vegetables cultured in agricultural sites in Arequipa, Peru\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Health risk assessment from heavy metals in crops grown and marketed by family farmers in the mining region of Moquegua, Peru\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "The presence of toxic elements such as arsenic (As), cadmium (Cd), and lead (Pb) in croplands remains a persistent and pressing concern that continuously threatens global food security and public health ( ). Moreover, these contaminants impose significant economic challenges for nations, leading to substantial financial burdens ( ). Dietary exposure to these toxic elements and the associated health risks have been extensively investigated over the years, with numerous studies documenting their impact on various organ systems. These include the cardiovascular, nervous, endocrine, renal, and immune systems, with prolonged exposure potentially leading to chronic disease and carcinogenic outcomes ( ; ; ; ; ; ; Jomova et al., 2025). Research by highlights a growing academic focus over the last decade on health risks linked to chemical element contamination in food samples, including As, Cd, and Pb. This trend is reflected in the increasing use of various indices, such as Estimated Weekly Intake (EWI), Estimated Daily Intake (EDI), Target Hazard Quotient (THQ), Hazard Index (HI), Maximum Safe Consumption Quantity (MSCQ), and Target Carcinogenic Risk (TCR). These indices complement each other to provide a more comprehensive assessment of health risks. EWI and MSCQ help determine if the intake of these potentially harmful elements remains within established safety limits, while THQ and HI estimate the chances of non-carcinogenic health effects by comparing long-term exposure with reference thresholds. TCR, in turn, is used to assess the probability of developing cancer over a lifetime of exposure. Such indices have been widely applied worldwide in the assessment of contaminants across diverse food matrices, including grains ( ; ) and flour products ( ), seafood ( ; ), dairy products ( ; ), chocolates ( ), and fruits and vegetables ( ; ; ; ). However, information on this topic remains limited in several countries, including Peru. In Peru, this issue is particularly significant due to the prominent role of family farming in the country's food system. Family farmers own more than 70 % of the land dedicated to temporary food crops and contribute a similar proportion to the national food supply ( ). These farmers actively participate in ‘farm-to-pot’ or ‘farm-to-table’ markets, where they sell fresh, locally grown products directly to consumers. Such markets are not only essential for regular food purchases but also reflect a cultural and deep-rooted preference for fresh and local produce, making food safety a priority. In different regions of Peru, family farming and mining coexist as primary economic activities. Despite this, health risk assessments regarding potential contamination of food produced and sold in these areas remain scarce. One such region is Moquegua, situated in the southern part of the country. This region is a key contributor to national copper production ( ) and is projected to experience increased demand in the forthcoming years ( ). The lack of comprehensive studies on this issue is contributing to growing consumer distrust regarding the safety of agricultural products. According to , perceived food safety risks have a negative impact on consumers' willingness to purchase. This can lead to a reduction in demand, ultimately affecting farmers' incomes, particularly those engaged in small-scale or subsistence farming. From a public health perspective, further investigation into the safety of locally grown produce is necessary, especially since fears about toxic elements entering the food chain tend to escalate in areas with extensive copper mining activities, as noted by . Therefore, this research aimed to evaluate the health risks associated with As, Cd, and Pb in locally grown produce from family farming, directly sold to consumers through a farm-to-table system in Moquegua, Peru. These elements, hereafter collectively referred to as heavy metals (HMs) for simplicity, were assessed in terms of both non-carcinogenic and carcinogenic potential, utilizing established assessment indices, including EWI, THQ, HI, MSCQ, and TCR. To provide a broader perspective on overall food safety in the region, the study also includes supplemental information from an analysis of pesticide residues conducted on 67 samples of commonly consumed foods. As a pioneering study in Moquegua, the results are preliminary and provide a foundation for further research aimed at protecting public health and guiding actions to enhance local food safety and security. This study was conducted in the Moquegua region of southern Peru, which covers only 1.2 % of the country's territory (15,733.9 km ). It is the second smallest region, with an estimated population of 201,129 inhabitants ( ). Known for its significant copper deposits, Moquegua plays a crucial role in Peru's status as the world's second-largest copper producer ( ). In addition to copper, Moquegua also produces other important metals such as gold, silver, molybdenum, and selenium ( ). As a result, mining has become the primary economic activity. Mining deposits and agricultural activities are widespread throughout the region, occurring in various agroecological environments that range from coastal areas to the highlands, where a diverse range of food crops is cultivated ( ). The overlap of these two land uses has contributed to social tensions, particularly regarding environmental concerns and, more specifically, water governance ( ; ; ). This issue is further compounded by Moquegua's location in an arid zone, where water scarcity has historically been a source of stress ( ). Despite these challenges, agriculture remains a key component of the region's economy. It is predominantly carried out by small-scale farmers, who typically manage plots of less than 5 ha and continue contributing significantly to the region's agricultural output. To support these farmers and facilitate local agricultural commerce, the capital city of Moquegua hosts a traditional market called ‘From Farm to Pot’ ( ). Serving as a platform for producer-to-consumer interaction, the market is organized by the Ministry of Agriculture and Irrigation (MINAGRI) through Agro Rural. It operates partially on Fridays and fully on Saturdays, attracting family producers from the region's three provinces: Mariscal Nieto, General Sánchez Cerro, and Ilo. Set up in public spaces, it allows them to sell a variety of agricultural, livestock, agro-industrial, and artisanal products without intermediaries. Given its role as a cultural and commercial hub that connects local producers with consumers, this market was selected as the sampling site for the present study. The focus was on vegetables, which are exclusively grown in the Mariscal Nieto province, the leading vegetable producer in the region ( ). Aligning with the study's focus on local agricultural systems, dried corn (a traditional product obtained by naturally dehydrating corn for long-term storage while preserving the integrity of the grain) and table olives were also analyzed. Both are emblematic of the region's agricultural identity, primarily grown for local sale or self-consumption. Due to their specific cultivation areas, they were collected directly from producers' properties. Dried corn samples were gathered from the districts of Carumas (Mariscal Nieto Province) and Chojata (General Sánchez Cerro Province), while table olive samples were obtained from the El Algarrobal district (Ilo Province). These districts are exclusively known for cultivating these crops, positioning them as the primary hubs for regional production. presents a map showing the collected samples' origins. The selection of farmers for sample collection was based on convenience to ensure that the samples were representative of the Moquegua region. While the predominantly mostly features local agricultural products, it is important to note that some items may come from nearby regions, such as Tacna and Arequipa. To verify the origin of the products, an initial selection process was implemented in collaboration with local farmers and the Regional Directorate of Agriculture of Moquegua ( - Moquegua). Initially, a total of 103 vegetable sales booths were identified at the ‘From Farm to Pot’ market. However, only booths that offered products exclusively from the farmers' own harvests were considered, resulting in the selection of 59 booths. The criteria for selecting the farmers from whom samples were collected included the sale of specific vegetables (lettuce, spinach, carrot, broccoli, tomato, and celery), the requirement for production lands to be located in Mariscal Nieto Province, the necessity to sell their products weekly at the ‘From Farm to Pot’ market in Moquegua, consistent booth attendance, and the farmers' willingness to provide additional information for the study. For dried corn, 15 farmers were selected from the district of Chojata and another 15 from Carumas. These districts, situated at altitudes ranging from 2800 to 3500 m above sea level (masl), provided an ideal environment for selection due to the strong community cohesion among producers and the established tradition of verbal communication. This collaborative network facilitated the identification of farmers engaged in corn cultivation, who typically preserve dried corn for personal consumption and local sale for extended periods after harvest. For table olive producers, a similar procedure was applied in the district of El Algarrobal, where eight properties that cultivate olives and produce table olives for commercial sale were selected. These properties are located at altitudes ranging from 100 to 800 masl. In all three districts, field technicians from the Regional Directorate of Agriculture of Moquegua played a crucial role in locating and verifying the producers, thereby enhancing the accuracy and representativeness of the sample selection. Random sampling was conducted at each sales booth selected for convenience. Each sample was composed of multiple units to ensure a total weight of at least 1 kg, as established by the guidelines of the National Service of Agrarian Health of Peru ( ). This procedure was adapted to the market setting, where products are sold in small-scale quantities, and the total sample consists of multiple units to ensure representativeness. Each sample focused explicitly on the edible portion relevant to the analysis, and the number of samples collected varied depending on the availability of each food item (refer to ). Once collected, the samples were placed in polyethylene bags, sealed, and coded for identification before being transported to the laboratory for analysis. One portion of the food samples was analyzed for As, Cd, and Pb using inductively coupled plasma mass spectrometry (ICP-MS), following the EPA 200.3/EPA 6010B method, validated in 2016 for vegetable tissue ( ; US ). Another portion of the samples was used to quantify As, Cd, and Pb using atomic absorption spectrometry (AAS), as recommended by the Food and Agriculture Organization (FAO) in the General Methods of Analysis for Contaminants in Foods ( ). This analysis followed the guidelines of the Mexican Official Standard NOM-117-SSA1–1994 ( ). Table S1 in the supplementary material details the number of food samples analyzed using each method, based on the limits of quantification (LOQ) outlined in . Additionally, pesticide residues were analyzed in a subset of 67 samples to provide supplementary information on potential contaminants. While not the primary focus of the study, this analysis aimed to contribute to a broader understanding of the overall safety of agricultural products. Supplementary Material S2 describes the pesticide analysis and the detected residues with their concentrations. To ensure officially valid results, the chemical analyses were performed by ALS LS laboratory in Lima, Peru. This laboratory is accredited by the Instituto Nacional de Calidad del Perú (INACAL) in accordance with the Peruvian Technical Standard NTP-ISO/IEC 17025, which specifies general requirements for the competence of testing and calibration laboratories. This standard, developed by the International Standards Organization (ISO) and approved by national branches of the ISO and the International Electrotechnical Commission (IEC), ensures conformity assessment. Quality assurance and control followed ALS Laboratories' protocol, which includes instrument and reagent verification, sample processing, standard curve configuration, quantitative sample analysis, method detection limits, lower detection limits, and assessments of precision and accuracy ( ). For the health risk analysis, the per capita consumption of each food studied in the target region was determined using the most recent data available from the National Institute of Statistics and Informatics ( ). This information is presented in . The Estimated Daily Intakes (EDI) and Estimated Weekly Intake (EWI) indices are similar, but the key distinction is that the EDI evaluates exposure based on a single day, while the EWI is compared to the Provisional Tolerable Weekly Intake (PTWI). The Joint FAO/WHO Expert Committee on Food Additives (JECFA) established the PTWI, which specifies the maximum allowable limits for each chemical element (CE) based on its toxicity level ( ). EDI and EWI are calculated using represents the fresh weight concentration of an element in food, expressed as μg CE per gram of food, food ingestion rate is expressed in two ways: food mass in grams per day ( ) and food mass in grams per week ( ), whereas stands for the reference body weight, which averages around 70 Kg. It is important to note that can vary by country. Generally, the EWI index has been calculated for both adults and children ( ). To assess the potential risk of contamination, the ratio of EWI to PTWI is calculated. A ratio greater than 1 suggests a possible contamination risk ( ). The daily intake rate ( ) of a food is based on data from the official documents of each country, which account for the annual consumption of that food. In Peru, the annual consumption of food items per inhabitant is detailed in , indicating that the intake rate corresponds to the amount of food in grams consumed daily by each inhabitant, according to data from the National Institute of Statistics and Informatics ( ). Some chemical elements have their PTWI values expressed as μg Kg per body weight per week. These values are: 15 for total arsenic, 7 for cadmium (II), and 25 for lead (II) ( ). The Maximum Safe Consuming Quantity (MSCQ) is an index used to determine the safe amount of food containing a toxic chemical element (CE) that an individual can consume daily or weekly. This calculation takes into account the person's body weight ( ), the concentration of the CE in the food ( ), and the maximum consumption limits established by legislation. The MSCQ index is given by refers to the oral reference dose (mg of CE per kg per day) established by the USEPA or WHO as a safety limit, and represents the concentration of the chemical element present in the food being examined (μg of CE per gram of food). The relevant values are 0.0003 mg/kg BW/day for As, 0.0004 mg/kg BW/day for Pb, and 0.001 mg/kg BW/day for Cd ( ; ; ). To ensure that food does not pose a health risk to humans, the mass of food derived from the MSCQ index must exceed the daily intake rate ( ). The Target Hazard Quotient (THQ), an index used to assess the non-carcinogenic toxicological risk associated with consuming contaminated food, evaluates potential health risks over a resident's lifetime. It is determined by stands for the exposure duration factor (typically set at 70 years), represents the frequency of exposure, calculated as 365 days per year, and is the average time of food consumption, calculated as multiplied by 70 years. If the THQ value exceeds 1, it suggests a potential health risk, indicating that further evaluation, including the carcinogenic risk index, may be necessary ( ). The Hazard Index (HI) is a comprehensive measure used to assess the potential non-carcinogenic health risks associated with consuming food containing multiple toxic chemical elements. It is calculated by summing the Target Hazard Quotients (THQ) of all elements present in the food. Unlike the THQ, which evaluates the risk from a single contaminant, the HI provides a broader assessment, particularly when exposure involves several contaminants at once. If the HI exceeds 1, it indicates the possibility of adverse non-carcinogenic health effects ( ). The Target Cancer Risk (TCR) evaluates the potential risk associated with exposure to carcinogenic agents over a lifetime and is given by refers to the oral cancer slope factor, specific to each element. The values for the elements studied are as follows: 1.5 mg kg day for As, 6.1 mg kg day for Cd, and 0.0085 mg kg day for Pb ( ). TCR values below are considered negligible, while values between and are classified as acceptable concerning cancer risk. However, if the value exceeds , the risk of cancer becomes imminent ( ). The intake rate, a critical factor in calculating the risk index, is often based on regional food consumption patterns to ensure the values accurately represent the local population. Table S1 in the supplementary material summarizes the total number of samples analyzed, as well as the number of samples with concentrations above the LOQ. The results showed that Cd was the most prevalent metal, quantified in 69.1 % of the samples. In comparison, Pb and As were quantified in 41.5 % and 32.9 % of the samples, respectively. The details regarding the maximum and minimum values can be found in Table S3 in the supplementary material. summarizes how the median levels of HMs varied based on the type of sample analyzed. In the case of As, the concentration order was: carrot (up to 0.0969 mg/kg) > spinach (up to 0.0767 mg/kg) > broccoli (up to 0.04850 mg/kg) > lettuce (up to 0.0383 mg/kg) > tomato (up to 0.0323 mg/kg). For Cd, the order was: dried corn (up to 0.1420 mg/kg) > spinach (up to 0.1250 mg/kg) > lettuce (up to 0.1390 mg/kg) > celery (up to 0.0470 mg/kg) > broccoli (up to 0.0700 mg/kg) > carrot (up to 0.1620 mg/kg) > tomato (up to 0.0620 mg/kg). Finally, for Pb, the concentration order was: lettuce (up to 0.1272 mg/kg) > carrot (up to 0.1343 mg/kg) > broccoli (up to 0.1910 mg/kg) > spinach (up to 0.1550 mg/kg) > tomato (up to 0.1380 mg/kg). Existing literature commonly uses either EWI or EDI in assessments, with EWI providing a more direct comparison to the PTWI. This allows for the evaluation of safe weekly intake limits for specific contaminants. Accordingly, presents the ratio of EWI to PTWI for adults and children based on the median values for each food item analyzed. The minimum and maximum values can be found in Table S4 of the supplementary material. All ratios for both children and adults were below 1, indicating that the intake of these HMs from food is within acceptable limits. The highest ratios for As and Pb were observed in carrots, while for Cd, it was in dried corn. presents the THQ and HI results for each food item analyzed, based on their median values. The minimum and maximum values can be found in Table S5 of the supplementary material. All values are well below 1, suggesting that consuming these locally grown agricultural products does not pose a significant health risk from lifetime exposure to As, Cd, and Pb. Even the HI, which combines the THQ of As, Cd, and Pb, was below 1, with the highest median values found in carrots (0.18141). Similarly, the Global Target Hazard Quotient (GTHQ), which reflects the combined risk of exposure to all analyzed HMs in the considered foods ( ), was also below 1. presents the MSCQ for adults and children, based on the median values for each food item analyzed. The minimum and maximum values are provided in Table S6 of the supplementary materials. The MSCQ values indicate that median concentrations of As, Cd, and Pb in all food samples remain within safe intake levels, as the mass of food based on the MSCQ index is within the daily intake threshold. presents the results for the TCR associated with the analyzed food items, based on the median values for each item. The minimum and maximum values can be found in Table S7 of the supplementary materials. All median values for the TCR fall within the range of , spanning from for spinach, celery, and broccoli to for carrots. While these values do not indicate an imminent health risk, they also do not fall within the negligible risk range, placing them in an intermediate risk category. However, as shown in Table S7, there are individual samples with TCR values that suggest a potential health risk, particularly in carrots (up to ), tomatoes (up to ), and lettuce (up to ). These findings call for attention to public health policy considerations. The median concentration of As varied, with the lowest level found in tomatoes at 0.0241 mg/kg and the highest level in carrots at 0.0699 mg/kg. For Pb, concentrations ranged from 0.0322 mg/kg in tomatoes to 0.0655 mg/kg in lettuce. In terms of Cd, the minimum concentration recorded was 0.0103 mg/kg in tomatoes, while the maximum concentration was 0.104 mg/kg in dried corn. All these concentrations are within the permissible limits established by the and the MERCOSUR Technical Regulation on Maximum Limits of Inorganic Contaminants in Food ( ). Among the HMs analyzed, Cd was notably prevalent, appearing in nearly 70 % of the samples, including vegetables from the Mariscal Nieto Province and dried corn from the Chojata district. Pb and As were also quantified in a considerable proportion of samples (41.5 % and 32.9 %, respectively), but were exclusively found in vegetables. Cd is recognized as a common contaminant in agricultural soils and, consequently, in food crops. Its presence is often associated with the long-term application of phosphate-based fertilizers intended to enhance crop yields ( ), as well as the use of poultry manure and inorganic fertilizers like NPK 20–20–20, which are commonly applied in Peruvian agriculture ( ). Environmental factors, such as soil and water contamination, can also contribute to its prevalence ( ). reported on the concentrations of these HMs in the agricultural soils of the Moquegua region, comparing them with the Peruvian environmental quality standards (ECA), which set limits at 70 mg/kg for Pb, 1.4 mg/kg for Cd, and 50 mg/kg for As. In some areas of the Chojata district, Cd concentrations in the soil exceeded the limit. However, in the Mariscal Nieto Province, the levels of these HMs were generally below the ECA thresholds, although some Cd concentrations approached the regulatory limit. Nonetheless, a more comprehensive analysis of HMs in agricultural soils in this province is necessary, along with further investigations into other potential sources, such as irrigation water. Despite the quantification of HMs in a notable proportion of the samples, it is important to contextualize these findings in terms of dietary exposure. Most of the analyzed food products, except for dried corn, are consumed at higher rates than the national average ( ), making them particularly relevant for assessing potential health risks. However, the EWI and MSCQ values were below established thresholds. Additionally, the THQ and integrated values such as the HI and GTHQ suggest a low consumption risk for residents over their lifetime. This is particularly significant in the context of family farming, as the results suggest that, despite agricultural production taking place in a mining area, the presence of the analyzed HMs in local food does not appear to pose a major non-carcinogenic dietary risk, contrary to what might be expected. Research on health risks associated with HMs in food is limited in Peru, particularly for crops grown in mining areas, making direct comparisons challenging. However, it is important to note that our findings are consistent with results from other regions in Peru with environmental concerns, where non-carcinogenic risks were also not reported. This includes artichokes irrigated with contaminated water in Junín ( ), vegetables such as peppermint, coriander, garlic, and leeks grown in areas historically irrigated with untreated sewage in Arequipa ( ), and potatoes grown in high-altitude areas of Junín where important mines are located ( ). In terms of carcinogenic risk, the TCR values obtained in this study are neither negligible nor unacceptable but fall within an acceptable range based on the per capita consumption analyzed, suggesting that further evaluation is needed to ensure long-term safety. These values may be influenced not only by the concentrations of HMs but also by higher local consumption patterns compared to national figures for most of the analyzed food items ( ). As shown in Table S7 of the supplementary materials, TCR values also indicate that some individual samples fall within a concerning risk range. For tomatoes and lettuce, the elevated risk is solely due to Cd, as Pb and As concentrations were below the LOQ. This was observed in tomatoes when Cd concentrations reached 0.0620 mg/kg and in lettuce at 0.139 mg/kg. Similarly, identified tomatoes and lettuce as among the products of major concern due to elevated Cd concentrations, based on samples collected from markets in the San Martín region of the Peruvian Amazon, suggesting a pattern of concern for these vegetables. In the case of carrots, the TCR median value ( ) was close to the threshold associated with potential carcinogenic risk (1 × 10 ). However, six samples exceeded this limit. In two of them, As, Cd, and Pb were all quantified, with Cd being the major contributor to the risk. Specifically, in the first sample that exceeded the threshold, Cd reached 0.0411 mg/kg, Pb was 0.1343 mg/kg, and As was 0.0892 mg/kg. In the remaining four samples, the elevated TCR was solely attributed to Cd, with concentrations ranging from 0.1060 to 0.1620 mg/kg. Notably, these results correspond to isolated cases and do not reflect the overall trends identified across all analyzed samples. These concentrations indicate a potential risk that warrants attention, even though the levels are below the permissible limits established by and . A CR value of 1 × 10 suggests a cancer risk probability of 1 in 10,000 ( ), underscoring the importance of further monitoring despite compliance with regulatory standards. Given that Cd contributes more to the overall risk than As and Pb in certain samples and is frequently detected, it is important to enhance its surveillance. According to the International Agency for Research on Cancer (IARC), for the non-smoking general population, food is the primary source of Cd exposure. Cd and its compounds are classified as Group 1 carcinogens, indicating sufficient evidence of carcinogenicity in humans ( ). While the strongest evidence links Cd exposure to lung cancer, studies also suggest associations with pancreatic, bladder, and kidney cancers ( ; ). In studies assessing health risks associated with exposure to HMs through food consumption, accounting for actual intake is essential, as it directly influences exposure estimates. A limitation of this study is the use of official per capita food consumption data from 2012, the most recent available for the Moquegua region. While this dataset provides a valuable reference, it is important to consider that dietary patterns may have changed since then. For example, provide an analysis of shifts in the consumption of various food groups in Peru between 2008 and 2021. Consequently, the estimates presented in this study may be subject to variation should there be significant alterations in consumption trends. Future assessments would benefit from expanding the range of analyzed items to include a wider variety of locally consumed foods, particularly those of high consumption (e.g., tubers and cereals), as well as sources of drinking water and animal products. Although the present study did not report non-carcinogenic risks, carcinogenic risks were kept within an acceptable range, though not negligible. Previous health risk assessments in Peru have identified carcinogenic risks even in the absence of non-carcinogenic concerns. This has been observed for As in native potatoes ( ) and in vegetables such as leek, mint, and cilantro ( ). Therefore, a broader scope of analyzed elements in food matrices in the Moquegua region would provide a more comprehensive understanding of dietary exposure to HMs and the associated health risks. Additionally, distinguishing between rural and urban consumption patterns could offer valuable insights into exposure disparities and inform more targeted public health strategies. Our results are significant for two main reasons. First, they enhance the trust between producers and consumers in local markets, which are essential sources of fresh food for many rural and urban communities without intermediaries. Demonstrating the safety of food products boosts consumer confidence, which is crucial for family farmers. This trust encourages consumers to choose locally produced goods over alternatives, ultimately supporting the local economy and securing the livelihoods of farming families. Incorporating key regional products such as dried corn and table olives, which are emblematic of the Moquegua region, further enhances the relevance of this study. The fact that the concentrations of As, Cd, and Pb in table olives were below the LOQ, along with the negligible health risk associated with dried corn, reinforces the safety and quality of these local foods, strengthening the identity of Moquegua's agricultural products. As the first health risk analysis conducted in the Moquegua region, this research expands the limited evidence on the concentrations of HMs in crops at harvest ( ). It contributes to the broader discussion on public policy aimed at addressing food contamination risks and supports ongoing assessments, as well as the development of regulations to enhance food safety measures nationwide. This study also examined pesticide residues to enhance the understanding of overall food safety in the Moquegua region, given the potential cumulative health risks of exposure to both HMs and pesticides ( ). The levels of pesticide residues, outlined in Table S2, were compared against the Maximum Residue Limits (MRLs) established by the Peruvian Ministry of Health ( ). However, not all identified pesticides have defined limits, and some food products lack specific regulatory thresholds, pointing to a gap in food safety standards. It is recognized that chemical hazards are among the public's top concerns regarding food safety, often ranking above microbiological risks ( ), which draws attention to the urgency of addressing these issues through policy and oversight. Given that Peru plays a central role in the global mining landscape, especially amid projected increases in copper demand (Valente et al., 2021), developing robust regulatory frameworks becomes increasingly important to protect food safety and public health. Regulation is essential for anticipating population exposure to contaminants and guiding preventive strategies in regions affected by extractive industries. Periodic monitoring expands the evidence base needed for regionally tailored health risk assessments, which are essential to support the development and implementation of such regulatory frameworks. Up-to-date data are key for informing public policies that protect consumer health and strengthen trust in local production, particularly in areas like Moquegua, where the intersection of mining and family farming presents unique challenges to food security and safety. Therefore, efforts to update official national food consumption data patterns are paramount to support consistent regulatory decision-making across regions. This study is the first of its kind to investigate the levels of As, Cd, and Pb in selected food crops produced by family farmers and marketed through a local initiative known as ‘From Farm to Pot’ in the distinctive mining region of Moquegua, Peru. The evaluation of non-carcinogenic risk did not reveal any indication of significant health threats from consuming the analyzed foods. However, in terms of cancer risk, although the results do not indicate an imminent risk, the values identified are not negligible and should be subject to continuous monitoring. Among the samples analyzed, carrot showed the highest median carcinogenic risk value, at , which remains below but close to the commonly accepted threshold of . Although As, Cd, and Pb all require attention, the consistent influence of Cd on elevated TCR values points to its relevance in public health evaluations. Therefore, these findings can serve as a preliminary basis for future research to broaden the scope of dietary risk assessments and support the development of targeted public health interventions. Additionally, this study reports pesticide residue concentrations in food samples to support more comprehensive food safety analyses. Writing – review & editing, Writing – original draft, Visualization, Validation, Project administration, Methodology, Investigation, Funding acquisition, Data curation, Conceptualization. Writing – review & editing, Methodology, Investigation, Formal analysis, Conceptualization. Writing – review & editing, Investigation, Formal analysis. Validation, Methodology. Writing – review & editing, Validation, Supervision, Investigation, Formal analysis.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Not Used\n",
      "* **AIモデルの判断:** Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 100 / DOI: 10.1016/j.theriogenology.2019.12.012  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Data supporting the spectrophotometric method for the estimation of catalase activity\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Effect of carboxylated poly L-Lysine as a cryoprotectant on post-thaw quality and in vivo fertility of Nili Ravi buffalo (Bubalus bubalis) bull semen\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "The technology of artificial insemination (A.I.) is extensively used for the propagation of elite germplasm to a large number of females in the dairy and beef industry across the globe [ ]. Structural and functional competence of cryopreserved semen has vital importance in fertility propagation through AI to disseminate desired valuable genetics for improvement in the production and reproductive efficiency of domesticated animals especially bovines. The efficient process of semen cryopreservation results in long-term preservation of semen and later for its desired use [ , ]. Frozen-thawed sperm possessing intact motility apparatus, active mitochondria, integral plasma and acrosomal membranes [ ], and unfragmented DNA [ ] ensure optimum fertility through AI [ ]. Conversely, freezing and thawing instigate a detrimental and irreversible cascade of changes in the sperm; thus, compromise its integrity and survival within the female reproductive tract after insemination [ ]. Sperm cells come across these damaging effects due to disruption of plasma membrane integrity [ ], initiation of cell death pathways through necrosis and apoptosis [ ], osmotic disturbance across membranes [ ], reactive oxygen species (ROS) [ ], mechanical damage due to ice crystals and cold shock [ ]. This damage is more augmented due to re-crystallization during thawing [ ]. Buffaloes (Bubalus ) have a lower fertility rate with cryopreserved semen under field conditions [ , ] due to poor freezibility [ ] and decreased sperm survival [ ]. More occurrence of cryo-injuries in buffalo sperm as compared to cattle [ ] is due to a higher proportion of polyunsaturated fatty acids (PUFA) in membranes [ ]. More PUFA make the sperm membranes vulnerable to freezing and thawing stresses [ , ]. The augmentation of freezing procedures is required to protect buffalo sperm from cryo-injuries. Formation of intra and extracellular ice crystals during freezing inflicts injuries to sperm by causing fragility to membranes through mechanical damage, disruption of osmotic balance and later exposing sperm genome to injurious changes [ ]. The cryoprotective agents prevent harmful damage to sperm at subzero temperatures. Glycerol, being permeable cryoprotectant, has a tremendous beneficial role in freezing extenders to preserve germ cells of multiple species. For bovines, the usual concentration of glycerol is 6–7% in freezing extenders. Apart from usefulness, it has been reported that glycerol exerts toxicity on spermatozoa when used at higher concentrations. It is obvious that it influences the cell volume, changes in integrity and permeability of cell membrane, and actin cytoskeleton [ ]. Besides this, contraceptive effect due to higher concentration of glycerol in freezing extenders on the sperm has also been reported [ ] which could be due to the osmotic shock following rapid loss of glycerol from frozen-thawed spermatozoa in female’s reproductive tract [ ]. Therefore, it is imperative to decrease the glycerol concentration in freezing extenders and explore new cryopreservation regimes for efficient cryopreservation of buffalo bull sperm. Carboxylated Poly- -Lysine (CPLL) is classed as the ampholytic polymer compound and composed of conversion of 65 mol % of amino groups to carboxyl groups. The cryoprotective properties of CPLL have been investigated and found higher as compared to various cryoprotectants for cattle sperm, oocytes [ ], embryos and different somatic cells [ , ]. Moreover, it is reported that CPLL exerts less cytotoxic effects than dimethyl sulfoxide and possesses better cryopreservation efficiency. It was also found that risks of damage by ice recrystallization during freezing and subsequent thawing were reduced by using CPLL [ ]. These better cryoprotective effects of CPLL have not been investigated for buffalo bull sperm freezing regimes. To best of our knowledge, no comprehensive report is available for use of CPLL for freezing of buffalo bull sperm. Therefore, the present study has been focused to observe the effect of CPLL as a cryoprotectant in freezing extenders containing optimum and less glycerol on post-thaw semen sperm structure, function, DNA integrity, lipid peroxidation, catalase concentration and fertility of Nili-Ravi buffalo bull semen. Nili Ravi buffalo bulls (n = 3; age = 5–6yr), regular semen donors, were housed at Semen Production Unit (SPU) Qadirabad, Pakistan (ISO 9001:2015: RICI) to be used in the present study. The managemental and housing conditions were kept optimum and uniform for each bull. Seasonal green fodders at 10% of body weight, 3 kg concentrate and water were offered on a daily basis to each bull. A regular health check program is operated at SPU which also includes scheduled vaccination and deworming regimes. For preparation of the CPLL solution, Epsilon poly -Lysine (PLL, Macklin Chemical Co. China) was dissolved (25%; w/v) in Mili-Q water. Then 1.3 gm of succinic anhydride was mixed per 10 mL of PLL solution and then heated at 50 °C for 1 h. Tris-citric acid (TCA) buffer was prepared by dissolving Tris 199.80 mM, citric acid monohydrate 69.75 mM, fructose 55.56 mM, benzyl penicillin 1000 IU/mL and streptomycin sulphate 1000 μg/mL. Later, it was divided into two aliquots to dissolve glycerol in two concentrations (5 and 7%; v/v) termed as G5 and G7. Both groups contained chicken egg yolk as 20% (v/v). Each glycerol extender (G5 and G7) was further subdivided to five aliquots to dissolve different concentrations of initially prepared CPLL (0, 0.25, 0.5, 0.75 and 1%; v/v) termed as C0, C0.25, C0.5, C0.75 and C1 respectively. Group containing 7% glycerol without CPLL (C0) was termed as control. In this way, total of ten groups were made as the factorial of 2 glycerol concentrations (G5 and G7) × 5 CPLL concentrations (C0, C0.25, C0.5, C0.75 and C1). All chemicals were acquired from Sigma Aldrich Co. MO, USA except separately stated. The present study was conducted during the breeding season (October–December). Semen was collected from each donor bull using an artificial vagina which was maintained at 42 °C, twice a week, (replicates = 5). After collection, each semen ejaculate was immediately shifted to a water bath maintained at 37 °C for initial evaluation (volume, percentage motility, and concentration). Semen volume was measured by calibrated graduated glass tube. Sperm motility was measured under phase-contrast microscope (BH-2, Olympus, Japan) and sperm concentration was measured through semen photometer (Accucell IMV, France). Ejaculates having a minimum 1 ml volume with >500 × 10 /mL sperm count and 65% motility were selected for further processing. Individual ejaculates of each bull were diluted with Tris-based extenders (mentioned above) to a final concentration of 50 × 10 motile sperm/ml. Each diluted sample was cooled to 4 °C in 2 h, filled in 0.54 ml French straws and equilibrated for 4 h at 4 °C. Programmable Freezer (Digitcool, IMV France) was used for the freezing of the semen after equilibration. Freezing rate was followed as holding of semen at 4 °C for 2 min, from 4 °C to −20 °C at 10 °C/min, from −20 °C to −120 °C at 30 °C/min and final holding at −120 °C for 1 min. After that, these straws were directly plunged into LN and stored until analyzed. Post thaw semen evaluation assays were performed at Animal Reproduction Laboratory, University of Veterinary and Animals Sciences Lahore, Pakistan. For analysis, three straws per replicate of each bull were thawed at 37 °C for 30 s and pooled in a tube for each bull separately to avoid straw variation; and processed for analysis as given below. Sperm motility, velocity distributions, and kinematics were analyzed through computer-assisted sperm motion analysis (Frame Rate: 60HZ/sec, Number of Frames: 30, minimum cell contrast; 15, minimum cell size (pixel) 8, cell intensity, threshold straightness 80, medium VAP 25 μm/s, low VAP cut off 5.0, VSL 0.05 μm/s). A drop of 7 μl semen sample was placed on a pre-warmed glass slide. This slide was positioned at stage warmer (Tokai Hit; Japan) and observed at 200X. The recorded parameters included sperm total motility: TM (%), progressive motility: PM (%), average path velocity: VAP (μm/s), curvilinear velocity: VCL (μm/s), straight-line velocity: VSL (μm/s), lateral head displacement amplitude: ALH (μm), straightness: STR (%), linearity: LIN (%) and beat cross frequency: BCF(Hz). At least 200 sperm were assessed for each motility parameter in CASA. Assessment of sperm viability [ ] was done through the Propidium Iodide assay. For this purpose, 50 μL of TCF (Tris 2.42 g, citric acid 1.34 g and fructose 1 gm dissolved in 100 mL distilled water) were mixed with frozen-thawed semen (50 μl) and then centrifuged at 300 g for 5 min. Sperm pellet was re-suspended in TCF to make a final volume of 47.5 μL, supplemented with propidium iodide solution 2.5 μl; (10 μg/mL in distilled water) and incubated for 5 min at 37 °C in dark. For the determination of viability, a drop of sperm-PI mixture was placed on a glass slide, covered with a coverslip and 200 sperm were observed under the fluorescent microscope (Olympus CX41). Sperm containing red fluorescence at the head region were denoted as dead sperm while sperm without fluorescence were counted as viable sperm. Plasma membrane integrity was assessed through the Hypo Osmotic Swelling test (HOST). The HOST solution contains sodium citrate 0.735 g, fructose 1.35 g and 100 ml of distilled water. Frozen-thawed semen sample (50 μl) was mixed with 500 μl of HOST solution and incubated at 37 °C for 40 min. A 5 μL drop of semen sample was placed on a slide and analyzed under phase-contrast microscope (Olympus, BH-2, Japan) at 400× magnification. At least, 200 sperm per semen sample were counted. Sperm with coiled or swollen tail were considered for intact plasma membrane which are biochemically active [ ]. For assessment of acrosomal integrity, thawed semen sample (500 μl) was mixed with 50 μl of formal citrate solution (1 mL of 37% formaldehyde solution and 2.92-g tri-sodium citrate dihydrate, dissolved in 100-mL distilled water). 200 spermatozoa for each sample were counted under phase-contrast microscope at 1000× magnification for normal apical ridge. Number of spermatozoa with sharp crescent type appearance as apical ridge was counted as intact acrosomal membrane [ ]. Sperm DNA integrity was evaluated through the Acridine Orange staining method. Briefly, the stock solution of AO was prepared by adding 1 g of AO (sigma St. Louis, MO, USA) in 1000 ml of AO stock solution (40 ml of citric acid anhydrous solution, and 2.5 ml of 0.3 M Na HPO solution). Egg yolk extender was removed from 100 μl semen sample by adding distilled water and this sample was centrifuged at 4000 rpm for 7 min. A small drop of semen sample was placed on a glass slide and smear was prepared. Slide was air dried and fixed by putting in freshly prepared corny solution (three parts of ethanol and one part of glacial lactic acid) at 4°c for 2 h. Slide was air dried after the fixation and put in AO stain for the next 3 min. Slide was rancid with tap water and evaluated immediately under fluorescence microscope (Olympus BX41). Spermatozoa shined green at head region were considered having normal DNA integrity, while those shining other than green (from yellow to red) were counted as DNA damaged [ , ]. Sperm mitochondrial membrane potential was assessed through modified Rhodamine (Sigma Aldrich USA) assay. 50 μL TCF buffer (Tris 2.42 g, citric acid 1.34 g and fructose 1 gm dissolved in 100 mL distilled water) was used for the dilution of semen sample (50 μL) and centrifuged at 3000 rpm for 5 min. Rhodamine 5 μl (10 μg/mL) was added to sperm suspension and incubated at room temperature for 20 min in dark. At least 200 spermatozoa were counted for mitochondrial membrane potential. Sperm which showed green fluorescence at mid piece were characterized as active mitochondria with higher membrane potential [ ]. Semen straws were thawed in a water bath at 37 °C for 30 s. After thawing, straws were placed in a glass beaker (500 ml) and sonicated with the help of a BMS ultrasonic liquid processor (ULP-750) by placing the probe in the glass beaker. For complete sonication, a total of 8 cycles with 30 s cooling period between each cycle was completed. Semen sample was placed in a 3 ml Eppendorf tube and centrifuged for 5 min at 3000 rpm. The supernatant was removed carefully and processed further for the analysis of catalase. After centrifugation, 50 μl supernatant was placed into a cuvette containing 500 μl of H O (Test sample) and into another cuvette containing 500 μl H O (control test). Besides a blank (550 μl H O) and a standard (500 μl H O , 50 μl H O) cuvettes were prepared. All cuvettes were incubated at 37 °C for 3 min and supplemented with 2 ml of ammonium molybdate to stop the chemical reaction further. Finally, 10 μl from each cuvette was checked for absorbance at 374 nm using spectrophotometer (UV-2800; BMS; USA). Catalase activity (kU/L) was calculated by the following equation. Where t = total time S° = absorbance of standard tubes, S = absorbance of test tubes, M = Absorbance of control test VT = Total Volumes of reagents in test tubes and VS = volume of semen [ ]. Lipid Peroxidation in semen was assessed by thiobarbituric acid reactive substances (TBARS) assay through the estimation of malondialdehyde (MDA) [ ]. Frozen thawed semen sample (200 μL) was mixed with 200 μl of Sodium Dodecyl Sulphate (8%) and 750 μl of acetic acid (20%). Thiobarbituric acid (1.5 ml) and distilled water were added to make the final volume 4 ml, heated at 95°c for 1 h and cooled at room temperature. N-butanol (1.5 ml) was added and centrifuged at 4000 rpm for 15 min. Finally, the supernatant layer was taken to read the absorbance against an appropriate blank at532nm wavelength using a spectrophotometer. The concentration of malondialdehyde (MDA; nmol/ml) was calculated from the standard curve against the known concentration of 1,1,3,3-tetraethoxypropane and expressed as nmol/ml. For fertility, 205 adult cyclic buffaloes (age = 5–8 years) possessing no reproductive problem were selected. Based upon better post-thaw results, two groups were selected [(G7C0 (control) and G5C0.75 (5% glycerol + 0.75% CPLL)] to compare fertility. Inseminations were performed by the same inseminator to avoid human variation (n = 103 for G7C0 and n = 102 for G5C0.75). Pregnancy diagnosis was performed per rectum at least 60 days post insemination by the same technician. All data were analyzed for normality using the Shapiro-Wilk test. Data for post-thaw semen parameters was analyzed through 2 × 5 factorial ANOVA [(2 concentrations of glycerol (5 and 7%) × 5 concentrations of CPLL (0, 0.25, 0.5, 0.75 and 1%)] using PROC MIXED procedure. Replicates and bulls were considered as random variables. Data of these semen parameters were presented as Mean ± S.E.M. Multiple comparison of groups was conducted using Tukey’s test and a probability level of P˂0.05 was considered as significant. The data for fertility was compared using chi-square analysis. All statistical analysis was performed in SAS University Edition (SAS Inst. Inc., Cary, NC, USA). The data for post-thaw total and progressive motility has been shown in . Both parameters differed (P < 0.05) due to the interaction of glycerol and CPLL. Total motility was found highest (P < 0.05) in the G5C0.75 group while progressive motility was higher (P < 0.05) in C0.25, C0.5, and C0.75 when used with G5. Both motility parameters were found lowest (P < 0.05) in G5C0 as compared to control (G7C0). Total motility was higher (P < 0.05) in C0.5 and C0.75; and progressive motility in C0.5, C0.75, and C1 when used in G5 as compared to G7 at the same concentration of CPLL. Results for post-thaw plasma membrane integrity, acrosome membrane integrity, viability, and mitochondrial membrane potential after the addition of CPLL in combination with glycerol are shown in . PMI [ (c)] was found highest (P < 0.05) in the group (G5C0.75) where the extender was supplemented with 0.75% CPLL while reducing the concentration of glycerol to 5% as compared to control (G7C0) and other treatment groups. It was also recorded higher (P < 0.05) in C0.5 and C1 in reduced concentration of glycerol (G5) as compared to G7 interacting with the same concentration of CPLL. It was found lowest (P < 0.05) when 5% glycerol is used without the addition of CPLL (G5C0) as compared to G7C0. Post-thaw acrosomal integrity [ (d)] and mitochondrial membrane potential improved (P < 0.05) due to 0.75% CPLL in 5%glycerol (G5C0.75) as compared to control (G7C0) and other groups. Both parameters were found higher (P < 0.05) when CPLL C0.50 and C1 was added with 5% glycerol G5 as compared to G7 at the same concentration of CPLL and control group (G7C0). shows that viability is higher (P < 0.05) in C0.5, C0.75, and C1 groups when used with G5 as compared to G7 at the same concentration of CPLL and control group (G7C0). Sperm viability [ (b)] is lower in group (G5C0) when 5% glycerol is used without the addition of CPLL as compared to control (G7C0) and other treatment groups. However, the data for the post-thaw DNA integrity illustrated in , shows that the parameter did not differ (P < 0.05) significantly due to the interaction between CPLL and glycerol. However, DNA Integrity [ (a)] was found lower in the G5C0 group when 5% glycerol is used with no CPLL. Moreover, data for the CASA parameters average path velocity (μm/s), straight-line velocity (μm/s), curvilinear velocity (μm/s), linearity (%), straightness (%), amplitude of lateral head displacement (μm) and beat cross frequency (Hz) is presented in . Average path velocity and Straight-line velocity (μm/s) were found higher (P < 0.05) in groups containing 0.75% CPLL and 5% glycerol (G5C0.75) as compared to Control (G7C0) and other groups. Both parameters were found lower (P < 0.05) in G5C0 as compared to G7C0 control. Linearity (%) and straightness (%) were found higher (P < 0.05) in G5C0.75 as compared to control (G5C0) and other treatment groups. Straightness (%) was also higher (P < 0.05) in C0.5 and C1 when 5% glycerol G5 is used as compared to G7 at the same concentrations of CPLL. Lowest linearity and straightness (%) were observed in group G5C0 having 5% glycerol with no CPLL. However, VCL was higher (P < 0.05) in the control group than in other treatment groups. The results of post-thaw analysis for biochemical profile Lipid peroxidation (MDA level) catalase activity are shown in . Both parameters also differed (P < 0.05) due to the interaction of glycerol and CPLL. Lipid peroxidation (MDA level) was lower (P < 0.05) in groups C0.5, C0.75 and C1 when G5 was used as compared to G7 at the same concentrations as compared to control group and other treatment groups. Higher lipid peroxidation was observed in G5C0 as compared to control. Catalase activity was observed highest (P < 0.05) in the G5C0.75 as compared to control G7C0 and other groups. It was found higher in C0.5 and C0.75 groups when 5% glycerol (G5) as compared to G7 at the same concentrations of CPLL. is showing the data on the fertility rates in buffaloes after insemination with control group G7C0 and G5C0.75. Fertility rates [56.8% (58/102) vs. 36.9% (37/103)] were found higher (P < 0.05) in buffaloes inseminated with semen cryopreserved using (G5C0.75) 0.75% CPLL in combination with reduced glycerol concentration (5%) as compared to control G7C0 (7% glycerol) respectively. To our knowledge, this is the first detailed study to investigate the effectiveness of carboxylated poly -Lysine (CPLL) as cryoprotectant used with different concentrations of glycerol on post-thaw quality and fertility of Nili Ravi buffalo bull semen. The reduction in the fertility potential of sperm due to damaging processes of freezing and thawing processes is crucial [ , ]. Sperm cell membrane is considered to be the highly susceptible structure for cellular damage during cryopreservation due to lipid peroxidation of the polyunsaturated fatty acids [ ]. The buffalo bull spermatozoa are more prone to cryo-injuries [ ]. Additionally, to avoid toxic effects of glycerol in freezing extender, we hypothesized that the use of CPLL would enhance freezability and later fertility of buffalo bull sperm with reduced glycerol concentration. The most salient result of the present study was significantly improved post-thaw plasma membrane integrity when CPLL was used in combination with a reduced concentration of glycerol. The reason for these better results is because of the membrane-binding property of CPLL thus, protecting it from cryo-injuries during the freezing process. CPLL has very low permeability to the sperm cell plasma membrane [ ] and it protects the cells membrane as an extracellular agent [ ]. In the present study, extender having 5% glycerol and 0.75% CPLL (G5C0.75) significantly decreased the sperm cell plasma membrane damage. It is reported that CPLL has a protective effect on the spermatozoa membrane. Similar findings were observed when CPLL was used for the cryopreservation of oocyte [ ], bovine fibroblasts cells and cumulus cells, murine L929 cells and rat mesenchymal cells [ ], mouse embryo and Human induced pluripotent cells [ ]. CPLL possesses anti-freeze proteins like properties [ , , ]. Anti-Freeze proteins inhibit the formation and recrystallization of ice crystals in the intracellular spaces during freezing and thawing. So the cellular damages by these crystals could be reduced [ , ]. Anti-Freeze proteins bind with ice crystals through prism the surface and cause the ice crystals to be more hexagonal [ ]. High sperm cell viability percentage was observed in this study when CPLL, having anti-freeze properties, was added in the semen extender. It is due to the less damage to spermatozoa by ice crystals formation in intracellular spaces during cryopreservation. These findings get support from the previous technical note of the effect of anti-freeze polyaminoacid (carboxylated poly -Lysine) on bovine sperm [ ]. Sperm Motility is considered to be the major but not the only prediction parameter for fertility. It has a positive correlation with sperm cell viability, mitochondrial and cell membrane integrity [ ]. Computer-assisted sperm analysis (CASA) is an important tool to help comprehensively analyzing the different motion characteristics and kinematics of spermatozoa. Significant increase in sperm total motility, progressive motility and sperm kinematics (VSL, VAP, Linearity, Straightness, BCF) was observed when 0.75% CPLL is used in combination with 5% glycerol. The exact reason for this drastic improvement is still not cleared but it is probably due to the protection of the spermatozoa cell membrane from cryo-injuries during freezing due to AFPs like membrane binding property of CPLL [ , , ]. It was clearly observed in the study that CPLL gave the best results when it was used in combination with glycerol. Similar results were observed in a previous study [ ]. It is normally considered that bulls with high fertility rates have less than 2% DNA damage. Moreover, cryopreservation and subsequent thawing process also induce injuries to the chromatin structure of spermatozoa [ , ]. The Results of this study showed that the addition of CPLL in the semen extender caused less than 2% damage to chromatin structure. Furthermore, it is shown in the results that mitochondrial integrity, acrosome integrity, and normal apical ridge were also improved by adding CPLL. It is very intelligible to mention that sperm cell membrane stability plays a pivotal role in improved cell viability and it also acts as a barrier that protects the internal structures such as cytoplasm, mitochondria, acrosomal region and chromatin structure from cryo-injuries [ ]. The presence of high levels of polyunsaturated fatty acids makes buffalo bull spermatozoa highly susceptible to cryo-injuries. During the cryopreservation and thawing process, different ultra-structural changes occur in spermatozoa [ ]. PUFAs in the sperm cell membrane undergo lipid peroxidation, consequently, excessive production of ROS occurs which disrupts the anti-oxidant defense status of the sperm cell and seminal plasma. Hence the quality of semen and sperm fertilization ability is affected [ , ]. Moreover, sperm mitochondria have been indicated as the main source of ROS in spermatozoa [ , ] and also considered to be among the sperm structure prone to cryo-injuries during freezing [ , ]. Lipid peroxidation causes high levels of ROS that increase sperm damage because sperm plasma membranes are rich in polyunsaturated fatty acids in buffaloes [ ]. It is reported that ROS production during cryopreservation was associated with a reduction in sperm motility and DNA damage [ ]. Malondialdehyde (MDA) is a biomarker for lipid peroxidation [ ]. The present study revealed that the addition of CPLL at the level of 0.75% in extender and reducing the amount of glycerol to 5% significantly reduced the level of MDA production. It is due to the spermatozoa cell membrane stability due to membrane binding property of CPLL. Anti-oxidant enzyme activity i-e, catalase activity was found higher when semen was preserved using CPLL. It is because of the low level of lipid peroxidation and membrane stability. It is evident that the cryopreservation process causes membrane damage and increases lipid peroxidation [ , , ]. As a result of this peroxidation, alteration in membrane integrity and increased cell permeability may cause a reduction in the level of anti-oxidative enzyme activity [ , ]. Sperm membrane integrity, livability, and motility are found to be negatively correlated with the concentration of sperm MDA [ , ]. The high lipid peroxidation of the plasma membrane during freezing and thawing induces increased loss of membrane integrity and membrane leakiness. As a result of this successive leakage, loss of intracellular anti-oxidant enzymes occurs. In this study, the subsequent fertility rate was drastically improved when CPLL is added with 5% glycerol in semen extender. Low toxic property of CPLL may be considered as the major reason for this improvement. CPLL was found non-cytotoxic even at a concentration of 20% to L929 cells [ ] or a concentration of 25% to human dermal fibroblasts [ ]. Moreover, a better proliferation rate was observed when fibroblasts cells, cryopreserved using CPLL as cryoprotectant, were cultured without removing CPLL. Hence, CPLL was proved as a cryoprotectant that does not need to be removed. This is not the case with other cryoprotectants i-e, DMSO [ ]. Another reason for this increased fertility is due to high plasma membrane integrity. Developmental ability is majorly affected by the plasma membrane integrity [ ]. Therefore, the use of CPLL in buffalo semen extender is considered to reduce the cytotoxic damages on spermatozoa during freezing and thawing [ ]. It is found that some cryoprotectants which are permeable to the cell membrane, move into the cells and cause rapid dehydration of the cells i.e. glycerol. This removal of water is helpful for protection during freezing. But this rapid dehydration causes damage to the cells. So they me be cytotoxic to the living cells [ ]. Therefore, the addition of CPLL helped in reducing the quantity of glycerol and moderating the rapid change in osmotic pressure [ ]. These effects may have been major reasons for high fertility rate. Conclusively, this study shows that the addition of CPLL (0.75%) with a reduced concentration of glycerol in freezing extender can enhance structural and functional parameters of buffalo bull sperm along with a decrease in lipid peroxidation and increase in catalase activity of sperm. Ultimately, all these parameters enhanced fertility of Nili Ravi buffalo inseminated with semen cryopreserved with CPLL in freezing extender. Further studies are needed to explore this protective effect of CPLL on organelles of sperm and mechanisms to decrease sperm death.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Not Used\n",
      "* **AIモデルの判断:** Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 102 / DOI: 10.1016/j.rse.2023.113578  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Global 1 km × 1 km gridded revised real gross domestic product and electricity consumption during 1992–2019 based on calibrated nighttime light data\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "A first Chinese building height estimate at 10 m resolution (CNBH-10 m) using multi-source earth observations and machine learning\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "Accurate measurement of building height is essential for understanding the impacts of urbanization on the urban environment. Building height is correlated with urban energy use ( ), greenhouse gas emissions ( ; ) and human wellbeing ( ; ). Furthermore, it is a crucial variable in volumetric analysis ( ), population mapping ( ) and living conditions such as per capita space availability ( ). Building height also has a significant impact on urban climate ( ), including urban heat islands effects ( ; ; ), solar radiation ( ; ) and wind speeds (Miao et al., 2009). Accurate mapping of building height is therefore an important basis in improving our understanding of urban processes. In recent decades, two-dimensional (2D) urban morphology, including urban boundaries, the extent of impervious surface, and human settlement footprints, have received considerable attention and resulted in many high-resolution and global products ( ; ; ; ). There are, however, relatively few studies or products available for three-dimensional (3D) urban structures, and most of these focus on particular cities with spatial resolution limited to 0.3-1 km ( ; ; ; ). Even scarcer are 3D high-resolution data for urban structures in large and connected areas ( ). Currently, open-source and freely available earth observations are widely used for 3D mapping of urban morphology. For example, using Sentinel-1 backscatter data, modelled building height at 500 m spatial resolution for major cities in the US. presented a building height map for China at 1 km-spatial-resolution based on Sentinel-1 data and spatially-informed Gaussian process regression. However, the accuracy and application of such models is constrained by the backward scattering coefficient which is influenced not only by building height but also by the surface characteristics of construction materials, the composition of land-cover surrounding the buildings, and by the roughness characteristics of buildings or trees ( ; ). It is possible to address these limitations, , developed a method to estimate building height for China based on the Advanced Land Observing Satellite (ALOS) World 3D-30 m (AW3D30) DSM ( ). As illustrated by , who combined Sentinel-1A/B and Sentinel-2A/B time series data to construct a model of building height for the whole of Germany. However, China has a more complex urban 3D structure, greater degree of building heterogeneity, and a wider distribution of high-rise buildings ( ) and accordingly the method requires further modification to produce a reliable estimate building height at the national scale. An accurate map of building height is a basic requirement to support urban environmental research and planning, the more so in China with its prolific and rapid scale of urbanization. To date, however, there is no high resolution (<30 m) map of building height on a national scale. In filling this research gap, this study aims to develop a first Chinese building height map at 10 m resolution (CNBH-10 m) based on data from an open-source earth observation platform analyzed using machine learning. The main research objectives of the study are as follows: As summarized in , the independent variables used to estimate building height include radar data, optical data, night-time light data, population data, topographic data, and settlement distribution data. Each variable and preprocessing step is described in detail below. We did not filter the variables for further regression because the RF model is insensitive to multivariate linearity ( ). Sentinel-1 Ground Range Detected (GRD) scenes were used to estimate urban building height which provides data from a dual-polarization C-band Synthetic Aperture Radar (SAR) instrument. Sentinel-1 imageries are available at 10 m resolution and a short revisit time (6–12 days) ( ). In this study, two polarization bands, VV and VH, and dual-polarization information derived VVH ( ) were used for further building height estimation. All Sentinel-1 SAR GRD data are available on the GEE platform ( ), and all the data on GEE were processed using Sentinel-1 Toolbox ( ) to generate a calibrated, ortho-corrected product. A total of 56,821 scenes of Sentinel-1 data were used in this study. a shows the total observations of Sentinel-1 data from 2019 to 2021. The dual-polarised information derived VVH was calculated as follows: Where was set to 5 based on a former study ( ). Phased Array Type L-band Synthetic Aperture Radar (PALSAR) is an active microwave sensor which has a multi-polarization configuration, and is widely used for the estimation of vertical structure in vegetation. In this study, global 25 m PALSAR/ PALSAR-2 yearly mosaic data from 2019 to 2021 were obtained from the GEE data catalog (Collection Snippet: “JAXA/ALOS/PALSAR/YEARLY/SAR”). To ensure high-quality data, the SAR images with the lowest response to surface moisture were prefer used for creating the annual product using the mean composite method ( ). In this study, a total of 160,023 scenes of Sentinel-2 were utilized. The QA60 bitmask band was used to mask out poor-quality observations caused by clouds (Bit 10) and cirrus clouds (Bit 11) ( ) for each image. b shows the total number of cloud free observations of Sentinel-2 data during 2019 to 2021. In addition to Sentinel-2 bands, six other spectral indices were considered as independent variables, including Normalized Difference Vegetation Index (NDVI) ( ; ), Enhanced Vegetation Index (EVI) ( ), Land Surface Water Index (LSWI) ( ), Modified Normalized Difference Water Index (MNDWI) ( ), Normalized Difference Built-up Index (NDBI) ( ), and Combinational Shadow Index (CSI) ( ). Equations used to calculate the indices are as follows: , , , , , , are the surface reflectance of the red, green, blue, near infrared, shortwave infrared, aerosols and water vapor bands of the Sentinel-2 MSI sensor. LUOJIA 1–01, a new nighttime light (NTL) data satellite launched by China in 2018, has a spatial resolution of 130 m ( ). In this investigation, we utilized LUOJIA night light images from 2018 as input data to derive estimations of building heights. To improve the accuracy of NTL, we calculated the Vegetation Adjusted NTL Urban Index (VANUI) ( ) based on Eq. to mitigate the oversaturation effect of NTL. The World Settlement Footprint (WSF) layer for 2019 is a global human settlement distribution product at a ground resolution of 10 m derived from Landsat-8 and Sentinel-1 data. In this study, the settlement coverage derived from WSF 2019 was extracted as an independent variable for building height estimation and was also used to mask the final CNBH-10 m product to remove to remove non-built-up pixels. We chose WSF due to its superior ability to remove roads between buildings, as well as its high user accuracy and accuracy of area estimation compared to other datasets such as ESA WorldCover, ESRI Land Cover, and GHS-BUILT-S2 ( ). c shows the distribution of WSF in 2019. Considering the large extent of mainland China's latitude and longitude range, from 73°33′E to 135°05′E and from 3°51′N to 53°33′N, we incorporated the potential impact of variations in solar elevation angle on the model in our selection of variables for building height inversion. We included building location (longitude and latitude) and topographic information (DEM and slope) as key variables, as these factors significantly affect the solar elevation angle. In addition, population size was considered as an independent variable. More detailed information on the data sets used here is presented in . All variables were resampled to 10 m using the nearest neighbor resampling method for building height modeling and estimation. The vector building footprint data, which incorporate the information relating to the number of floors for 62 cities in China for the year 2018 ( d) were collected from Baidu map services ( ). To obtain the building height for each building, the number of building floors was multiplied by 3 m ( ; ). report an accuracy of 86.8%, with a mean height deviation of approx. 1 m, for this dataset. In order to acquire sample points of building footprint for further analysis, we used the method (Eq. ) from to convert the vector building height data of single buildings to 10 m spatial resolution grid data. In order to enhance the quality of samples and mitigate errors arising from incomplete building measurements, the analysis excluded samples within a 50-m radius of the sampling point that exhibited a difference of >25% between the building footprint vector data and the WSF building distribution in terms of area. In this study, a total of 22,909 samples were used to train (70%) and validate (30%) the building height model. where and represent the height and area of individual building patches, respectively, while denotes the total area of all building patches within the statistical area. illustrates the workflow developed for the building height estimation based on multi-source, multi-temporal, all-weather earth observations. The approach consists of three main sections. First, the preprocessing of independent variables was conducted by combining multi-temporal, multi-spectral, multi-window, and multi-statistical methods. Second, the Random Forest (RF) model was used to construct and optimize the estimation model. Third, the validation of the simulations was referenced against real building height data. All Sentinel-1 and cloud free Sentinel-2 time series were temporally aggregated to enhance the richness of information. This processing step employed the spectral-temporal variability approach ( ). We calculated the maximum, minimum, and standard deviation of the time series of Sentinel-1, Sentinel-2, and their derivatives variables for the three-year period from 2019 to 2021. To capture shadow features across different levels of building heights, we employed multi-window local statistics ( ) that utilized both spectral features and radar data. We applied maximum and mean statistical methods to circles with radii of 50 m, 100 m, 150 m, and 200 m. The RF regression model ( ) was used to estimate building height, where a total of 519 variables were set as predictors, and 22,909 samples were used as training samples. The number of trees (Ntree) and the number of features randomly selected to split each node (Mtry) are two crucial parameters of the RF model. Increasing Ntree can enhance the performance of random forests. Since the RF classifier is computationally efficient and non-overfitting, Ntree can be set to the highest feasible value ( ). In most of the studies reviewed here, the Ntree value was set at 500, as the errors of the classification tree stabilize before this number is reached ( ). When Mtry is small, the model's variance decreases, but the bias increases, as some critical features may be ignored. When Mtry is large, the variance of the model increases, but the bias decreases as the model considers more features. Previous research suggests using the square root of the number of variables as the value for Mtry ( ). Therefore, in this study, we set Ntree and Mtry to 600 and the square root of the number of variables, respectively. In order to understand the importance of different variables for the building height model, the relative importance of each variable was evaluated using the Mean Decrease in Gini (MDG) method ( ). For 2019, WSF data were used to define the settlement footprint of the CNBH-10 m map. The RF regression model was performed on the Google Earth Engine (GEE) platform ( ). To assess the accuracy of the estimated building height, three indicators were calculated including R, Root Mean Square error (RMSE) and Mean Absolute Error (MAE) based on 30% of the reference samples. In this study, we evaluated the accuracy of the model using both the least squares (LS) regression model and the weighted least squares (WLS) regression model. The WLS model assigns weights to each building height category, with data points appearing more frequently in the height category having a lower weight, resulting in a more precise estimation of the slope of the regression line. is the estimated building height value, and is the reference building height value. illustrates the relative importance of each independent variable in the building height estimation model. The cumulative relative importance ( a) indicates that the CSI makes the largest contribution (15.1%) to the estimation model, followed by VH (7.8%). Overall, the contribution of optical, radar, and other data to the building height estimation model is 76.6%, 18.2%, and 5.3%, respectively. According to the average degree of importance of each independent variable ( b), building location (latitude, longitude, DEM, slope), and settlement density also contribute to the simulation, albeit less so. The importance of information gathered by windows of different sizes varies for building height models, as the relative importance of the 50 m, 100 m, 150 m, and 200 m scales are 34.2%, 32.5%, 22.4%, and 5.6%, respectively. To summarize, optical information exhibits the highest level of importance in combination with the most significant scales of 50 and 100 m. The results in show the progression of estimation accuracy as the number of variables increases. A total of 519 models were constructed by incrementally adding variables based on their relative importance, and the change in model accuracy was assessed as the number of independent variables increased. The results indicate that the inclusion of a certain number of variables significantly improves estimation accuracy, but that further increasing the number of variables eventually reachs a plateau at which accuracy can no longer be improved. Therefore, a balance between efficiency and high estimation accuracy can be achieved by including a specific number (121 in this study) of variables in the model. illustrates the relationship between reference and estimated building height. Our study confirms the remarkable generalizability of the RF-based building height model across various cities. Using the least squares regression model, the mean values of R, RMSE, and MAE were 0.7, 7.6 m, and 6 m, respectively. Meanwhile, the WLS regression models produced mean values of R, RMSE, and MAE at 0.7, 6.2 m, and 5.2 m, respectively. The three strongest correlations based on WLS regression models are obtained for the dataset in Nantong ( = 0.87), Beijing ( = 0.82) and Tangshan (R = 0.8). Wuhu (4.1 m), Baoding (4.2 m), and Quanzhou (4.5 m) exhibit the smallest RMSE. The validation results for all samples exhibit a strong statistical relationship between the estimated and reference building heights ( = 0.77 using both least squares regression and WLS regression models). In the least squares regression model, the model achieved an uncertainty (RMSE) of 7.4 m and MAE of 5.8 m, in the WLS regression model the RMSE was 6.1 m and MAE was 5.2 m. shows the distribution of accuracy obtained from two evaluation models. Further results of the accuracy verification for individual cities can be found in the supplementary Fig. S1 and Table S1. indicates the height distribution of buildings in China in 2020. The results reveal that megacities in eastern China. More specifically in the Yangtze River Basin and Delta, the Beijing-Tianjin-Hebei region and Guangzhou-Shenzhen-Hong Kong regions have the greatest concentrations of high-rise buildings. As would be expected, large and medium-sized urban centers also have signify taller buildings. The CNBH-10 m product demonstrably provides accurate representations of building heights in several urban areas, as illustrated in . A comparison with high-resolution satellite imagery reveals that the product performs well in estimating point, linear, and clustered arrangements of high-rise buildings and accurately reflects low buildings in older urban areas, such as the historic city of Beijing. Based on the CNBH 10 m product, the mean building height at city level ( a), the high-rise building area ( b) and total building volume ( c) are depicted for China. Macau (22.3 m) has the tallest, and Hong Kong (22.1 m) has the second tallest average building height. The larger metropolitan conurbations in China, i.e. Beijing, Shanghai, Guangzhou, and Shenzhen have mean building heights of 12.8 m, 18.0 m, 15.2 m, and 17.9 m, respectively. The distribution of the total area of high-rise buildings (above 24 m) in each city shows that the Beijing-Tianjin-Hebei region and the Yangtze River Delta region have more high-rise buildings, among which Shanghai has the most with 209.79 km , followed by Beijing (144.14 km ), Suzhou (110.99 km ) and Chongqing (106.29 km ). There is a marked difference between the distribution of accumulated building volume and average building height at city scale. Cities with greater mean building heights are located especially along the coast and in central and southwestern China, while the largest building volumes are found in the most densely populated cities, such as Shanghai (298.4 10 m ), Suzhou (266.8 10 m ) and Beijing (266.2 10 m ). Using cities as a statistical unit, we compared population and GDP with building morphology, including mean building height, high-rise building area and accumulated building volume ( ). The results show that building height generally does not correlate well with population or GDP, although high-rise building area and building volume exhibits a strong correlation with two socio-economic parameters. This may be due to the fact that most of the tall buildings are commercial sites and not located in residential areas. Population figure actually exhibits the most significant correlation with the high-rise building area (R = 0.66, -value<0.01) and building volume (R = 0.78, p-value<0.01), while GDP also have very strong correlations with these two parameters. The backscattering coefficient is considered to have a strong correlation with building height, as demonstrated in previous studies ( ; ). Accordingly, in this study we incorporate HH, HV polarization data from PALSAR to improve the accuracy of building height estimation. The complexity of urban morphology and urban building material differences introduces uncertainties if using only the backscatter coefficients for large scale and high resolution building height estimation. So in this study we also applied long time series optical data and explore application of the shading index. The results demonstrate that the backscatter coefficient and shading index are the most important variables in the building height estimation model. In considering the large scale and complex topography of this national-scale study, information on building location and topography, as well as longitude, latitude, and DEM were used to show that taking account of the relative contributions of these variables to the building height model can improve the generalizability of large-scale building height estimation. When estimating large scale building heights in China, which is more heterogeneous than those in European countries, the multi-window statistical approach used in this study effectively accounts for the model variables, such as shadows for high-rise and low-rise building. The study indicates that it is possible to improve the potential of building height estimation in such complex scenarios. In this study, we compared the estimated 10 m building height with existing sets of products, including 30 m ( ), 500 m ( ), and 1000 m building heights ( ). shows the distribution of building height observation data for six representative cities in China and compares the results across the four building heights. The results indicate that the 10 m building height product provides better detail and more accurately reflects the distribution pattern of building heights compared to other building height product. Furthermore, the 500 m and 1000 m building height products fail to reflect building height information at the block scale, thereby demonstrating the superiority of our 10 m building height product. Additionally, we randomly selected 20,000 sample points and analyzed the correlation between the four sets of building height products ( ). To ensure comparability of building height data across different resolutions, we resample the higher resolution data using the bilinear interpolation method to match the spatial resolution of the lower resolution data. Our findings show that the 10 m building height product has a good correlation with the 30 m and 1000 m building height products, with R values of 0.69 and 0.71, and RMSE values of 5.8 m and 4.4 m, respectively. However, the findings of this study exhibit a low correlation with the 500 m building height product, with an R value of only 0.41 and an RMSE value of 12.8 m. This may be due to the inclusion, in the 500 m building height product, of information on nonbuilding surfaces such as streets and parking lots ( ). This inclusion also explains the poor correlation between the 500 m and the 30 m and 1000 m building height products. Compared with the lower resolution building height products of previous studies ( ; ; ), the 10 m spatial resolution building height data presented here demonstrates the feasibility of fine-grained urban 3D morphological characterization. Moreover, CNBH-10 m products provide potentially important baseline data with a wide range of applications, such as studies of urban microclimate. For example, previous researchers have indicated that the building complexity and the mixture of building types can influence urban ventilation and energy balance, and thus have effect on urban heat accumulation and release ( ). CNBH-10 m also has great potential in research on urban morphology. Specifically, there are numerous studies on the impact of urban expansion and urban 2D landscape patterns on urban ecology and the environment ( ; ). However, the discontinuity and high economic cost of urban 3D data collection have made it difficult to quantify these impacts in cities. Together with the 3D building morphology metrics proposed in previous studies ( ; ), CNBH10 m has great potential to fill this gap. Despite these promising results, several limitations of the methodology need to be acknowledged. Due to the complex 3D structure and high degree of heterogeneity of cities, there are several uncertainties in the estimation of building height and these need to be carefully taken into account when applying the methodology. For example, additional shadows caused by trees and overpasses between buildings may affect the estimation of building height, future efforts to mitigate these uncertainties can be pursued through the utilization of multi-angle remote sensing techniques or the acquisition of higher-resolution satellite imagery ( ; ; ). The data resampling method used in the study may also have an impact on the results of the building height estimation. Here, the WSF dataset was used as the base map for building distribution, and a masking process was applied to the final CNBH-10 m product. The accuracy of the base map has a significant influence on the accuracy of the building height product, especially for the building volume estimation. Future research could explore the combination of multiple building distribution products, such as GHS-BUILT-S2 ( ), to improve the accuracy. Due to the use of a pixel-based method for estimating building height, the final results of building height estimation exhibited some noise. Moreover, the multi-window approach used for processing input variables resulted in some smoothing effects in the building height products. To enhance the accuracy of building height estimation, future studies may consider utilizing object-based methods or employing post-processing techniques on building height products, in conjunction with more precise building boundaries such as vector boundaries of individual buildings ( ). The use of three-year remote sensing imagery for building height inversion in this study may also introduce uncertainties in the results due to rapid urban development in China. Moreover, due to computational limitations and mapping efficiency, this study only used the RF model for building height estimation and comparison, while further refinements may be achieved through deep learning methods. Although nighttime lighting and population data are also used as variables for building height estimation in the study, they do not contribute well to the building height estimation model, which is likely due to the low spatial resolution of these data. This study demonstrates the potential of using earth observation data for national-scale building height estimation and establishes a high degree of accuracy of simulated building height based on RF regression modeling. Specifically, a total of 519 different feature variables were derived from earth observation data collected in all-weather conditions, and building heights were analyzed based on multitemporal, multispectral, and multiscale geospatial big data. The shading index, the backscattering coefficient, and the location of the building are the most significant contributors to the China-wide building height estimation model which has the potential for universality, transferability and reliability in terms of accuracy. Estimated and observed building heights exhibit R, RMSE and MAE values of 0.77, 6.1 m and 5.2 m respectively. In summary, the method and outputs indicate the potential of multi-source, multi-time, multi-window algorithms and cloud-based computing platforms for large-scale, refined building height mapping. The CNBH-10 m product can be applied to a wide range of urban process studies, such as urban climate, energy consumption and population estimates. The CNBH-10 m product is fully open source and freely available ( ). Data curation, Formal analysis, Methodology, Software, Validation, Visualization, Writing – original draft. Methodology, Validation, Writing – review & editing. Methodology, Writing – review & editing. Methodology, Writing – review & editing. Methodology, Writing – review & editing. Writing – review & editing, Data curation. Writing – review & editing, Data curation. Data curation, Visualization. Funding acquisition, Project administration, Resources, Supervision, Methodology, Writing – review & editing.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Used\n",
      "* **AIモデルの判断:** Not Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 115 / DOI: 10.1016/j.chiabu.2023.106299  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Multiple Indicator Cluster Surveys: Delivering Robust Data on Children and Women across the Globe\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Do parenting behaviors intended as discipline vary by household religious affiliation in Cameroon?\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "Parenting behavior intended for discipline (including behaviors considered abusive) vary substantially across and within countries. For example, household survey data from 33 low- and middle-income countries (LMICs) found a range of exposure to physical punishment from 24 % in Bosnia & Herzegovina to 86 % in Yemen among children 2–14 years of age ( ). In addition to variability across countries, there is substantial variability within countries ( ). Advancing knowledge about parenting behavior variability may provide greater understanding of child rearing and family function. Additionally, it may inform initiatives aimed at reducing children's exposure to violence in the home, a key component of target 16.2 of the Sustainable Development Goals ( ). Religion is one factor that may account for some of the variability in parenting behavior, however, there are substantial research gaps. The peer-reviewed literature examining the relation between parent disciplinary behavior and religion appears predominately restricted to high-income countries (HICs) with an almost exclusive focus on Christianity. Furthermore, it typically only considers a limited range of disciplinary behaviors. Within this literature there has been a particular interest in comparing households affiliated with Protestant denominations, particularly conservative subtypes, with other groups (e.g., Catholics). It has been proposed that Conservative Protestants may place greater emphasis on child obedience and literal interpretation of the Bible than other Christian groups, which might account for a hypothesized greater use of corporal punishment ( ; ; ). Literal interpretation of the biblical passages (Proverbs 13:24) ( ), would be particularly relevant. There have been several United States (US)-based studies examining this potential religion-disciplinary relationship. For example, in a study of 132 parents of three-year olds from a midsized southwestern city in the US, parents with a Conservative Protestant affiliation reported spanking or slapping their child more frequently than other groups (Mainline Protestants, Catholics, non-affiliated), although there was no adjustment for potential confounders ( ). In a large nationally representative US sample of households with a child under 12 years of age, parents with a Conservative Protestant affiliation compared to other affiliations (specifics of others' affiliations not provided separately), reported spanking or slapping their child more often even after adjusting for potential socioeconomic and demographic confounders ( ). In another national US sample, spanking was independently associated with parents having a Protestant affiliation (no subgroups identified) compared with parents having a Catholic affiliation (child mean age: 9 years), adjusting for potential confounders ( ). Whereas most of the work in this field appears to be based on US samples, there are a few examples from other HICs, but none from LMICs. In a nationally representative Canadian sample, parents with a Conservative Protestant affiliation had higher odds of endorsing use of “physical punishment” when a child misbehaved than other groups (Mainline Protestants, Catholics, and no religion) for children 2–5 and 6–9 years old while adjusting for socioeconomic variables (although this relationship was not found for 10–11 year olds) ( ). In a sample from Northern Ireland of parents of 4–6 year olds ( = 371), those with a Conservative Protestant affiliation scored higher on a measure of hitting or smacking their child compared to those with Mainline Protestants and Catholic affiliation ( ). There does not appear to be similar peer-reviewed publications examining parent disciplinary behavior among household affiliated with Islam, in HICs or LMICs, despite Islam being the second most common world religion ( ). However, some ideas may be gleaned from work by Islamic scholars. The Quran and the hadith (reports attributed to the Prophet Muhammad's words and deeds) are the two core sources of principles for Islam ( ; ). Some authors have proposed that there may be ideas from these texts that influence parenting ( ; ). “ (Muslim prayers) ” (Sunnah.com, n.d.) is one potentially relevant hadith. However, there does not appear to be an equivalent or specific directive in the Quran with regard to physical punishment of children as with the earlier cited biblical passage. Additionally, there is some suggestion that punishment may be discouraged before the age of seven, potentially related to ideas about younger children not being at an “age of responsibility” as reflected in Islamic educational/training ideas (e.g., “tarbiyyah”) ( ). This may result in a more distinct age effect on parenting behavior compared to households with other religious affiliations. There is also a hadith emphasizing affection and mercy towards children that may influence parenting ( ). Furthermore, it has been noted that there is not the Christian notion of “original sin” in Islam and children are perceived as innocent, i.e., lacking adult culpability of committing sinful acts ( ). A further limitation in the peer-reviewed literature on religious affiliation and parenting behaviors is the tendency to only report a single type (e.g., spanking) and/or a single broad category of physical or corporal punishment ( ; ; ). This overlooks the possibility that the relationship may differ by type of physical punishment or between physical and non-physical disciplinary behavior. A partial exception to the gap in this type of inquiry is a study that examined the use of yelling by Conservative Protestants in the US. It has been proposed that if yelling is more common in this group, along with physical punishment, the approach would be consistent with an authoritarian parenting style ( ), whereas it would not if physical punishment was delivered in combination with positive non-physical parenting ( ). The finding of a lower prevalence of yelling, in addition to some evidence of theological conservativeness related to higher prevalence of hugging and praising, may lend support to the latter ( ; ). Examining the combined use of non-physical discipline with physical punishment may identify whether Protestant groups more frequently use a blended approach to disciplinary practice. This study purposefully selected Cameroon, a LMIC, to extend the examination of the relation between parenting behavior and religious affiliation as it has sizeable populations of different major religious groups, including Muslims. Cameroon is a Sub-Saharan country of approximately 27.2 million (2021 estimate) and is classified as a lower-middle income country ( ). Cameroon is reported to have a high prevalence of child punishment. Based on a UNICEF index, 85 % of children (aged 1–14 years) experienced any physical punishment and/or psychological aggression by caregivers in a one-month period based on a 2014 national survey ( ). This study has two main questions: (1) Does the prevalence of parenting behavior (inclusive of three domains [i] physical punishment, [ii] psychological aggression, and [iii] non-violent discipline) vary by head-of-household religion in Cameroon?; and (2) Are the odds of exposure of a child to these parenting behaviors independently associated with head-of-household religion in Cameroon after adjusting for socioeconomic and demographic variables and the belief in the necessity of physical punishment in child rearing? Two hypotheses are proposed for question two. First, it is hypothesized that the odds of exposure will be higher for children in Protestant than Catholic and Muslim households, and Catholic than Muslim households, for the following specific parenting behaviors: (i) spanking, (ii) hitting with an object, and (iii) a combination of the three domains of discipline. Higher odds for Protestant households would be consistent with some of the HIC studies cited above, and a lower odds for Muslim households would be consistent with ideas identified by Islamic scholars cited above. These three behaviors were selected as spanking is a frequently studied behavior in HICs, hitting with an object directly ties to the above cited biblical passage, and combined discipline extends the above cited work that examines the relation between parent behavior and religious affiliation beyond physical punishment. No hypotheses were proposed for the analysis of other parenting behaviors. Second, it was hypothesized that a significant age-by-religion interaction would be found such that there would be lower odds of use of these parenting behaviors for younger than older children in Muslim households compared to Catholic and Protestant households. This hypothesis stems from the suggestion that Islamic teaching may encourage a less punitive approach towards younger children compared to Christians. The Multiple Indicator Cluster Survey (MICS) program is an initiative facilitated by UNICEF to collect health and social data on nationally representative samples of children and families in LMICs ( ; ). MICS are approved by ethics committees within participating countries with informed consent obtained from all participants. Survey data without any personal identifiers are made available for secondary analysis. A two-stage sampling approach is used which entails (i) randomly selecting enumeration areas, from existing census of participating countries, based on probability proportional to size, and (ii) randomly selecting households from within the selected enumeration areas. For selected households, there is a suite of questionnaires depending on household membership. Relevant for this study, the questionnaire module on parenting behavior intended as discipline was embedded in the household survey. Parenting behavior was asked of an adult in the household (typically the mother), but only if the households had a child between 1 and 14 years of age. If there was more than one eligible child, one child was selected randomly to be the focus of the module questions. The 2014 MICS dataset for Cameroon (Cycle 5) was used for this study ( ). Of the sampled households with a child 1–14 years of age ( = 6073), the following inclusion criteria were applied. First, all questions in the parenting behavior module had to be complete (5905/6073, 97.2 %). Second, the head-of-household identified religious affiliation as either Catholic, Protestant or Muslim ( = 5012/5905, 84.9 %). This religious restriction resulted in the ineligibility of 893 households (other Christians [ = 423], no religion [ = 207], Animists [ = 190], other religion [ = 71], missing [n = 2]). Third, all considered covariates had to be complete ( = 4978/5012, 99.3 %). The adult respondent was asked about the use of 11 types of parenting behaviors for the selected child with the following preamble “ ”. Each question has a response option of “yes” or “no”. UNICEF classifies the 11 behavior types into three domains: (i) “Physical Punishment”, (ii) “Psychological Aggression” and (iii) “Non-violent Discipline” ( ). The Physical Punishment domain is composed of six physical punishments (some of which would be classified as physical abuse depending on the jurisdiction): (i) , (ii) (“spanked”, for short), (iii) e.g. (“hit with object”), (iv) (“hit on head”), (v) (“hit limb”), and (vi) (“beat up”). The Psychological Aggression domain is composed of two behaviors: (i) “ ” (“yelled”) at the child, and (ii) “ ” (“name calling”). The Non-Violent Discipline domain is composed of three behaviors (i) (“privileges”), (ii) (“explained”), and (iii) (“gave something”) ( ). In addition to reporting the frequency of use of each individual behavior, a composite measure for each domain was calculated: “any physical punishment”, “any psychological aggression” and “any non-violent discipline”. Finally, a measure to capture diversity of practice is based on reporting at least one item from the three domains (“Combined Approach”). This set of parenting behaviors has face validity and has been vetted by an international committee ( ). However, there are no reported psychometric properties on the specific items or the composite scores. Items were drawn from the Parent-Child Conflict Tactics Scale ( ; ). A single question was asked of the respondent as to religious affiliation of the head-of-household. Respondents indicating Catholic, Protestant or Muslim were included in this study. The respondent was asked a single question: with response options of “ ”, “ ” or “ ”. “Don't know” responses were infrequent and were dropped for analysis. The two child-specific variables were (i) child age, reported in years, and (ii) child sex, reported as male or female. There were five household-specific variables (i) head-of-household sex, reported as male or female, (ii) head-of-household education, classified in the available database into four categories: none, primary, secondary, and higher, (iii) location of residence, coded as rural or urban in the available database, (iv) number of children 1–17 years of age in the household categorized into 1, 2, 3, 4, 5, 6+, and (v) wealth quintile, available in the dataset and constructed from questions on a series of household assets used across all MICS studies and described elsewhere ( ). The wealth index is divided into quintiles from the poorest to the richest. The Complex Sampling Program in SPSS (version 27) was used for all statistical analysis to adjust for the MICS complex sampling design. This program accounts for the use of cluster sampling, stratification, and weighting in analysis of MICS survey data. Weighted percentages are presented using household weights to provide nationally representative estimates. The presence of associations between the primary variable of interest and parenting behavior was evaluated using the Rao-Scott chi-square test ( ) used in complex sampling with significance set at < 0.05. The association of the primary variable of interests and covariates, including two-way interaction terms (main predictor - child age) on the criterion variable, was expressed by odds ratios (OR) with 95 % confidence interval (CI) and calculated by logistic regression models within the complex sampling program. Model diagnostics were performed, and all the assumptions for modelling were assessed and met. All covariates were considered in the modelling, but only variables that were significantly associated with the criterion variables remained in the model. -values were corrected for multiple-hypotheses testing using Bonferroni correction for the hypothesized outcomes ( = 3), and at the end variables with -value < 0.02 (a = 0.05/3 = 0.022) remained in the final model. For the examination of the relation of other parenting behaviors, the value was not corrected for multiple statistical testing and was set at p < 0.05. A total of 4978 (unweighted) households met the inclusion criteria. Using weighted data, Catholic households were the largest subgroup (41.6 %), followed by Protestant (30.9 %) and Muslim (27.6 %) households. Groups differed significantly on several socioeconomic and demographic variables as summarized in . Muslim households had the highest percentage of households located within a rural area, falling within the lowest quintile, and having a head-of-household with no formal education. Catholic households had the lowest percentage located within a rural area and with a head-of-household with no formal education. The prevalence of each parenting behavior by household religion is summarized in . There were no differences across the three groups in the prevalence of being spanked. In contrast, hit with object significantly differed across groups with the lowest percentage occurring within Muslim households. The combined approach also significantly differed across groups with the highest percentage occurring in Protestant households. Several of the other parent behaviors also differed across groups. This included, higher prevalence on all three composite measures (i.e., any physical, any psychological, and any non-violent discipline) in Protestant household, whereas the prevalence was lowest in Muslim households for these same three composite measures. In addition, respondents in the Muslim households had the lowest percentage which endorsed the belief that physical punishment was necessary in child rearing. Unadjusted and adjusted logistic regression models for evaluating the association of the three parenting behaviors (identified in the hypotheses) and household religion are presented in . In the unadjusted models being hit with an object and a combined approach were statistically associated with household religion, whereas being spanked was not. Children in Muslim households had lower odds of exposure to being hit with an object and a combined approach in comparison with both Catholic and Protestant households. Additionally, children in Catholic households had lower odds of exposure to being exposed to a combined approach compared with Protestant households. In the adjusted model, being spanked was still not associated with household religion ( ). For being hit with an object, the interaction between household religion and child age was significant. The relationship between household religion and this behavior was not constant over one-year increments in child age. These relationships are specified in and illustrated in . For example, the odds of exposure to being hit with an object for a two-year-old in a Muslim household was decreased by a factor of 0.65 [95 % CI: 0.46, 0.92] compared to a similar child (with respect to other variables in the model) in a Protestant household. For the combined approach, children in Muslim and Catholic households had lower odds of exposure compared to Protestant households, although no difference was found comparing Muslim and Catholic households ( ). Among the covariates, the respondent's belief in the role of physical punishment in child rearing was associated with increased odds of each of the three parenting behaviors. Girls had lower odds of exposure for being hit with an object. Head-of-household education levels and household wealth quintiles demonstrated a mixed relationships with the different parenting behaviors ( ). For example, there was some increase in odds of spanking with increasing wealth, whereas the wealth index was dropped from the multivariate model for hitting with an object given its lack of a statistically significant relationship. Examination of the relationship between household religion and other parenting behaviors is summarized in . Full model details are available in Supplement Tables A1–4. In adjusted models, children in Catholic compared with Protestant households had lower odds of exposure to 7 of 12 parenting behavior types (hit on limb, beat up, yelled at, any psychological aggression, given explanation about behavior, gave something else and any non-violent). A similar pattern was seen for exposure of children in Muslim versus Protestant household except for the beat up variable which was not significantly different, and taking away privileges which was significantly lower for children in Muslim households. Children in Muslim households had different odds of exposure for three parenting behaviors compared to Catholic households (higher odds of being beat up, and lower odds of being given something else to distract and losing privileges). Among these 12 parenting behaviors, a significant age-by-religion interaction was only found for name calling, in which the pattern included lower odds of exposure to this practice at younger ages for children in Catholic compared to Protestant households and lower odds of exposure for older children in Muslim households compared with Catholic households. Consistent with one of the study hypotheses, children in Protestant households had higher odds of exposure to a combined parenting approach than Catholic and Muslim households. Also consistent with one of the hypotheses, there was a child age-by-household religion interaction effect that included younger children in Muslim households having lower odds of exposure to being hit with an object than younger children in Protestant households. However, inconsistent with study hypotheses, there was no relationship found between spanking and household religion, in addition, there were few differences between Catholic households and the other groups. It may be that children in Protestant households had higher odds of exposure to a combined disciplinary approach driven in part by prevalence differences in psychological aggression and non-violent practices, and not solely by physical punishment differences. Unfortunately, little is known about the relation between religious affiliation and parenting behavior beyond physical punishment. One exception is a small US study cited earlier that found that Conservative Protestants differed from others groups only on spanking/slapping and not on eight other non-physical disciplinary behaviors ( ). In contrast, a national US study found that a lower frequency of yelling at one's children was related to Conservative Protestant affiliation ( ). This result, in combination with another US study which found higher prevalence of hugging and praising of children among Conservative Protestants, led its authors to conclude that Conservative Protestants should not be categorized as employing a typical authoritarian parenting style, but rather a “neotraditional” style ( ; ). However, children in Protestant households in this Cameroonian study had greater odds of being yelled at. Yet, Cameroonian Protestants also had greater use of non-violent discipline. The more prevalent use of a combined approach by the Cameroonian Protestant group may reflect an effort to employ multiple strategies to attain child obedience, a value that has been proposed to be particularly salient for Protestant households, at least for conservative subtypes in the US ( ). The 11-item disciplinary module in MICS surveys, which is also embedded in many Demographic and Health Surveys (DHS), should allow advancement in examining the use of a combined parenting approach by religion in other LMICs. The finding of variability in the relationship between household religion and different individual parenting behaviors in this sample is important as it implies that studies that only consider spanking or only rely on a single composite measure of physical punishment, (e.g., “any physical punishment”) may miss significant relationships with other disciplinary behaviors. Within this study, neither of these variables was associated with household religion in adjusted models (i.e., spanking and “any physical punishment”), whereas significant associations were found with individual physical punishment and other parenting behaviors (e.g., gave something else to do). It should also not be assumed that spanking, a particular focus in HICs, should necessarily be a core physical punishment used throughout LMICs. Whereas it was the most common physical punishment in this Cameroonian study, other physical punishments (e.g., hitting with an object) have been found to be more prevalent than spanking in other countries (e.g., Colombia) ( ). Nevertheless, it has been noted that spanking appears to have developed independently in different cultural groups ( ) and was found to be one of the more common physical punishments in a study of 33 LMICs ( ). If the link between certain punishments and Protestant households is thought to be a function of biblical literalism, one might anticipate that the association would be more specific to hitting with an object given the specification of hitting with a rod in a biblical passage, whereas there is no specific mention of spanking in the Bible. There was partial support for this hypothesis in this Cameroonian study, although it was only significant at younger child ages. Potentially, a more robust relationship would be found if the study had been able to identify a conservative subgroup from the larger Protestant group, or even more so if there had been an index of biblical literalism. One study in a US population found a measure of “theological conservatism” (composed of the extent of agreement with the statements that “ ,” and “ ) demonstrated an independent relationship with spanking/slapping and reduced the strength of association between Conservative Protestant affiliation and this parenting behavior ( ). Differences in being hit with an object between Muslim and Protestant households for younger children may in part be a function of the idea outlined in the introduction about younger children in Muslim households potentially being protected from punishment given related teachings and beliefs. However, this same age effect was seen for Catholic compared to Protestant households. Although there was limited evidence for an age-by-religion effect in this study, it is recommended that future studies examine this and other interaction effects. This could build on substantial evidence of some age effects on the prevalence of different parenting behaviors. For example, in UNICEF's study of 33 LMICs, most countries had a pattern of violent discipline being most prevalent for 5–9 year olds compared to 2–4 and 10–14 year olds, however, several countries demonstrated other age patterns ( ). In a more refined analysis by age, spanking of younger children in Cameroon was more prevalent than in Nigeria up to age five years, but then spanking prevalence drops substantially with age in the Cameroon whereas there is little drop with age in Nigeria ( ). The parenting-belief question was the only variable demonstrating a significant relationship in adjusted models for all parenting behaviors. Whereas it was expected that there would be an association with physical punishment, and this is consistent with findings in other studies ( ; ), it was not anticipated that it would also be associated with non-physical punishment behaviors. This may be in part due to a confounding effect. Additionally, this pattern may also indicate that a single binary belief variable is too crude to show a unique association with the specific use of physical punishment. Nevertheless, it signals that further investigation into measures of attitudes and beliefs related to different punishment types in LMICs is warranted. These could be examined in their relationship with religious affiliation as well as stand-alone variables. The Attitudes Towards Spanking questionnaire ( ; ) is an example of one measure that has aimed to examine these dimensions further for one particular physical punishment type in HICs. How such measures relate to religious affiliation would be informative. Another concept to prioritize in future studies is perceived normativeness, which has been identified as a factor that may explain some of the variability in the relationship between physical punishment and adverse child outcomes across groups ( ). Perceived normativeness by parents and children was identified as moderating some of the relationship (i.e., between physical punishment and adverse child outcomes) within a multicountry study ( ). Examining the influence of perceived normativeness within one's own religious group may be an important next step to further advance understanding of the influence of this contextual variable. Finally, there is a need to consider other contextual factors. This study only considered a single country. Potentially there are additional country level or cultural subgroup within countries that need to be considered within which religious affiliation may be embedded. For example, are there aspects of religious affiliation (whether they be Christian or Muslim) which are unique to Cameroon, or are the relationships found in this Cameroonian study generalizable to other countries with a similar religious mix? A preliminary examination of the relation between religious affiliation and physical punishment in the country of Chad found that children in Muslim households had lower odds of exposure to both spanking and being hit with an object compared to those in Protestant households in adjusted models ( ). Unlike Cameroon, the Muslim population is in the majority in Chad ( ). There are several limitations to this study that should be considered. First, there is a lack of psychometric assessment of the discipline module questions. Although they are derived from an instrument that has established psychometric properties, the Parent-Child Conflict Tactics Scale ( ), there are substantial differences between these instruments (e.g., differences in response options) ( ). Although the items have face validity and have been used in multiple international settings and vetted by an international committee ( ), additional studies are needed to assess the items' performance in different setting to determine the pattern of reliability, as well as their validity in indexing actual parenting behaviors. Second, this study lacked indicators to index additional aspects of religiosity that may influence parenting behaviors. Other studies have included indicators of religiosity beyond group affiliation. For example, a study in Turkey found a measure of religious saliency (importance of religion in the respondent's life) was associated with valuing obedience to authority and good manners in children ( ). Additionally, some studies in HICs have examined variables such as biblical literalism and theological conservatism that may be more proximal in influencing parenting than broader religious grouping ( ; ; ). This current study also did not include variables for subcategories of Muslim groups (e.g., Sunni, Shia). In one study of Muslim identity which included the Cameroon, 27 % and 3 % in the Cameroonian Muslim sample identified as Sunni and Shia, respectively, while 40 % identified “just a Muslim” ( ). Several implications may follow from this study's findings. From a research perspective, determining whether the patterns identified in this study are replicated in other countries, particular in those which have sizeable mixes of these religious groups, would allow determination of generalizability across countries, or whether these patterns are more specific to religion affiliation within a given country. This could be accomplished using existing datasets from MICS and DHS. However, there is also the need for primary data studies, using both qualitative and quantitative methodologies. Qualitative studies could allow the exploration of parent and child perspectives. An example of this approach is a recent study using focus groups of parents and children in Zimbabwe to explore their perspectives on differences between discipline and abuse ( ). Primary quantitative studies could allow examination of measurement of the influence of religiosity, as well as attitudes about the use of physical punishment as has been done in HICs ( ; ). Obtaining perspectives of religious leaders could also generate further insights. A recent mixed method study of religious leaders in Guatemala, Uganda and Senegal helped to identify variation in perspectives on physical punishment as well as child rights ( ). There are also implications that may inform intervention ideas. In US settings, one approach aimed at reducing reliance on physical punishment has been to attempt to work within religious establishments. For example, one intervention found providing research evidence on physical punishment in combination with “progressive biblical interpretations” resulted in significant changes in attitudes about spanking, particularly among Conservative Protestants ( ; ). If such presentations were delivered by religious leaders, they might be more influential than more general government-directed health education initiatives, particularly in settings with extensive and diverse religious affiliations such as in Cameroon. This might be considered within each of the major religious groups as the use of “any physical punishment” was reported for the majority for each group considered in Cameroon. Overall, it appears that there is substantial reliance on use of physical punishment in Cameroon. There is even more reliance on yelling by each group. Given that both physical punishment and reliance on verbally abusive behaviors are associated with poorer child outcomes ( ; ; ; ), these might be prioritized by intervention efforts. It is also noted that there is extensive use of non-violent discipline in Cameroon, with particularly prevalent use of “explained wrong behavior” by all three groups. Building on the current use of non-violent practices (which is relatively prevalent and presumably acceptable by many), may be an important point of intervention, rather than predominant reliance on prohibition-only messages. Exploring the extent of use of other non-violent discipline behaviors and their possible promotion is further warranted.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Not Used\n",
      "* **AIモデルの判断:** Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 119 / DOI: 10.1016/j.jsg.2025.105445  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Fault Scarp Dating Tool - a MATLAB code for fault scarp dating using in-situ chlorine-36 supplemented with datasets of Yavansu and Kalafat faults\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Multi-millennia slip rate relationships between closely spaced across-strike faults: Temporal earthquake clustering of the Skinos and Pisia Faults, Greece, from in situ 36Cl cosmogenic exposure dating\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "Understanding the temporal behaviour of active faults is crucial in deciphering the mechanisms that control continental deformation. It is now recognised that slip rates on active faults fluctuate through time, with periods of rapid slip lasting multiple millennia alternating with periods with less rapid slip, or even no slip. The periods of rapid slip are associated with surface displacements that are too large to be produced by a single earthquake slip (e.g. ; ), suggesting the existence of temporal clusters of surface faulting earthquakes (>Ms 6.0) ( , ; ; ; ; , ; ). Conversely, relatively low slip rate periods indicate temporal earthquake anticlustering, where a lack of large magnitude surface-faulting earthquakes in given time periods produces a slip rate that falls below the background long-term slip rate (e.g. ; ; ; ; ). It has been shown that the periods of rapid slip associated with the temporal clustering of surface faulting earthquakes can be out-of-phase on faults located across strike from one another (across-strike faults). Such out-of-phase behaviour has been documented on faults spaced 100s of kilometres apart across strike, such as the Los Angeles-region faults and the eastern California shear zone in USA ( ; ), Garlock, San Andreas and Eastern California Shear Zone fault system in USA ( ), and North Anatolian, Kunlun and Denali Faults ( ). The behaviour has also been identified on faults spaced 10s of kilometres across strike, such as the Milesi, Malakasa and Fili Faults in Greece ( ), the Mt. Vettore and Leonessa Faults ( ) and the Maiella, Scanno and Pescasseroli Faults in Italy ( ), and the Wairau, Awatere, Clarence and Hope Faults in New Zealand ( ). Microstructural evolution, including annealing and strain-hardening, and interaction through stress transfer have been invoked to explain such behaviour. However, little is known about whether active across-strike faults spaced only a few kilometres apart exhibit similar out-of-phase slip relationships, and what mechanism(s) might be operating to produce these slip rate fluctuations. This unknown is important because many examples exist where faults spaced only a few kilometres apart have ruptured simultaneously or closely-spaced in time (e.g. the 1915 Pleasant Valley, 1959 Hebgen Lake, 1954 Dixie Valley and Fairview Peak, and 1981 Gulf of Corinth earthquakes; ; ; ; ; ). For examples spaced 100s of kilometres apart across strike, it has been suggested that out-of-phase slip may be explained by cycles of strain hardening and annealing within shear zones in the middle to lower crust beneath the brittle active faults ( ; ). An example of such out-of-phase slip at this scale has been identified between the Los Angeles-region faults and the eastern California shear zone (USA), where faults are too far apart to be interacting through static stress transfer ( ; ; ). The activation of a temporal earthquake cluster is suggested to occur when annealing of a viscous shear zone, through the introduction of new strain-free grains, weakens the shear zone to an extent that causes deformation to migrate to that location. Strain hardening during slip then develops and increases resistance to deformation through microstructural evolution on the shear zone. Strain hardening continues until deformation migrates to a shear-zone across strike that has become annealed, initiating a temporal cluster of surface faulting earthquakes in that new location ( ; ). Thus, the across-strike shear-zones have differing microstructural histories. For examples spaced 10s of kilometres apart across strike, it has been suggested that out-of-phase slip may be explained by fault interactions where slip induces differential stress changes on neighbouring across-strike fault/shear-zones which result in the switching between periods of temporal clustering and anticlustering ( ; ). Examples of this have been reported for active normal faults in the central Apennines, Italy ( ; ). It is suggested that fluctuations in differential stress on shear-zones during interaction induce changes in viscous strain rate, quantified by flow laws linking stress and strain-rate (e.g. ). These changes in viscous strain-rate at depth on shear-zones then produce changes in the slip rates on overlying brittle faults. and showed that the magnitudes of differential stress changes calculated for periods of increased and decreased slip rate were sufficient to explain observed fluctuations in slip rates constrained with Cl cosmogenic dating on fault planes. Thus, the across-strike shear-zones have differing stress transfer histories. However, for examples of faults spaced only a few kilometres apart, it is likely that they share the same underlying shear-zone, because envisaged shear-zone thicknesses of a few kilometres at most are similar to across strike fault spacing ( ; ; ). This may imply that it is unlikely that individual faults have differing annealing, strain-hardening or differential stress histories for viscous deformation at depth. Thus, the question arises as to whether such closely-spaced faults show out-of-phase behaviour, prompting this study. In this study, we investigate slip relationships between two active normal faults spaced only 1–2 km across strike in central Greece, the Skinos and Pisia Faults, which ruptured within hours of each other during the 1981 eastern Gulf of Corinth earthquake sequence (Ms 6.7 and Ms 6.4) ( ). To achieve this, we (1) reconstruct the slip rate histories of both faults over the last ∼20 kyr using Cl cosmogenic exposure dating, (2) examine their temporal slip relationships, revealing alternating periods of high and low slip rates as well as both out-of-phase and simultaneous slip relationships, and (3) explore the mechanisms driving these interactions and their implications for seismic hazard. By understanding the slip behaviour of closely spaced faults, this study provides critical insights into single and multi-fault earthquake scenarios through identifying periods in which one fault ruptures or periods in which both across-strike faults rupture simultaneously or closely spaced in time. These findings are essential for improving seismic hazard assessments in regions with complex fault networks, particularly where closely spaced faults may interact and potentially trigger multiple earthquakes. Extension in Greece occurs in crust thickened by Alpine thrusting alongside slab-roll back of the north-dipping Hellenic subduction zone between the African and Eurasian Plates ( ; ) and dextral motion of the North Anatolian strike-slip Fault ( ; ; ; ; , ; ). This tectonic setting is associated with extension throughout Greece for the past ∼5 million years, predominantly in a north-south orientation, resulting in the formation of normal faults ( ; ; ; ; ; ; ). Extension in the Corinth rift is accommodated by normal faults predominantly striking ∼ E-W and producing extension in a N-S direction (e.g. ; ; ; ; ; ; ; ). GNSS-derived extension rates in the eastern region of the gulf, the region we study, indicate a rate of 5–6 mm/yr, contrasting with a higher rate of 10–11 mm/yr in the western region (see , their Fig. 7 for the regional GNSS vectors) ( a). The footwalls of the normal faults consist predominantly of Mesozoic limestones, with Neogene sediments found in some areas, along with rare outcrops of ultrabasic ophiolitic rocks and deep-sea cherts in places ( ; , ; ; ; ). The hanging walls are occupied by sediments formed from the erosion of footwall rocks, consisting of Neogene-Holocene fluvio-terrestrial sediments, including alluvium, colluvium and marine deposits at coastal sites ( ; ; ; ; ; ; ; ; ; ). Seismicity in central Greece and the Peloponnese due to normal faulting is widespread, with examples known from as far back as at least 464 BC, documented by the ancient Greek philosopher Plutarch who reported a devastating earthquake in the Sparta region ( ). More recent historical records have shown frequent, moderate-to-large destructive earthquakes (<∼Ms 6.7; ). The most recent and well documented destructive earthquakes in the eastern Gulf of Corinth region were in February–March 1981 ( b). During the 1981 eastern Gulf of Corinth earthquake sequence, slip on the Pisia, Skinos and Kaparelli normal faults was associated with surface rupturing during three earthquakes on the 24th and 25th February, and 4th March with Ms = 6.7, 6.4 and 6.4, respectively, causing 22 fatalities ( b) ( ; ; ). Following detailed analyses of the surface ruptures and seismicity produced by the 1981 earthquakes, the 24th and February 25, 1981 earthquakes were attributed to the Pisia and Skinos Faults ( ; ; ; ). The approximately 25 km long Pisia Fault is arranged in a left-stepping, across-strike and geometry relative to the approximately 30 km long Skinos Fault, which is likely connected along strike to the Psatha Fault ( ). The Pisia, Skinos and Psatha Faults are considered to be part of the ∼40 km-long South Alkyonides Fault System ( ; , , ; ; ) ( c). In the zone of overlap, the Pisia and Skinos Faults are separated by less than 1–2 km across-strike, with both faults displacing a thrust sheet containing ophiolite by ∼450 m each ( d and e). At depth, the geometry and arrangement of the faults are unknown, and it is unclear if the faults link at depth ( ; ). Along-strike to the east from this fault system, the 4th March earthquake occurred on the south-dipping Kaparelli Fault ( b). For the Pisia and Skinos Faults, determining which ruptures were associated with each of the earthquakes is challenging, because both earthquakes occurred within hours of each other overnight, at 20:53 and 02:35 local time ( ). The ruptures were mapped in the days and weeks after the earthquakes ( ; ; ). Ruptures on the Pisia and Skinos Faults were partially re-mapped in 1994 and 1995 ( ), and then mapped in more detail in 2022, revealing semi-continuous surface ruptures that extended ∼8–10 km on each fault within the zone of overlap ( fi) ( ). The ruptures produced a single maximum asymmetric profile on the Pisia Fault with a maximum coseismic throw of 223 cm, and a double maxima profile on the Skinos Fault with a maximum throw of 109 cm and 130 cm ( ). Comparing the two sets of ruptures across-strike revealed that the faults were spatially anticorrelated which implies interaction between the faults, because coseismic slip deficits on one fault are compensated by slip maxima on the other fault, and vice versa ( fii,iii) ( ). Summing the two throw profiles from the 1981 ruptures across strike in the zone of overlap revealed a single maximum symmetric bell-like profile (combined discretised maximum displacement of ∼2 m) implying that the two faults worked together during the 1981 earthquake sequence ( fiv) ( ). However, examination of earlier ruptures preserved as a 2nd lichen stripe higher on the exhumed fault planes show that the along strike displacement profiles differed in previous earthquakes on the two faults implying non-characteristic earthquakes ( ; see also ). Although no other along-strike profiles for previous ruptures exist for the Skinos and Pisia Faults, paleoseismological analysis of the two faults has revealed multiple earthquakes on both faults during the Holocene ( ; ). Cosmogenic Cl exposure analysis of a carbonate fault scarp on the Pisia Fault suggests the occurrence of six to eight moderate to large paleoearthquakes during the Holocene with more slip back to ∼30 ka ( ). Modelling by revealed an average slip rate of 0.5–0.6 mm/yr over the last ∼7.3 kyrs, and a period of higher slip rate of up to 0.8–2.3 mm/yr between ∼7 and 10 ka. It is worth noting that the sampled scarp of the Pisia Fault in began to form earlier than the widely accepted view for the timing of the demise of the high erosion rates that characterised the Last Glacial Maximum (LGM) at 15 ± 3 ka ( , ; ; ). Since scarp preservation could only begin once erosion rates no longer exceeded fault slip rates, the scarp age of ∼30 ka being older than the age of the end of the high erosion rates suggests that regional variations influenced the timing of the end of the high erosion rates. For example, suggested that differences in latitude and elevation may have caused the colder, high erosion rate conditions of the LGM to cease earlier at some fault scarp sites, allowing scarp formation and preservation to commence sooner. The slip rate variations determined by on the Pisia Fault have been verified by mapping elevations of radiocarbon-dated Holocene coastal notches which were deformed along the strike of the fault associated with earthquake clusters and quiescence implied by Cl on the Pisia Fault (Robertson et al., in review). For the Skinos Fault, palaeoseismic trenching and associated radiocarbon ages on ruptured sediments of the Vamvakies fan, also known as Bambakies fan, situated on the Skinos Fault (e.g. ) suggest up to six previous paleoearthquakes which were comparable to the displacements produced during the 1981 earthquakes ( ). An average throw rate was calculated to be 0.7–2.5 mm/yr on the Skinos Fault over the last ∼1.5 kyrs with a suggested earthquake recurrence of 330 years ( ). However, this paleoseismic record for the Skinos Fault is less complete or absent prior to 6–8 ka, or possibly as far back as 12.4 ka, due to the Vamvakies fan only forming when rising Holocene sea level achieved its current elevation, thus no earthquakes prior to this are likely to be recorded in its stratigraphy ( ; ; ; ). This highlights the need for a record of slip on the Skinos Fault that can be compared with that of the Pisia Fault whose record stretches back possibly as far as 30 ka ( ), prompting this study. Measuring Cl concentrations can help us understand the slip history of a fault because, after the demise of high erosion rates associated with the LGM, fault scarps began to accumulate and preserve Cl through interactions between calcium atoms and cosmic radiation. As a result, higher concentrations of Cl are found at the top of exposed fault planes within fault scarps because the higher parts have been exposed for a longer duration ( , ; ) (illustrated in a and b). Cl begins to form in carbonate at depths of up to 10 m or more beneath the surface due to exposure interactions between the carbonates at depth and incoming muons ( c) ( ; ; ; ; ; ). However, overall the rate of production of Cl increases towards the surface where the spallation of calcium atoms due to interactions with incoming neutrons becomes the dominant Cl production process ( ; ). In general, below a depth of 2 m, the rate of production is below ∼5 at(atoms)/g/yr due to shielding by, for example, surrounding bedrock and colluvium ( ; ). Again, in general, at the surface, the Cl production rate in limestone is ∼48 at/g/yr (e.g. ), but varies with elevation and latitude ( ; ). The values stated above are approximate and more detailed production rate scenarios need to be calculated for individual sites using knowledge of production scaling with latitude and elevation produced by global and temporal variations in the Earth's magnetic field and local atmospheric thickness above mountainous topography ( ; ; ). One way to deal with uncertainties in the above in the absence of measured cosmogenic production rates is to iterate the production values during modelling, for example, as implemented in the code by . The modelling attempts to replicate the measured Cl derived from Accelerator Mass Spectrometry and attempts to constrain the exhumation history of the rock samples as they were exhumed up through the sub-surface Cl production zone. Additional considerations include the site shielding due to topographic slopes, rock densities, the elemental composition of the rock targets and the colluvium through which some cosmic radiation passes, variations in the geomagnetic field over time, assumptions about production rates with depth (as shown in c), and other parameters, along with the inherent uncertainty associated with all these factors. The model results must then be interpreted by considering quantitative results such as the least squares solution for the comparison between the measured and modelled Cl concentrations, the ensemble of top least squares solutions, and the posterior distribution of highest likelihood solutions. Aspects of such modelling are explained in the next section. We note that while Cl dating is valuable for identifying changes in slip rates on faults over multiple millennia (e.g. ; ; ; ; ), there are a range of other methods available for analysing fault activity, such as paleoseismic trenching (e.g. ; ) and U-Pb dating of fault zone cements (e.g. ). However, the advantage of Cl cosmogenic exposure dating of limestone fault scarps is that it provides temporal coverage lasting 10s of millennia with the resolution of timing of slip of ±0.5 kyrs ( ). We sampled the carbonate Skinos fault plane for Cl cosmogenic dating to provide a dataset that can be compared with the dataset for the Pisia Fault, sampled by ( ). The Skinos Fault site was selected because, like the Pisia site, it lies within the zone of overlap between the two faults and is situated less than 2 km from the Pisia Fault site ( c). The exact site was ultimately chosen based on exhibiting the distinct geomorphic characteristics of a tectonically exhumed fault scarp, as required for a Cl sample site ( ). These observations include confirming that the scarp offsets a slope that has not been affected by erosion or sedimentation. In detail, this means that we do not sample unless it can be confirmed that there are no incised gullies in the immediate footwall or the hanging wall, the lower slope to fault plane contact is horizontal and not prone to along-strike mass wasting, and no landsliding is present in the immediate vicinity of the sample sites. These observations confirm the scarp was exposed through surface faulting, rather than erosion/sedimentation or mass wasting. As per other previous examples (e.g. ; ), we collected multiple samples up the free face in the orientation of the fault slip vector, with sample spacing of 23–40 cm and greater towards the top of the fault, with 50 and 60 cm spacing for the two highest samples ( aii). We also excavated an ∼80 cm deep trench and sampled the carbonate fault plane in the sub-surface. Site characteristics used to model the Cl data are shown in and tabulated in Electronic . The magnetic field scaling parameters for the sample site, determined using , and based on latitude and altitude, are provided in Electronic . Sample preparation and measurement procedures on Cologne AMS, and including the relevant major and trace element chemistries of the samples are given in Electronic alongside elemental data from ICP-OES. The composition of the colluvium used in our study is derived from the average chemical composition of colluvium samples collected in central Italy that have similar provenance ( ). The Italian Apennines have comparable climate conditions and parental Mesozoic carbonate rocks and volcanic inputs with those found in central Greece. Following previous papers ( ; ; ; , ; ), we utilised the code to model the slip history of the Skinos Fault and re-model the slip history of the Pisia Fault. This code is designed to recover slip rate variations on faults from continuous or discontinuous sampling. The code is provided with the site characteristics and knowledge and uncertainties associated with Cl production, and proposes fault slip histories, calculates the implied Cl profiles for the proposed slip histories, and compares these with the measured Cl concentrations using a Bayesian Markov Chain Monte Carlo (MCMC) approach ( ). The code operates iteratively, conducting simulations of slip histories hundreds of thousands to millions of times. The reader should bear in mind that Cl production factors are not precisely known, so we chose to use the code in order to iterate these values rather than selecting single values (compare with ; ; ). The code iterates colluvial density, production rates, and attenuation lengths associated with spallation and muonogenic Cl production and displays the results as histograms from the posterior distributions ( ). The code accounts for uncertainties in input parameters and model set-up, and tests for convergence onto a stable result using two parallel Markov chains (see for more details). Unlike methods which predefine the number of slip events and displacement sizes (e.g. ; ; ), the code iteratively searches for these parameters. The iterative approach facilitates identification of periods of high or low slip rates, potentially associated with temporal earthquake clustering, rather than individual earthquakes. Additionally, proposed slip histories are drawn from a Brownian-passage-time model (BPT) of earthquake recurrence over a time period set by the user. We have chosen 120 ka for the start of the model runs, with the code choosing the time when erosion rates decreased associated with the demise of the last glaciation through iteration of this value guided by the fit of predicted Cl concentrations to measured Cl concentrations. The BPT model allows the possibility of both constant and fluctuating pre-LGM slip rates to generate possible Cl concentrations from this time, rather than using a single value for pre-exposure or glacial period slip rate (see ; ). The modelling ranks slip-histories using the fit of proposed Cl concentrations to the measured values, using the least squares solution, an ensemble of the top 1000 least squares solutions (or another number of choice), and the posterior distribution of highest likelihood solutions, showing the median and 90 % confidence bands after a 50 % model run burn-in. Full model results for the two sites are presented in . The age of the fault scarp is not required to be pre-defined in the code and we do not force the code to constrain the slip history to be only after the LGM because it has been shown that the age of different fault scarps likely vary based on factors such as elevation and latitude ( ). Once the two parallel Markov chains in the Beck code have converged, as quantified using the Gelman-Rubin test (see Electronic Supplement S2ab and S2bb), we then interpret the initiation and termination of each cluster or anticluster from model output (compare ). These interpretations are based on the consideration of three indicators from the modelled Cl slip history (e.g. see ). Increases (cluster initiation) and decreases (cluster termination) in slip rate are identified using the following: (1) steepening and shallowing staircase patterns in the least squares solution, which offers the best fit to the data; (2) concave-up and concave-down inflections in the 90 % confidence lines derived from the full posterior distribution; (3) the density of models derived from, for example, the top 10000, 2000 and/or top 100 least squares solutions, to be chosen by the user. The interpretation of the duration of a period of clustering or anticlustering and the amount of slip in a cluster is therefore somewhat subjective due to the possible bias of the user, but we argue that clear signals of fluctuating slip rate can be identified within the results, accepting a likely ∼±0.5 kyrs of uncertainty associated with the timing of our interpretations. Also, we only interpret periods of rapid slip as earthquake clusters where the slip accumulated is too large to be accommodated by single earthquakes ( ), defined by the maximum expected coseismic slip for a given fault as per length-displacement scaling ( ). Anticlusters are periods of relatively low to no slip between clusters, and these periods have slip rates that are lower than that defined by the total slip since slip was preserved, as defined by the modelling of the Cl data. It is challenging to calculate the exact slip rates implied by our modelling because of the subjective choice of how to weight the (1) least squares slip history, (2) the ensemble of least squares slip histories or (3) the median slip history from the posterior distribution. To address this, we used a combination of all three statistical solutions to aid our interpretations of the slip rates through time. To aid visualization of the results, we show representative gradients of different slip-rates as part of the key in aii, bii, and 5ai, bi. The slip rate uncertainty we assign of ±0.5 mm/yr was constrained by iterative adjustments of pixel dimensions in the least squares models during visualization. Since pixel size is a subjective choice, the uncertainties themselves are subjective. The Cl sampling site on the Skinos Fault was chosen from the structural mapping of the fault zone and 1981 ruptures conducted by ; their site 324. The key field measured parameters used for modelling include a fault dip of 59°, a post-glacial slip of 14.6 m, a trench depth of 0.73 m, and lower and upper slope dips of 35°. The chosen site meets the Cl sample site selection criteria of , confirming that the scarp was exhumed by surface faulting rather than sedimentation, erosion, and post-depositional disturbance. The chosen site exhibits the following attributes: (1) The hanging wall cut-off is horizontal and un-incised, lacking sediment cones, which rules out burial by sediment supply from the footwall; (2) the fault plane forms a sub-horizontal contact with the lower slope, so this excludes localised mass movement both perpendicular to the fault and along strike; (3) the lower slope is free of incised gullies, so this excludes the possibility of erosion or sedimentation contributing to exhumation or burial through time; and (4) similarly, the upper slope is preserved and planar with no incised gullies so as to exclude movement of material from the footwall to the hanging wall after scarp formation. These characteristics confirm that fault plane exposure resulted from surface faulting after slope stabilisation, with slope stabilisation occurring due to the reduction of free-thaw action associated with the climatic conditions associated with the LGM ( ; ; ; ; ) ( aiv). The concentrations of Cl increase up the dip and slip vector azimuth of the fault planes and this is due to more Cl being formed through longer exposure up the fault scarp as the fault slips ( ai,bi). Disturbances in this trend can be produced by variations in the Ca concentrations, as well as changes in slip rates. However, we measure the Ca concentration for each sample and the code is provided with this information. The concentration profiles differ for the Pisia and Skinos sites with the highest measured values at the Pisia site reaching 4.75 × 10 at/g, whereas the topmost samples from the Skinos site exhibit lower concentrations of 2.1 × 10 at/g. This is due to a combination of the different extents of the preserved and hence sampled fault planes, but also due to differing exposure histories and we show the results of modelling in aii and 4bii and our temporal cluster interpretations in ai and 5bi. Note that we did encounter one anomalously low Cl measurement at +118 m on the Skinos Fault (see ). Examination of this sample showed that the anomalously low Cl in this sample is coincident with local fractures. It is likely that the local fractures contain low Cl because fractures are pathways for the influx of meteoric water which might precipitate a young calcite vadose cement (mineral precipitate from infiltrating water in the unsaturated zone above the water table). This would lead to lower overall Cl concentration in the overall bulk sample. As this sample was probably altered in its Cl concentration by non-tectonic causes, we excluded it from the modelling. The results of the MCMC modelling, shown in , reveal fluctuations in slip rate through time and shows the parameters that were iterated during the modelling. The results show that surface slip has progressively produced growth of the fault scarps. Slip rates on both faults were variable over multiple millennia and some constraints have been resolved on the timing of when slip started to be preserved, that is, the age of the preserved scarp ( ). Surface slip has been preserved since at least ∼20 ka (see Electronic Supplement S2af and S2bf) but the certainty of older slip history is limited by the availability of preserved fault plane exposures. The upper part of the fault plane was eroded, so no sampling was possible of its earliest exposed surfaces, as indicated by the measured scarp profiles ( aiii, biii). The Skinos Fault site has a preserved free-face of only 4.2 m, compared to 8.45 m at the Pisia site, which may contribute to the less clear early slip history on the Skinos Fault. However, the older part of the slip history is also constrained to a degree by the overall shape of the Cl profile and hence the exhumation history of the younger samples. Thus, in we mainly concentrated our interpretations on the younger parts of the slip histories which are likely to be better constrained. In addition, while the Pisia samples were extracted with closely-spaced samples, and the Skinos samples were less closely spaced, this and previous studies (see Electronic and , their supplementary material S3c) show that degrading the number of samples up to a point does not affect the overall slip history, confirming the robustness and reliability of the results. The slip rate histories inferred from Cl modelling suggest that the two faults have experienced different alternating periods of rapid slip and little or no slip over the last ∼20 ka ( ai,bi). Based on the modelled results, we interpret that the Pisia Fault had a low slip rate of ∼0.5–0.75 mm/yr from ∼20 ka to 9.6 ± 0.5 ka, which was followed by a relatively high slip rate of ∼1.25 mm/yr from 9.6 ± 0.5 ka to 5.2 ± 0.5 ka ( bi). At 5.2 ± 0.5 ka, the Pisia Fault slows to a low slip rate of ∼0.25 mm/yr or less, maintaining this relatively low slip rate until 2.0 ± 0.5 ka. Subsequently, from 2.0 ± 0.5 ka to the present day, the slip rate on the Pisia Fault sees an increase to ∼1.25–1.5 mm/yr from its previously lower slip rate. Regarding the Skinos Fault, we observe a very low slip rate of ∼0.25 mm/yr or less from ∼20 ka to 6.4 ± 0.5 ka ( ai). At 6.4 ± 0.5 ka to the present day or possibly around ∼1.0 ± 0.5 ka, the Skinos Fault accelerates to a very high slip rate of ∼2.0–3.0 mm/yr. These values on the Skinos Fault are consistent with throw rate estimates from a paleoseismic trenching site located 8 km along the strike of the Skinos Fault on the Vamvakies fan (0.7–2.5 mm/yr over the last ∼1.5 kyrs; ), which closely align with the throw rate derived in this study (∼1.7–2.6 mm/yr over the same period, converted from a slip rate of ∼2.0–3.0 mm/yr and a fault dip of 59°). A notable observation we make from comparing the slip rate histories of the two faults is that, at times, specifically from ∼9.4 ka to ∼6.4 ka and ∼5.2 ka to ∼2.0 ka, their patterns of alternating periods of slip are out-of-phase with each other; one fault has high slip rates whilst the other fault has low slip rates and vice versa ( ). This out-of-phase relationship seems to dominate and is especially clear in the pattern of slip rates displayed by the median of the posterior distribution of highest likelihood solutions ( aii,bii) at ∼4.0 ka as the Skinos Fault had a maximum slip rate of ∼2 mm/yr whilst the Pisia Fault had a minimum slip rate of near 0 mm/yr. At other times, we observe periods of simultaneous slip of the two faults, for example from 6.4 ± 0.5 ka to 5.2 ± 0.5 ka and 2.0 ± 0.5 ka to the present day or possibly ∼1.0 ± 0.5 ka ( ai,bi); both faults have relatively high slip rates at the same time. During the first instance of simultaneous slip, the Pisia Fault seems to be slowing down whilst the Skinos Fault seems to accelerate. Conversely, during the second instance of simultaneous slip from 2.0 ± 0.5 ka to possibly the present day whereby the Skinos Fault appears to be slowing, whilst the Pisia Fault appears to be accelerating. Overall, these results suggest a long-term alternation between periods of out-of-phase slip and periods of simultaneous slip, and during the latter, their slip rates switch in opposite directions, with the faults exhibiting either acceleration or deceleration. Note the exact values for slip rate in aii and 5bii are challenging to interpret because this figure only shows the results from the posterior distribution which will include results from immediately after reverse jumps in the Markov chains, so the exact values are difficult to extract but generally the pattern shows that the faults are out-of-phase at ∼4.0 ka. The slip rate patterns can also be visualised with regard to best fit between the measured and modelled Cl concentrations using the least squares solutions and ensembles of least squares solutions ( ). Our overall finding is that sampled parts of the Pisia and Skinos Faults show temporal periods of alternating out-of-phase or simultaneous slip on a multi-millennial timescale. This alternating slip behaviour through time exists alongside knowledge that both faults (a) ruptured in the 1981 earthquake sequence with spatially out-of-phase slip ( f; ), (b) share very similar total offsets of Alpine thrust sheets that developed on the millions of years timescale ( e), (c) share similar offsets across post-demise of the LGM faults scarps that developed over the last few tens of millennia ( aiii and 3biii), and (d) share the same regional stress and velocity fields ( ; ; ) ( ). The close proximity of the two faults, which form an arrangement with an along-strike overlap of ∼8–10 km and an across-strike spacing of 1–2 km ( ), together with spatially out-of-phase slip on the timescale of a single earthquake sequence and the temporally out-of-phase slip over multiple millennia, suggest interactions between the two fault structures. Below we discuss the possible reasons for this complex fault behaviour. We have documented, periods of temporally out-of-phase slip on the Skinos and Pisia Faults, and, as mentioned above, this phenomenon has been reported by other authors (e.g. ; ; ; ). However, unlike other examples whereby the across-strike faults are likely to have separate shear zones at depth due to across-strike distances of 100 s km ( a; e.g. ) or 10 s km ( b; e.g. ; ) between the faults, the proximity of the closely spaced (less than 1–2 km of across-strike separation) Skinos and Pisia Faults implies that it is unlikely that they have separate shear zones. Thus, it is unlikely that the out-of-phase slip on the Skinos and Pisia Faults can be explained by different overall conditions for their shear zones, such as annealing versus strain-hardening ( ) or differential stress changes inducing changes in strain rate ( ; ); another mechanism(s) must be at work to produce the out-of-phase slip ( c). We suggest that the out-of-phase slip relationship and periods of simultaneous slip on both faults imply that the faults are working together as one on a millennial timescale, so that the overall slip rate is shared between the faults to accommodate the regional strain rate. Both faults can slip at the same time, perhaps with slip dispersed through an anastomosing network of brittle faults above the brittle-viscous transition which connects the Skinos and Pisia Faults ( cii-vi). In other words, at times it appears that the slip rate on the underlying shear-zone makes its way upwards through the interconnected anastomosing network of brittle faults at the brittle-viscous transition, and eventually separates upwards onto the discrete Skinos and Pisia Faults ( civ,vii). At other times, the faults slip separately, perhaps because slip through an anastomosing network targets the weaker fault, causing the weaker fault to slip without the other fault slipping ( cii,iii,v,vi). Contrasts in strength could be caused by, for example, heterogeneities in the fault gouges in the upper crust or perhaps even fluid involvement. If there is low frictional resistance on both faults due to the gouge having undergone strain-softening during its microstructural evolution, both faults may slip during the same time period. Alternatively, if there is relatively high frictional resistance due to strain-hardening of one fault's gouge, the other fault might slip. Changes in the frictional resistance in gouges may result from microscale processes such as crystallographic orientation changes, grain-size variations, or fluid content changes (e.g. ; ; ; ; ; ), or the influence of rate and state friction ( ). While our study does not include microscale analyses, similar frictional processes have been observed on multiple faults in the Corinth Rift including the Skinos Fault. For example, carbonate fault rocks on these faults have been shown to undergo cyclic grain-size reduction and induration phases, and cyclic production of crystallographic preferred orientations leading to grain-boundary migration and annealing, influencing frictional resistance over time ( ; ). In other fault zones, grain-size reduction and grain attrition has been suggested to reduce stress concentration at grain-to-grain contacts, which can lead to gouge to lock-up and hence strain-hardening ( ). In contrast, introduction of new grains plucked from the damage zone can re-set the grain-size distribution, increasing stresses at grain boundaries, which can lead to strain-softening ( ). Additionally, the introduction of fluids can either lubricate the slip by reducing normal stresses (e.g. ; ) or promote cementation or diffusive mass transfer, potentially locking or unlocking slip ( ). The influence of rate and state friction can also produce pulses of alternating slip rate due to changes in the smoothing factor (see , their Fig. 4e). We suggest that these complex changes in fault zone rheology may influence how slip is distributed within the anastomosing network of faults that includes the Pisia and Skinos Faults. Another scenario may be that, although a single relatively narrow shear zone ∼1–2 km wide is likely to exist at depth beneath the brittle-viscous transition under the Pisia and Skinos Faults, perhaps at times different portions of this shear zone are at different stages of the dislocation creep and annealing cycle. Dislocations piling up at grain boundaries in one portion of the shear zone could promote strain-hardening, whilst elsewhere within the same shear zone annealing could produce new strain-free grains that promote renewed dislocation creep ( ; compare with ); this may activate either the Pisia Fault or Skinos Fault at any one time. At other times the rheology of the shear-zone might be uniform across its extent promoting simultaneous slip on both the Pisia and Skinos faults. Whatever the cause of the alternating slip behaviour on the 10 –10 years timescale, in the case of the Skinos and Pisia Faults, shared growth is implied by the fact that both faults have slipped by similar amounts over several million years evidenced by the similar ∼450 m offsets of features in the pre-rift geology such as the thrust carrying the ophiolite ( b). The faults also share the same post slope stabilisation slip of 14.6 m which has developed over the last few tens of millennia ( aiii, biii and c; Electronic Supplement S2af and S2bf). On the coseismic timescale of a single earthquake sequence, shared fault growth has been evidenced by the spatially out-of-phase throw profiles from the 1981 earthquakes, where slip deficits in the set of ruptures from one fault seem to be filled in by slip maxima in the set of ruptures from the other fault, immediately across-strike, and vice versa ( e) ( ). Thus, over timescales of a few seconds to hours during single earthquake sequences, over timescales of a few tens of millennia since slope stabilisation, and over timescales of a few million years associated with offsets or pre-existing thrusts, neither fault seems to dominate slip. Combining this evidence, we suggest that the two faults may, in general, share slip through being connected at depth by a shared shear zone and an anastomosing network of faults. In other words, the Skinos and Pisia Faults appear to be acting as a single combined fault-zone/shear-zone. These complexities in the temporal and spatial patterns of slip over different timescales are important because other examples of normal faults with similar structural geometries exist worldwide, and they may show similar complexity in behaviour. For example, in the Basin and Range Province of the US, the Pleasant Valley Fault ( ), Hebgen Lake Fault ( ) and Dixie Valley and Fairview Peak Faults ( ; ) all experienced historical earthquake rupture of across-strike faults spaced by distances of just a few kilometres, similar to that for the Skinos and Pisia Faults. We have discussed possible reasons for the complexity of slip over different time periods, but clearly further work is required to constrain the reasons for out-of-phase and simultaneous slip on faults that are so closely spaced across strike that they are likely to share the same underlying shear zone. In particular, such research could help better understand the probability of across-strike faults co-rupturing and help identify the fault configurations that could either increase or decrease the likelihood of subsequent earthquake ruptures occurring on closely spaced across-strike faults. Multi-fault ruptures, as implied by our hypothesised mechanism(s) and identification of simultaneous slip on both faults, may suggest that the faults could rupture individually or both rupture together. The latter would likely produce a higher magnitude of earthquake shaking. In addition, in terms of determining the nature of segment boundaries for the assessment of seismic hazard, it has been suggested that some segment boundaries may either be persistent, meaning they always stop the migration of ruptures, or non-persistent, meaning they sometimes allow ruptures to jump across them ( ; ; ; ; ; ). For example, one estimate suggests that a segment boundary of 3–4 km across will be persistent, with examples with smaller dimensions being non-persistent ( ). Indeed, it has since been proposed that specific size parameters, such as these dimensions of fault segment boundaries, determine whether ruptures can jump from one fault across a segment boundary onto a neighbouring fault ( , ). However, for the Pisia and Skinos Faults, our findings indicate that at times ruptures can jump between the two faults, separated in time by just a few hours, whilst at other times they do not, indicating that the structure can alternate between being persistent or non-persistent. Another critical factor highlighted by our work that should be considered for the assessment of seismic hazard, is that while the regional extension rate is probably constant through time, it is likely to be partitioned differently between individual structures through time (e.g. ). This is evidenced by our finding that slip rates fluctuate over multi-millennial timescales. For example, using inferred average slip rates for the Skinos Fault (0.25 mm/yr, 2.5 mm/yr, and 1.38 mm/yr) and the Pisia Fault (1.25 mm/yr, 0.25 mm/yr, and 1.38 mm/yr) across time slices of 10–6 ka, 6–2 ka, and 2–0 ka, and applying fault dips of 59° and 62°, respectively, we calculated combined heave rates of 0.72 mm/yr (10–6 ka), 1.40 mm/yr (6–2 ka), and 1.35 mm/yr (2–0 ka). These results reveal significant temporal variability in heave rate, and thus extension rates, with the combined rate nearly doubling from 0.72 mm/yr (10–6 ka) to 1.40 mm/yr (6–2 ka), before a slight decrease to 1.35 mm/yr in the most recent time slice. We argue that these heave-rate changes reflect changes in local strain accumulation and shifts in activity between faults. These combined heave rates are significantly lower than the GNSS-derived extension rate of 5–6 mm/yr for the eastern Gulf of Corinth ( ), indicating that the Pisia and Skinos Faults alone cannot account for the full extension measured by GNSS, implying that multiple other active faults, probably offshore, contribute to the total strain budget of this region. However, our findings of slip rates varying through time suggests that faults in seismically active regions likely experience temporal changes in slip rates. These findings underscore the importance of incorporating temporal variability in local extension rates into probabilistic seismic hazard models to better capture the seismic hazard posed by fault systems with complex, time-dependent behaviour. Overall, our work provides new insights into the seismic hazard represented by closely-spaced faults and highlights the need for further studies of earthquake recurrence on such structures over multi-millennial timescales. This study analysed the multi-millennia slip rate histories of the closely spaced Skinos and Pisia Faults in Greece using Cl cosmogenic exposure dating. Our findings reveal a relationship of alternating out-of-phase slip and simultaneous slip, indicating interaction between these faults over millennial timescales. We recommend similar studies of fault behaviour over different timescales should be conducted on other examples of closely spaced across-strike faults. Analysing slip relationships between across-strike faults using fault slip histories over multi-millennia, as demonstrated herein, may prove crucial for understanding earthquake activity, the faulting process and the potential seismic hazard associated with such complex fault systems. Writing – review & editing, Writing – original draft, Methodology, Investigation, Funding acquisition, Formal analysis, Conceptualization. Writing – review & editing, Methodology, Investigation, Formal analysis, Conceptualization. Writing – review & editing, Methodology, Investigation, Formal analysis, Conceptualization. Writing – review & editing, Supervision, Methodology, Investigation, Funding acquisition, Formal analysis, Conceptualization. Writing – review & editing, Supervision, Methodology, Investigation, Funding acquisition, Formal analysis, Conceptualization. Writing – review & editing, Funding acquisition, Conceptualization. Writing – review & editing. Writing – review & editing. Writing – review & editing. Writing – review & editing, Methodology. Writing – review & editing, Investigation. Writing – review & editing, Investigation. Writing – review & editing, Investigation. Writing – review & editing. Writing – review & editing. Writing – review & editing. Writing – review & editing, Investigation.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Not Used\n",
      "* **AIモデルの判断:** Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 123 / DOI: 10.1016/j.compag.2024.108680  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Crop Origins and Phylo Food: A database and a phylogenetic tree to stimulate comparative analyses on the origins of food crops\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Harnessing quantum computing for smart agriculture: Empowering sustainable crop management and yield optimization\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "According to the , the global population has steadily increased from 2.5 billion in the 1950 s to 8 billion in November 2022. Further, it predicts a rise to above 9.7 billion people by 2050 and 10.4 billion by 2080 ( ). In addition, demonstrates that the 20 percent rise in global population is due to the significant advancement of human society, economies, and healthcare, as well as a broad spectrum of factors, including an increase in human lifespan, high urbanization, acceleration of migration, and changes in fertility rates. Additionally, statistics showed that in 2021, 2.3 billion people experienced food insecurity, with up to 828 million facing hunger. Addressing food insecurity and meeting the rising demand for food requires modern agriculture to increase its productivity levels. Upon close analysis of the statistics, it is clear that one solution to achieve this is addressing the issues in modern agriculture. In that regard, finding new approaches to enhance agricultural productivity worldwide is imperative. While the issues of poor agricultural productivity, food insecurity, and population growth persist in contemporary modern society, smart agriculture and dynamic farming practices have been reviewed and incorporated within conventional farming designs to improve agricultural outcomes. The inclusion of intelligent computing designs within the farming plans significantly facilitates the curbing of challenges that contribute to poor agricultural outcomes and food shortages due to low yields. Incorporating quantum computing within farming initiatives aims to leverage quantum phenomena, such as entanglement and superposition, to improve intelligent farming techniques' computing efficiency, speed, and accuracy ( ). Alongside various technological tools applicable in quantum computing, smart agriculture leverages quantum computing for precision agriculture ( ), genomic analysis ( ), control of crop quality, pest, and disease management ( ), and yield prediction ( ). Another potential application of quantum computing that could be particularly important in agriculture is weather prediction ( ). All of these applications are predicted to improve agricultural productivity significantly, hence being considered a solution to the imminent food shortage that might be caused by global population growth. Furthermore, with agricultural revolution 4.0, which refers to the latest trends in agriculture, utilizing precision farming, the Internet of Things (IoT), and big data may improve farming efficiency and address climate change and population growth issues. illustrates an example of Agriculture 4.0. Tailoring the approach to suit each plant's unique response to external factors such as soil and weather can ensure that every plant receives the personalized care it requires. This benefits farmers and end consumers by utilizing the optimal number of resources. However, this approach would necessitate an enormous 2,000,000 sensors per square kilometer, which is beyond the current capacity of creating 5G networks, given only two plants for every square meter. Many sensors are necessary for each plant to receive photosynthetically active radiation (PAR), which ranges between 400 and 700 nm. The PAR Energy beams provide readings in W/m , a good quality light plants need for germination and growth ( ). Artificial intelligence, big data, IoT, robots, and virtual and augmented reality are used in agriculture to improve efficiency and environmental responsibility. According to , the technologies demonstrate that a typical IoT setup is vital for smart agriculture. The illustration in shows that by embracing data-driven strategies, farmers can increase crop yields, save money on inputs, and reduce crop loss. Global Smart Farm Market is expected to witness substantial growth, with estimated market size projections from USD 12.80 billion in 2021 to USD 33.69 billion by 2029 ( ), reflecting the increasing adoption of innovative farming strategies driven by information and communication technologies. As a result, intelligent farming is geared toward improving the sustainability of farming methods ( ). The premise or rationale of integrating emerging technologies, including IoT, big data, AI, and robots in farming, arises from the risk-averse nature of agriculture in the contemporary world. Moreover, the highly-anticipated 6th Generation of the Internet of Things, or 6G-IoT, is set to revolutionize efficient communication protocols and extend the coverage of IoT networks. Additionally, combining AI and advanced computing capabilities allows for more active and sophisticated device-to-network interaction beyond mere data transmission. The 6G-IoT framework is poised to offer revolutionary solutions for smart cities, remote usage, and industrial Internet of Things, impacting the next generation of smart agriculture for the better. To effectively enable Agriculture 5.0, the technologies essential for meeting the 6G-IoT requirements must first be identified, as depicted in . As a result, diverse, innovative agriculture applications have emerged within the last decade, where applications have been recognized in various ways ( ). For instance, to alleviate the risks of inaccurate weather forecasts, smart agriculture is aiding in solving the problem and helping farmers make appropriate decisions ( ). Moreover, posit that farmers risk losing crop yield due to incorrect weather forecasts, which lead to untimely decisions regarding when to plant, apply fertilizer, and work in the field. However, reiterate that farmers receive accurate insights on when to plant, apply fertilizer, and manage pests by adopting intelligent technologies. A further emerging technology in smart agriculture regards the use of quantum computing and high-performance computing to enhance productivity within agriculture ( ). align with the assertion of above by stating that using quantum sensors helps enhance the evaluation of plant growth and production, thereby reducing resource requirements. illustrates elements of a quantum internet. Additionally, using quantum-enabled precision agriculture improves farming efficiency, enhancing fertilizer production and reducing carbon generation from farming processes ( ). also reports that quantum computers can bypass crossbreeding activities in genes and identify the genes responsible for crucial plant and animal production features. As a result, gene editing can be undertaken within a shorter period using quantum computers. In a different research, argues that quantum computing can facilitate the development of next-generation fertilizers, enhance field monitoring, and provide accurate weather forecasts. Despite the diverse findings reported, argue that more research needs to be undertaken in this domain, hence motivating the current study. The core focus of this study paper is to investigate the potential use of quantum computing technologies in smart agriculture. The following objectives will be addressed: i. To evaluate the capabilities of quantum computers and how they could be used in smart agriculture. ii. To examine the existing challenges faced in smart agriculture. iii. To investigate future directions regarding the uptake of quantum computing technology in intelligent farming. The guiding research question for the current research is: What are the potential applications of quantum computing technologies in intelligent farming, and what are the expected future directions? This research is vital in uncovering the potential of quantum technologies in smart farming. The findings will improve farming practices by identifying new ways to integrate these technologies. The research team is excited to contribute to the discussion on the possible applications of quantum computing in smart agriculture. Scholars interested in the research area can also rely on the findings from this study to develop further research related to the research problem. The current paper is divided into six sections, with the first introducing the research's aim and purpose. The second section delves into the contrasts between quantum computing and conventional computing technologies. The third section details the methods and materials utilized in collecting and analyzing data in the research. The fourth section presents the results, while in the fifth section, the results and the underlying research objectives are discussed, and recommendations for practice and future work are provided. The conclusion of the findings obtained in the review paper forms the last section of the research paper. Quantum computing encapsulates a multidisciplinary knowledge and practice that leverages the complex benefits of computer science, mathematics, physics, and quantum mechanics. The know-how enables modern computers to identify and solve complex problems within varied fields ( ). provides a glimpse into the groundbreaking world of quantum computing, where problems are solved efficiently. Quantum-mechanical concepts employ entanglement and superposition to gain a processing advantage over classical computing. Notably, demonstrated the efficacy of quantum–mechanical modeling of the well-designed nanostructure properties. The authors considered nanoparticle composition’s preliminary preparation effectiveness and the parameters used to fertilize plants using pores developed in the gas medium. Soil fertilization is ineffective because most nutrients do not reach the root system. Therefore, the quantum–mechanical model helps create direct feed and increase crop yields. The Noisy Intermediate Scale Quantum (NISQ) era reached a turning point with the proof of quantum supremacy. The subsequent logical development is quantum computers' advantage in solving a real-world problem far more effectively ( ). However, Quantum decoherence and qubit interconnections are two main obstacles to achieving full quantum potential in the NISQ period. Quantum decoherence is losing a phenomenon’s smallest discrete unit or information from a system into the environment, usually modeled as heat. Conversely, qubit interconnection is the integrated circuit of the basic information unit in quantum computing ( ). The study of quantum computing is currently quite hot, developing quickly in all directions. A thorough analysis of the existing literature on smart agriculture using quantum computing is vital in comprehending issues the technology has addressed and its challenges. This article critically analyzes the literature on quantum computing, suggesting taxonomy and gaps for smart agriculture. Quantum AI applies advanced artificial intelligence, machine learning, and intelligent computing to make resourceful decisions and actions within complex agricultural fields. Fundamentally, Quantum AI integrates machine learning algorithms and artificial intelligence to attain higher computation power to solve complex problems associated with massive data sets. Likewise, quantum sensors use quantum mechanics to measure the physical world and gather sensory information from atoms and light photons in their natural environment. Quantum communication involves using advanced quantum physics to transmit data through qubits and protect information channels through quantum cryptography. Implementing quantum algorithms as software is necessary to create practical QC applications. Following the Software Development Life Cycle (SDLC) is crucial for developing dependable software. Conversely, quantum software development is still in its infancy; therefore, it is yet to have a life cycle. An SDLC entails project planning, gathering requirements, designing, coding, testing, deploying, and maintaining ( ). However, one can use quantum programming languages to implement quantum algorithms at a lower level, such as creating quantum circuits with quantum gates. Quantum computing employs advanced aspects of quantum physics, such as interference, entanglement, and operational mechanics, to deal with complicated problems. It differs significantly from conventional computing, such as an AI's backend hardware. In the future, scientists will incorporate quantum technologies in AIs to enhance learning algorithms. According to , the operational principles of traditional computing are different from quantum computing modes. Traditional computers rely on binary systems, where bits can only be in one of two states: 0 or 1. Conversely, quantum computers have the advantage of multitasking and undertaking a wide range of processes simultaneously at a high speed. Additionally, IoT sensors are vital in smart agriculture, where an extensive network of microcontrollers is necessary. Quantum computing benefits more than conventional hardware systems, such as IoT sensors, because of quantum entanglement and superposition ( ). Quantum entanglement allows qubits separated over unbelievable ranges to operate immediately. Moreover, quantum computers achieve high processing gain by combining quantum interposition and superposition ( ). The quantum superposition principle indicates that if a physical entity is in one of several configurations, the correct state combines all the possibilities. Quantum computing uses qubits that can be in multiple states and locations simultaneously. It exponentially increases the amount of information that can be represented and processed in a quantum computer over a traditional computer ( ). A combination of quantum computing and IoT-based devices, including wireless sensors, robots, and drones, manages farm pests and diseases. The technologies undertake real-time modeling, monitoring, and disease forecasting ( ). IoT-linked disease management depends on image processing and detection. Furthermore, the technique employs field sensors and remote sensing imagery to record pest incidences and plant health. Quantum computing and IoT-based automation setups trap, count, and categorize pests ( ). Moreover, quantum computing enables farmers to control agricultural robots with multispectral image-sensing features and precision spraying nozzles. Quantum computers can solve complicated problems much faster than conventional computers. Moreover, unlike traditional processors, they have the advantage of multitasking ( ). Individuals utilizing them can run a wide range of different processes simultaneously without compromising speed. Ray Johnson, the director of Regetti Computing located in Berkeley, California, uses quantum computing to simultaneously calculate multiple and complex weather variables to enhance crop yields ( ). Moreover, the current industries use an energy-intensive approach known as Haber - Bosch process to produce ammonia for manufacturing agricultural fertilizers ( ). Powerful quantum computers determine the most efficient catalyst combination to create ammonia at low cost and energy. illustrates precision agriculture (PA), which uses quantum computing, information technology, proximal data gathering, and remote sensing. The technology optimizes returns and reduces environmental impacts. The user-centered design (UCD) describes how farmers influence the procedure's shape. Quantum computers are also regarded as more versatile in their use of algorithms, a capability not present in traditional computers. shows that quantum computers can effectively solve simple and straightforward problems like traditional computers and highly complex issues related to scientific simulations, cryptography, finance, and artificial intelligence. While reviewing the weaknesses of quantum computers, established that modern quantum computers have high sensitivity to electromagnetic fields, thermal changes, and repetitive collisions with air particles, which exacerbate the potential of the qubit to lose its quantum properties. point out that quantum computers are susceptible to temperature changes and electromagnetic (EM) fields and would need methods like full noise abatement chambers to avoid potential instability. The failure to make such provisions could lead to erroneous results despite their more advanced computational power and speed than traditional computers. Artificial Intelligence (AI) demonstrates the simulation and automation of natural aspects of life. suggest that bioinspired AI algorithms are applied in agriculture to enhance crop production and reduce the environmental impact. In support of the assertion above, states that the inclusion of IoT and artificial intelligence within smart farming allows the system of physical items to connect and communicate via the internet. According to , integrating AI and IoT in intelligent technologies and image processing capabilities has significantly contributed to the early identification of crop diseases. Moreover, agricultural robots and drones with computer vision systems handle weeds, sow seeds, detect infections, and facilitate irrigation. The superiority of quantum computers over traditional computers shows that the widespread use of AI and IoT in its different applications within the quantum computing framework should lead to even better outcomes. In agriculture, stated that AI and IoT help real-time assessment of crop pests and plant health. The farm environment advocates for optimal resource management, encompassing pesticide control, irrigation supervision, fertilizer regulation, and water quality analysis. IoT connectivity with suitable networks reduces unnecessary human interventions and optimizes resources ( ). The agriculturalists can also use IoT and low-cost sensors to monitor light, temperature, and water flow in the farm. Quantum computers help manage excessive amounts of data, uncover patterns, and detect anomalies in the cultivated area. Applying AI technology within smart greenhouses optimizes crop yields, efficient fertilizer use, and pest control through robotic systems and smart agriculture practices. Further, the potential use of Uncrewed Aerial Vehicles (UAV) and dynamic robot systems within the smart greenhouses efficiently automates energy management and planned operations ( ). In concurrence, assert that using UAVs and robotics in smart agriculture showcases the potential application of quantum technology to monitor and relay real-time data. Farmers increasingly use the technologies to monitor soil, weather, and crop data, providing insights on optimizing crop yields. Quantum technology inhibits crops' abiotic stress and predicts the prevailing weather conditions. Traditional agriculture concentrated on observing and harvesting farm produce to understand the plant and breed. However, showed that quantum computers enable farmers to undertake additional analyses, such as predicting weather patterns and soil moisture levels. It allows agriculturists to adjust their irrigation and fertilization in real-time, ensuring efficient use of resources. Additionally, the technology is highly effective in crop monitoring, precision farming, and livestock management ( ). Thus, the diverse areas of potential application of quantum technology in agriculture show that embracing them can provide an essential mechanism for enhancing crop production and mitigating several hazards. The datasets in classical computers are bits with a value of one or zero. Conversely, quantum computer datasets are qubits simultaneously holding a linear grouping of one and zero ( ). The current research utilized a narrative secondary review approach to gather the necessary data. A narrative review is a critical and thorough summary of published studies on a specific topic. It gives an accurate guide to known issues about a topic. Moreover, the narrative review helps establish a methodological and theoretical framework for the research ( ). The paper evaluates quantum computers' capabilities, challenges, and future direction in smart agriculture. Therefore, a narrative review is suitable for the study because it explores the existing debates, appraises earlier findings, identifies gaps, and speculates future interventions. The narrative review corresponds to the literature search method and includes a specific question and summary of studies ( ). The research starts by identifying the main research question and guides the collection of relevant data. The following subsections depict the steps taken to conduct the secondary research for this review paper. Considering the world of smart agriculture and quantum computing, the researcher wanted to answer the question: What can quantum computing bring to smart farming? This review paper explores the potential uses and future directions of quantum computing in smart agriculture. It also addresses the challenges associated with its adoption. By reviewing relevant articles, the researcher is assured of achieving the goals of this study. The second step entailed the search strategy, where essential aspects, including the keywords, databases, and exclusion criteria, were defined. According to , developing a search strategy is an iterative process with continual refinement and assessment of the keywords used in the research and their usefulness based on the search results. also reveal that adopting a protocol is essential in defining the search strategy since searching for relevant literature can be rigorous and complex. In the review, the databases selected include Science Direct, MDPI, Scopus, and Sage. The choice of the databases arose from their effectiveness in allowing access to relevant and up-to-date articles on the research topic. Keywords included; potential applications, challenges, quantum computing, IoT, AI, Big Data, technologies, and smart agriculture. The keywords were combined using Boolean logic operators such as AND and ORto enhance the scope of the search process ( ). Some of the applied search terms include; Potential applications AND quantum computing AND smart agriculture, Challenges AND quantum computing, Challenges AND smart agriculture, AND quantum computing AND smart agriculture. In the third phase, 650 articles were identified after utilizing keywords and search terms in the different scientific databases. Inclusion and exclusion criteria narrowed the number of articles in the study ( ). The next aspect was focusing on articles published in English to avoid additional work requirements that would emerge from translating the articles. Of the 650 retrieved sources from the mentioned databases, 43 studies were included in the current study through themes relevant to answering the research question, as shown in the results chapter. The present chapter critically reviews the 43 sources selected following the criteria specified in the previous chapter. To ensure all the objectives are met, the sources are analyzed under themes aligned explicitly in each aim presented in this journal paper. The selected sources are analyzed under three overall themes: how AI, IoT, and Big Data are revolutionizing smart agriculture with Quantum Computing, current challenges, and future directions in implementing quantum computing in smart agriculture. Quantum computing is a game-changer in today's economy, with the ability to tackle complex problems and boost efficiency in various industries ( ). surveyed the quantum computing state in the agricultural, automobile, and aerospace sectors and determined the Quantum Technology and Application Consortium's (QUTAC) contribution to the ecosystem. The paper identified 24 cases to understand how QC aligns value chains, including software and hardware solution providers, the industry, and investors. The finding was that QC successfully optimized supply chains, transportation, and financial operations. The research also noted that QDataset, including 52 high-quality simulations, helped in quantum control, tomography, and spectrography. The large-scale dataset is a quantum computer benchmark for developing algorithms for applied agricultural settings. Furthermore, studies by and have shown that firms have benefited from the cost-effective solutions of quantum computing. According to , nature provides fascinating quantum marvels, including magnetism, Bose-Einstein condensation, and superconductivity. Scientists can theoretically define and explain all features of a substance from quantum systems rules, but they need help grasping the laws and predicting the datasets. RumexWeeds datasets help AI-based technologies detect weeds from agricultural fields in grasslands by matching 5,510 image sequences with 2.3 MP resolutions. Combining AI and quantum computing emulates structural and investigational data collection theoretical designs. With such a promising future, the widespread adoption of quantum computing will revolutionize agribusiness operations. Quantum computing techniques have become more fitting for the RumexWeeds dataset application due to the vast amount of information it generates. The real-world RumexWeeds dataset is designed to detect grassland weeds, including species like Rumex crispus and Rumex obtusifolius. This dataset comprises entire image sequences encompassing temporal aspects and considering various perspectives of the same object, as documented. Quantum computing exhibits substantial potential in expediting the processing and analysis of large RumexWeeds datasets, surpassing the capabilities of classical counterparts. Its ability to handle numerous permutations and combinations optimizes complex systems. Quantum computers efficiently simulate image sequences, resulting in accurate outcomes. The heightened levels of accuracy and efficiency empower farmers involved in grassland management to make well-informed decisions regarding eliminating weeds. Quantum computers are revolutionizing various fields, including machine learning ( ). According to and , quantum algorithms can process vast amounts of data and detect patterns that classical computers cannot. stated that linking quantum computing with machine learning allows farmers to learn patterns from existing information and understand unknown data. The article utilized the Sen4AgriNet dataset to train agricultural monitoring applications using 42.5 million images. The research noted that a highly refined machine-learning algorithm is vital in analyzing extensive amounts of updated data. Besides, quantum computing exploits mechanical interaction processes and manipulates data to solve problems. Satya Nedella, Microsoft CEO, used a corn maze analogy to explain the application of quantum computers. QC found the path through the corn maze by unlocking remarkable parallelism. The technology took all routes in the corn maze simultaneously and exponentially lowered the number of steps needed to solve the problem. Quantum computing holds significant promise for enhancing pattern recognition in big data, particularly through Grover's Algorithm, which efficiently sifts through vast datasets and improves the AI's ability to recognize intricate arrangements. This potential is evident in applications like the Sen4AgriNet dataset, specifically designed for agricultural purposes with machine and deep learning capabilities ( ). The dataset has been instrumental in training models to monitor farmland by classifying crop types, extracting and counting parcels, and assessing the evolution of crop phenology. As highlighted by , Quantum-enhanced sensors contribute to this process by providing high-resolution imaging capabilities and precise pattern recognition. This quantum technology has proven effective in monitoring crop health and evaluating plant stress, showcasing its potential in advancing agricultural monitoring and management. As noted by , the lightning-fast processing speeds of quantum computers mean that professionals can gain new insights from extensive data in record time. research also highlights the use of quantum computing in weather forecasting for applications such as disaster management, emergency response, and agriculture. Get ready to witness exceptional advancements in technology with quantum computing. summarizes the findings of a few sources evaluated under the theme of AI, IoT, and Big Data as Elements of Quantum Computing applications in Smart Agriculture. An extensive summary of all sources analyzed under this theme is presented in . Quantum computers process vast datasets and conduct complex simulations, enabling farmers to establish data-driven decisions that augment yields. Quantum robots optimize crop planting and harvesting and monitor soil and plants' health. Researchers have combined principles of quantum computing with robotics to develop robots that make decisions based on complex datasets. proposed an IoT-based smart agriculture and automatic seed-sowing robot, delivering high efficiency and precision in sowing soybeans, pigeon peas, and groundnuts. The research used the LabelMe dataset, which contained 40 JPG original soybean images, and used them to detect the soybean quality. Merging quantum computers and IoT also enhances selective harvesting. The strategy involves harvesting crop parts that meet specific quality thresholds. For example, the technology sorts and harvests barley with fixed protein content and those meeting the size criteria. Farmers should connect IoT and QC applications to small autonomous crop harvesters, which can gather crops and transport them to stationary processing systems. The technologies enhance precise sorting, lowering labor requirements and reducing crop yield wastage. Quantum computing also transforms agricultural production through digital twins (DT). According to , a DT is a scientific version of a process, environment, or product that runs in real time. The innovative approach uses advanced modeling and simulations for greater accuracy and detail. In agriculture, state that a DT implemented in soil and irrigation systems fosters a digital representation of agricultural soil data. In addition, it predicts fundamental soil and water component requirements in growing crops. DT uses quantum computing datasets to replicate real-world entities according to predefined metrics and goals. The gadget must align with ecological dependencies to give positive feedback loops at the algorithm and system design levels. add that DTs differ from models and other simulations, as they are digital equivalents of physical products that involve bi-directional data flow between the actual entity and the virtual solution. The studies by and suggest that DTs duplicate physical objects in both the real and virtual worlds while acting independently. However, the two entities can exchange data in a bi-directional manner. The synchronization benefits identified in the DTs' are essential in optimizing the physical assets by enhancing bilateral communication between the physical and cyberspace ( ). Such real-time models are widely adopted in improving agriculture through diverse applications, as described in the sections below. Quantum computing enables digital twins to explore sensed data, statistical models, and real-time elaboration to predict and optimize physical assets’ behavior. According to , digital twins (DTs) enhance crop productivity within controlled environment agriculture applications. They improve decision-making in greenhouses and plant factories. The authors underscore the emerging benefits of DT in integrating real-time data into simulations to guide decision-making in managing different types of crops. Furthermore, the DT controls microclimate to optimize total productivity via intensive AI. Similarly, added that DTs augment resilience and flexible automation within vertical farming. Quantum computing can embed data collection and analysis capabilities and enhance digital twins. It uses a random algorithm to impute the hour of the day, missing temperature, and the neighboring data sensors. The algorithms predicted daily plant and fish growth. Quantum computing has the potential to analyze vast amounts of data and make predictions based on complex algorithms in smart agriculture. sought to establish how quantum computing assists in crop optimization. It merged quantum computing with machine learning and assessed their application in modern agriculture. The research used the Plant Centroids dataset with 2000 images to examine how the algorithms optimized crop production and waste reduction through informed watering, planting, and harvesting decisions. Bayes Net algorithm attained 99.56 % classification accuracy, while Naïve Bayes Classifier achieved 99.46 % precision when linked to quantum computers. The researcher established that using quantum computing accurately predicts the optimal amount of water and nutrients for crop growth. Leveraging quantum-based machine learning offers insights into the optimal levels of phosphorus, potassium, and nitrogen for efficient crop growth and development. The Plant Centroids dataset, containing annotations of emerging stem points of sugar beets captured in NIR and RGB images, as documented by , plays a crucial role in this process. Accurately detecting the emerging stem point is essential for precisely positioning fertilizing tools at the center of the plant. In addition to optimizing fertilization strategies, quantum machine learning contributes to the secure and tamper-proof transmission of sensitive agricultural data, encompassing information about soil fertility and crop health ( ). This technology not only enhances the efficiency of sugar beet fertilizations but also reduces resource wastage, demonstrating its potential for sustainable and precise agricultural practices. Similarly, confirmed quantum computing enables real-time tracking of water levels. The research used quantum computer-based 3D-printed microfluidics and digital microfluidic platform to undertake the automated soil nitrate nitrogen detection. The device portrayed rapid detection of 200 s, low reagent consumption of 40 µL, and reduced detection limit of 95 µg/L. The relative error between the ultraviolet spectrophotometry measured and detected concentrations was 20 %. The technology helped farmers take immediate corrective actions, leading to higher crop yields. The findings showed that quantum computing sensors detect the slightest nutrient and humidity changes and provide farmers with real-time feedback. As a result, farmers can adjust their growing conditions before any damage is done to their crops. From the insights, applying quantum technology in controlled environments minimizes the risk posed by fluctuations in environmental conditions. The Quantum-based Light Dependent Resistor (LDR) sensor is designed to capture the light intensity passing through a soil sample solution. Quantum computers can improve LDR sensors' efficiency by detecting variations in light intensity and converting this information into electrical signals, as outlined. Furthermore, the computational capability of quantum systems enables the LDR sensor to swiftly and efficiently detect nitrogen content in soil samples through complex simulations. This quantum computing-aided LDR sensor circuitry proves valuable for farmers, processing electrical signals and converting them into numerical scales ranging from 0 to 1024, where a high numerical value indicates low nitrogen content in the soil ( ). The quantum sensors provide real-time continuous data streams, empowering farmers to make immediate decisions regarding soil fertilization based on the observed nitrogen content. According to , quantum computing enhances the efficient use of resources than traditional farming. The authors indicate that using quantum computing analytical and predictive capabilities enables the global agricultural sector to optimize productivity and reduce resource wastage. Such outcomes lead to higher profitability in agriculture because farmers produce more using the least possible resources on their farms. In that regard, the use of quantum computing technology in regulating energy use in agriculture could lead to the overall environmental sustainability of the sector. This argument is based on the fact that optimizing energy use could reduce greenhouse gas emissions related to agricultural production. Quantum computing avoids excessive energy consumption in controlled farms through heat exchanger network systems (HENS). The tool controls cooling and heating using cold and hot utilities. Furthermore, the analysis should examine how quantum computers and digital twins affect crossbreeding and gene transfer. Crossbreeding and gene data offer crop varieties and better yields. However, the farmers must trail gene combinations and monitor performances to determine which plants have strong genetics ( ). The study wanted to impute molecular phenotypes at the level of functional gene pathways and cell-type proportions. It performed weighted gene correlation network analysis (WGCNA) to cluster genes based on marker profiles and decompose bulk transcriptome data. It is common for scientists to press between 1000 and 20,000 features, but quantum processing minimizes the clustering to about 100 features. Quantum computing bypasses crossbreeding and directly points out the genes responsible for crucial features. However, the conventional algorithm only uses mathematical approaches and a defined genetic dataset to determine the fitness of a crossbreed. Quantum computing addresses intricate combinations and permutations concurrently, optimizing intricate classifications. Its efficacy is particularly beneficial in processes demanding simultaneous considerations, such as genetic analysis, as highlighted by . Experts extrapolate intermediate phenotypes at the levels of gene sets, encompassing functional pathways and cell-type magnitudes. One illustrative method, the weighted gene correlation network analysis (WGCNA), employs hierarchical clustering to derive co-expression modules ( ). Quantum heuristic approaches and speedups excel in efficiently searching through extensive tree spaces and clustering over 20,000 features. The precise processing of big data is crucial in accurately determining intermediate phenotypes. reaffirmed that quantum computing algorithms identify the genes responsible for desirable traits such as drought tolerance, disease resistance, and higher yields. The population utilized in the study entailed 190F 2:3 progeny lines from the F1s between the Indian-origin drought-resistant Nagina 22 rice and the US-bred drought-sensitive rice. The breeding effort focused on incorporating drought-resistant characteristics in rice, making it suitable for farrow irrigation. The research used quantum computing to assess the extent to which drought-tolerant parent N22 contributed vital alleles for all the qualitative traits loci, except qGN5.1 and qGN3.2 for the number of grains per panicle. The findings demonstrate that quantum computing applications in rice enable it to thrive in a controlled environment previously considered unsuitable for agriculture. Quantum computing has the potential to expedite the process of genome assembly by introducing innovative computation methods. These algorithms exhibit high efficiency and speed, particularly when handling extensive datasets ( ). Quantum computers excel in swiftly executing complex calculations, aiding researchers in the more efficient processing of genomic information related to drought-resistant rice. Furthermore, quantum computing facilitates genome assembly by allowing scientists to explore novel genetic sequencing and assessment strategies, as highlighted. This capability enables the analysis of distinctive characteristics and patterns within the genetic data associated with drought-resistant rice. Likewise, agree that quantum computing helps advance genetics in agriculture. The research used the Barley ExpDB dataset and quantum computer to assess the magnitude and nature of prevailing genetic variability in barley germplasm conserved in the Indian National Genebank (INGB). The dataset revealed that BH959 (85) was responsible for spike emergence. Moreover, BH959 (1 2 8), BHS352 (1 2 6), and DWRB101 were precursors for maturity. Besides, BH959 (57), BHS352 (4.49), and BWRB 1010 accounted for hundred-grain weight. While the article did not evaluate the specific impacts of genetics on productivity, it is plausible that such quantum computing developments will significantly increase the amount of food produced globally. BarleyExpDB is a web-accessible dataset seamlessly incorporating Barley's transcriptional profiles across various growth and development stages. The integration of quantum computing has empowered scientists to query genes of interest precisely through sequence similarity, as outlined. Quantum technology further contributes by presenting expression data in diverse formats such as heat maps, gene ontology, and functional descriptions. The remarkable capacity of quantum computers to swiftly process large datasets and discern patterns expedites RNA sequencing, as highlighted. This computational prowess proves invaluable to field researchers, facilitating the efficient processing and analysis of the dynamic expression patterns of transcribed genes. In smart agriculture, quantum computing monitors crops and makes data-driven decisions, improving agricultural production and sustainability. elaborate on the fundamental concepts, applications, and wide-ranging review of the currently developing quantum computing. The research uses Clarivate Analytics to undertake a bibliographic analysis that covers the comprehensive literature on quantum computing and agricultural engineering topics from 54 598 papers published between 1990 and 2020. The outcome indicated that two-fifths of the publications focused on applying quantum computing to crop monitoring in the 1990 s. However, the number of publications increased three times by the 2000s. Currently, agriculturalists monitor crops to determine the optimum fertilizer, water, and temperature the produce needs. Quantum computing enables sensors to undertake more precise crop health and growth measurements, leading to better decision-making by farmers. Quantum sensors leverage the fundamental properties of light and atoms to measure crops' health and growth accurately. Applications employing particles as probes can precisely quantify various factors, including magnetic fields, accelerations, time passage, and rotation, as indicated by . Quantum computing plays a crucial role in enhancing these sensors, allowing for monitoring crops' photosynthesis rates and growth with unparalleled precision. Additionally, AI-powered algorithms contribute to the capabilities of quantum robotics, enabling the analysis of extensive sensor data. These algorithms facilitate the generation of predictive models for crop management, as highlighted by . Farmers can benefit from real-time detection of disease outbreaks and nutrient deficiencies, empowering them to take immediate corrective measures for enhanced crop health and productivity. Similarly, quantum computer vision and deep learning models are applicable in detecting and classifying infections in sugarcane plants. presented a quantum-behaved particle swarm optimization based on the deep transfer learning (QBPSO-DTL) concept to detect and classify diseases in sugarcane leaves with high accuracy. The QBPSO-DTL technique involved designing an optimal region and a growing segmentation to discover the affected portion of the leaf image. Moreover, the research utilized the SqueezeNet feature extractor and applied the deep stacked autoencoder (DSAE) model as a classification architecture. The article carried out the DSAE model’s hyper-parameter tuning through the QBPSO algorithm. The QBPSO-DTL model used the testing and training datasets with diseased and nondiseased captions. The training dataset had 80 images under the nondiseased and diseased classes. The results depicted that the QBPSO-DTL model had a validation accuracy of 99.48 % while the training accuracy was 89.65 %. Information on crop health acquired through quantum computing indicates developing problems and advocates corrective measures. The QBPSO-DTL approach comprises several subprocesses, including preprocessing, hyperparameter tuning, and classification. Quantum variation circuits play a crucial role in binary classifiers by providing high accuracy with limited parameters, as emphasized. Quantum computers, with their ample computational capacity, can perform multiple tasks simultaneously. In applications, they excel in highlighting targeted regions and preprocessing small sections of images. Furthermore, quantum computing assists sensors in optimally segmenting disease-affected plants, showcasing its potential to enhance precision and efficiency in plant health analysis. Likewise, the remotely sensed multisource information and light detection data fusion aid farmers in monitoring disparities in parts of a specific plant. used spectral imaging to describe and evaluate remote sensing know-how for field crops. The research used quantum computers to help monitor crop development in the virtual constellation of satellite images. It used the DiaMOS plant dataset with 3505 images of leaves and fruits. The models based on reflectance-associated virtual images had a pixel range of between 5 and 30 m. The operational utilization of dense time series of multispectral imagery at a spatial resolution enables monitoring crop biophysical parameters. The authors revealed that quantum computing has profoundly improved crop monitoring through the ability to analyze satellite imagery. The advancement of crop assessment, satellite image accessibility, and accurate forecasting enhance the prediction of crops’ parameters, including water requirements. Integrating quantum machines and deep learning contributes to the improved classification and recognition of diseases in plant foliage. Notably, the DiaMOS dataset has played a role in detecting infections in pear leaves and fruit, as demonstrated. Quantum deep learning techniques facilitate automatic feature extraction and classification, effectively expressing image characteristics, as highlighted. This software supports complex deep-learning architectures, empowering farmers to detect diseases in crops with high precision. The combination of quantum computing and deep learning holds promise for advancing disease detection and management in plant health. affirmed that quantum algorithms quickly analyze large amounts of GIS data. The research combined quantum computing and deep learning models to review GIS-associated data about crop status, diseases, and soil types. The study used the Wuhan hyperspectral image (WHU-Hi) dataset for training and evaluation of deep learning models to classify crops. The collected hyperspectral images were at different altitudes and locations through devices mounted on two UAV types. It also used the Weedmap datasets and multispectral information for classifying sugar beet crops and weeds. It entailed at least 10,000 multispectral aerial images with diverse resolutions using two camera types. The analysis shows that quantum computing and UAV images help stakeholders responsible for large plantations to identify patterns and anomalies in crop growth that may not be visible to the naked eye. In hyperspectral imaging (HSI), unique labels are assigned to pixels to distinguish various land cover categories. The WHU-Hi dataset, curated by , stands out for its hyperspectral information, particularly in crop classification, achieving high precision. Scientists utilized nano-hyperspace sensors to collect datasets from farms showcasing diverse crop types. The introduction of quantum computing brings forth a new paradigm for HSI-related deep learning models. The Quantum-inspired Spectral-Spatial Network (QSSN), as proposed by , efficiently and accurately extracts HSI features in a shorter time span. The QSSN leverages quantum representations to depict HSI cuboids and retrieve joint spectral-spatial structures, showcasing the potential for quantum computing to enhance the efficiency and capabilities of hyperspectral image analysis. Furthermore, quantum computational intelligence and data mining are increasingly prominent in smart agriculture. and employed the soft computing method of fuzzy cognitive maps (FMC) to predict cotton yields. They used the methodology to evaluate 360 cases during five subsequent years. The results showed that the crop simulation model effectively predicted the potential impacts of soil attributes, climate change, and management practices. Crop models are effective mathematical representations of crop growth and development, predicting yields and identifying factors that can impact crop growth. Crop models are progressively being used to make critical decisions in agriculture that are useful in boosting yields. Specifically, used quantum computer models to investigate farmers' best procedures for capturing and developing multispectral aerial images from Unmanned Aerial Vehicles (UAV). The methodology entailed measuring sugarcane crop crops' vegetation indexes correlating with vegetal strength, foliar mass, and number of stems. The Green Normalized Difference Vegetation Index (GNDVI) and the Normalized Difference Vegetation Index (NDVI) datasets generated using the Sequoia camera identify sugarcane parcels with vegetal strength and high density. Multispectral cameras generated high-resolution images and represented them in 3 cm per pixel. Quantum computing helped identify furrows in sugarcane plantations from the obtained data and index map. highlighted that quantum computing allows farmers to make data-driven decisions about planting, fertilizing, and harvesting crops. Modeling capabilities of quantum computing improve the probability of farmers getting high yields. Quantum machine learning is pivotal in addressing intricate tasks within mechanized sugarcane harvesting through advanced computational techniques. Utilizing computer simulations, this technology optimizes the processes involved in sugarcane harvesting and transportation, as demonstrated by . Quantum machine learning proves beneficial in preventing machine traffic-related damage to sugarcane planted in rows, offering farmers a tool for optimizing plantation operations. Beyond optimizing harvesting processes, quantum machine learning is instrumental in developing early warning systems. These systems can alert farmers to potential machine traffic within the plantation, allowing for proactive measures. Various machine learning algorithms, including regression, artificial neural networks, and decision trees, as outlined by , are applicable in predicting changes in the farm environment. Farmers can leverage these algorithms to create models for developing comprehensive sugarcane management plans, enhancing efficiency and sustainability in sugarcane cultivation. Pest management is another area where quantum computing is applied in smart agriculture. Accurate and real-time exposure to orchard pests is necessary in the evolving fruit industry. developed an orchard pest dataset, PestImgData, using a web crawler, data augmentation, and specimen image collection. PestImgData had 24,796 color images and addressed seven types of orchard pests. The researchers used the “You Look Once” (YOLO) version 5 algorithm and PestImgData to detect and localize pests at an accuracy of 92.86 %. The research proposed applying an automatic system, including a smartphone IP camera, for detecting insect pests. The approach is anchored on YOLO object architecture, such as YOLOv5, YOLOv3, YOLOR, and YOLO-Lite. The study collected 7046 images under different background conditions and illuminations. The researchers trained and tested the object recognition systems with diverse parameters and compared the models. Analysis of the images using quantum computers demonstrated that YOLOv5x depicted 98.3 % precision in the IP-23 dataset. Quantum computing algorithms help analyze data related to pest populations, providing farmers with more valuable insights into managing pests. Likewise, created localization and classification methods for managing pests. The authors trained the “You Look Once” (YOLO) version 5 model using an optimal learning hyperparameter to localize pest-infested areas in a plant image. The YOLOv5 detector had three fundamental parts: the backbone, neck, and head models. The YOLOv5 head entailed sixteen convolutional, one focus, and one spatial pyramid pooling. YOLOv5 had sixteen convolutions, two up-sampling, four concatenations, and one detection layer. The study employed quantum machine learning to classify the pest images into Paddy with or without pests. The quantum computers had fifteen layers displaying two-qubit accuracy. According to the results, the Paddy with pests dataset had 135 files, while the Paddy without pests had 513 files. Besides, the IP102 dataset utilized to recognize insects had 75,000 images categorized into 102 classes. The transfer learning models, VGG16 and MobileNet, classified the data with 93.83 % accuracy. Quantum computing algorithms are used to calculate data from the sensors and cameras placed in fields to identify pest infestations and determine the best action to address them. Quantum computers excel in classifying datasets through quantum-enhanced machine learning (QEML) algorithms. These algorithms, as described by , leverage orthogonal transformations and quantum gates to transform supervised learning processes efficiently. This application of quantum technology significantly reduces the time required for classifying and counting pests. Quantum machine learning also empowers sensors to simultaneously organize pests while capturing their real-time behaviors with maximum accuracy, as highlighted by . Accelerating algorithm execution and representation space, facilitated by quantum heuristics in QEML, allows for swift and efficient processing. Quantum computing, in this context, provides farmers with the capability to detect pests before they inflict damage on fields, mitigating potential crop yield losses. The integration of quantum-enhanced machine learning proves instrumental in enhancing the precision and timeliness of pest detection in agriculture. As a relatively new technology, the evidence presented in most studies shows that adopting quantum computing in smart agriculture still faces many challenges. provides a summary of some of the sources that were analyzed under this theme. An extensive table with summaries of all the studies used is presented in ( ). One major challenge in using quantum computing in smart agriculture is more expertise and infrastructure. Numerous scholars, including and , agree that quantum computing is a highly specialized field requiring significant expertise and resources. According to , quantum computing is a relatively new technological development and needs to be better understood by the masses in all sectors of the global economy, including agriculture. As a result, it follows that the adoption of the technology in the specific context of smart agriculture could be higher because most of the stakeholders in the industry need to possess the necessary competencies to use it. Additionally, in the evaluation presented under the previous theme, the comparison of information presented in sources such as and proves that the scope of quantum computing in smart agriculture is far-reaching, meaning that it is challenging for most people to learn and use the technology in all its different applications. Although quantum computing holds great promise, it remains a nascent field with limited hardware and software availability worldwide, making it difficult to test and optimize quantum algorithms in a variety of fields, including smart agriculture ( ). While some argue that these resources are available in sufficient quantities ( ), their scarcity remains challenging. However, they argue that what needs to be improved is the standardization of hardware, software, and algorithms that are used in quantum computing. The lack of standardization makes it difficult for farmers and agricultural organizations to effectively use quantum computing in their operations as they may need to create customized solutions for their exact needs. In this regard, the perceived scarcity of hardware and software mentioned by and could result from the fact that it is difficult for different stakeholders in agriculture to find the specific quantum computing options they need, since in most cases, the technology needs to be customized for each user. Standardizing the hardware and software for the agricultural sector could thus overcome the perceived scarcity of the technology's resources. Quantum computing and standardization are beneficial for smart agriculture because it sets out particular products. The farm produce will have identical size and nutritional values. Moreover, the plants and animals perform similarly and undergo the same safety measures. Standardization also simplifies the safety requirements and sustainability concerns in precision agriculture. Quantum computing has the potential to revolutionize data analysis and processing capabilities ( ). However, it also introduces new vulnerabilities and risks to data privacy and security, as stated by the same sources. More specifically, point out that quantum computing can break traditional encryption methods, making sensitive data vulnerable to attack. This argument is supported by , who point out that quantum computing has made it much easier to breach most software-dependent security systems, meaning that it increases the risk of unauthorized individuals, such as hackers accessing critical data about organizations in a wide range of contexts including smart agriculture. In this regard, stakeholders in the agricultural sector might be reluctant to adopt the technology for fear of being exposed to security breaches by malicious parties. Another challenge in using quantum computing in smart agriculture requires specialized quantum sensors and hardware. According to recent research by and , quantum sensors are essential for the growth of quantum computing technology in smart agriculture. These sensors are adept at measuring physical properties at a quantum level, making monitoring soil and water quality possible. Despite the many benefits, quantum sensor development and implementation are still in the early stages, and various unresolved technical challenges need addressing. summarizes some sources that provided insights addressing future directions for quantum computing in smart agriculture. The full summary table is provided in . Quantum machine learning (QML) is a subdivision of computer science that employs the principles of quantum physics and algorithms to solve problems that classical computers cannot handle. According to , QML uses qubits with the capacity to tackle complex issues. On the other hand, conventional machine learning (CML) is an image ordering strategy that entails acquisition, pre-processing, mining, and cataloging. Although there are not yet many widespread applications of quantum computing in agriculture, articles such as and indicate that there are some promising areas where it can be used to promote agricultural production, including land cultivation. According to , one potential application of quantum computing is land cultivation, where the technology will be able to analyze and optimize soil quality through quantum machine learning algorithms from the information presented in sources, such as and . As a result, they are well-suited for analyzing large datasets related to soil health. Integrating AI, IoT, and Big Data in quantum computing will contribute significantly to environmental sustainability. argues that the application of AI in agriculture, along with IoT, can use crop sensors to collect and transmit data about weather, soil moisture, plant growth, and other environmental conditions. In their respective studies, and show that quantum computers could analyze such farm data, providing farmers with information on the optimum quantities of irrigation water, fertilizer, and pesticides to use. This argument is supported by , who point out that machine learning algorithms in quantum computers can use data collected using AI and IoT systems to monitor crop health, detect early signs of disease, and identify pests, reducing the need for harmful pesticides. Apart from promoting sustainability in the farming of crops, the evidence presented in different research studies shows that quantum computing and other technologies, such as IoT and AI, could be used to promote environmental sustainability in livestock rearing. and argue that livestock farming significantly contributes to global greenhouse gas emissions. point out that ruminant animals produce methane during digestion. reveal that there has been ongoing research to establish ways to develop low-methane food additives for Livestock. According to , quantum computing can help develop low-methane food additives by simulating complex chemical reactions during digestion. , in corroboration, points out that this simulation can identify the specific compounds that contribute to methane production and help develop additives that reduce methane emissions. Creating low-methane food additives for livestock is a promising solution to reduce greenhouse gas emissions from the agriculture sector. The principles of quantum computing revolutionized how experts approach computational problems and made scientific discovery accurate, efficient, and powerful. In addressing the first objective, the evaluations presented in this research show that quantum computing has been successfully utilized in smart agriculture applications. It allows farmers to use developments such as hydroponics and aeroponics more effectively in rural and urban settings. Likewise, explained that quantum computing fosters efficient management of water and soil nutrients. Technology facilitates the need to prepare and till the land ready for cultivation. Moreover, quantum computing reduces the time and increases the accuracy required to evaluate and modify crop genetics. The average time it takes to crossbreed and see fruiting in new varieties is between five and seven years. However, with quantum computing, scientists can analyze the genetic makeup of crops with incredible speed and accuracy. According to , quantum computing pinpoints genes with desirable traits and cultivates new breeds using techniques such as gene editing. With the success of using quantum capabilities within the healthcare and drug discovery industries for proteins and DNA, transferring these practices to agriculture for food security is a logical progression. In addition, Digital Twin (DT) helps address food security issues globally by improving production efficiency. DT is a virtual representation of a system or object that is an indistinguishable counterpart of real-world products or processes for simulation, testing, integration, and monitoring ( ). DT's precision agriculture ensures optimal resource application for maximum yield and minimum environmental impact, a win for farmers and the planet. Land degradation is also a significant challenge, as it can lead to decreased productivity and increased vulnerability to pests and diseases, thus adjusting farming practices as needed. Quantum computing helps prevent further land degradation and maintain productivity levels. Smart agriculture can utilize quantum computing capabilities to increase the yield for precision agriculture, control the quality of crop production, perform genomic analysis, manage pests and diseases, and predict weather patterns and crop yields ( ). This finding is consistent with , and who explained that technological tools leverage quantum computing capabilities to perform exceptional functions that increase the output and quality of crop production. Moreover, the findings indicated that quantum computing is applicable in generating a digital twin and offering predictions on weather forecasts. The know-how helps prevent abiotic stress emanating from unpredictable weather patterns. Agriculture has recently witnessed several technological advancements with the adoption of digital twin technology. These digital replicas, or virtual models of physical systems, allow farmers and agricultural experts to test new strategies and experiment with different ideas to optimize operations in a risk-free environment. By replicating the physical world in a digital environment, agriculturalists can fine-tune and improve traditional practices, from crop rotation, soil management, irrigation, and predicting weather patterns. This technology enables farmers to streamline farming processes, optimize yields, control costs, and improve the quality of products. Digital twins are a game-changer for farmers, allowing them to remotely track essential data such as soil moisture levels and temperature to improve overall crop health. Farmers effectively identify issues and make necessary changes to irrigation and fertilization levels to ensure success and prevent disease outbreaks by monitoring plant and livestock health ( ). Livestock farmers can also use digital twins to monitor animal health, reduce mortality rates, and improve breeding techniques. Digital twin technology contributes to more sustainable and efficient farming. Similarly, the potential to deploy UAVs and robotic systems in agriculture enhanced efficient automation and overcame abiotic stress ( ). Quantum technology establishes digital twins and collects data about the agricultural soil and water requirements in growing crops. Controlled farming is a modern trend involving growing plants in a controlled environment using technology. , and agree that controlled farming involves growing crops in managed surroundings where the temperature, humidity, and light are regulated. It allows farmers to create ideal growing conditions for each crop, resulting in higher yields and better-quality produce. In , the control capability of quantum computing has also been identified in various contexts, as established in studies such as and . Specific elements of quantum computing, such as simulation capabilities, are highlighted by in achieving much better control and dynamics outcomes than traditional systems. Such capabilities that are found by and have been successfully adopted in the specific context of smart agriculture to maintain conditions as close to ideal as possible. Therefore, the control capability of quantum computing is one of the critical enablers of controlled farming, which promises to significantly contribute to food security by allowing year-round production of crops without being limited by the weather or seasonal changes. One area of concern for the findings drawn from sources such as and is that they need to present a comprehensive list of examples of some of the specific areas in which quantum computing has been successfully applied to promote control. This challenge is well addressed by , who shows the use of quantum computing, artificial intelligence, and the Internet of Things in specific controlled farming contexts such as hydroponics and aeroponics. Similar control of water and nutrients is an essential application of quantum computing in aeroponics, as established by . In explaining how technology is integrated into controlled farming, show that sensors utilizing AI and quantum computing technology are installed in greenhouses and then used to collect data on environmental conditions. Quantum computing algorithms use this data to adjust to achieve optimum conditions depending on the specific crops. In the domain of controlled farming, therefore, evidence in the literature shows that automation of control of environmental conditions is the main contribution of quantum computing. In the context of controlled farming, it is evident that quantum computing has enabled full automation of environmental conditions. In some cases, however, the objective of having the technology is not to take away the responsibility of farming from humans but rather to help make decisions. In this regard, another critical application of quantum computing in smart agriculture, as established in sources such as and and confirmed in studies like and El , is to assist farmers and other key stakeholders in agriculture to make informed decisions. Farmers have a convenient tool when they use quantum computing algorithms to analyze vast amounts of information regarding soil constitution, weather patterns, and other determinants to nail down the best circumstances for growing crops. By employing quantum computing algorithms, farmers can analyze copious amounts of data that would have been bothersome to manage otherwise. Quantum computing can help farmers save time and money while achieving the best crop yields. The study by shows that one of the critical challenges in agriculture is predicting crop yields and identifying the optimal times for planting and harvesting crops. However, and show that capabilities such as yield prediction will be possible shortly based on the current advancements observed in quantum computing. This piece of technology, if properly utilized, can enhance farmers' decision-making prowess about their planting, fertilizing, harvesting, and yield enhancement strategies. Another critical decision that is made primarily on open-farm agriculture is related to how to deal with pests. In this regard, show that quantum computing has been applied in detecting and identifying pests before they can spread to the extent of leading to farm losses. As established from evaluating sources such as and , information on pest infestations allows farmers to decide when and the specific pesticides to use. Additionally, establish that AI-based pest detection systems have been found to help farmers identify potential pest infestations and make early decisions on how to respond. In addition to identifying pests, show that quantum computing algorithms can also help determine which pesticides to use, in what quantities, and how to apply them for maximum effectiveness while minimizing environmental risks and costs. The findings of this research derived from sources such as and agree with sources such as and that early detection of potential infestations enable farmers to decide to use pesticides on the affected sections of their farms before the issue of pests becomes widespread. Early detection and the timely decision-making made possible by quantum computing allow farmers to save significantly on pesticides since they can take action before large sections of their farms are affected. Traditional crop monitoring methods involve physical surveys and manual data collection, which are labor-intensive and time-consuming. However, the evidence presented by shows that using satellite imagery in quantum computing has made crop monitoring more efficient and cost-effective. An important question to consider at this point is how satellite imagery is used to acquire information that can be used to make decisions in smart agriculture. The findings from the analysis of studies, such as and , show that satellite imagery provides a lot of data within a very short period. While this is a good thing from a data collection point of view, argue that analyzing these images could take a very long time when traditional methods are used. This assertion is confirmed by , who point out that satellite imagery provides an overwhelming amount of data that would be very difficult to analyze quickly. In this regard, and agree with that quantum computing has made it possible for satellite data to be analyzed within a very short time and provide insights on weather patterns, soil conditions, and health of crops. By examining this information, farmers and agricultural experts can make important decisions, such as the kind of crops suited for specific areas that still need to be farmed. In their research, confirm this observation by showing that farmers can accurately predict yield by analyzing satellite imagery data using quantum algorithms. This information helps them to plan harvesting, storage, and processing more efficiently. Another benefit of this overall viewing of farms is that farmers can use quantum algorithms to monitor crop health in real time. The algorithms can analyze images and detect changes in plant color, leaf density, shape, and other indicators of crop health. With this information, farmers can decide on the specific actions to take, including seeking assistance from agriculture experts. Generally, the analytical capabilities made possible by using quantum computing in satellite imagery make farmers more responsive to their farms. The decisions informed by the analyses promise to increase the level of productivity in farms. Quantum image processing uses the amplitude encoder (QIPIE), which is highly efficient in image space programming. The approach is faster than the traditional methods because it has exponential gains in several qubits it employs to store and process the satellite image. Quantum computer processes a 2D image of 512 squared pixels by employing QIPIE to represent it in 18 qubits ( ). On the other hand, classical computers use a single byte to code each pixel, requiring 256 K Bytes. Despite its advantages, quantum computing also entails some costs. According to , the core goal of sustainable farming is to heal and grow the health of the system's components, which includes the naturally farmed environment, the food produced, and the humans involved. believe that quantum computing can bring any benefit but stresses that they do not search for use for a technology merely because it exists. The know-how needs to align with the specified goal of sustainable farming. Moreover, the over-dependency on technology in food production poses a significant threat to human and natural processing as it challenges our very being, raising ethical and existential questions. This development is a cause for concern for farmers emotionally attached to their farms and traditional practices. Therefore, sustainable farmers may not be inclined to adopt certain technologies because they believe in human endeavor over machine-based inputs. Depending on individual worldviews, this sentiment may lead to societal costs if quantum computing becomes widely adopted in the agricultural industry. Considering these societal and ethical impacts while implementing new technology is vital as this technology may be used for the betterment of humanity. Studies, including , identify quantum computing as a recent technology, even more so in agriculture. One of the direct outcomes of this developing technology is the need for sufficient expertise in this field. According to and , the complexity of quantum computing requires specialized knowledge and skills that are not readily available in the agriculture industry. As a result, there are fewer quantum computing experts in the agricultural sector to aid in customizing the technology of various farm functions. However, this challenge is short-term because more people are gaining the required knowledge in quantum computing, especially in the specific context of smart agriculture, as the technology becomes more valuable in the agricultural sector. While a lack of expertise could be a profound challenge, it will be brief because agriculture experts are acquiring the necessary technical competencies to handle different quantum computing applications adequately. The other challenge directly resulting from the immaturity of quantum computing technology is the limited availability of hardware and software. Sources such as and agree that quantum computing has inadequate hardware and software to allow widespread utilization of the technology in smart agriculture. The scarcity of quantum computers and the fact that the agricultural sector is not among the first movers to adopt new technologies implies that widespread adoption of the technology in the industry will only be achieved when the technology becomes mainstream. Secondly, shows that quantum computers are still costly, and so is the specialized software that runs in them. Therefore, the cost implications of the technology would bar most farmers from using it even if it was readily available in the mainstream market. Nevertheless, the increase in quantum computing hardware and software is inevitable going forward, meaning that the technology will become cheaper with time, leading to mass adoption in agriculture. As the world progresses towards a more advanced technological landscape, some ambiguity arises regarding the definition of a DT and the threshold for labeling technology. The scientific community has tried classifying technologies based on their proximity to becoming a DT. However, it remains quite challenging to distinguish whether a system meets the qualifications required for a DT. However, a DT should have essential features such as analytical capabilities, observation mechanisms, and an interface for various agricultural applications. Like any other application that relies on the Internet, quantum computing in agriculture has been argued to expose the sector to security breaches. The findings derived from sources such as , and show that, indeed, the security of information on the quantum system is one of the critical considerations in the adoption of the technology in agriculture. Security estimation is essential in software or web application assessment, administration, and control. examined how quantum computing defines and characterizes software security. The authors used cryptography or algorithms to secure medical devices, financial organizations, automobiles, and military weapons. The research addressed software durability, which suggests the capacity to execute an item on the schedule. The findings demonstrate that cryptosystems have the potential to collapse when engineers develop large quantum computers. For example, Google created the Sycamore Processor 53 qubits, and the innovation can potentially render cryptosystems obsolete. The research inspires stakeholders to thoroughly scrutinize security factors in web and software applications to determine their durability. Quantum mechanics attempts to elaborate the inconsistencies in system behaviors at micro and macro scales. The development of quantum computers to 65 qubits is weakening the security of cryptographic algorithms. examined quantum computing-associated attacks and potential future effects. Computer scientists can use the Grover Algorithm to discover a key for scripted plaintext. Grover Algorithm is a wide-ranging quantum search procedure for an amorphous database. An adversary can integrate an Advanced Encryption System (AES) into a quantum computer and find keys to encrypted texts. Google and IBM have ambiguous plans for upscaling quantum computers. In 2020, IBM developed a quantum computer containing 65 qubits and intends to make a 1000-qubit system by 2024. The landmark will pave the way for quantum supremacy and increase its threats to software security. Quantum computing broke encryption codes for the first time in the 20th century by introducing the SHOR algorithm. Recent advancements in quantum computing have improved networks' integrity, availability, and confidentiality. Conversely, the transformation from classical to quantum computation comes with cartographic implications, including the risks in transaction methods. explore the advanced quantum computing's aptitudes to pose massive risks to network security. The research noted that elliptical curve cryptography helps secure blockchain and IoT. Shor's algorithm makes it easy for intruders to discover a private key. Secondly, Bruce Schneier developed a systematic cryptographic block cipher called Blowfish in 1993, approximately between 32 and 448 bits. It makes long keys and sub-keys that enhance complexity and inhibit attacks. Quantum computing can break the Blowfish algorithm only by one or two factors. Equally, confirmed that security threats undermine the adoption of quantum technology in agriculture. The researchers itemized the security features of precision agriculture (PA) and smart farming (SF). Thereby, they reviewed the cyberattacks that were likely to violate the security system. Furthermore, the authors presented a taxonomy of cyber risks to PA and SF based on their relationship to different phases of the Cyber-Kill Chain (CKC). The study found that many PA and SF attacks used HTTP, DNS, and additional standard network protocols for communication. Hackers utilized backdoors dependent on the protocols because they could not penetrate the server ( ). Therefore, farmers should undertake frequent password changeovers, information encryption, and software updates. However, quantum computing enables attackers to jam the SF network's radio frequency. The virtual assailants also misconfigured SF and PA reporting systems to reflect invalid farm management information. point out that the primary concern is the risk of cyber-attacks on the systems that rely on quantum computing. These attacks could compromise the integrity of the collected and analyzed data, leading to inaccurate results and potentially adverse consequences for farmers. Even more profound, according to , is the fact that quantum computing has made it much easier for hackers to break even the most complex encryption systems within a very short time. The links of such security threats with quantum computing make many stakeholders in the agricultural sector cautious about adopting it, hence its slow progress in the industry. Any disruption to the hardware and software used in quantum computing could result in significant downtime for smart agriculture systems, potentially costing farmers valuable time and money. According to , such analytical capabilities will play an essential role in establishing nutrient deficiencies with greater accuracy and speed, which will further enhance the ability of farmers to make informed decisions on actions to take. This argument is supported by , who point out that the ability to measure and analyze soil pH, moisture content, and nutrient levels in real time will allow farmers to optimize fertilizer application and improve crop yields. However, there needs to be more certainty about when such capabilities will be possible. Quantum computing combined with soil moisture sensors is practical in soil quality analysis because it monitors and displays water percentages available for plants. The sensor transmits the data to the Raspberry Pi and uploads it to the cloud. The quantum computer collects data continuously and determines the crop suitable for a specific soil type ( ). The farmers can link quantum computers to a time domain reflectometry sensor (TDR) to get soil quality output in the form of waves and utilize it to assess the average moisture content. With the threat to the climate caused by environmental degradation, it is necessary to ensure all activities, including agriculture, are carried out in a manner that is not damaging to the environment. From the evidence presented in research like as , and Ramachandran et al. (2022) that combining IoT, AI, and big data with quantum computing will lead to the optimization of farming practices which will lead to better use of resources in farms. More specifically, using IoT sensors, confirms that capabilities such as real-time monitoring of moisture levels in the soil will be possible. Such data can be used in quantum algorithms to determine the optimal amount of water to use in irrigation. Such systems are necessary for excess water to be used in farming. In that regard, combining quantum computing, AI, IoT, and big data could improve water sustainability in the future. Similar outcomes should be expected when using pesticides and fertilizers, as established in the studies by and . Additionally, the use of quantum computing in the formulation of livestock feeds is projected in studies, such as , and , to reduce methane emissions from livestock rearing. This will reduce the overall greenhouse gas emissions from the agricultural sector, contributing to overall sustainability. Therefore, the findings of this research confirm that quantum computing plays a significant role in ensuring the environment is effectively preserved for future generations. The first limitation of this study is associated with the use of secondary data. While the survey has extensively reviewed existing literature to understand the potential areas of application of quantum computing in smart agriculture to increase crop yield, it would benefit more from first-hand perspectives from industry experts, developers of this technology, and farmers. Moreover, biases associated with the methods employed in each reviewed study may have been transferred to this study. A systematic evaluation of the quality of evidence could have helped inform the conclusions drawn in this study. However, given the nascency of research into this area, limiting the reviewed sources to only primary literature may not have been feasible. Nonetheless, the researcher ensured the reviewed sources were sourced from credible sources to determine transferring biases to this research. Further research is recommended to enhance our understanding and implementation of quantum computing in smart agriculture. Firstly, there is a need to focus on developing specialized quantum algorithms explicitly tailored for agricultural applications. This research should explore the design of algorithms for crop yield optimization, pest and disease prediction, climate modeling, and resource allocation in agriculture, leveraging the unique capabilities of quantum computing. Quantum computing ensures adequate resource reallocation by identifying farm areas that need more or less fertilizer, water, or any nutrients essential for the plants. The technology optimizes the utilization of resources and promotes crop yields. The AI-driven quantum machine collects and analyses images to detect patterns in the farm. Furthermore, the AI algorithm and quantum computing automate crop monitoring and soil analysis. Additionally, attention should be given to the design of quantum computing hardware specifically suited for agricultural contexts. Research efforts should investigate architectures and technologies that address the scalability, stability, and error correction challenges faced by quantum systems in agriculture. Developing robust and efficient hardware solutions will be crucial for practical implementation. Field testing and validation are essential to validate the real-world impact of quantum computing solutions. Collaborative efforts with farmers and agricultural organizations should be undertaken to implement quantum-based technologies on farms and assess their effectiveness in improving crop productivity, resource efficiency, and overall sustainability. This empirical research will provide valuable insights into the practical implications and benefits of quantum computing in agriculture. Another critical research area is integrating and analyzing diverse agricultural data sources within quantum computing frameworks. Researchers should investigate methods to effectively process and extract meaningful insights from large-scale agricultural datasets, including sensor data, satellite imagery, historical records, and real-time environmental data. Developing techniques to analyze and utilize this data using quantum algorithms will be crucial for informed agricultural decision-making. Such research should examine the cost-effectiveness, sustainability, and long-term implications of implementing quantum-based solutions in different farming systems and regions. This analysis will provide a clearer understanding of the economic and environmental viability of quantum computing in agriculture. Lastly, researchers should investigate the implications of integrating quantum computing in agriculture regarding ethics, legality, or social. This research should address concerns related to privacy, data ownership and control, equity, and potential impacts on rural communities and farming practices. Understanding and addressing these ethical and social implications will be vital for the responsible and equitable deployment of quantum computing technologies in the agricultural sector. By pursuing further research in these areas, we can deepen our knowledge and unlock the full potential of quantum computing in smart agriculture. These recommendations provide a roadmap for advancing the field and contribute to sustainable and efficient farming practices. The application of quantum computing principles in smart agriculture is currently being researched. This research aims to investigate the potential of quantum computing to improve the accuracy and efficiency of agricultural practices. This study's implications highlight quantum technologies' substantial influence on regulated farming practices, output optimization, and high crop yields. Farmers can comprehensively enhance their farming practices by leveraging quantum computing methodologies and optimizing their yields and output efficiency. The application of these cutting-edge quantum technologies has sparked a renewed interest in the agricultural industry, generating fierce discussion and debate surrounding the potential benefits and implications of their widespread application. These findings, therefore, provide a solid foundation for further research and exploration of quantum computing in the innovative agriculture landscape. Quantum computing has been identified as a valuable asset in several industries, including supply chain management, transportation, and finance. Upon analyzing essential resources, it is clear that the optimization capabilities of quantum computing can have a positive effect on a wide range of sectors. Moreover, quantum machine learning algorithms have proven proficient in processing extensive data and identifying intricate patterns. With this increased ability to handle large amounts of complex information, quantum machine learning has become a powerful tool for businesses seeking advancement in the current technologically driven world. Quantum computing offers tremendous potential in agriculture, particularly in optimizing crop productivity: precision seed management, plant and soil health monitoring, and reduced labor requirements. The ability to leverage digital twins in controlled environment agriculture provides an opportunity to integrate real-time data and simulations to enhance decision-making, ultimately improving overall crop yield as quantum computing has the potential to revolutionize smart agriculture; some challenges, such as specialized expertise and limited technology, need to be addressed. Additional research and development are required to fully realize the benefits of quantum computing in agriculture. Quantum computing can transform agriculture by optimizing resource allocation and enabling better decision-making. Combining quantum computing with AI, IoT, and big data can enhance productivity, promote sustainable farming, and contribute to global food security. As the field continues to evolve, further exploration and implementation of quantum computing technologies in agriculture will be necessary to unlock its full potential and promote a more sustainable and productive future. The research recommends the critical need to research the environmental and human-based factors that significantly affect the effective installation of smart agriculture technologies such as hydroponics and aeroponics. Soil quality analysis and review of the settings under which the installation is done require critical research and survey processes to understand the farms' natural characteristics. While different farms are characterized by varying soil qualities, topography, and climatic regions, the current research recommends considering these underlying factors to ensure the correspondence and relevance of the smart computing technologies installed within the respective farms. Writing – review & editing, Writing – original draft, Supervision, Resources, Project administration, Methodology, Investigation, Formal analysis, Data curation, Conceptualization. Writing – review & editing. Investigation. Writing – review & editing, Supervision. Supervision.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Used\n",
      "* **AIモデルの判断:** Not Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 127 / DOI: 10.1016/j.quascirev.2023.108368  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Last Interglacial sea-level proxies in the western Mediterranean\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Holocene vertical velocity fields in the calabrian arc (southern Italy): New insights from uplifted sea-level markers in the Crotone Peninsula\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "The Calabrian Arc defines the upper plate of the Ionian Subduction System. In this geodynamic setting, the Ionian oceanic lithosphere is underthrusted beneath the Calabrian continental block and the Tyrrhenian microplate (e.g., ) ( ). Evidence of such dynamics is a Wadati-Benioff plane dipping down to 400 km and an active volcanic arc in correspondence with the 150 km isobath of the subducting slab. From a tectonic point of view, the arc is now experiencing uplift as illustrated by instrumental records ( ; ; ). Uplift is thought to have started since Serravallian-lower Tortonian (13.8–11 Ma) times, as witnessed by the age of the highest marine deposits in the Sila Massif ( ; ; ISPRA, CARG 561-San Giovanni in Fiore) and continued through the Pleistocene, producing the well-known flights of marine terraces along much of the coast ( ; ; ; ; ; ; ). Fossil sea floors from the earliest Pleistocene can now be found up to 1300 m elevation ( ), but to constrain uplift on a shorter period, the elevation of the MIS 5.5 marine terrace, representing the Last Interglacial Maximum (≈125 ka), is often considered (e.g., ; ). Deposits of this age, related to a sea-level ≈8 m higher than the current one ( ), can now be located at elevations of up to tens of meters ( and references therein) ( ). The highest elevation for a MIS5.5 marine terrace occurs in the Strait of Messina, where the inner edge of the MIS 5.5 terrace reaches an elevation up to 175 m. At Capo Vaticano, the position of the MIS 5.5 marine terrace is debated. According to , , and , it is around 90–120 m in the SW sector and 48–60 m near Briatico and Vibo Marina. In contrast, , , believe that the elevation of the MIS 5.5 marine terrace varies even between 216 and 285 m. In other areas of Calabria, the MIS 5.5 marine terrace elevation is lower: in the Tyrrhenian coastal area near Gioia Tauro, it is about 65 m ( ) while it is only 9–12 m at Diamante ( ). On the Ionian coast between Locri and Catanzaro, it has been tentatively placed at 92 m through stratigraphic correlation with a section at Reggio Calabria ( ; ). This value would agree with the 100 m terrace stratigraphically dated to MIS 5.5 at San Leonardo di Cutro on the Crotone Peninsula ( ; ). Similarly, in the Gulf of Sibari, deposits related to the last interglacial have been dated by aminostratigraphy at an elevation of 114 m ( ). Vertical uplift rates close to or greater than 1 mm/yr are thus generally achieved throughout much of the Calabria region. However, Upper Pleistocene stratigraphic markers do not provide accurate information on the current state of the vertical velocity field of an area. For this purpose, Holocene sea-level markers, in such active tectonic frames, are commonly searched. They can reveal high-frequency fluctuations in the long-term trend but also, they represent a powerful way to understand mechanisms and dynamics of vertical movements of the Earth's surface. Unfortunately, Holocene sea-level markers in Calabria are limited to a few sites. This is mainly due to the physiography of the Calabrian Peninsula. Generally, the presence of a sandy coast prevents the preservation of ancient coastlines through time. Only on the Tyrrhenian side, between the Messina Strait and Capo Vaticano, where rocky coasts are more common, have a significant number of Holocene sea-level markers been preserved and investigated. reported Holocene uplift rates between 1.6 and 2.7 mm/yr and between 1.2 and 1.6 mm/yr in two Tyrrhenian-side Calabrian localities (4 & 7 in ). reported new evidence of raised Holocene coastlines from the Calabrian Messina Strait (5 & 6 & 8 in ). From those sites, sea-level markers between ≈2 and ≈5 ka were related to coastlines currently between 1 and 3 m in elevation. The authors interpreted their position as the dual result of recurring coseismic displacements and periods of steady uplift. In all these sites, the curve of sea-level rise predicted by the glacio-isostatic adjustment (GIA) model of was used to account for paleo sea level. In addition, evidence of raised Holocene coastlines from Capo Vaticano (2 & 3, ) were described by , by considering bio-geomorphological sea level markers. They identified four discrete levels between ≈0.7 and ≈ 2.3 m above mean sea level (a.m.s.l.) dated between ≈2 and ≈6 ka. The abrupt change between each level was considered to have occurred coseismically. In contrast, for the northern side of Capo Vaticano (Site 1 in ), supposed steady uplift in the last 1800 yr BP. Based on archaeological remains of structures with a clear relationship with the sea level, namely a fish tank and a breakwater, they proposed a continuous uplift rate of 0.65 mm/yr. Both the papers refer to the curve for paleo sea-level correction. On the Ionian side of Calabria, the only evidence of Holocene uplift is reported by , by considering bio-geomorphological sea-level markers. Along the beach of Soverato (near Copanello in ), these authors found a beachrock reaching a maximum elevation of 1–1.5 m. In the Crotone Peninsula, around Le Castella village (Site 9 × in ), the authors reported an abrasion platform at 1 m a.m.s.l. However, datable material was lacking in both these localities. The only radiocarbon dated sample is from the bay of “Spiagge Rosse” (Site 10 × in ), where an emerged beachrock can be observed up to 1.0 ± 0.2 m a.m.s.l. A sample of calcareous algae yielded an age of 2990 ± 60 yr BP (not calibrated), providing the means to calculate the only Holocene uplift rate value along the entire Ionian coast of Calabria. In this context, it is clear that the Holocene uplift history of the Ionian coast of Calabria is poorly understood, even if the region has been suggested to be a potentially suitable place to search for Holocene evidence of uplift ( ; ; ). In this study, the Ionian coasts of Calabria between the village of Copanello, to the south, and the promontory of Capo Colonna, to the north, were explored for Holocene sea level markers ( ). Field observations confirmed numerous traces of past sea level, at elevations well above the current mean sea level (MSL), several of which contained appropriate material for dating. Following corrections for paleo sea level, we consider this new suite of Holocene uplift constraints, together with constraints from other locations in Calabria and longer-term uplift rates, to evaluate previously posed hypotheses on the uplift mechanisms. The Crotone Peninsula is the easternmost point of the Calabria region. Outcropping units belong to the onshore portion of the Crotone Basin. It extends offshore for several kilometres and it is interpreted to be a forearc basin, developed between the Ionian accretionary complex and the Calabrian terrain ( ; ; ). Sediment deposition within the basin started since the Serravallian (13.82–11.63 Ma) ( ) and continues currently offshore. Two main lithologies outcrop in the Crotone Peninsula: a marly-clay unit known as the “argilla marnosa di Cutro” (marly clay of Cutro) and the younger Pleistocene calcarenites, including the Sant’Anna Lake (Chibanian), Soverito (Tyrrhenian), and Capo Cimiti (upper Pleistocene) synthems (ISPRA CARG project 577-Isola di Capo Rizzuto and 571-Crotone). The “argilla marnosa di Cutro” is a marine unit representing a deep-sea environment, dated to the Upper Pliocene (Piacenzian) - Lower Pleistocene (middle Calabrian) ( ). It widely crops out in the study area, representing the substrate of the Middle Pleistocene and Upper Pleistocene synthems. These latter deposits overlie the “argilla marnosa di Cutro” through a sharp angular unconformity. Furthermore, because the Middle-Upper Pleistocene deposits are much less erodible than the underlying clay, they form a classic caprock-like morphology. When this caprock is dismembered as result of erosion, the “argilla marnosa di Cutro” crops out, typically hosting large fields of badlands. The Middle-Upper Pleistocene deposits, mainly consisting of calcarenites, form the prominent step-like profile of marine terraces recognizable along the entire Crotone Peninsula. The main lithology of the Middle-Upper Pleistocene synthems consists of a brownish calcarenite that shows high-energy shallow-water sedimentary structures, like cross and hummocky stratification. Bedding is usually sub-horizontal, but meso-scale structures can be observed. Sometimes, a mollusk-rich bio-facies characterizes the calcarenites, even though in some places they are barren. Some Middle-Upper Pleistocene terraced units are made of reef carbonates, consisting of calcareous algae, serpulids, bryozoa, mollusks shells, and corals. Given these features, the depositional system of the Pleistocene terraced units can be commonly ascribed to a shallow water environment, within the shoreface water-depth ( ). Besides these Quaternary deposits, the coastal area of the Crotone Peninsula hosts some younger deposits. For marine deposits, younger ages can be inferred based on their elevation, which is lower than the lowest Pleistocene marine terrace in the area (MIS 3 at 9 m, see Sec. ). For example, along “Spiaggia dei Gigli” (GIG in ), low-elevation, partially stabilized sands are reported as Holocene dune deposits (ISPRA, CARG 577-Isola di Capo Rizzuto) ( ). Terraced Middle-Upper Pleistocene units form extensive surfaces gently dipping toward the sea. These represent fossil sea floors, and they should be laterally continuous to a wide extent. However, intense disruption of such marine surfaces is often observed in the study area, mainly due to the effects of high-angle, WSW-ENE striking normal faults ( ; ISPRA, CARG 577-Isola di Capo Rizzuto). Before the extensional regime, compressional deformation occurred in the study area. The last compressional phase in the Crotone Peninsula is recorded by gentle folding in the Piacenzian-middle Calabrian “argilla marnosa di Cutro” ( ). According to and , seismic lines and bathymetric data offshore the Crotone Peninsula reveal other evidence of compressional tectonics. There, a bathymetric relief, known as Crotone Swell, overlies a sub-horizontal reflector emerging at the base of the swell. The classic interpretation of the Crotone Swell is a hanging-wall of an ESE-verging thrust. However, according to a new interpretation, the thrust at the base of the swell, as well as the onshore normal faults affecting the marine terraces, are attributable to a megalandslide ( ). Five levels of Pleistocene terraces can be observed in the Crotone Peninsula ( ). Numerous dating techniques have been applied in the Crotone Peninsula to define the age of the different marine terraces and to explore the interaction between Pleistocene sea levels and vertical movements. A summary of the literature data on elevation, the chronological constraints, and the uplift rates of these marine terraces is reported in . In the Crotone Peninsula, the first (highest) terrace level is the most extensive. Its elevation is in the range of 161 ± 9 m a.s.l. ( ), but the inner edge reaches almost 200 m outside the study area. The age of this marine terrace has been associated to MIS 7-MIS 7.5 (Belluomini et al., 1988; ; ), although recent IRSL datings do not exclude ages as old as MIS 9 ( ). The second marine terrace level has its maximum elevation in the range of 88 ± 7 m a.s.l. ( ). A lower elevation is reached in the central sector of the Crotone Peninsula, where normal faults downthrow the terrace. The age of the second terrace level has been constrained by several dating methods. The presence of Senegalese fauna in the marine deposits of this terrace points to an age referable to MIS 5.5 ( ; ). This age is also supported by OSL dating of 116 ± 10 ka ( ). The MIS 5.5 age is further corroborated by recent infrared-stimulated luminescence (IRSL) dating 125 ± 7 ( ). The third marine terrace level has an elevation mostly between 30 and 40 m, but it reaches up to 50 m NW of Le Castella. In some places, the lateral discontinuity of this terrace is the result of normal faulting ( ). Assigning an age to the third terrace level is tricky. According to aminostratigraphy and stratigraphic correlation, it was initially thought to be referable to MIS 5.3 (Belluomini et al., 1987). However, none of the subsequent datings fall within the range of this isotopic stage. Instead, a sample dated with IRSL to 83 ± 4 ka links the lower portion of the terrace to MIS 5.1 ( ). Since a series of overlapping surfaces within the same terrace are geomorphologically recognizable (to the west of locality, ), another interpretation is possible. According to , and , this terrace was formed between MIS 5.3 and MIS 5.1. The different recognizable terrace edges would thus be the remnants of relative sea level fluctuations within this time interval. The fourth marine terrace level occurs along the Capo Rizzuto promontory and in the Capo Colonna area. The elevation of this marine terrace is lowest at Capo Rizzuto (22 ± 4 m), while it reaches 33 ± 10 m at Capo Colonna. Recent dating with the IRSL method to 84 ± 9 ka associates the terrace to MIS 5.1 ( ). The fifth marine terrace level occurs in the Le Castella area at 10 ± 5 m elevation. first dated these deposits using optical stimulated luminescence (OSL). Three samples returned an age of approximately 45–50 ka, which would associate the terrace with MIS 3. More recently, dated the same terrace with OSL to an age of 64 ± 3 ka, confirming the same isotopic stage suggested by . Recently, terraced marine deposits in the area of the Strait of Messina have also been referred to MIS 3 ( ). Two field campaigns were performed in the study area, in April and September 2021. April campaign allowed us to perform accurate observations given the special low tide conditions (−42 cm a.m.s.l.), while most of the sampling for radiocarbon dating was performed in September. Elevation measurements were made mechanically with a tape measure. Uncertainty can be evaluated in the range of ±0.2 m. Each sea-level marker was measured with respect to the current mean sea level (MSL). The position of the latter was identifiable considering the maximum curvature of the active notch and the living algal cover at the same level. When the considered section shows no active notch, the measurement is taken from the sea level at that time. After that, the measured sea level is corrected with respect to the elevation of the currently active notch at the location closest to the studied section. Several types of bio-geomorphological sea-level markers have been used to evaluate the paleo shoreline elevation. Erosional forms such as notches, marine sediments, and also suspended fossil riverbeds provided accurate indications of paleo coastline position, even though they were unable to provide age constraints. Dated samples were therefore collected from marine deposits related to these geomorphological features. AMS radiocarbon dating was performed on carbonate material by Beta Analytic, by using C techniques. Conventional ages were calibrated to the MARINE20 calibration curve and a local reservoir effect correction was applied. A value of ΔR = −26 ± 70 was used using the average of the four nearest reservoir effect estimates ( ). The value adopted clearly refers to calibration on the new MARINE20 curve. Calibration was performed using the MATLAB function “MatCal” ( ). We created a database collecting all Holocene sea level markers from Calabria. The ages were calibrated with the same global curve and local reservoir effect according to the parameters clarified above. The new calibration was not possible, however, for the samples dated by , because the original paper does not report the “conventional” radiocarbon age, which is necessary to make the calibration. The datings in the original paper are reported as calibrated, but it is not clear which curve was used. Thus, one must consider that the ages of these samples may not be perfectly comparable with those of the rest of the database. We presume that any shift in age is a few tens or hundreds of years, at most. In the database, and in this paper, we use the term “sea-level marker” as a constraint of the relative sea level (RSL) in space and time, with a clear relation to the ancient paleo-mean sea level (MSL). The second part of the definition is related to the fact that a sea-level marker may accurately represent the coastline at its time (Sea Level - SL), or it may not have this accuracy. Sometimes “sea-level markers” define a lower limit (Lower Level - LL) or an upper limit (Upper Level - UL). This concept is also critical in the interpretation of the results in terms of uplift rate. An uplift rate obtained from a LL will be a minimum estimate, while that obtained from a UL will be a maximum estimate. The vertical velocity obtained from a SL marker, on the other hand, will be the most accurate. This terminology is also associated with a color code that is consistent throughout the next figures. So, the sea level markers are coloured as follows in the graphs: SL = green, LL = blue, UL = red. We then define the term “site” as a series of sea-level markers that refer to the same coordinates. This can happen when multiple sea-level markers are found in the same vertical section. But it can also happen when the coordinate resolution cannot discern individual sea-level markers. For example, the GIG site in the database includes 10 sea-level markers. Some of them are in the same vertical section, others are too close each other so that they can be described by the same coordinates. However, the main feature of the site is related to the RSL curves produced by the GIA models. Since the site is defined by geographic coordinates, it means that a site is associated with a unique RSL curve. Glacial isostatic adjustment (GIA) models are used to estimate the difference of the sea level between the current one and that at the paleo shoreline age. They depend on several inputs, primarily including the viscoelastic structure of the Earth and the deglaciation history of the polar ice caps since the last glacial maximum. We applied two widely used GIA models: ICE-6G (VM5a) and ICE-7G (VM7). Relative sea level curves calculated with these models were obtained through the open-source SELEN 4.0 software ( ). The reliability of the curves produced by using the SELEN 4.0 freeware is ensured by tests on both real data (e.g., ; ) and synthetic data ( ). Both models showed excellent performance in approximating stable coastlines of the Western Mediterranean ( ; ) and in the Eastern Mediterranean ( ). We also considered the ANU curve in the comparison of vertical velocities of other sectors of Calabria, because all previous works used the ANU model (e.g., ; ; ). The main difference between the ICE-xG and ANU curves is that the former has much smaller RSL rising rates in the late Holocene than the ANU model. This difference is due to the different deglaciation history between the two models but also to a different lower mantle viscosity value ( ; ). In essence, the ICE-xG curves are higher than those from the ANU model (see Sec. ) and therefore the uplift rates calculated based on the latter are always bigger. It is worth noting that recent findings on stable coastlines indicate that ICE-xG models outperform the ANU model (Melis et al., 2018; ). Using the results of this model to interpret vertical tectonics may lead to erroneous conclusions. We modelled the vertical coseismic displacement of a fault in the Crotone area to test some of our hypotheses about the mechanism of the uplift. We used the model for finite deformations from a rectangular source in a homogeneous elastic half-space. Our results come from a simple solver of the Okada equation ( ) in MATLAB (Beauducel, 2022). The code provides results on a simple grid of Cartesian axes; our only modification was to show them on geographic axes. The tectonic model requires nine fault parameters as input: strike, dip angle, length, amplitude, depth of hypocenter, geographic coordinates of the fault center, total slip, rake, and opening angle (zero, in case of faults). Some of these parameters were identified using the Database of the Italian Seismogenetic Sources (DISS) (DISS 3.3.0 - DISS Working Group, 2021). Specifically, the location, geometry (strike and dip angle), fault length and width were identified in this manner. As for slip, this must be scaled to the size of the fault. For this purpose, we used the relations of . According to empirical equations, the moment magnitude (Mw) is related to both the mean slip [S(m), Eq. and Eq. ] and the total rupture area [RA (km ), Eq. ]. Arranging Eq. and Eq. gives the relationship between average slip and rupture area in Eq. . Traces of past sea levels were found on three of the five investigated sites ( ), where carbonate samples were collected for radiocarbon dating. Starting from south, and moving north-eastward ( ), we present below a description of the investigated sites. Further details can be found in in the Supplementary Material. Spiaggia dei Gigli is a long sandy beach between Le Castella and Capo Rizzuto ( A). Numerous lines of evidence of a relative sea level higher than the present one has been observed in this locality. Along the eastern sector of the beach, there are large blocks of Upper Pleistocene calcarenite, fallen down from the cliff, lying and embedded in the current beach sediment ( ). These blocks show a wave notch about 1 m above the current MSL. The lateral continuity of the fossil notch is extensive. Often, it can be found on all sides of the blocks, forming notch-mushroom morphologies ( C–D). The vertical width of the notch reaches more than 70 cm, while the depth is about 30 cm. Moreover, the notch is in some places associated with a wide abrasion platform, which can reach more than 3 m in horizontal width ( B). In addition to the notch, there are also sandy marine deposits that are clearly referable to a RSL higher than the current sea-level. These deposits were studied through dedicated sections crossing the beach zone ( A). In Section ( , see for location), there is a cemented sand bank in the innermost part of the beach. It contains numerous shells or fragments of cardites, mainly sp. These mollusks have often been used as Holocene and Pleistocene sea-level markers, since they live in water depths between 0 and -2 m in brackish marginal-marine lagoons ( ; and references therein; ). Moreover, the organization of these sediments in thin layers allows us to exclude that the deposition occurred by storm events or other high energy processes. On top of this marginal-marine deposits there is a cemented sandy layer rich in Helicidae remains pointing to a terrestrial environment. Moving up section, cross-bedding structures characterize these terrestrial deposits, which are referable to aeolian sands as dune cords (aeolianites). Finally, up section, loose sandy sediment of the active dune covers these aeolian fossil dunes. The highest sp. In the lagoonal marginal-marine deposits was sampled as GIG 1 for radiocarbon dating. Its elevation, between 2.1 and 2.5 m a.m.s.l. Represents a lower constraint for the shoreline (which must have been above that level). The resulting age is 7099–6621 cal Yr BP. In the aeolianites, on the other hand, a sample of Helicidae, taken as GIG 2 at 2.5–2.9 m a.m.s.l. Yielded an age of 2698–2578 cal Yr BP. In Section 2 ( ), a mushroom-like notch is developed at 1.0 ± 0.2 m a.m.s.l., around an Upper Pleistocene calcarenite block. At the base of the notch, covered by the modern sandy sediment, there is a well-cemented sandy deposit, organized in cm-thick strata. The lower portion is a cobble conglomerate of calcarenites and sp. Shells. Given the distribution of the sediment, it is likely that it was the deposit related to the same sea level as the notch. A shell of sp. Was sampled as GIG 3a (3237-2769 cal Yr BP). We also sampled a calcareous encrustation made of algae and serpulids as GIG3b. It was attached on the notch, just below the maximum curvature ( ). It yields an age of 2610–2090 cal Yr BP. Section ( ) shows one more notch of the same 0.9–1.1 m a.m.s.l. Coastline, which produces a mushroom-shaped block. It is sorrounded by a consolidated marine deposit made by large pebbles and remains of large mollusc shells. Some lithodome boreholes can be observed on it. The radiocarbon C dating of a pectinide fragment (GIG 7a) provided an age of 7173–6715 cal Yr BP. In a niche on the notch, a calcareous algal encrustation was taken (sample GIG 7 b) and dated to 4841-4385 cal Yr BP. Section ( ) shows similarities with Section 1. In this section, a well cemented unit made of conglomerate, rich in sp. Shells, crops out in the beach zone, just below the modern beach deposits. Close to the current sea level, the stratification slightly dips seaward by a few degrees. It becomes sub-horizontal as it approaches the dune zone. The bedding and sedimentological characteristics of the conglomerate are typical of a foreshore and beach environment. Its distribution at the base of the notches also supports its proximity to palaeo sea level. However, the presence of sp. Allows us to hypothesise the presence of a lagoon environment close to and probably connected with the sea. Going upsection, the conglomerate layers give way to weakly cemented sands, similar to the sandy unit of Section . Thin layers of fairly cemented sand host fragments of sp. Up to a maximum elevation of 2.5 m. Above that, sedimentary structures associated with an aeolian environment become prevalent and the first fossils of terrestrial gastropods appear at 3.3 m elevation. GIG8a and GIG8b are samples of the uppermost sp. Shell and the lowest continental gastropod (Helicidae), respectively. Their ages are 7058–6579 cal Yr BP for the former and 5317-5051 cal Yr BP for the latter. In the same gulf between Le Castella and Capo Rizzuto, a little to the east of Spiaggia dei Gigli, Spiaggia di Selene is developed at the mouth of a stream. Its riverbed is directly dug into the “argilla marnosa di Cutro”. The equilibrium of the watercourse with its current base level is evidenced by the small alluvial plain that develops around the stream. This small alluvial plain is located at sea-level and is overlain by loose deposits of beach sand and coastal dune. At approximately 1.8 m a.m.s.l., a consolidated sandy unit overlies, through a sharp angular unconformity, the “argilla marnosa di Cutro”. These consolidated sands exhibit structures typical of aeolian transport and are completely barren of fossils. However, their lithostratigraphic position, the lithological characteristics, and the proximity to the Spiaggia dei Gigli, allow us to correlate this deposit to the terrestrial sandy unit that we were able to date through samples GIG 2 and GIG 8a. The unconformity between the “argilla marnosa di Cutro” and the overlying aeolianites clearly traces a paleo-riverbed like the currently active one. The fossil riverbed is in disequilibrium with the current base level, as it is cut by the currently active river channels ( ). This is the site already described by . Along Spiagge Rosse beach ( ), a conglomerate beachrock crops out from the current sandy sediment. The beachrock is formed by several slabs dipping towards the sea with inclinations between 8° and 11°. The lithology of the deposit is variable ( ). At the base of the slabs there are conglomerate facies with large pebbles (10–30 cm) and many remains of large molluscs. In the upper part, on the other hand, a calcarenitic sediment prevails. The boundary between the two facies is not always clear. Often, they can be found interfingered with each other in patterns that are not easy to schematize. There are also laminated carbonate concretions that cover the top of the beachrock and some cavities inside it ( ). These consist entirely of calcareous algae and serpulids, forming small carbonate mounds or coating the rock surface. The sample dated by was taken from one of these features. We also dated these encrustations (sample SPR2, 4251-3735 cal Yr BP). We also collected SPR1, a shell of sp. From the highest point of the beachrock (1.3 ± 0.2 m a.m.s.l.), which yielded an age of 3020–2542 cal Yr BP ( ). Beachrocks are known to have a submerged part and an emerged part. In the next site (Spiagge di Curmo, Section 4.1.5), for example, we studied a currently active beachrock that reaches up to 60 cm a.m.s.l. We believe that the portion of beachrock sampled at Spiagge Rosse is the submerged portion, as it is commonly covered by algal and serpulids encrustations that can only live below the MSL. Spiaggia di Curmo is developed at the foot of a cliff formed by Upper Pleistocene calcarenites resting on top of the clays of the “argilla marnosa di Cutro”. This stratigraphic setting has important hydrogeological implications, as it determines the formation of a freshwater aquifer within the Upper Pleistocene calcarenites. In the Spiaggia di Curmo, freshwater comes out from the calcarenites at the base of the cliff, then crosses the beach zone flowing into the sea. The mixing of freshwater, rich in calcium carbonate, and seawater within the beach sediment promotes the precipitation of calcite in the pores of the sediment and the formation of beachrocks (e.g., ; ). Along the beach of Curmo, in correspondence with freshwater flows, it is possible to observe an active beachrock, whose elevation does not exceed 60 cm a.m.s.l. ( ). The grain size, detrital grain composition, their colour, and especially the mollusk remains of the beachrock are identical to that of the modern unconsolidated beach sands. Both the beachrock and the beach sand are rich in and shells. The C dating of a shell (sample CUR1) from the beachrock, returning a Post 1950 age, confirms the observation we previously made on the beachrock and the current beach sands. In addition to sea-level markers developing at the current sea-level, other markers related to a higher RSL were found on the same beach. A well-cemented cliff breccia and conglomerates, made by meter-scale calcarenite blocks, with a sandy or coarser matrix rich in marine shells ( ), were found on the Curmo beach. One of the calcarenite blocks retains traces of a notch at about 1.2–1.3 m a.m.s.l. At the notch elevation, a trochid shell (CUR2, see A) was collected from the conglomerate. It returned a C age of 3755–3282 cal Yr BP. Slightly lower than the notch at 1.2–1.3 m a.m.s.l., attached to the conglomerate, calcareous encrustations made by algae and balanids were found ( B). A balanid shell (sample CUR3) returned a C age of 2600–2076 cal Yr BP. The presence of balanids itself is not an indication of sea level, as these organisms can also live in the emerged spray zone ( ). However, the association with the calcareous algae suggests that these organisms lived at or slightly below MSL. Along the same beach sector, marine conglomerate with a small abrasion platform was found at an elevation of up to 2.9 m ( C). Several shells of sp. (sample CUR5) were found on this abrasion platform. CUR5 yieldee a C age of 4677–4146 cal Yr BP. Although no dating was possible for this site, this locality provides the northernmost evidence on the Crotone Peninsula of a higher relative sea level ( ). Similar to Spiaggia di Selene, a suspended riverbed was found between 1.7 and 3.3 m higher than the currently active riverbed. Different from the Spiaggia di Selene site, in this locality, a 1.5 m thick fluvial deposit made by well-rounded pebbles unconformably overlies the “argilla marnosa di Cutro”. To define the uplift rates and the vertical movements of the Crotone Peninsula, ages of dated samples were plotted against RSL curves calculated with GIA models ( ). All samples fall above the RSL predicted by GIA models. This means that a tectonic uplift component must be responsible for the current position of the sea level markers considered in this work. The calculated uplift rate for each sea level marker is given in . In this section, the sea-level markers from other Calabria localities (see ) are compared with the corresponding RSL curves ( ). In all the localities we are comparing, the sea-level markers fall above the sea-level prediction of the GIA models. Only for the Briatico locality (BRI, ) there is no evidence of uplift. The details of the obtained vertical velocities can be found in and . The slower uplift rates in the younger markers could lead to the conclusion that the uplift rate of the Crotone Peninsula has decreased over time. But the slower uplift could be simply related to a mathematical issue concerning the calculation of the uplift rate. Because age is in the denominator in this calculation, the more the denominator decreases, more the sensitivity of the uplift rate to discrete uplift events increases. shows a graph for better understanding this concept. Suppose we have a series of sea level markers with ages between 100 and 10,000 yrs at similar elevation, about ±1 m one from each other. Suppose next that there is a discrete increase in uplift so that, within a short time (≪ age of the marker), all coastlines acquire the same increase in uplift. Recalculating the uplift rate after that event, one would get higher uplift rates, although the increase in uplift rate would be different, depending on the age of the considered coastline. Younger coastlines would acquire a much higher increase in uplift rate, while older coastlines would acquire a small or no increase in uplift rate. For example, considering a 1000-year-old coastline affected by a discrete uplift of 0.2 m, it would acquire 0.2 mm/yr of uplift rate. In contrast, a coastline with an age of >5–6 kyr, affected by the same increase in uplift, would experience either a little or no change in uplift rate. That is, younger shorelines are much more sensitive to uplift increase than shoreline a few thousand years older. The transition between stable and unstable uplift rates would appear to occur at around 5–8 kyr ( ). It could be said that uplift rates evaluated for shorelines younger than 5 ka are estimates on a short term, whereas shorelines older than 8 ka are more likely reflect the long term uplift rate. Therefore, in the Crotone Peninsula, where Holocene sea-level markers of 2–5 ka show lower uplift rates with respect to older markers, it could be assumed that shorelines of this age lack a certain amount of uplift. A future, discrete uplift event of ca. 50 cm affecting the Crotone Peninsula would balance of all the Holocene and even late Pleistocene uplift rates. By the same reasoning, the variability of the uplift rate in different parts of Calabria can be partly explained with the graph of . In this figure, the differences in uplift rates can be examined in space and time, where all the uplift rates are plotted with the corresponding age. Sea level markers are also coloured according to the distance from the Messina Strait (geodetic distance, in km). Areas within 20 km from the Messina Strait (dark blue, blue, and green dots in ) show uplift rates averaged over short time periods that are greater than those averaged over longer time periods. In contrast, the Capo Vaticano area shows a similar trend of time-averaged uplift as the Crotone Peninsula. Considering the sensitivity of the calculated uplift rate to the averaging time period, one could interpret these differences through a deficit or surplus of recent uplift. The coastlines of the Messina Strait may have acquired a discrete uplift pulse in recent times, so they are not yet rebalanced with the stable uplift rate value calculated over a longer term. The numerous active faults roughly parallel to the coast are ideal candidates for generating some amount of discrete uplift at the surface ( , ; ). The Capo Vaticano area would instead have a small uplift-rate deficit that could be filled by a future discrete uplift event. Some points show anomalous uplift velocities compared to the average of the area. The vertical velocity of samples D5b and D5c ( ) is offscale likely because storm waves or tsunamis ( ) have carried marine remains to elevations well above the paleo-MSL. point to the magnitude 6.3 earthquake of 853 AD as the responsible tsunamigenic event. Also, marker CV5 is well above the average for its area. The authors ( ) do not explain its anomalous elevation with respect the samples from the same coastline. In our interpretation, we cannot rule out that it may be either affected by carbon contamination, or its elevation may include a component of unrestored coseismic dislocation. Finally, BRI, the most recent sea level marker, brings with it some interesting information, as it has not been displaced from its original position. Its velocity, which is not exactly zero, is due solely to the prediction of the GIA model, which associates the age of the sample with a slightly lower RSL than the current one. This marker indicates that there was virtually no uplift in ca. 1.6 kyr in this area. Our result is therefore different from the value of 0.65 mm/yr obtained by . The difference is solely due to the adopted GIA model. The RSL curve adopted by was fitted with the ANU model ( ), which is much lower than the ICExG curves that we consider more reliable (See Section ). The sea level markers in can be organized into two main clusters. A first cluster includes the oldest sea level markers (GIG 6a, GIG 6 b, GIG 1, GIG 7a, GIG 8a), having an average age of 6815 cal Yr BP. Given their overlapping ages and similar elevations, these sea level markers could be related to a single paleo-coastline. We define this paleo-level as CL1. However, the precise elevation of this ancient shoreline cannot be precisely defined, as individual elevations range from 1.2 to 2.4 m. Moreover, apart from GIG 7a, the other markers do not precisely mark the sea level but rather a level that was, albeit slightly, below it. The age of GIG 7a, however, is possibly not reliable. Looking at , we note that the sample was taken from a deposit embedded in the notch at 1.0 ± 0.2 m elevation. That is, the deposit from which GIG 7a was derived must necessarily have been contemporary with or older than the notch. But this notch marks the coastline that we refer to the second cluster of sea-level markers, which we call CL2. This second cluster includes GIG 3 b, GIG 3a, CUR 3, SPR 1, SPR 0 and has an average age of 2615 cal Yr BP. We therefore believe that GIG 7a, which is a pectinid fragment in a marine conglomerate, was resedimented in a coastal environment referred to the second cluster of sea-level markers (CL2 coastline). However, a number of samples remain excluded from CL1 and CL2, as they are between these two main coastlines. Understanding the position of the RSL in between CL1 and CL2 is more complex, although there are several equally valid interpretations to explain them. The main issue concerns the sea level marker CUR 5 at 2.9 m. This sample would indicate that at 4279 cal Yr BP, the RSL must have been at least at that elevation. But at about the same age there is GIG 7 b, at a much lower elevation, compatible with CL2 coastline (1.0 ± 0.2 m). We rule out the possibility that this is the same coastline subsequently displaced, which would require a tectonic element capable of producing 1.9 m of throw in about 4.2 kyr. A fault with these characteristics would be a regional structure with a clear morphological expression that instead is not detectable either in the field or from digital elevation models. The age of at least one of the two sea level markers is therefore incorrect. CUR 5 is a Patella sp. Shell that could be affected by a resedimentation or contamination problem. For example, it could have arrived at that elevation with some storm waves. Or the age of GIG 7 b could be wrong: this, however, is an encrustation of calcareous algal and serpulid, sessile organisms that were taken directly from inside the notch of CL2. GIG 7 b therefore cannot have been resedimented, but may have been affected by carbon contamination. On the other hand, this encrustation should return an age like that of the CL2 group, i.e., 2615 cal Yr BP. The mismatch strengthens the contamination hypothesis, but it could also be that the CL2 notch at the age of GIG 7 b (4613 cal Yr BP) was already active. Since we cannot find a solution, we must necessarily work out two different hypotheses about the location of the intermediate coastline between CL1 and CL2. Each of these hypotheses provides different constraints on the shoreline position around 4–5 ka. Therefore, a different uplift path must be analyzed for each hypothesis. In , we have attempted to draw two of the infinite paths that can be defined by the constraints from CUR 5 and GIG 7 b. To make these reconstructions reasonable, however, it must also be considered that uplift occurred almost homogeneously throughout the study area. This limitation is compatible with the geology of the area and the arrangement of sea level markers: the markers are located within a few kilometers of each other, and no significant tectonic features were detected between their locations. The same CL2 coastline elevation at the three different localities (GIG, SPR and CUR) allows us to assume the absence of significant tilting or disarticulation of the uplifting block. Uplift path 1 (UP1 – ) - In this simulation, the information from CUR 5 is considered valid, not that from GIG 7 b. The first uplift constraint is given by the continental sample GIG 8 b at 5305 cal Yr BP. Between the age of CL1 (6789 cal Yr BP) and GIG 8 b, the sea level rose 2.1 m. But sample GIG 8 b was sedimented in the terrestrial environment and is only 1.5 m above CL1. Therefore, there must have been a minimum uplift of 0.6 m in the meantime. However, the uplift could have been even up to 1.6 m depending on the paleo-depth of GIG 6 b. This is the uplift constraint value obtained between 6789 and 5205 cal Yr BP; above this uplift value, the vertical distance between GIG 8 b and the other sea-level markers would no longer be respected. The second uplift constraint is CUR 5 at age 4278 cal Yr BP. This sample was supposed to be at or just below sea level at its age. Assuming it was at sea level, we would have at least 0.6 m of uplift during this second time interval (5205-4768 cal Yr BP). Then, at 3003 cal Yr BP, the activation of the CL2 coastline constrained by sample GIG 3a. GIG 3a is located 2.3 m vertically below GIG 8 b. For the distances of the sea-level markers to be respected, and considering the sea-level rise of 0.5 m between CUR 5 and GIG 3a, we have to assume an uplift of 2.8 m between 4268 and 3003 cal Yr BP. The sea level remains stable in CL2 until 2300 cal Yr BP. Then a rapid uplift event of about 1.1 m may have occurred, leading to the abandonment of CL2 (details of this will be discussed later). We thus arrive at the current arrangement of sea-level markers. According to this reconstruction, in the period between CUR 5 and GIG 3a, the RSL was between 2.9 and 1 m a.m.s.l. This reconstruction is consistent with constraints for the sea-level markers CUR 2 and SPR 2, since sea level must have been at their elevation or slightly higher. Uplift path 2 (UP2 – ) - In this simulation, the information from GIG 7 b is considered valid, but not that from CUR 5. The uplift path remained unchanged until 4613 cal Yr BP. At that point, RSL was established at CL2. This coastline would remain active from 4613 to 2300 cal Yr BP, even though the sea level rose by about 0.8 m during that time period. It could be that a small amount of uplift kept the CL2 shoreline active, but it could also be that the notch migrated vertically along with sea level. This hypothesis cannot be dismissed, as 0.8 m of sea level rise in 2300 years is a low rate, consistent with the rate of notch erosion ( ). In the period between GIG 7 b and GIG 3a, the RSL always remained at 1 m a.m.s.l. CUR 2 and SPR 2 are thus sea level markers of the CL2 coastline. Given the limited number of constraints, these reconstructed uplift histories are affected by many uncertainties. However, some solid points can be stated. One of the main problems in reconstructing the uplift path of an area concerns how to connect the constraints obtained from different sea-level markers. We could connect the different constraints with a straight line to indicate that the area was affected by a steady uplift. A different option is to assume that the total uplift of an area was achieved with a series of coseismic pulses, as might have been the case for the CL2 coastline. However, we cannot rule out periods of accelerated uplift rate, i.e., a hybrid solution between the two previous options. Unfortunately, the ecological characteristics of the Mediterranean do not provide such high resolution RSL tracers. Discontinuous, or even reverse, vertical velocities appear to be typical of various subduction arcs. For example, U–Th dating on corals in the Sumatra Arc allows for high temporal resolution of RSL variations. The paleo-sea level record reveals a strongly discontinuous uplift process, consisting of rapid coseismic uplift events and interseismic periods of weak subsidence ( ). However, the summation of vertical contributions over the long term leads to a general uplift of the sector. In any case, coseismic uplift events along the coasts of subduction arcs are common. For example, along the Chilean subduction zone, a co-seismic uplift of 1.8 m with a Mw 8.1 earthquake was recorded ( ). Coseismic uplift of at least 1 m affected the coastline of the Peninsula de Nicoya, within the northern Costa Rican forearc, during the M 7.7 Nicoya subduction earthquake of October 5, 1950 ( ). Subsequently, in the postseismic period, four decades of gradual subsidence reversed a significant portion of this uplift ( ). In the Mediterranean subduction zones, earthquakes on reverse faults also produced 0.7 m of coseismic uplift in Algeria (Mw 6.8, ), whereas in the Eastern Mediterranean (Crete subduction zone), the cosismic uplift due to the Mw 8.3 earthquake of 365 C E ( ) was as much as 9 m! U plift of fossil coastlines along the Tyrrhenian side of Calabria has been interpreted as due to a series of coseismic pulses as well ( ), driven by elastic rebound along the hanging walls of a series of coastal normal faults. In the case of Ionian Calabria, a co-seismic uplift mechanism would explain not only the short- and long-term difference in the uplift rates, but also the evidence for rapid abandonment of the CL2 coastline. It is less understood which source, if any, is capable for giving a such large displacement at the surface. The DISS seismogenic source database (DISS 3.3.0 - DISS Working Group (2021)) reports a reverse fault as potential active seismogenic source, offshore the Crotone Peninsula ( A). This structure is called the Crotone-Rossano Fault (CRF), and is a complex source formed by three segments with a slightly differing strikes ( A). Following the characteristics of the CRF and assuming different magnitudes, we modelled the vertical displacement field in the case of activation of such a structure. We used the model for finite deformation from a rectangular source in a homogeneous elastic half-space (see methods, Section ). The maximum possible magnitude, as reported by the catalogue ( ), is Mw 7.3. This earthquake would occur in case of a total rupture of the CRF, which shows dimensions of 80 × 9 km. In our model, the geometry of the fault has been simplified to a single rectangle of the above dimensions. For this reason, it must be considered that the deformation obtained in our model is geographically shifted from that which would be obtained in the real case of activation. Thus, our results are useful only for a qualitative evaluation of the deformation pattern at the surface. To have a more precise geographic location of the vertical deformation, the real geometry of the seismogenic source should be included in the model. Our model shows that an earthquake of Mw 7.3 sourced by a rectangular fault placed offshore the Crotone Peninsula can generate a maximum of 1.4 m of uplift ( B). If we considered the true complex geometry of CRF, this maximum displacement would probably be centered exactly on the coast of the Crotone Peninsula. Furthermore, it can be seen in C and that it is not necessary to activate the entire fault to have significant vertical deformation. Even with a Mw 7.0 to 7.1 earthquake, which would rupture between 47% and 63% of the entire CRF fault, there would be an uplift of 0.71–0.92 m ( ). Moreover, over longer time periods, the vertical deformation resulting from multiple high-magnitude earthquakes would be summed in the intersecting sectors. An earthquake of Mw 6.5, for example, would originate from a rupture of ≈20% of the CRF fault plane and would produce ≈30 cm of vertical displacement at the surface. However, two or three earthquakes of this magnitude, rupturing patches at different depths, could generate a larger uplift signal at the surface. Thus, a cluster of earthquakes of different magnitudes may also represent a source of uplift. If they occurred over a short time-span, the uplift may not be properly coseismic, but it can be considered quasi-impulsive. If a seismic event of Mw > 7 or a series of smaller events are capable of uplifting the Crotone Peninsula, it remains to be investigated whether or not earthquakes with these characteristics are present in the historical record of the Crotone Peninsula. The Crotone Peninsula has been inhabited for thousands of years, as evidenced by a great number of archaeological sites (i.e., Capo Colonna, Capo Piccolo, Le Castella). Therefore, it can be assumed that an earthquake of this magnitude left traces both in ancient and modern settlements. Many earthquakes have been historically detected in the area, but only one, which occurred in the 3rd century CE (macroseismic Magnitude Mm 6.6; ), had an epicenter compatible with the activation of a CRF patch ( A). Unfortunately, this earthquake does not have a well-constrained source ( ). According to the authors, the earthquake was responsible for the ultimate collapse and abandonment of the Roman settlement of Capo Colonna ( A). The collapse of this settlement has been dated to the first half of the 3rd century CE, as indicated by the age of the coins and pottery sherds found below the collapsed roofs. Given its epicentral location, this earthquake would be a strong candidate for the source of the coseismic uplift of CL2 shoreline, but its age is too young. According to the CUR 3 and GIG 3 b sea-level markers, the youngest limit for the abandonment of CL2 shoreline is around the second half of the 1st BCE, while the earthquake occurred at least 250–300 yr later. The earthquake may, however, have contributed to bringing CL2 sea-level markers to their present elevations. suggested that there may have been other seismic events before this one in the area. Indeed, one of the domus in the Roman settlement at Capo Colonna was restored and then abandoned for no obvious reason in the first half of the 1st century CE. Given these archaeological observations, a cluster of discrete uplift events might be favoured over a single seismic event to explain the Holocene uplift of the Crotone Peninsula. Given the many uncertainties surrounding the coseismic uplift of the CL2 sea-level markers, it is not possible to determine whether or not it was actually affected by an impulsive uplift. The CRF is indeed a seismogenic source capable for producing this deformation, but, as stated before, there is no correspondence between the chronological constraints of the uplift (1st century BCE, second half) and those of the Capo Colonna earthquake (3rd century CE, first half). However, further investigation on the uplift mechanism of this area should be addressed, since the confirmation of an active seismogenic source could significantly contribute to improve our understanding of the seismic and tsunami hazard of the Ionian coastal area of Calabria. 15 new AMS C dates allow us to constrain details of the Holocene uplift history of the Ionian coastal area of Calabria, particularly for the Crotone Peninsula. Unlike the Tyrrhenian side of Calabria, the Ionian coastal area has long remained under-studied. Only one sea-level marker indicating Holocene uplift was dated by . Despite uncertainties due to the limited temporal resolution of the methodology used, as well as the discontinuous dataset, our new data on the Holocene sea-level markers of the Crotone Peninsula allow to draw the following conclusions. Despite these insights, some questions related to the Holocene uplift history of the Crotone Peninsula remain unanswered. For example, the Holocene uplift path is not completely understood, and the specific causes of uplift are not yet identifiable. Some evidence suggest that the most recent fossil shoreline gained its current elevation with a rapid pulse of uplift. If confirmed, we need to identify a mechanism capable of producing surface deformation and uplift in a very short or even impulsive event. Our tectonic model reveals that the CRF is a seismogenic source capable for producing up to 1.4 m of coseismic uplift with a Mw 7.3 earthquake. Whether or not it is responsible for CL2 shoreline uplift is unclear. A 3rd century CE earthquake may have contributed to raising the CL2 shoreline to its current elevation, even though it occurred 250–300 years later then the abandonment age of the shoreline. Another option is that a cluster mechanism of >Mw 6.5 earthquakes can account for the shoreline uplfit, but evaluating this hypothesis will require more evidence. Future research is needed to further investigate the uplift mechanism that affected the Ionian side of the Calabrian Arc in the late Holocene, considering that any seismogenic source could still be active. If it is the case, the seismic and tsunami hazard in the area should be updated and more deeply investigated. In addition, given the current rate of sea level rise, if the uplift path of the Crotone Peninsula has shown long periods of uplift rate close to zero, the flooding hazard in the area would need to be assessed. A first step in resolving the open questions arose from this work is the integration of paleo-shoreline studies with geodetic measurements of the current vertical velocity field of the Crotone Peninsula. DISS 3.3.0 - DISS Working Group (2021). Database of Individual Seismogenic Sources (DISS), Version 3.3.0: A compilation of potential sources for earthquakes larger than M 5.5 in Italy and surrounding areas. Istituto Nazionale di Geofisica e Vulcanologia (INGV). . Beauducel F. (2022). Okada: Surface deformation due to a finite rectangular source ( ), MATLAB Central File Exchange. ISPRA CARG project 577-Isola di Capo Rizzuto and 571-Crotone ( ) ISPRA CARG project 561-San Giovanni in Fiore . Fare clic o toccare se si considera attendibile questo collegamento.\"> .\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Used\n",
      "* **AIモデルの判断:** Not Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 130 / DOI: 10.1016/j.ijdrr.2024.104605  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Development of CMIP6-Based Climate Scenarios for Japan Using Statistical Method and Their Applicability to Heat-Related Impact Studies\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Nationwide evaluation of changes in fluvial and pluvial flood damage and the effectiveness of adaptation measures in Japan under population decline\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "Flood disasters cause severe economic losses. According to the Emergency Events Database (EM-DAT) [ ], 22 % of the damage costs due to natural disasters worldwide from 2000 to 2020 can be attributed to floods. To reduce such flood damage, there is a need to evaluate flood damage and its countermeasures and implement strategic measures based on the assessment results. Many researchers [ ] have evaluated flood damage as a product of hazards, exposure, and vulnerability. As these aspects change over time [ ], there is also a need to evaluate flood damage and corresponding countermeasures by considering future changes. Future changes in flood damage are projected to increase owing to the increased hazards associated with climate change and increased exposure due to population growth [ ], and the measures needed to address increased flood damage have been assessed [ ]. Moreover, exposure is expected to decline in countries with severe population declines, such as Japan [ ]. Changes in hazards in Japan are projected to include increased rainfall [ ]. Therefore, future fluctuations in flood damage in Japan are determined by the difference between reduced exposure and increased hazards. In previous studies, fluvial and pluvial flood damage and the effects of countermeasures throughout Japan have been estimated considering changes in hazards due to climate change [ , ]. However, changes in exposure due to demographic changes have not been considered. Therefore, in Japan, there is a lack of knowledge for implementing strategic measures considering both climate and demographic changes. Assessments of future flood damage and countermeasures that consider climate and demographic changes align with Goals 11 and 13 of the Sustainable Development Goals adopted by the United Nations [ ]. In addition, as part of integrated flood management (IFM), the Japanese government has fortified preparations against floods [ ]. This assessment contributes to the promotion of effective IFM in Japan. The shared socioeconomic pathway (SSP) [ ] was developed as a socioeconomic scenario that provides projections of future demographic changes and has been widely used in climate change research. The SSP was originally developed as a narrative scenario, and in subsequent studies, the population and other factors were quantified [ , ]. Studies using quantified SSP data have also been conducted globally. For example, Arnell and Lloyd-Hughes [ ] used SSP population projections to assess the impact of climate change on populations exposed to flooding. Dottori et al. [ ] used spatially distributed population SSP datasets [ ] to estimate the impact of global warming on economic losses resulting from fluvial flooding. Kam et al. [ ] used the same datasets as those employed by Dottori et al. [ ] to quantify the future risk of population displacement due to fluvial flooding. Although these studies were conducted on a global scale, the SSP was developed at the global scale and is thus unsuitable for national-scale assessments. Therefore, in previous studies, the impact of severe population decline, such as that in Japan, may have been overlooked. A revised version is required to conduct detailed impact assessments on a national scale in Japan [ ]. Chen et al. [ ] proposed a Japanese version of the SSP (hereafter referred to as the JSSP) that corresponds to the global version. Yoshikawa et al. [ ] used the JSSP proposed by Chen et al. [ ] to provide a population and land use dataset that enables comparisons of impact assessments with other sectors. The JSSP facilitated the assessment of the impacts of climate and demographic changes on a national scale in Japan. This assessment highlights the importance of considering the impact of population decline on flood damage and the effectiveness of countermeasures on a national scale because few studies have been conducted based on the SSP architecture on a national scale [ ]. Given this context, in this study, we estimated changes in fluvial and pluvial flood damage due to climate and demographic changes across Japan and evaluated whether such changes significantly impact fluvial and pluvial flood damage. In addition, we focused on adaptation measures considered in previous adaptation studies and conducted a nationwide assessment of adaptation measures considering climate and demographic changes for the formulation of strategic measures under population decline. In this study, we applied a similar methodology to that of Yamamoto et al. [ ] and Yanagihara et al. [ ] to evaluate fluvial and pluvial flood damage costs throughout Japan. Considering climate change, they assessed changes in fluvial and pluvial flood damage costs and adaptation measures throughout Japan. With the use of inundation analysis with a spatial resolution of 7.5″ × 11.25″ (approximately 250 × 250 ), they estimated damage costs based on inundation depths. The major differences between this study and previous studies are the change in climate scenario data from Coupled Model Intercomparison Project Phase 5 (CMIP5)-based data to CMIP6-based data and the consideration of demographic changes based on the JSSP in inundation analysis and damage cost calculations. The impact of demographic changes was reflected by estimating the associated land use changes. shows a flowchart of the overall methodology. We first describe the input datasets in Section , and each process of the methodology is described in Section to Section . We used rainfall data involving the generation of equal-probability extreme discharge in all rivers across Japan as the input rainfall data for the baseline period in fluvial flood inundation analysis, similar to Yamamoto et al. [ ]. Yamamoto et al. [ ] created rainfall data with a spatial resolution of 7.5″ × 11.25″. We used rainfall data with return periods of 5, 10, 30, 50, 70, 100, 150, and 200 years created by Yamamoto et al. [ ]. The rainfall data of Yamamoto et al. [ ] were converted from the extreme rainfall data in the next section using the methodology of Tezuka et al. [ ]. The methodology of Tezuka et al. [ ] was proposed to calculate fluvial flood inundation risk across Japan. For example, if extreme rainfall with a return period of 100 years is applied to the river basin, the discharge in the downstream areas would have a much greater return period. Therefore, Tezuka et al. [ ] developed a method for creating rainfall data involving the generation of equal-probability extreme discharge in all rivers across Japan by reducing extreme rainfall according to catchment area. A complete description can be found in . These rainfall data have been used by other researchers to assess climate change impacts and adaptation measures for fluvial flood damage in Japan [ ]. Because pluvial flood inundation occurs in areas that surround areas receiving rainfall, we used the extreme rainfall data for Japan prepared by Kawagoe et al. [ ] as the input rainfall data for the baseline period in pluvial flood inundation analysis, similar to Yanagihara et al. [ ]. These rainfall data provide the probabilistic rainfall for each return period at a spatial resolution of 30″ × 45″ (approximately 1 km × 1 km). These rainfall data were created using 24-h precipitation data from 1980 to 2000 at 1024 weather stations and estimated average monthly precipitation from April to November from 1971 to 2000 with a spatial resolution of 30″ × 45″. Extreme rainfall was obtained by frequency analysis using the generalised extreme value distribution at 1024 weather stations, and regression equations between extreme rainfall and the maximum average monthly precipitation from April to November were developed to estimate extreme rainfall with a spatial resolution of 30″ × 45″. Extreme rainfall data with return periods of 5, 10, 30, 50, and 100 years were used. To create rainfall data that reflect the impact of climate change, we used bias-corrected climate scenario data for the Japan region (hereafter referred to as NIES2020) [ ] via a cumulative distribution function-based downscaling method based on CMIP6. Daily precipitation data for five global circulation models (GCMs) and two scenarios (SSP1-RCP2.6 and SSP5-RCP8.5) were obtained from NIES2020. The five GCMs used were MRI-ESM2-0, MIROC6, ACCESS-CM2, IPSL-CM6A-LR, and MPI-ESM1-2-HR. These GCMs were selected by Shiogama et al. [ ] using a statistical method to capture the prediction range of GCMs in the CMIP6 for the Japan region. There was one ensemble member per GCM, and the spatial resolution was 1 km × 1 km. The data periods used ranged from 1981 to 2000 (baseline period), 2031 to 2050 (near future), and 2081 to 2100 (end of the 21st century), similar to those used in previous research [ ]. According to Ishizaki et al. [ ], in NIES2020, the root mean square error of MIROC6 before bias correction was 2.19 mm/day in June–August and 1.71 mm/day in December–February, and after bias correction, the root mean square error reached 0.32 mm/day in June–August and 0.13 mm/day in December–February. Therefore, the bias correction of NIES2020 for precipitation performed well, and the NIES2020 climate scenario data were reliable. Similar to previous studies [ , ], elevation data and ground slope data with a spatial resolution of 7.5″ × 11.25″ provided by the national administration [ ] were used. Elevation data were used for inundation analysis, and ground slope data were used for damage cost calculations. River data provided by the national administration [ ] were used. These river data are line data showing the river centreline. River section type information was assigned to the river data. Japan has two major types of river systems: those designated by the national administration and those designated by prefectural governments. Rivers in the river systems designated by the national administration are classified as rivers managed by the national administration (hereafter referred to as class A directly controlled sections), rivers managed by prefectural governments (hereafter referred to as class A designated sections) and rivers managed by other local governments. Rivers in river systems designated by prefectural governments are classified as rivers managed by prefectural governments (hereafter referred to as class B river sections) and rivers managed by other local governments. River data were used to designate river channel cells with a spatial resolution of 7.5″ × 11.25″ for inundation analysis. Cells with river data were designated river channel cells. In addition, the cells with the largest areas of rivers and lakes in the land use data at a spatial resolution of 7.5″ × 11.25″ were designated river channel cells. River section type information for the river cells designated using the land use data was obtained to match the surrounding river data. The method of designating river channel cells was the same as that adopted by Yamamoto et al. [ ]. With the use of these river channel cells, they determined the channel depth at which flood control safety floods could flow without overflowing in each river section using the method of Tanaka et al. [ ] (Section ). In addition, Strahler's stream ordering framework for the same river data calculated by Yamamoto et al. [ ] was used. The building perimeter data provided by the national administration [ ] were used. The year of creation of the building perimeter data varies from region to region and ranges from 2014 to 2023 at the time of data acquisition. The building perimeter data were used to calculate the house occupancy ratio and the house size for inundation analysis. The 2016 land use type data provided by the national administration [ ] were used for inundation analysis and damage cost calculations. These land use data were the same as those for the JSSP baseline year period [ ]. The spatial resolution was 3″ × 4.5″ (approximately 100 × 100 ), and 12 land use types were recorded. The land use types were (1) paddy field, (2) field, (3) forest, (4) barren land, (5) land for buildings, (6) roads, (7) railways, (8) land for other uses, (9) rivers and lakes, (10) beach, (11) sea area, and (12) golf courses. We calculated the area of each land use as 7.5″ × 11.25″ from the land use type data. Land for buildings was classified into residential areas or offices when calculating damage costs. Similar to Yamamoto et al. [ ], the data for areas with designated land use in 2011 provided by the national administration [ ] were used; cells in areas designated as exclusive residential areas were classified as residential areas, while other cells were classified as offices. Chen et al. [ ] first developed a narrative scenario reflecting the Japanese situation based on workshops by local researchers and the government. They then selected population data from a widely used database of future populations in Japan that is the most consistent with the JSSP population. In addition, municipality and JSSP population scenarios were created to match the total population of the JSSP [ ]. We used these municipality and JSSP population scenarios. The JSSP1 and JSSP5 population data for 2050 and 2100, respectively, and population data for the baseline year 2015 were used, as the JSSP population scenario is the only scenario that corresponds to the global version of the SSP, which reflects the situation in Japan. Yoshikawa et al. [ ] estimated the future area of each land use at a spatial resolution of 30″ × 45″ using municipality and JSSP population scenarios. Future land use data based on the JSSP developed by Yoshikawa et al. [ ] were used in a climate change impact study [ ]. The estimation method of Yoshikawa et al. [ ] is also applicable to a spatial resolution of 7.5″ × 11.25″, so we could obtain the area of each land use in 2050 and 2100 for JSSP1 and JSSP5 at a spatial resolution of 7.5″ × 11.25″. describes the estimation method for future land use data used by Yoshikawa et al. [ ]. The future land use data include the same 12 land use types as in the baseline period. Using the same method as in the baseline period, the land for buildings was classified into residential areas or offices. We used future land use data to adjust Manning's roughness coefficient, the house occupancy ratio, and the asset value of each cell during inundation analysis and damage cost calculations. The land use data for 2050 and 2100 were used to analyse the near future and end of the 21st century, respectively. Population distribution data with a spatial resolution of 30″ × 45″ retrieved from the 2015 Population Census [ ] were used to determine the areas subject to land use controls (see Section ). The 1:500,000 scale landform classification maps provided by the national administration [ ] were used as the landform classification data. The landform map shows areas of flat ground, slopes, and alluvial fans on the slope. Landform classification data were used to set the ridge heights of the paddy field dams (see Section ). Future rainfall data used for inundation analysis were estimated by the methods of Yamamoto et al. [ ] and Yanagihara et al. [ ]. Using the following equations, we estimated future rainfall data. and are the rainfall data with a return period of years in the future period (near future or at the end of the 21st century) used for fluvial and pluvial flood inundation analysis, respectively. and are the rainfall data with a return period of years in the baseline period used for fluvial and pluvial flood inundation analysis, respectively (Section and Section ). and are the daily precipitation with a return period of years for the future and baseline periods, respectively, in the GCM data from the NIES2020. and were obtained by frequency analysis of the annual maximum daily precipitation in the GCM data from the NIES2020 for the future and baseline periods, respectively. The generalised extreme value distribution was used as the probability distribution type, and the probability weight moment method was used for parameter estimation [ ]. Eq. and Eq. were applied to each cell with spatial resolutions of and , respectively. In Eq. , the spatial resolution of is approximately 250 × 250 , which does not match the spatial resolutions of and of 1 km × 1 km. Therefore, and were converted to a spatial resolution of approximately 250 × 250 , assuming the same daily precipitation in 1 km squares. In Eq. , the spatial resolution of is the same as that of and . The fluvial flood inundation analysis was conducted according to the methods of Yamamoto et al. [ ]. A two-dimensional unsteady flow model [ ] was used for the inundation analysis. A two-dimensional unsteady flow model was uniformly applied to rivers and floodplains, and inundation analysis was conducted while simultaneously analysing the entirety of Japan. The grid size was 7.5″ × 11.25″. Furthermore, a two-dimensional unsteady flow model has been applied and validated in the Naruse River basin in Japan [ ]. In fluvial flood inundation analysis, the rainfall data for fluvial flooding were given for 24 h, and the rainfall intensity was given as a constant [ ]. This occurs because the rainfall data for fluvial flooding were adjusted to produce extreme discharge by providing 24 h of rainfall data [ , ]. Inundation analyses were carried out using rainfall data for fluvial flooding with return periods of 30, 50, 100, and 200 years to obtain the inundation depths due to fluvial flood inundation with return periods of 30, 50, 100, and 200 years, respectively. Inundation from rivers occurs when floods exceed the flood control safety levels established for each river section described in Section . The water depth in the sea cells was set to 0 as the boundary condition between the land and sea areas. The fluvial flood inundation model includes the effect of pluvial flooding but does not account for inland water drainage facilities. This occurs because the consideration of inland water drainage facilities may result in fluvial flooding underestimation, as fluvial flooding is reproduced by collecting rainfall across the river basin. Manning's roughness coefficient in each cell was obtained via weighted averaging of the square of Manning's roughness coefficient values for all land uses by the area of each land use. Manning's roughness coefficient according to land use was determined as described in Yanagihara et al. [ ]. The house occupancy ratio and the house size on building land were calculated for each administrative district in 2016 using building perimeter data provided by the national administration [ ]. The house occupancy ratio in each cell can be obtained as follows: is the porosity in each cell, is the house occupancy ratio in each cell, is the house occupancy ratio of building land, is the area of building land in each cell, and is the area of land other than buildings in each cell. The other parameters used for the analysis were the same as those in Tezuka et al. [ ]. The elevation data used for inundation analysis indicated depressions. According to Mark [ ], the natural occurrence of depressions in elevation data with a spatial resolution of 10 or more is rare, except in glacial or karst areas, and depressions can generally be considered errors. Water is retained in depressions, so inundation analysis based on elevation data with depressions differs from the real phenomenon. Furthermore, in some cases, the downstream elevation was greater than the upstream elevation in the river channel cell. In this case, the water in the river channel ceases to flow. Therefore, we corrected the elevation of the depression and the river channel cell. shows the procedure for the correction of elevations. In the following, we explain the correction of elevations using the variables in . In the first step of depression removal in (a), the elevation of a depression cell ( ) was replaced by the average elevation of the two directions from the lowest of the cells along four directions adjacent to the depression cell ( , ). This ensured that the cell with the lowest elevation functioned as the runoff destination. In the second step in (b), the river channel elevations were smoothed to eliminate water not flowing through the river channel cell. The elevation of the river channel cell ( ) was smoothed by setting it to the average elevation of the adjacent river channel cells along the four directions ( , ) and itself. The smoothed rivers were those for which the flood control safety level was defined. Finally, the elevation of the depression cell among the river channel cells ( ) was the average elevation of the two directions from the lowest of the four directions of the river channel cells adjacent to the depression cell ( , ). This removed depressions caused by river channel smoothing. The method described above is the same as that of Yanagihara et al. [ ]. In the elevation data used for fluvial flood inundation analysis, the elevation of the river channel cell reflects the elevation of the surrounding area and does not include the depth of the channel. Therefore, when fluvial flood inundation analysis was carried out using these elevation data, the inundation depth ignored the flood control safety level of the rivers. Therefore, to consider the flood control safety level of rivers throughout Japan, Tanaka et al. [ ] surveyed the design scale of the river systems listed in the basic policy for river development formulated by the national administration and obtained the design flood control safety level for each river section type. In addition, Tanaka et al. [ ] and Yamamoto et al. [ ] used this design flood control safety level to reflect the flood control safety level in fluvial flood inundation analysis across Japan. The method is as follows: first, inundation analysis was conducted using rainfall data for fluvial flooding for each return period under the condition that the elevation of the river channel cells was lowered to prevent water from overflowing the river channels, and the maximum water depth in the river channel cells could be obtained. This maximum water depth was defined as the depth of channel excavation required to carry the extreme discharge for each return period without overflowing the river channel. Next, the elevation corrected in the previous section was reduced by the depth of channel excavation required to carry the extreme discharge without overflowing the river channel, depending on the flood control safety level for each river section type. Specifically, the flow capacity in all river channels was assumed to be 50 % of the design flood control safety level. In other words, it was assumed that channel excavation and levee works achieved up to 50 % of the design level in all river channels. In this study, this method was partially improved and applied. First, the river section types were respecified using Strahler's stream ordering framework, referring to Yanagihara et al. [ ]. Previous researchers [ , ] have used river section types of river data provided by the national administration. Among these river data section types, the section types of some rivers are discontinuously specified. In this case, as the flood control safety level decreases from upstream to downstream, the water in the river channel could be dammed in the inundation analysis. Given that this does not match the actual conditions, we respecified the section types using Strahler's stream ordering framework. The correspondence between the stream order and section types was determined to be the one most consistent with the river section types of the river data provided by the national administration for each river system. We set the flood control safety level for class A directly controlled sections, class A designated sections, and class B river sections. Second, we improved the assumption regarding the flood control safety level. The national administration publishes the levee development rate for class A directly controlled sections of each water system [ ]. The levee development rate is the proportion of sections where levees of the design scale have been built to the sections where levees are needed. In this study, instead of setting the flood control safety level at 50 % of the design flood control safety level, the flood control safety level was set to the design flood control safety level multiplied by the levee development rate for class A directly controlled sections. However, as the levee development rate is not disclosed for other river sections, the flood control safety level was assumed to be 50 % of the design flood control safety level with reference to previous studies [ , ]. Given that the levee development rate for class A directly controlled sections managed by the national administration is approximately 70 %, the values for other river sections managed by the prefectural governments are lower than this value due to their smaller budgets. Therefore, 50 % is a reasonable value. However, if the levee development rate for class A directly controlled sections is less than 50 %, the development rate for class A directly controlled sections and the development rate for class A designated sections are assumed to be the same. The return periods for the depth of channel excavation required to carry the extreme discharge without overflowing the river channel in this study were 5, 10, 30, 50, 70, 100, 150, and 200 years. Therefore, linear interpolation based on the relationship between the depth of channel excavation required to carry the extreme discharge without overflowing the river channel and the annual exceedance probability was used to obtain values for other return periods. Pluvial flood inundation analysis was conducted according to the methods of Yanagihara et al. [ ]. Pluvial flood inundation analysis was carried out in the same way as in Section , except for the rainfall and the water depth of the river settings. In the pluvial flood inundation analysis, extreme rainfall data were given for 24 h, and the rainfall intensity was constant [ ]. In addition, the water depth of the rivers was set to 0 in the inundation analysis [ ]. These rivers provide a certain flood control safety level. This assumption enabled us to focus only on pluvial flooding. We conducted inundation analyses using extreme rainfall data with return periods of 5, 10, 30, 50, and 100 years to obtain the inundation depths due to pluvial flood inundation with return periods of 5, 10, 30, 50, and 100 years. The original pluvial flooding model assumed a situation where river levels are high and where rainwater cannot be drained [ ]. However, such pluvial flooding is accounted for in the fluvial flood inundation model. Therefore, we considered only pluvial flooding due to rainfall exceeding the rainwater drainage capacity by assuming that rainwater can drain into rivers. The parameters from Section and the corrected elevation data from Section were used for the pluvial flood inundation analysis. Pluvial flood inundation analysis was conducted by subtracting the rainfall amount according to the maintenance level of inland water drainage facilities from the rainfall amount used for pluvial flood inundation analysis [ ]. This value was then used to obtain estimates considering inland water drainage facilities [ ]. In Japan, the construction of inland water drainage facilities is planned in many areas to provide an adequate response to rainfall with a return period of approximately five years [ ]. Additionally, the urban inundation countermeasure achievement rate, which is an index of urban inundation countermeasures provided by sewage systems in Japan, is based on whether they can process rainfall with a return period of approximately five years [ ]. Accordingly, Yanagihara et al. [ ] conducted an analysis based on the assumption that all rainfall with a return period of five years during the baseline period can be drained, referring to the existing maintenance target. In this study, a pluvial flood inundation analysis was conducted according to this assumption. The damage costs were calculated with reference to the Manual for Economic Evaluation of Flood Control Investment [ , ]. This manual is generally used to evaluate the economic efficiency of flood control projects in Japan. General property damage and agricultural product damage were included in the calculation of the direct damage cost. The damage cost was calculated by multiplying the asset value of the flooded cell by the damage rate according to the flood depth. Economic damage due to flooding was assumed to occur in paddy fields, fields, residential areas, offices, and golf courses. It was assumed that there would be no economic damage due to flooding for other land use types. We describe the calculation of asset values in Section and the damage rates by inundation depth in Section . We calculated asset values by multiplying the area of paddy fields, fields, residential areas, offices, and golf courses by the asset value per unit area for each land use. The asset values include the asset values of rice plants in paddy fields, crops other than rice plants in fields, houses in residential areas and offices, household items in residential areas, depreciation and inventories in offices, and depreciation and inventories in golf courses. The asset value per unit area for each land use in the baseline period was calculated as shown in , using a partial modification of the method of Yamamoto et al. [ ]. Yamamoto et al. [ ] calculated asset values by assigning one land use type to each cell at a spatial resolution of 7.5″ × 11.25″, whereas in this study, the area of each land use in each cell was considered to calculate asset values. All asset values are expressed in 2004 prices. The area of each land use in the baseline period was used to calculate asset values in the baseline period. To calculate future asset values, asset values per unit area for each land use in the future are needed. However, no such data correspond to the JSSP. Therefore, in , we investigated the relationship between the rate of change in population and the rate of change in asset values per unit area for each land use over the most recent five years using the available data. The results showed a statistically significant positive correlation ( < 0.01) between the rate of change in population and the rate of change in the asset value of household items per unit area of residential area. In other words, the greater the population decline is, the smaller the asset value of household items per unit area of residential area. On the other hand, there was no statistically significant correlation ( > 0.10) between the rate of change in population and the rate of change in other asset values per unit area. The relationship between the rate of change in population and the rate of change in asset values of household items per unit area of residential areas is correlated but highly variable ( (d)). Therefore, using this relationship to estimate the future decline in the asset value of household items per unit area of residential area due to population decline may lead to large estimation errors. Thus, we assumed that the asset value of household items per unit area of residential area in the future would be the same as that in the baseline period to avoid underestimating damage costs without considering the decrease in the asset values per unit area and to be on the safe side. Other asset values per unit area were assumed to be the same as those in the baseline period because no relationship with population change was found. The area of each land use in the future period based on the JSSP was used to calculate the asset value in the future period. The asset value of each cell in future periods differed based on the change in land use, which in turn was caused by demographic changes. Damage rates by inundation depth were used with partly updated values from the Supplementary Information in Yamamoto et al. [ ]. In response to the 2020 revision of the Manual for Economic Evaluation of Flood Control Investment [ ], we used the revised values of the damage rates of house damage and depreciable/inventory asset damage by flood depth. The damage rates of house damage vary according to the ground slope. The damage amount was evaluated based on the expected annual damage cost after factoring in the occurrence probability. Using the fluvial flood damage cost with return periods of 30, 50, 100, and 200 years, the expected annual fluvial flood inundation damage cost was calculated by Eq. [ ]. is the expected annual fluvial flood inundation damage cost, is the return period, and is the fluvial flood damage cost with a return period of years. The return periods corresponding to are 30, 50, 100, and 200 years. Using the pluvial flood inundation damage cost for return periods of 5, 10, 30, 50, and 100 years, the expected annual damage cost for pluvial flood inundation was calculated by Eq. [ ]. is the expected annual damage cost for pluvial flood inundation, is the return period, and is the pluvial flood inundation damage cost with a return period of years. The return periods corresponding to are 5, 10, 30, 50, and 100 years. The following adaptation measures, which have been considered in previous adaptation studies [ , , ] across Japan, were assessed: improvement in the flood control level of rivers, improvement in the maintenance level of inland water drainage facilities, land use control, piloti buildings (houses on stilts), and paddy field dams (paddy field storage). An improvement in the flood control level of rivers is assumed to be achieved through river channel excavation and levee development. Inland water drainage facilities include storm sewers and pumping stations. Land use controls envisage forcibly removing residents from areas of high fluvial flood risk. Piloti buildings are a type of architecture in which the first floor of a building is an external space with only columns. Therefore, in piloti buildings, damage due to flooding to the first floor does not occur. Paddy field dams can reduce downstream runoff by storing rainwater in paddy fields. We also assessed the cases in which all the adaptation measures were implemented. Improving the flood control level of rivers entails fluvial flood countermeasures, improving the maintenance level of inland water drainage facilities is a pluvial flood countermeasure, and other measures include fluvial and pluvial flood countermeasures. The design level of the adaptation measures was determined with reference to previous studies [ , , ]. First, the flood control safety level should match the design flood control safety level due to improvements in the flood control level of rivers [ ]. Second, it was assumed that the improvement in the maintenance level of inland water drainage facilities would enable drainage of rainfall with return periods ranging from five years (baseline period) to 10 years (baseline period) across Japan [ ]. Third, it was assumed that of the building land inundated by at least 3 during fluvial floods with a return period of 200 years during the baseline period, cells with a population density of 10,000 people/km or lower were subject to land use controls and suffered no damage [ ]. Fourth, cells with land for buildings in which above-floor flooding (i.e., 0.45 or more) [ , ] occurred in fluvial or pluvial flood inundation analysis with a return period of 30 years were replaced with piloti building cells [ ]. When calculating the damage cost in a piloti building cell, the inundation depth was reduced by 3 , assuming that the buildings had been raised by 3 due to the piloti building. Therefore, inundation analysis did not consider the effect of replacing the building with a piloti building [ , ]. Fifth, we applied the simple paddy field dam model of Chai et al. [ ] to reflect the potential effects of fluvial and pluvial flood mitigation measures in all paddy fields throughout Japan in inundation analysis [ ]. The modelled paddy field dam used a free-drain type water fall control device. One water fall control device was installed per 10,000 m . To reflect the impact of the paddy field dam on the inundation analysis, we set the original rainfall in all cells other than the paddy field cells, and the drainage volume of the paddy field dam model was set for the paddy fields. shows the calculation conditions for the paddy field dam model. The landform classification map was used to define the different ridge heights for flat ground, slopes, and alluvial fans on slopes. This occurs because the water storage effect of paddy fields differs depending on the topography. Finally, when all the adaptation measures were implemented, if the land use controls and piloti building target areas overlapped, the land use controls took precedence. The total benefits were calculated for the assessment period of 2025–2100. The social discount rate was set to 4 %, as described in the Manual for Economic Evaluation of Flood Control Investment [ , ]. Furthermore, to examine the impact of the social discount rate, the total benefits were also calculated for social discount rates of 3 % and 5 %. To assess the impact of the construction period, construction periods of 1, 5, 10, 15 and 20 years were adopted for paddy field dams, and 10, 20, 30, 40 and 50 years were adopted for other adaptation measures. The construction period for paddy field dams was shorter than that for other adaptation measures because paddy field dams are easy to implement. The benefits for the baseline period, the near-future period, and the end of the 21st century were defined as the benefits in 2000, 2050, and 2100, respectively. The benefits in 2000, 2050, and 2100 were linearly interpolated to obtain the benefits for the 2026 to 2049 and 2051 to 2099 periods. The benefits gradually increased, assuming that the design-level benefits occurred at the end of the construction period. show the changes in the fluvial and pluvial flood damage costs due to climate change. The scenarios in which only rainfall was adjusted using climate scenario data were denoted as representative concentration pathways (RCPs). RCP2.6 corresponds to the scenario in which the 2 °C target of the Paris Agreement has been achieved. RCP8.5 corresponds to the worst-case scenario in which we increase the use of fossil fuels. Under both RCPs, the future fluvial and pluvial flood damage costs across Japan increased due to climate change. The peak rate of change in the fluvial flood damage costs across Japan in the future period was +24 % (+5 %–+56 %) under RCP2.6 and +43 % (+25 %–+76 %) under RCP8.5. Additionally, the peak rate of change in pluvial flood damage costs across Japan was +53 % (+18 %–+109 %) under RCP2.6 and +90 % (+52 %–+151 %) under RCP8.5 in the future. In the near future, both RCPs will experience similar changes in damage costs, but these changes will diverge towards the end of the 21st century. At the end of the 21st century, the fluvial and pluvial flood damage costs decreased from the near future under RCP2.6 and increased from the near future under RCP8.5. These temporal changes in fluvial and pluvial flood damage costs across Japan coincide with the temporal changes in rainfall projections shown in . Despite the large variation between GCMs, Welch's -test showed that the fluvial and pluvial flood damage costs under RCP8.5 were significantly greater than those under RCP2.6 at the end of the 21st century at a significance level of 10 %. On the other hand, no statistically significant differences in the damage costs in the near future or at the end of the 21st century were found at the 10 % level of significance. However, the temporal trend of the results for four of the five GCMs under RCP2.6 and all GCMs under RCP8.5 matched the temporal trend of the results for the damage cost obtained from the average value of the damage costs derived from the five GCMs. Overall, the rate of change in the pluvial flood damage cost was greater than that in the fluvial flood damage cost. To understand this result, we conducted a sensitivity analysis of the damage cost with respect to rainfall ( ). Our results indicated that the cost of pluvial flood damage was more sensitive to rainfall than the cost of fluvial flood damage. shows that the future fluvial and pluvial flood damage costs increased in nearly all prefectures. The spatial trend in the rate of change by prefecture is the same as the spatial trend in the rainfall projections shown in . In the near future, the rate of change under RCP2.6 was greater than that under RCP8.5 in 55 % of the prefectures for fluvial flood damage and 47 % of the prefectures for pluvial flood damage. show the changes in fluvial and pluvial flood damage costs due to climate and demographic changes. The scenarios that combined RCPs and JSSPs are denoted as JSSP-RCP scenarios. The fluvial and pluvial flood damage costs in the near future are expected to increase under both JSSP-RCP scenarios. At the end of the 21st century, fluvial flood damage costs decreased under both JSSP-RCP scenarios; pluvial flood damage costs decreased under JSSP1-RCP2.6 and increased under JSSP5-RCP8.5. The peak change rates of the future fluvial flood damage costs across Japan were projected to reach +4 % (−12 %–+30 %) under JSSP1-RCP2.6 and +9 % (−2 %–+30 %) under JSSP5-RCP8.5. The peak change rates of the pluvial flood damage costs across Japan in the future were expected to reach +29 % (+1 %–+75 %) under JSSP1-RCP2.6 and +38 % (+26 %–+67 %) under JSSP5-RCP8.5. The change in the damage cost under both JSSP-RCP scenarios will be almost the same in the near future, but towards the end of the 21st century, JSSP1-RCP2.6 will experience a more marked decrease in fluvial and pluvial flood damage than JSSP5-RCP8.5. This is because, compared to JSSP5-RCP8.5, JSSP1-RCP2.6 has conditions conducive to reduced damage costs, i.e., low hazard ( ) and progressive population decline ( ). shows that fluvial and pluvial flood damage increases or decreases differently according to prefecture. From , prefectures with high rates of change in damage cost have high hazards and low population decline, while those with low rates of change in damage cost have low hazards and high population decline. Additionally, the rate of change under JSSP1-RCP2.6 was greater than that under JSSP5-RCP8.5 for fluvial flood damage in 43 % of the prefectures and for pluvial flood damage in 38 % of the prefectures in the near future. shows the number of prefectures where the damage costs decreased due to climate and demographic changes. Overall, it could be estimated that fluvial and pluvial flood damage costs will decrease in approximately 70 % or more of the prefectures at the end of the 21st century. In contrast, the future fluvial and pluvial flood damage costs per capita increased across Japan and in almost all prefectures, as shown in . The peak rate of change in the fluvial flood damage cost per capita across Japan in the future period was +24 % (+5 %–+55 %) under JSSP1-RCP2.6 and +47 % (+26 %–+83 %) under JSSP5- RCP8.5. Additionally, the peak rate of change in the pluvial flood damage costs per capita across Japan in the future period was +55 % (+21 %–+110 %) under JSSP1-RCP2.6 and +101 % (+58 %–+168 %) under JSSP5-RCP8.5. The rate of change in the fluvial and pluvial flood damage costs per capita exhibited a similar spatiotemporal trend to that of the rate of change in damage costs with climate change. In the near future, the rate of change under JSSP1-RCP2.6 was greater than that under JSSP5-RCP8.5 in 57 % of the prefectures for fluvial flood damage and 45 % of the prefectures for pluvial flood damage. shows the change rates of the fluvial and pluvial flood damage costs when adaptation measures are implemented considering climate and demographic changes. We found that for both fluvial and pluvial flooding, the reduction in damage caused by piloti buildings was greater than that caused by conventional measures for improving the flood control level of rivers and the maintenance level of inland water drainage facilities. In addition, the results showed that the effects of paddy field dams were small compared to those of other adaptation measures. Therefore, in future scenarios where the damage cost increases without adaptation measures, in many cases, the damage cost can be reduced by individual adaptation measures other than paddy field dams. With all adaptation measures, future fluvial and pluvial flood damage costs can be reduced by approximately 80 %. Next, similar to the previous section, an adaptation assessment was conducted from the perspective of the damage per capita. shows the rates of change in the fluvial and pluvial flood damage costs per capita across Japan when adaptation measures are implemented. For comparison, cases in which only climate change was considered were also assessed. The reduction in fluvial and pluvial flood damage costs per capita across Japan due to adaptation measures was almost the same whether climate change alone or climate and demographic change were considered. The differences in effectiveness between adaptation measures are the same as those in . On the other hand, we found that it is difficult to reduce the future damage cost per capita by individual adaptation measures, except for piloti buildings. All adaptation measures can reduce the future damage cost per capita by approximately 70 % for fluvial flooding and approximately 80 % for pluvial flooding. shows the change rates of the total benefits of each adaptation measure for fluvial and pluvial flood damage across Japan due to climate and demographic changes at a social discount rate of 4 %. shows the results for two patterns: considering only climate change and considering both climate and demographic changes. The rate of change in the total benefits was calculated by comparing the total benefits with or without consideration for future changes. Overall, irrespective of the construction period, the total benefits when both climate and demographic changes were considered were lower than those when only climate change was considered. The total benefits of the adaptation measures for fluvial flood damage increased with climate change, except for paddy field dams. The total benefits of all adaptation measures for fluvial flood damage decreased with climate and demographic changes. Additionally, the total benefits of the adaptation measures for pluvial flood damage increased with climate change, except for the improvement in the maintenance level of inland water drainage facilities. Due to climate and demographic changes, the total benefits of improving the maintenance level of inland water drainage facilities and paddy field dams decreased, while the total benefits of land use control and piloti buildings increased. The results with social discount rates of 3 % and 5 % are shown in . The social discount rate does not affect the trends described above, but the higher the social discount rate is, the smaller the impact of climate and demographic change on changes in total benefits. We applied the inundation model developed in previous studies [ , ], which can be used to analyse all of Japan. In the application, we modified the assumptions required to analyse all of Japan to ensure its rationality. In addition, in the asset value calculations, the calculation method of several asset values per unit area was modified, and the differences in the area of each land use in each cell were considered. Furthermore, the damage rates by inundation depth in the damage cost calculation were updated to the latest version. In previous studies [ , ], the validity of the estimated damage costs was validated by comparing the actual annual average damage cost and the expected annual damage cost by prefecture. We therefore adopted the same approach. The actual annual average damage cost was calculated from the Statistical Survey on Flood Damages (1980–2000) [ ]. Deflators were used to adjust the prices to 2004 prices. shows the relationship between the actual annual average damage cost and the expected annual damage cost by prefecture. also shows the relationship between the actual annual average damage cost and the expected annual damage cost estimated by previous studies [ , ] by prefecture. First, comparing the results of this study with those of previous studies, the magnitudes of the expected annual damage cost for fluvial flood damage in this study and those of the previous study [ ] are almost the same. On the other hand, the expected annual damage cost for pluvial flood damage in this study is smaller than that in Yanagihara et al. [ ]. This is because the present study estimated damage costs under the condition that rainwater can drain into rivers, whereas the previous study [ ] estimated damage costs under the condition that rainwater cannot drain into rivers. Next, we compare whether the expected annual damage cost in this study or the previous study represents a spatial trend in the actual annual average damage. shows the Spearman's rank correlation coefficients between the actual annual average damage and the expected annual damage cost by prefecture in this study and previous studies [ , ]. Spearman's rank correlation coefficient was used as the correlation coefficient to eliminate the influence of outliers. shows that the expected annual damage cost for fluvial flood inundation in this study represents a better spatial trend of actual annual average damage than that in a previous study [ ] due to the modifications considered in this study. The results for pluvial flood inundation are almost the same between the present study and the previous study [ ]. Finally, the expected annual damage cost was greater than the actual annual average damage from the statistical survey in both this study and previous studies [ , ]. The reason for this is that the spatial event probability has not been considered [ ]. In other words, this study and previous studies [ , ] have generated floods with equal return periods everywhere. On the other hand, this approach makes it possible to compare flood risks of the same magnitude between locations. Similar to this study and previous studies [ , ], the damage cost used to calculate the benefits of river improvement projects estimated by the administration was approximately 20 times the actual annual average damage [ ]. Previous studies [ , ] focused on the rate of change in the damage cost due to climate change and adaptation measures to reduce the uncertainty of the estimation results. Similarly, we focused on the change rates of the damage costs and total benefits of adaptation measures due to climate and demographic changes. In this study, it was shown that the costs of fluvial and pluvial flood damage across Japan increase due to climate change. In addition, the fluvial flood damage costs at the end of the 21st century will decrease under both JSSP-RCP scenarios, and the pluvial flood damage costs at the end of the 21st century will decrease under JSSP1-RCP2.6 due to climate and demographic changes. Furthermore, the results showed that due to climate and demographic changes, the fluvial and pluvial flood damage costs at the end of the 21st century would decrease in approximately 70 % or more of prefectures. This suggested that at the end of the 21st century in Japan, the impact of demographic changes on fluvial and pluvial flood damage costs will exceed that of climate change in many prefectures. This study also analysed the damage cost per capita to better understand the impact of flood damage on people's lives. As a result, even if the damage cost decreases, flood countermeasures are necessary as the per capita fluvial and pluvial flood damage costs are projected to increase. These findings supplement the findings of previous studies [ , ] that focused only on the impact of climate change. In these studies [ , ] using CMIP5-based climate scenario data, the fluvial and pluvial flood damage costs across Japan were estimated to peak and decrease in the near future under RCP2.6 and increase towards the end of the 21st century under RCP8.5. The same temporal trends in fluvial and pluvial flood damage costs were observed in this study using CMIP6-based climate scenario data for four out of five GCMs under RCP2.6 and for all GCMs under RCP8.5. In previous studies [ , ] of the United States, where the population is expected to increase, it was shown that the impact of demographic changes on flooding was greater than that of climate change. Similar findings were obtained for Japan, where the population is expected to decline. According to Yamamoto et al. [ ] and Yanagihara et al. [ ], individual adaptation measures, including improvement in the flood control level of rivers, improvement in the maintenance level of inland water drainage facilities, land use control, and piloti buildings, were estimated to hardly reduce the increased fluvial and pluvial flood damage costs due to climate change in the near future and at the end of the 21st century. In contrast, it was found that even individual adaptation measures could, in many cases, reduce future fluvial and pluvial flood damage costs because the increased flood damage due to climate change decreased with population decline. However, it is difficult to reduce future fluvial and pluvial flood damage costs with most individual adaptation measures when focusing on damage costs per capita. In addition, it was estimated that the total benefits considering both climate and demographic changes were lower than those considering only climate change ( ). This suggests that demographic change reduces the effectiveness of adaptation measures. In Japan, where population decline is predicted [ ] and tax revenues are expected to decrease [ ], limited budgets must be effectively used in the implementation of flood control measures. The results therefore emphasise the importance of adaptation planning that considers not only climate change but also demographic change. Concerning changes in the effectiveness of adaptation measures with future changes, Dottori et al. [ ] reported that the benefits of raising dikes, detention areas, flood proofing, and relocation for fluvial flooding increase with global warming in many European regions. This is attributed to an increase in the amount of damage avoided due to the increased flood frequency and exposure [ ]. Applying this finding to this study of Japan, it could be considered that demographic change reduces the total benefits because the reduced exposure due to population decline decreases the avoided damage. When only climate change was considered, the total benefits increased, except for paddy field dams for fluvial flooding and the improvement in the maintenance level of inland water drainage facilities for pluvial flooding, due to the increase in the avoided damage. Adaptation measures that do not increase the total benefits due to climate change are those that reduce the effectiveness of adaptation measures due to increased rainfall. Indeed, it has been reported that paddy field dams exhibit application limitations [ ]. This study has limitations. First, the results are affected by uncertainties in the GCMs. The five GCMs we used could capture the prediction range of the GCMs involved in the CMIP6, but it should be noted that the uncertainty in the GCMs influences our results. Second, the JSSP-based socioeconomic projection and flood damage projection are independent. There are cases where population growth in flood-prone areas has been observed because of flood countermeasures [ ]. However, we cannot consider such demographic dynamics in our results. This dynamic is referred to as the levee effect and occurs because the reduction in flood frequency due to flood countermeasures decreases the community's flood memory and encourages economic development in flood-prone areas [ , ]. Adaptation studies for fluvial flooding using the global version of the SSP [ , ] suffer the same limitations. The application of sociohydrological modelling of levee effects in Japan is limited, and there are issues to be resolved [ ]. Notably, our adaptation assessment results are affected by levee effects. Given the levee effect, the damage estimated in this study may increase when adaptation measures are implemented, and adaptation measures that could reduce future flood damage could be less effective than indicated by our results. Our assessment of adaptation measures emphasises the importance of adaptation planning that considers not only climate change but also demographic change, which we consider even more important due to the demographic dynamics caused by the levee effect. We used several assumptions in our methodology. The first is that the fluvial flood inundation model includes the effect of pluvial flooding but does not consider inland water drainage facilities. Therefore, the impact of pluvial flooding may be overestimated. The results of the fluvial flood inundation model should be considered as assuming the worst-case scenario in which the inland water drainage facilities cease to function. Next, the pluvial flood inundation model only considers pluvial flooding due to rainfall exceeding the rainwater drainage capacity. Pluvial flooding, which occurs when river levels are high and rainwater cannot drain away, cannot be considered in the pluvial flood inundation model, and such pluvial flooding is included in the fluvial flood inundation model. Therefore, the definition of pluvial flooding in the pluvial flood inundation model in this study should be taken with caution. Finally, the asset value per unit area in the future period was assumed to be the same as that in the baseline period in the calculation of damage costs because no such data correspond to the JSSP. Using the available data, we found no relationship between population change and changes in the asset value per unit area except for the asset value of household items in residential areas during the most recent five years. On the other hand, the asset value of household items per unit area of residential area tended to decrease as the population decreased. Therefore, future changes in the asset value per unit area may be influenced more by the population decline observed in this study. Future research should focus on examining the interaction between demographic changes and adaptation measures. In addition, further studies should also examine in depth changes in asset values per unit area. Furthermore, the feasibility of adaptation measures was not examined in this study, which is similar to previous studies [ , , ], because we focused on changes in the effectiveness of adaptation measures due to both climate and demographic changes. In the future, adaptation costs should be calculated, and the feasibility of adaptation measures should be examined. In this study, future changes in fluvial and pluvial flood damage were estimated considering climate and demographic changes. We also evaluated five adaptation measures, improvement in the flood control level of rivers, improvement in the maintenance level of inland water drainage facilities, land use control, piloti buildings, and paddy field dams, considering climate and demographic changes. The major conclusions of this study are as follows. The study results could be useful for predicting future changes in fluvial and pluvial flood damage and for examining the effectiveness of adaptation measures. Although the most reliable climate scenarios and socioeconomic scenarios available were used in this study, the impact of their uncertainties on the results should be noted. Conceptualization, Data curation, Formal analysis, Methodology, Software, Validation, Visualization, Writing – original draft, Writing – review & editing. Conceptualization, Funding acquisition, Methodology, Project administration, Resources, Software, Supervision, Validation, Writing – review & editing. Data curation, Formal analysis, Methodology, Validation, Writing – review & editing. Data curation, Formal analysis, Validation, Writing – review & editing, Visualization. Formal analysis, Methodology, Validation, Writing – review & editing. Data curation, Resources, Writing – review & editing.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Not Used\n",
      "* **AIモデルの判断:** Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 135 / DOI: 10.1016/j.dib.2021.107531  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "United States wildlife and wildlife product imports from 2000–2014\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Dataset of seized wildlife and their intended uses\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "The presented data covers the illegal trade (i.e., wildlife seizures; ) of 4,899 distinct taxa across three kingdoms ( ). The most diverse taxonomic kingdom was Animalia (n = 4,026 taxa), followed by Plantae (n = 871), then Fungi (n = 2). We identified c. 71% of the taxa to the level of species (or more specific) and c. 95% of taxa to the level of genus ( ). In total, our dataset represents 3,361 species. We used GBIF (Global Biodiversity Information Facility) to standardize taxonomy and obtain upstream taxonomic information . We standardized biological and resource use-types (e.g., “ivory”, “meat”, “live”) given by the three trade databases (TRAFFIC, CITES, LEMIS), resulting in 110 ‘standardized’ use-types. We further categorized these standardized use-types into 4 main categories (live, dead/raw, processed/derived, and unspecified) and 40 sub-categories for data summary purposes ( ; ; Table S1). The most diverse main categories of seizures (measured by the number of taxa) were “dead/raw”, followed by “live”, then “processed/derived” ( ). The most diverse sub-categories were: “live organisms or parts”, “dead organisms (whole body)”, and bone or bone-like body parts ( ). The most diverse standardized use-type was “live”, where over 2,127 distinct taxa were seized whole and alive (e.g., for the pet and ornamental plant trade; [ , ]), followed by seizures of dead wildlife. In total, we compiled 10,745 unique taxa-use combinations. We define a taxa-use combination as a unique combination of one taxa and one standardized use-type (e.g., bear claw). For taxa identifiable to the species level, we compiled 7,183 species-use combinations. We recorded multiple use-types for c. 37% of all seized taxa (n = 1,807 taxa); however, the majority of taxa had one use-type ( ). The most common taxa-use combinations, at the rank of taxonomic family, were: live seizures of orchids (Orchidaceae, n = 325 taxa); live seizures of cacti (Cactaceae, n = 136) and live seizures of Neotropical and Afrotropical parrots (Psittacidae, n = 126) ( ). The single species with the most use-types was the tiger ( ), which had 35 distinct use-types (e.g., bone, skin, genitalia; ). We retrieved the common names for each resolved taxa from GBIF, along with the common names associated with each taxa's upstream taxonomy. In total, we recorded 8,832 common names in the English language, and a further 37,507 common names in 125 other languages ( ). However, we found only 13 languages with over 1,000 common names. For approximately 7% of the common names returned, GBIF did not provide what language the common name was (i.e., the language field was left blank; n = 3,734 names). Two seizure databases (TRAFFIC and LEMIS) provided common names and one database (LEMIS) provided ‘generic’ names. A ‘generic’ name is either an alternative common name, regional name, trade name (a name used by traders but not the scientific and/or citizen science community), or the name of the family, order, or class of the taxa of interest. For example, Elephant would be a ‘generic’ name for the African bush elephant ( ). In total, we recorded 2,251 common names and 881 generic names from the trade databases (predominantly English language names). Of those, 727 common names and 247 ‘generic’ names were not found in the common names collected from GBIF. For each standardized use-type, we assigned ‘Internet friendly’ search terms that are relevant synonyms of each use-type. In total, we derived 304 search terms, where each use-type contained from zero (i.e., for “live” and “dead” seizures without a specified use) to eight use-specific search words, with a median of 2 search words per standardized use-type. We provide the above-described data in five tables that can be found in a public data repository ( ). The tables included are as follows: (i) taxa-use combinations, named “data/01_taxa_use_combos.csv” in the data repository, (ii) taxonomic key of GBIF taxonomy, named “data/02_gbif_taxonomic_key.csv”, (iii) common names provided by GBIF, named “data/03_gbif_common_names.csv”, (iv) common names provided by LEMIS and TRAFFIC, named “data/04_db_generic_common_names.csv”, and, (v) ‘Internet friendly’ search words associated with each use-type, named “data/05_use_search_words.csv”. We provide metadata describing each table and their fields in the data repository. These tables contain keys that allow for their combination (e.g., join or merge) to obtain a list of searchable keyword phrases tailored to one's requirement. For example, one can obtain a list of bird species that were seized as feathers, along with their common names. We provide R code, in the data repository, to demonstrate how to combine these datasets to obtain a list of searchable phrases. Our goal was to compile a comprehensive list of the wildlife taxa involved in the IWT (i.e., wildlife seizures) along with the purpose for which they were being traded (i.e., use-type). We chose to restrict our search to contemporary IWT (since 2010), because we intend this dataset to be used for searching the Internet, where trading wildlife is a relatively recent phenomenon [ , ]. We compiled wildlife seizure records from three major wildlife trade databases: (i) TRAFFIC's Wildlife Trade Portal (TRAFFIC, with permission; ), (ii) Convention on International Trade in Endangered Species of Wild Fauna and Flora trade database (CITES; ), and (iii) United States Fish and Wildlife Service's Law Enforcement Management Information System (LEMIS; see for more information on LEMIS). We obtained LEMIS through a Freedom of Information Act request to the United States government. Both TRAFFIC's and CITES databases are openly accessible. We restricted the date of wildlife seizures from 1 January 2010 to 31 December 2019, except for LEMIS, where our records stop at 31 December 2018. For all databases, we only extracted records labelled as seizures. For the TRAFFIC database, we extracted all records of ‘live’ or ‘dead’ seizures and the first 300 records (chronologically) from all other use-type categories. While these three databases are among the most comprehensive wildlife trade databases available, we note that each database has biases and limitations. TRAFFIC's database is largely derived from open source data (e.g., media and government press releases) and thus is not a comprehensive record of wildlife seizures. Further, TRAFFIC's records tend to be taxonomically biased towards charismatic species (e.g., ) and is spatially biased towards countries where TRAFFIC staff are based and collecting data from. The CITES trade database primarily contains legal trade records, but only a subset of participating countries have reported seizure records through the database. Even the countries that do report seizures in the CITES trade database may not do so in a consistent manner and, thus, there is no way to distinguish between seizures of illegal wildlife and legal trade in previously confiscated wildlife [ , ]. The LEMIS database is taxonomically comprehensive but only involves seizures of wildlife that are linked to the United States of America . Each seizure record gathered from the trade databases contained the use-type (i.e., intended usage of the wildlife). However, each trade database used slightly different words for the use-types. Thus, we standardized and consolidated the use-types between the three trade databases. Further, we provided ‘Internet-friendly’ search words associated with each use-type. These search words were either alternative names for the use-types used in one of the trade databases or synonyms of the use-type. For example, the search words we generated for the use-type “foetus” are “foetus”, “fetus”, “placenta”, and “embryo”. For the “live” and “dead” use-types, we did not assign any search words. We did not record the number of incidences for each taxa-use combination because there are likely duplicated seizure records between the three trade databases. We resolved the taxonomic names from each trade database to the Global Biodiversity Information Facility taxonomic database (GBIF; ). We automated the taxa resolution process using the R package . We manually resolved each taxa that was not matched through automation. We obtained upstream taxonomic information from GBIF (e.g., family, order, class, etc.). We collected the common names (i.e., vernacular names) from GBIF, for each taxa resolved to GBIF along with the common names for each upstream taxonomic unit. For example, for , we retrieved the species vernacular name (African Gray Parrot), the family common name (African & New World Parrots), the order common name (Parrots), and the class common name (Bird). In some instances, GBIF provided multiple common names per taxonomic unit (i.e., multiple species common names). For each English common name, we took the singular form (e.g., bears was converted to bear), using the R package . Further, we collected common names in other languages where available, from GBIF. In addition, two databases (TRAFFIC and LEMIS) provided common or ‘generic’ names (e.g., Parrot) of the taxa seized, and we included these names, as a separate table, in our dataset. We performed all data processing, analysis, and summaries in R (v. 3.6.3; ). We automated taxa resolution using the `get_gbif_id` function in the package (v. 0.9.95.91; ). We automated the collection of upstream taxonomic information from GBIF using the `classification` function from the package. We automated the collection of vernacular names from GBIF using the `name_usage` function from package (v. 3.3.0 ). We used the ecosystem of packages for general data processing, analysis, and plotting (v. 1.3.0 ). Ethics statements are not required for the presented data. Our work did not involve human subjects, animal experiments, nor collect data from social media platforms. Conceptualization, Methodology, Software, Investigation, Writing - Original Draft, Writing - Review & Editing, Visualization, Supervision Investigation, Data Curation, Writing - Review & Editing Investigation, Data curation, Writing - Review & editing Investigation, Data Curation, Writing - Review & Editing Investigation, Data Curation, Writing - Review & Editing Investigation, Data Curation, Writing - Review & Editing Investigation, Data Curation, Writing - review & editing Funding acquisition, Writing - review & editing Funding acquisition, Writing - review & editing Writing - Review & Editing Supervision, Funding acquisition, Writing - review & editing\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Used\n",
      "* **AIモデルの判断:** Not Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 141 / DOI: 10.1016/j.scitotenv.2022.161157  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Air pollution emissions from Chinese power plants based on the continuous emission monitoring systems network\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Widespread missing super-emitters of nitrogen oxides across China inferred from year-round satellite observations\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "Nitrogen oxides (NO ≡ NO + NO ) play a central role in the formation of fine particulate matter and ozone and have implications for climate change, human health, and life expectancy ( ; ; ; ; ). NO is typically emitted by combustion processes, particularly industrial activities (e.g., power or chemical plants) ( ; ; ). China is one of the largest NO emitters globally ( ). There is a heavy-tail distribution of NO emission sources in many localized regions (e.g., ∼5 × 5 km ) ( ; ). On this basis, super-emitters can be defined to be emission sources that are of very limited geographical scope but comprise highly concentrated plumes and dominate localized budgets. Hence, unique challenges for NO mitigation are presented by super-emitters, particularly by those due to abnormal operating conditions ( ; ). Efforts to guide super-emitter mitigation are complicated by large inconsistencies between emission estimates ( ; ; ; ). Bottom-up inventories tend to be designed at regional scales. In particular, aggregating point sources into regional grids challenge identifications of super-emitters. Besides, their foundations, such as activity data and emission factors, are often outdated for super-emitter representations ( ; ; ). By comparison, top-down attempts relying on accurate and up-to-date measurements present a more promising future ( ; ; ; ). Emissions from super-emitters tend to be the most well-known via continuous emission monitoring systems (CEMSs) ( ; ; ). Such infrastructure-level measurements encompass large power plants in China ( ). Nevertheless, there is a dearth of available regular measurements for localized super-emitters. Field campaigns are also spatially sparse and temporally infrequent, thus inapplicable to the super-emitters distributed over a large scale (e.g., nationwide) ( ; ; ). Satellite measurements have provided spatial patterns and magnitude of tropospheric NO vertical column densities (VCDs) on a global scale, central to improving our knowledge of the NO emission budget ( ; ; ; ; ; ). However, the ability to detect super-emitters has been limited by pixels far larger than 1 × 1 km , such as those from OMI (13 × 24 km ) ( ; ; ; ) and GOME-2 (40 × 80 km ) ( ; ; ). To date, the TROPOspheric Monitoring Instrument (TROPOMI) on the Sentinel-5 Precursor has an unprecedented spatial resolution of up to 3.5 × 7 km (3.5 × 5.5 km from August 2019 onward) and a high signal-to-noise ratio ( ; ; ; ). A representative top-down method combining a CTM with a Kalman Filter has tested the TROPOMI measurements at a sacrifice of spatial resolution (i.e., ∼0.25° × 0.25°) ( ), consequently focusing only on regional issues. Recent studies ( ; ; ) further preserved the benefit of its high spatial resolution, pioneering the quantification of known large emission sources. showed, for example, that such estimates agreed to within roughly 20 % of reported emissions in two large power plants, if the air mass factor (AMF) was correctly accounted for ( ). Even without any a priori knowledge of the locations, a handful of large emission sources can be exposed at the original TROPOMI resolution on a regional scale ( ). Therein, localized super-emitters, due to scales (i.e., ∼1 × 1 km ) smaller than the grids, might also be missed. Finally, emissions attributed to localized NO super-emitters have not been well isolated and evaluated nationwide. Here we develop an efficient, super-resolution (i.e., from 3.5 × 5.5 km to 1 × 1 km ) inverse model by capitalizing on a whole year (i.e., the year of 2019) of daily TROPOMI measurements, with a focus on identifying and quantifying NO super-emitters nationwide, down to industrial hotspots or parks (Methods) ( , S1 and S2). The whole work diagram is shown in Fig. S2h. Due to the fact that satellites typically sample irregularly in space, their data are suggested to be projected to regular spatial grids with much finer grids (1 × 1 km ) ( ). This process (called the oversampling method) has proven to be successful for emission identifications and noise elimination. Here a physics-based oversampling technique is used to average TROPOMI data (see ). On this basis, to further prevent the super-emitters from being omitted, a key advance of this model is to take into account the nonlinear NO VCD-Transport-Lifetime-Emission relationships ( ; ). Our results are validated by direct measurements (i.e., CEMS) ( ; ; ), top-down inverse estimates (i.e., the WRF-CMAQ model coupled with the Ensemble Kalman Filter) ( ; ; ), and comprehensive uncertainty analysis (Supplementary information). Hence, our survey is used to benchmark a state-of-the-art emission inventory MEICv1.3 (the Multi-resolution Emission Inventory for China) ( ) and also a state-of-the-art top-down inverse product. This work has important implications for revisiting NO budgets and facilitating emission mitigation. The TROPOMI instrument on the European Space Agency's Sentinel-5P satellite provides daily global coverage of slant column densities (SCDs) of NO with an unprecedented spatial resolution of up to 3.5 × 7 km (3.5 × 5.5 km from August 2019 onward) ( ; ; ; ). Its overpass time is close to noon (13:30 local time). Here the operational TROPOMI measurements with a high resolution were applied to analyze the finer scale spatiotemporal characteristics of NO , which can be expected to be of high signal-to-noise. We first filtered the TROPOMI pixels to ensure the data quality in advance. Pixels with cloud fraction above 30 % or a “qa value” (indicating data quality) below 0.75 were removed. We also excluded data with abnormal VCDs (i.e., >10 standard deviations from the average) ( ; ; ; ). Previous studies have evaluated this operational product globally and comprehensively. Other uncertainty analysis was presented in Supplementary information. The two-way coupled WRF-CMAQ model (i.e., the WRF-CMAQ model) was applied to provide three-dimensional wind fields at the spatial resolution of 5.5 × 5.5 km (comparable to the resolution of the TROPOMI instrument) ( ). Technically, the WRF-CMAQ model we applied here was the two-way coupled version, which was run as an integration ( ). The results formed the foundation of the following top-down NO emission estimates. Detailed model settings can be found in our previous papers ( ; ). Meteorological initial and boundary conditions were obtained from the European Centre for Medium-range Weather Forecasts (ECMWF) ERA5 reanalysis dataset with the spatial resolution of 1° × 1° and temporal resolution of 6 h. The analysis nudging option was switched on for temperature, humidity above the PBL, and winds at all model levels, thus being nudged to the meteorological driving data (i.e., the ERA5 reanalysis dataset) ( ). The horizontal domain of the model covered mainland China with a compatible horizontal resolution (5.5 × 5.5 km ) following a Lambert Conformal Conic projection ( a). In terms of the vertical configuration, 29 sigma-pressure layers ranged from the surface to the upper-level pressure of 100 hPa, 20 layers of which are located below around 3 km to derive finer meteorological and chemical characteristics within the planetary boundary layer. Anthropogenic emissions were obtained from MEICv1.3 ( ), which contained primary species (primary PM , SO , NO , CO, and NH ) from five anthropogenic sectors (i.e., agriculture, power plant, industry, residential, and transportation). Here we did not consider agriculture soil NO emissions. This is mainly because, although this source represents a key part of the total atmospheric NO budget, it is generally too diffuse to appear as hotspots in the satellite data. This inventory was initially designed with the spatial resolution of 0.25° × 0.25° and thus reallocated to match the domain configuration. In principle, the a priori emission inventory here significantly affects simulated chemical fields rather than meteorological fields. Therefore, it would not substantially affect our final estimates due to the fact that only simulated meteorological fields were used in the subsequent top-down inverse model (Eq. ). In addition to the simulated meteorological fields, the simulated chemical fields were used to evaluate our results. This verification process would not be affected by the a priori emission inventory substantially. This is mainly because the chemical fields are constrained by the TROPOMI VCDs. See Supplementary information for details. To achieve a super-resolution reconstruction for the one-year tropospheric NO VCDs, we applied the oversampling method ( ) to convert from the original satellite pixels to 1 × 1 km grid cells ( ). As demonstrated in previous studies associated with reshaping air pollutant distributions, this technique can exploit the fact that the location, shape, and orientation of the satellite footprints slightly varies from one orbit to another ( ; ). Thus, this variation can increase the resolution of the satellite-based analysis significantly, but still within the limitation due to the original TROPOMI resolution as well as the temporal sampling intervals. Initially, a grid size of 1 × 1 km was chosen. For each overpass, the TROPOMI footprint pixel coverage was calculated and, for computational reasons, approximated as an ellipse on a rectangular latitude–longitude grid. And then, we calculated the area-averaged value using all measurements overpassing the given cell. During this process, we also considered the influences of the footprint size that was set as a weight inversely proportional to the area of each footprint. Nevertheless, a measurement result would be regarded as erroneous and thus eliminated when it exceeded 10 standard deviations from the average ( ). As a result, 3.12 % of the measurements were eliminated in this way. Note that TROPOMI-based results can present both systematic and random errors ( ). Also, more available data might be of higher statistical power via temporal averaging in nature. Hence, we investigated the impacts of the temporal averaging windows on mapping the NO VCDs over the super-emitters. The main concern is whether one-year TROPOMI measurements could provide as much information on the NO VCDs around the super-emitters. Detailed results can be found in Supplement information. Here we developed an efficient, super-resolution (i.e., 1 × 1 km ) top-down inverse model on the basis of the one-year oversampled TROPOMI NO VCDs (Figs. S1 and S2). This model took into account their chemical and meteorological loss, including chemical loss and horizontal transport. Therein, the horizontal fluxes mainly considered time-averaged advection processes ( ). This model was assumed to be equilibrium ( ), the governing equation of which was thus shown as follows (Eq. ): Therein represents the tropospheric NO VCDs in the grid ( , ) of 1 × 1 km . denotes all ground NO sources, which combines anthropogenic, soil, and biomass burning NO emissions. represents the ratio of NO over NO concentration. In theory, the daytime NO chemical system reaches equilibrium rapidly and varies little. In this study, we set to be 0.76 ( ; ). The remaining terms in Eq. represent the potential NO sinks, including chemical, deposition, and horizontal loss. Given that the local overpass time of TROPOMI is close to noon (i.e., 13:30 local time), the chemical NO sink is dominated by the chemical loss reaction of NO with OH, which can be described by a first-order time constant and, thus, can be estimated from the measured itself as . indicates the lifetimes associated with deposition and chemical loss in nature. In theory, instantaneous NO lifetime is dominated by several factors, such as ozone levels and actinic fluxes, and also the NO concentration itself at the presence of high NO levels. As a result, the NO lifetime is linked with season and meteorological conditions. In the present study, the NO lifetime was assumed to be 4 h for all regions. For many of the hotspots, there can be substantial transport out of the box, in which case the effective lifetime would be smaller than 4 h. Given the high spatial resolution, the emission estimates of a grid cell derived in this way were more likely to be underestimated than overestimated. For specific remote sites, however, 4 h could be too short. We applied conservative assumptions using lifetimes of 1 h and 24 h to provide upper- and lower-bound emission estimates. The corresponding uncertainties are considered and shown as error bars in . Although more sophisticated methods, like regional CTM simulations, are available, they would not significantly optimize the high-resolution emission estimates for the super-emitters. ∙ denotes the time-averaged advection term. Therein represents the time-averaged wind vector, which is obtained from the WRF-CMAQ driven by the ERA5 reanalysis dataset. We adopted the averaged wind fields of the lowest 14 vertical levels (out of 157 levels in total). Such wind fields can represent the vertical altitudes from surface to about 500 m. Note that upward/downward NO and aircraft-/lightning-emitted NO are significantly lower than near-ground NO ( ; ), particularly over super-emitters. Despite this, they typically have longer lifetimes, present smoother horizontal distributions, and thus play an un-negligible role in shaping regional background. Hence, they would not substantially affect the identification of super-emitters but high-resolution emission inversions. To this end, we considered them as regional backgrounds and removed them from the NO VCDs so as to include only the emission fluxes of the point sources responsible for the hotspots. Such background correction is illustrated in the Supplementary information. In addition to the identification of the super-emitters, imaginary boxes around them were established, out of which the observed VCDs and the estimated emissions fell back to background values ( ). To make conservative emission estimates for the super-emitters, we applied the spatial integration method to account for small-scale (pixel-to-pixel) noises ( ). Specifically, only the fluxes (i.e., ) of the grids were summed that contained the super-emitters as their total emissions. These grids referred to the cells in the above assumed boxes ( ). In this study, the scales of the localized super-emitters were mostly smaller than those in previous studies. Accordingly, the assumed boxes were nearly close to the 5 × 5 km scale, also smaller than previous findings (e.g., the order of 10 × 10 km ). Overall, our emission estimates derived in this way were more likely to be underestimated than overestimated. Emissions of super-emitters and cities were also calculated in MEICv1.3 and used to compare with the satellite-based emission estimates. For 2016, this bottom-up inventory was established at a spatial resolution of 0.25° × 0.25° and of five anthropogenic sectors (i.e., agriculture, power plant, industry, residential, and transportation). According to the super-resolution results, the inventory was first re-gridded to the 1 × 1 km resolution. On this basis, the emission rates of the super-emitters were calculated in the same way as descripted above. Associated uncertainty analysis is presented in Supplementary information. We identified the super-emitters manually based on the combination of the tropospheric NO VCDs, the top-down NO emission estimates, and the high-definition Landsat 8 imageries ( and S3–S5). Here these imageries are updated to the year (i.e., 2019) of TROPOMI observations. First, we identified the localized maxima in the VCD and emission maps, which were significantly higher than the background value (see Supplementary information) within limited geographical scopes (i.e., <5 × 5 km ). Second, together with the visible imageries, we attributed the characteristic VCD and emission enhancements to industrial hotspots or parks. As a result, each super-emitter was approximated as a rectangle on the latitude-longitude grid. The approximate central, minimum, and maximum coordinates of which were recorded in . Third, although the visible imageries enabled us to ascertain the locations of the super-emitters, it was not possible to identify the industry type directly. Particularly, in China, where the industry was still rapidly developing, the associated bottom-up information may not be accessible in time. In addition, the Baidu Map allowed us to assign a name to each super-emitter. The usual choice was the name of the specific address recorded in the Baidu Map or the name of the nearest geographical area. We identified the super-emitters manually based on the combination of the tropospheric NO VCDs, the top-down NO emission estimates, and the high-definition Landsat 8 imageries ( and S3–S5). Here these imageries are updated to the year (i.e., 2019) of TROPOMI observations. First, we identified the localized maxima in the VCD and emission maps, which were significantly higher than the background value (see Supplementary information) within limited geographical scopes (i.e., <5 × 5 km ). Second, together with the visible imageries, we attributed the characteristic VCD and emission enhancements to industrial hotspots or parks. As a result, each super-emitter was approximated as a rectangle on the latitude-longitude grid. The approximate central, minimum, and maximum coordinates of which were recorded in Supplementary Table 1. Third, although the visible imageries enabled us to ascertain the locations of the super-emitters, it was not possible to identify the industry type directly. Particularly, in China, where the industry was still rapidly developing, the associated bottom-up information may not be accessible in time. In addition, the Baidu Map allowed us to assign a name to each super-emitter. The usual choice was the name of the specific address recorded in the Baidu Map or the name of the nearest geographical area. There remain two main uncertainty sources that significantly affect this identification process. First, although automated ways with uniform thresholds might result in more consistent and flexible identifications, no satisfactory criteria were found to identify super-emitters nationwide. This is mainly because, in localized regions, the NO budgets respond to not only super-emitters but also variable backgrounds. Second, it is possible that artifacts in the TROPOMI measurements might be mistaken as the super-emitters. Such errors are common in satellite-based measurements, and currently, their origins are still not completely clear. To this end, mutual corroboration between the TROPOMI-based measurement enhancements and the Landsat 8 visible imageries would help prevent such false positives to a large extent, as abovementioned. Here these imageries are updated to the year of TROPOMI observations. We further introduced the significance ratios of the VCDs to optimize the identification of the super-emitters. Specifically, the ratios of the one-year averaged VCDs to their standard deviations were calculated. For a candidate super-emitter, if the ratio map was projected to present the dipole distribution (Fig. S6), i.e., the ratios peaked in the centre and decreased outward, the enhancements of the VCDs would be mainly attributed to the super-emitter. If not, such enhancements would be regarded as noise-dominated and the candidate super-emitter would thus be eliminated. We must highlight that, within densely source areas (e.g., megacities), the emissions from numerous super-emitters would interfere with each other, vanish the VCD gradients, and thus disable our approach. Other uncertainty analysis can be found in Supplementary information. We applied an oversampling approach to exploit the variable spatial coverage of the satellite pixels (Methods). Consequently, we achieved the super-resolution (i.e., from 3.5 × 5.5 km to 1 × 1 km ) tropospheric NO VCDs based on the TROPOMI measurements in China ( a). We also zoomed in on five sub-regions (including Northwest, Northeast, North, Southwest, East, and South China) ( b–g). On this basis, the consequent NO emission map was derived by an efficient, compatible top-down inverse model (Figs. S1 and S2). In theory, this set of maps ( and S1) is distinct from previous outcomes obtained from early satellite-based surveys (e.g., those based on the OMI or TROPOMI measurements) ( ; ; ). This is mainly attributed to major improvements in the spaceborne instrument (i.e., TROPOMI) and the top-down model developed here. This integration is of more hyperfine resolution and takes these nonlinear meteorological and chemical effects into account (Methods). An illustrative comparison is explained in the next section. We isolated and identified hundreds (534) of super-emitters ( and S1) that were inventoried in only if they could be resolved unambiguously on the basis of the maps alone without the need for a priori knowledge. Comprehensive uncertainty analysis confirmed that the identification of super-emitters was robust (Supplementary information). We applied an oversampling approach to exploit the variable spatial coverage of the satellite pixels (Methods). Consequently, we achieved the super-resolution (i.e., from 3.5 × 5.5 km to 1 × 1 km ) tropospheric NO VCDs based on the TROPOMI measurements in China ( a). We also zoomed in on five sub-regions (including Northwest, Northeast, North, Southwest, East, and South China) ( b–g). On this basis, the consequent NO emission map was derived by an efficient, compatible top-down inverse model (Figs. S1 and S2). In theory, this set of maps ( and S1) is distinct from previous outcomes obtained from early satellite-based surveys (e.g., those based on the OMI or TROPOMI measurements) ( ; ; ). This is mainly attributed to major improvements in the spaceborne instrument (i.e., TROPOMI) and the top-down model developed here. This integration is of more hyperfine resolution and takes these nonlinear meteorological and chemical effects into account (Methods). An illustrative comparison is explained in the next section. We isolated and identified hundreds (534) of super-emitters ( and S1) that were inventoried in Supplementary Table 1 only if they could be resolved unambiguously on the basis of the maps alone without the need for a priori knowledge. Comprehensive uncertainty analysis confirmed that the identification of super-emitters was robust (Supplementary information). Spatially, these super-emitters are scattered on every corner of China, even in remote zones close to the frontiers (e.g., Northwest and Southwest China) ( ). As expected, such super-emitters concentrated over North, East, and South China. By combining the Landsat 8 images ( ), the identified super-emitters can be classified into two classes: industrial hotspots and industrial parks. When industrial parks were sometimes blended with a handful of anthropogenic sectors, like residential sources, they were taken together as individual super-emitters. Illustrative examples are shown in and S3 (Detailed spatial information is highlighted in Fig. S4). Industrial hotspots were consistently found to be associated with isolated industrial factories or power plants with one or more chimneys, as presented in the visible images. For instance, a typical localized NO emission maximum was found in a remote desert area in Northwest China ( a), coinciding spatially with a large industrial factory (Zeketai, Northwest China), as shown in the satellite image. It was adjacent to a small city (i.e., Xinyuan in Xinjiang Province) dominated by residential areas, but with much lower NO distributions and emissions (Fig. S4a). Longzhudou (East China) ( b) and Huaxin (Southwest China) (Fig. S3a) were similar examples of industrial hotspots. They were also located in mountainous areas but close to medium-sized cities (Ningde in Fujian Province and Lhasa in Tibet). The Jincheng Steel Group, located in North China, is a key base of the iron and steel industry ( c). NO emissions associated with coal-fired power plants were also identified, for example, in Rundajianeng (Northwest China) ( d). The second class, that of industrial parks, was mainly linked to intensive industrial facilities (sometimes blended with a handful of other anthropogenic sectors) but limited geographical extent (i.e., 1 × 1 km ), for which 150 parks were detected. They were mostly associated with chemical and manufacturing industries, such as oil and gas, iron and steel, and foundry production. Representative well-isolated examples include the parks in Baitong (Northwest China) ( e) and Shizuishan (North China) ( f). These super-emitters were clearly detected in the set of the maps, despite already large background concentrations. Chemical industrial parks, due to their large production, were often found to be geographically close to their distribution market. Nanbao (East China) ( g) and Jiaochuan (East China) (Fig. S3e) are the representative and bordered on the Beijing-Tianjin-Hebei region and the Yangtze River Delta region, respectively. Many industrial parks were also found near raw-material-related industries (e.g., oil fields and coal mines), thus conducive to building an agile supply chain. Moreover, such examples are abundant. For instance, the large petrochemical industrial parks, such as Longmen (Northwest China) (Fig. S3f) and Shengbang (Northwest China) (Fig. S3g), were also close to the coal bases (i.e., the Hancheng Coal Mine and the Shenmu Coal Mine). Note that there are countless enhancements of NO VCDs and emissions that cannot be linked clearly with clear super-emitters. Daqing (Northeast China) ( h) is such a representative, in which no super-emitter could be isolated in our approach. Rural regions were additional representatives, which were commonly found with high NO VCDs and emissions. These regions correspond to, for example, residential areas and small-scale manufacture clusters, such as Jiqingbao (North China) (Fig. S5a), Pangjing (East China) (Fig. S5b), and Chetian (Southwest China) (Fig. S5c). The primary NO sources might be attributed to scattered coal combustion ( ; ). Nevertheless, it was difficult to link those sources with well-isolated super-emitters. Also, NO emissions in cities represent a substantial part of the total atmospheric NO budget. Megacities, such as Beijing (Fig. S5d), Shanghai (Fig. S5e), Guangzhou (Fig. S5f), and Shenzhen (Fig. S5g), are the cases in point. Small-medium cities, such as Lhasa (Southwest China) (Fig. S5h), Shizuishan (Northwest China) (Fig. S5i), and Yongan (South China) (Fig. S5j), are no exception. The main sources can be consistently related to urban transportation, which, however, are too diffuse to emerge as individual super-emitters in our maps. In addition, in the one-year average of satellite detections, it was generally difficult to capture discontinuous and instantaneous biomass burning. Thus, hotspots dominated by biomass burning were excluded from this study. It is necessary to assess the role of the identified super-emitters in reshaping the localized NO budget. To this end, the annual NO emissions were calculated for all the super-emitters and 370 cities in China via our top-down inverse model ( , ). Comprehensive uncertainty analysis confirmed that the quantification of super-emitters was reliable and conservative. It is necessary to assess the role of the identified super-emitters in reshaping the localized NO budget. To this end, the annual NO emissions were calculated for all the super-emitters and 370 cities in China via our top-down inverse model ( , Supplementary Table 1). Comprehensive uncertainty analysis confirmed that the quantification of super-emitters was reliable and conservative. On this basis, we compared these with a state-of-the-art bottom-up emission inventory (i.e., MEICv1.3) (Methods). For remote small-sized cities, the bottom-up inventory often underestimated emissions by more than a factor of five. For example, some isolated super-emitters (i.e., industrial hotspots or parks), such as Luopu (Northwest China), Hejing (Northwest China), and Geermu (Northwest China), were missed ( and S7a and b). In turn, for 67 % of the cities, the emission fluxes agreed within a factor of two (85 % within a factor of three) and, importantly, when all large and medium-sized cities were considered, no major bias emerged ( ). Even, emission estimates for several megacities, such as Shanghai and Shenzhen, were less than those in MEICv1.3 to some extent (i.e., >−20 %). This could be attributed to persistent air pollution controls in China since 2013. On this basis, we compared these with a state-of-the-art bottom-up emission inventory (i.e., MEICv1.3) (Methods). For remote small-sized cities, the bottom-up inventory often underestimated emissions by more than a factor of five. For example, some isolated super-emitters (i.e., industrial hotspots or parks), such as Luopu (Northwest China), Hejing (Northwest China), and Geermu (Northwest China), were missed ( and S7a and b). In turn, for 67 % of the cities, the emission fluxes agreed within a factor of two (85 % within a factor of three) and, importantly, when all large and medium-sized cities were considered, no major bias emerged (Supplementary Table 1). Even, emission estimates for several megacities, such as Shanghai and Shenzhen, were less than those in MEICv1.3 to some extent (i.e., >−20 %). This could be attributed to persistent air pollution controls in China since 2013. In contrast, it was immediately clear that emissions from the super-emitters were largely underestimated in MEICv1.3 ( ). Of all the identified super-emitters, only 34 % agreed within a factor of three and, more importantly, only 67 % within one order of magnitude. Specifically, our emission estimates for the super-emitters exhibited a heavy-tail distribution, in which 20 % of the identified hotspots were responsible for >50 % of all the detected super emissions ( ). Compared to MEICv1.3, this satellite survey revealed three classes of super-emitters. First, 182 super-emitters were identified owing to a nearby localized maximum in the bottom-up inventory, despite exceptionally low emission fluxes. Representative examples were presented in Fig. S7, including Maligou (Northwest China) (Fig. S7c) for an industrial hotspot and Tuxianzhuang (North China) (Fig. S7d) and Peijiafen (North China) (Fig. S7e) for two industrial parks. Second, 189 super-emitters seemed displaced in MEICv1.3 in various degree, by at least one grid cell (i.e., ∼25 × 25 km ) (from the identified super-emitter centre), for example, Zhongjing (Southwest China) (Fig. S7f) and Guolemude (Northwest China) (Fig. S7g). Third, a number of super-emitters were essentially absent from the inventory because their fluxes in MEICv1.3 did not represent a localized maximum and at least one order of magnitude lower than our results. Notably, Nanzamu (Northeast China) (Fig. S7h), Chengbei (Northwest China) (Fig. S7i), Tanjiazui (South China) (Fig. S7j), and Mengsheng (Southwest China) (Fig. S7k) were captured by the one-year satellite observations. They represented the localized (i.e., 1– 10 km) maximum but were undoubtedly missed in the inventory. Such underestimations occurring at the super-emitters were partly related to the relatively coarse spatial resolution of the MEICv1.3 inventory. It might thus be more informative if we compare our results with bottom-up inventories with a finer resolution. Here we compared our results with another bottom-up inventory, i.e., the HTAP inventory. Similar to the MEICv1.3 inventory, it is a precious nation-wide and sector-wide inventory, but with finer resolution (0.1° × 0.1°), which can play a key role in verifying our results ( ). Fig. S8 presents that the HTAP inventory also underestimated the emissions from the super-emitters substantially. Besides, we compared total emissions over specific areas from our results with those from MEICv1.3. Specifically, for each super-emitter, a set of specific areas was defined as square areas centred on this super-emitter and with different side lengths (i.e., 1 × 1 km , 10 × 10 km , and 25 × 25 km ). The lowest side length (i.e., 1 km) denotes the exact scale of the super-emitter (see ), while the largest side length (i.e., 10 km) was comparable to the spatial resolution of MEICv1.3 (0.25° × 0.25°). Fig. S9 presents that these total emissions over specific areas from our results kept far beyond those from MEICv1.3. And the gaps were not narrowed equivalently with the specific areas expanding. In addition, as abovementioned, we explicitly examined the grids containing (surrounding) the identified super-emitters in MEICv1.3. It was clear that, for a number of the super-emitters, such grids did not include any emissions (e.g., , S7a–b, and h–k). Collectively, these results reinforce our main conclusions that such large gaps are mainly due to the existence of the super-emitters, and current bottom-up inventories do not adequately characterize super-emitters, typically underestimating (even missing) the super-emitters. It should be noted that, while current region-limited (e.g., for cities) or sector-limited (e.g., for heavy-duty trucks) inventories are typically of finer resolution (e.g., <0.25° × 0.25°) (e.g., , last access: 10 June 2022), they are difficult to verify the identified super-emitters that are typically distributed nationwide, far from the cities, and associated with industrial hotspots (or parks). This satellite-based study has the potential to capture the establishment or the discontinuation of industrial activities unambiguously. New or expanded super-emitters that emerged within the satellite measurements were found in this way. Correspondingly, the satellite-based emission fluxes were significantly higher (at least one order) than the bottom-up estimations. For instance, high NO emissions were observed over Xincheng (North China) (Fig. S10a) and Huagang (South China) (Fig. S10b). On the other hand, industrial plant closures were also detected, for example, over Shidian (North China) (Fig. S10c) and over Hunhe (Northeast China) (Fig. S10d). Consequently, the bottom-up emission inventory was likely outdated, overestimating the satellite-based estimates by >100 %. The one-year satellite observations, assisted by the top-down inverse model, made it possible to reveal and quantify the super-emitters. Also, it is noteworthy that, in the same way, a handful of the super-emitters therein could also be identified and assessed on shorter timescales (e.g., monthly). Eight representative super-emitters were presented in . More detailed information is shown in . Their monthly variations in the satellite-based emission estimates were found to be stable (i.e., <26 %) ( ). This indicates that even one-month satellite overpasses could track their yearly emission estimates and be capable of tracking their emission variations. Note that, for most of the super-emitters, the observed signals might be too weak to be identified on shorter timescales, mainly due to the limited data. The one-year satellite observations, assisted by the top-down inverse model, made it possible to reveal and quantify the super-emitters. Also, it is noteworthy that, in the same way, a handful of the super-emitters therein could also be identified and assessed on shorter timescales (e.g., monthly). Eight representative super-emitters were presented in . More detailed information is shown in Supplementary Table 2. Their monthly variations in the satellite-based emission estimates were found to be stable (i.e., <26 %) ( ). This indicates that even one-month satellite overpasses could track their yearly emission estimates and be capable of tracking their emission variations. Note that, for most of the super-emitters, the observed signals might be too weak to be identified on shorter timescales, mainly due to the limited data. Here we have focused on regional scales rather than large scales (e.g., a national scale) to examine the significance of the identified super-emitters. For a given super-emitter, its surrounding region was defined as a square area centred on the super-emitter and with a horizontal length scale of 50 km. Such a spatial scale was generally equivalent to that of a medium-sized county. On this basis, we compared the estimated emissions of the identified super-emitters with those of their surrounding regions. Several illustrative examples are shown in . We confirmed that the identified super-emitters play a dominant role in reshaping regional NO emission budgets. Specifically, super-emitters accounted for 3– 21 % of the regional NO emissions, most of which contributed up to 10 %. Hence, regional NO emission budgets would be overturned if the super-emitters are missed or largely underrated. By comparison, the contributions of some super-emitters close to cities (e.g., Jincheng, c) were relatively small but still prominent (>7 %). We compared our results with a state-of-the-art top-down NO emission inventory ( ) for representative super-emitters. This inventory relied on the same satellite observations (i.e., TROPOMI), but adopted a distinct inverse model (i.e., the DECSO algorithm) on a 0.25° × 0.25° resolution. Figs. S11 and S12 demonstrate that these two products are very similar in general spatial distributions and regional magnitude. In contrast, a key advance in this study was the considerable increase in spatial resolution. Of particular relevance are the three super-emitters, which can only be distinguished in our results. Whereas in-situ emission monitoring can provide a direct and valuable opportunity to support our emission estimates of the super-emitters, it is difficult to access a large dataset of CEMS in China. Here we sampled the CEMS data in 2019 at a handful (50) of representative super-emitters. The detailed information is recorded in . Therein, only hourly data (i.e., from 13:00 to 14:00, corresponding to the overpass time of the satellite) were included and averaged. On this basis, data were applied to evaluate the one-year averages of the emission estimates derived from our approach. Whereas in-situ emission monitoring can provide a direct and valuable opportunity to support our emission estimates of the super-emitters, it is difficult to access a large dataset of CEMS in China. Here we sampled the CEMS data in 2019 at a handful (50) of representative super-emitters. The detailed information is recorded in Supplement Table 3. Therein, only hourly data (i.e., from 13:00 to 14:00, corresponding to the overpass time of the satellite) were included and averaged. On this basis, data were applied to evaluate the one-year averages of the emission estimates derived from our approach. shows a good correlation between the emission estimates derived from our approach and the CEMS data. The correlation (R ) was up to 0.71 with a slope of 0.63. In turn, the regression analysis shows a low bias (−15.95 %) for our emission estimates, confirming that our results were conservative, consistent with previous findings ( ; ; ). Note that the alternative AMF, in which the vertical profiles and the surface albedos are optimized by super-resolution satellite-based measurements and CTM simulations, would help reduce these underestimations ( ; ; ). In addition, represents the recent advances in identifying NO emitters from space. It presents a global catalog of NO point sources by a fully automated iterative algorithm, yielding 451 point sources, and can be considered as a preferable reference ( ). We thus conducted a quantitative comparison in the results between and this study (Fig. S13). First, our results identified more super-emitters and can be used as another starting point for a more exhaustive inventory of global NO point sources. The main reason is that fully depended on an automated identifying algorithm, which can only work well at much more uncomplicated wind fields and topographies. For instance, this automated algorithm was difficult to identify super-emitters in mountainous regions, while manual efforts might be competent. Second, our estimates remained mostly lower than those in , while they were typically higher than the records in bottom-up inventories (as illustrated above). This indicates that our estimates were robust and conservative, as the conclusion of our uncertainty analysis (see Supplementary information). More importantly, these conservative results reinforce our main conclusion that numerous super-emitters are potentially missed in current state-of-the-art bottom-up inventories, and it is necessary to address the NO budget by revisiting super-emitters on a large scale. Here we develop an efficient, super-resolution (1 × 1 km ) inverse model for NO emissions based on whole-year TROPOMI satellite observations. Consequently, we resolve hundreds of NO super-emitters in virtually every corner in China, even in remote and mountainous zones. State-of-the-art bottom-up emission inventory (i.e., MEICv1.3 and HTAPv2) do not adequately identify these super-emitters. More than one hundred super-emitters are missed and the rest are generally underestimated substantially. The same is generally true of traditional top-down inverse methods. More importantly, we confirm that such super-emitters typically dominate the NO budget in a localized area (e.g., equivalent to a spatial scale of a medium-sized county). Although our dataset is incomplete nationwide due to the undetectable super-emitters on top of high pollution, our results imply that super-emitters contribute significantly to national NO budgets. Therefore, we provide the first evidence for widespread missing or underrated NO super-emitters in China and suggest an urgent need to revisit traditional bottom-up NO inventories. This essential dataset of NO super-emitters is not unique. A repeat of this study via other top-down inverse models may thus revise our dataset by supplementing super-emitters or updating their emission rates ( ). However, widespread missing NO super-emitters would remain regardless of how these models are designed, and large gaps (e.g., more than one order of magnitude) between bottom-up and top-down emission estimates would not be closed, at least regarding a number of super-emitters. Future works to optimize high-resolution emission inventories include extending time coverage, improving spatial resolution, and integrating multifarious top-down sensors and bottom-up information. Particularly, CEMSs remain largely absent for super-emitters. Widespread and sustained deployments of multi-tiered observational strategies, e.g., an integrated network of spaceborne, airborne, and CEMS measurements, could greatly advance scientific understanding of NO budgets and super-emitters. It is also necessary to introduce artificial intelligence (Machine Learning or Deep Learning, rather than manual cognition) to recognize potential super-emitter-driven daily divergences rapidly and impartially, owing to huge data volumes of daily VCDs ( ). The following are the supplementary data related to this article. Supplementary data to this article can be found online at . The codes are available upon request to corresponding author. : Conceptualization, Methodology, Writing, and Supervision. : Data curation, Software, Investigation, and Validation. : Reviewing and Editing.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Not Used\n",
      "* **AIモデルの判断:** Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 147 / DOI: 10.1016/j.agsy.2024.104243  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Gridded daily weather data for North America with comprehensive uncertainty quantification\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Enhancing simulations of biomass and nitrous oxide emissions in vineyard, orchard, and vegetable cropping systems\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "It's critical to mitigate climate change impacts, which becomes more challenging as population growth and urbanization intensify resource demands ( ). With the deadline approaching to meet the minimal climate change mitigation goals such as those in the Paris Agreement ( ), countries and states have set plans to reduce greenhouse gas emissions, recognizing the increasingly important role of agriculture (California Air Resources ; ; ; ; ; ). Agriculture is a significant contributor to global greenhouse gas emissions, particularly through the production of nitrous oxide (N O). Nitrous oxide has an estimated 100-year global warming potential and global temperature-change potential of 273 and 233 times that of CO , respectively ( ). Global N O emissions have increased to 17 Tg N O_N year with the fastest growth since the 1980s. From 1980s to 2016, 71 % of the rise in anthropogenic N O emissions was from agriculture, the majority of which was attributed to nitrogen (N) fertilizer applications ( ). Specialty crop (SC) systems, including vineyards, orchards, and vegetable cropping systems, have significant potential to reduce net emissions from agriculture lands due to their economic importance and extensive global land coverage (376.4 × 10 ha) ( ; ). However, N O emissions from SC systems have been less studied ( ; ; ; ; ), despite their potential as significant N O emissions sources. This is particularly concerning as they are among the highest value crops grown and can be input-intensive (e.g., irrigation and fertilization) ( ; ). Accurate accounting of land N O emissions remains challenging, despite ongoing efforts to improve monitoring and estimation methods ( ). In places where N O emission measurements are limited, N O emissions are likely estimated by the product of N input and emission factors (EF), either using Intergovernmental Panel on Climate Change (IPCC) Tier 1 guidelines (e.g., disaggregated emission factor averaged from 0.002 to 0.016; ), or crop- and region-specific emission factors when available ( ; ; ; ; ; ; ). Preferably, a Tier 3 method, such as process-based models with detailed soil, climate, and management information, may be used for potentially lower prediction errors ( ). Process-based models use commonly available information (e.g., soil texture, vegetation type, land management, weather) as well as informed plant- and site-specific parameters to simulate soil and plant processes along with C and N cycles for various ecosystems, including croplands, grasslands, rice paddies, forests, savannahs, and wetlands ( ; ). In the realm of SC systems, the current estimation of N O emissions are primarily based on the EF ( ; ; ; ; ). There were a few process-based modeling and calibrations from a limited range of crops, soils, climates, and management practices. and calibrated N O emissions of crops including almond, grape, beans, and lettuce in California using DeNitrification-DeComposition (DNDC) model. calibrated DNDC to simulate two years of greenhouse gas emissions including N O from a rice-vegetable cropping site in southern China. Another process-based model, Soil Water Heat Carbon and Nitrogen Simulator (WHCNS), was used to simulate N O emissions of common vegetables under greenhouse structures in northern China ( ; ). Last but not least, DayCent®, a widely adopted biogeochemical model, has been extensively used for simulating N O emissions across various scales, ranging from the national inventory of greenhouse gas emissions and sinks ( ) to global-scale simulations of croplands and grasslands ( ; ). Despite these efforts, there remains a significant hindrance to providing accurate regional simulations that include SC systems. This gap is highlighted by the lack of calibration and validation of process-based models for SC biomass dynamics and N O emissions, and is exacerbated by limited studies on the management effects on them ( ; ; ; ). There is a need to bridge the gap by completing process-based model calibration and validation for special crop biomass dynamics under various management, soil, and climate conditions. Besides environmental factors (soil and climate), crop-specific parameters are the driving forces in models for simulating N O emissions from different SC systems. Biomass measurements are currently more accessible than other variables such as soil mineral N in SCs ( ). Therefore, this study focused on calibrations on SC biomass over various management, soil, and climate, followed validation on both biomass and N O emissions. The objectives of this study are to: 1) conduct the DayCent® model calibration and validation for SC biomass dynamics in eight SC systems; 2) evaluate N O emission simulations by only calibrating crop-specific parameters; 3) simulate N O emissions for each SC production region in California as a case study. We reviewed field studies of vineyard, orchard, and vegetable cropping systems, which were also included in a meta-analysis we conducted in another study ( ). Based on the exhaustive literature review by , we narrowed down the studies to those with field measurements of adequate variables (biomass C, N, and N O) covering at least one growing season ( ). As a result, seasonal and annual N₂O emission measurements, along with biomass data of different plant organs from the same regions, were compiled for grape ( ), almond [ (Mill.) D.A. Webb], peach ( L.), walnut ( ), lettuce ( ), broccoli ( var. P.), cauliflower ( var. L.), and tomato ( L.) planting system. These observations were collected from fields spanning seven Koppen-Geiger climate types and five countries (the United States, Germany, Spain, France, and Australia). For the later analysis of treatment effects, 108 treatment levels were consolidated into 17 treatment groups by combining identical treatment factors ( ). The detailed descriptions of selected studies are in the Supplement and the full dataset is available in the Zenodo online data repository ( ). When only dry mass was measured, biomass C content for aboveground vegetable crops and berry fruit was assumed at 43 %; nut fruit and woody organs of orchard tree at 48 % ( ; ; ; ). Weighted averages of N O emissions by areas of tree/vine row and interrow were used. DayCent® ( ) is the daily time-step model derived from the CENTURY biogeochemical model and adopts a “Hole-in-the-pipe” concept to represent N O produced from nitrification and denitrification ( ). Soil and weather input data for the DayCent were retrieved from public databases when they were not available from the selected studies. Mean soil texture and pH were retrieved from the U.S. Department of Agriculture (USDA) Natural Resources Conservation Service and National Cooperative Soil Survey databases [R package: soilDB ( )]. Bulk density, saturation conductivity, fild capacity, and wilting point, when they were not reported, were calculated from soil texture ( ). Weather input data were from 1-km daily climate data of Daymet for the United States ( ) ( ) and of for Europe [R packages: daymetr, easyclimate ( ; )] based on the coordinates of experimental sites, unless data of nearby weather stations were reported or available online (see Supplement). A spinup period of 4000 years of representative native vegetation were simulated to reach equilibrium, followed by agricultural cropping since the 19th century till the beginning of experiment period of each study. Agricultural management practices were based on published experiment records. For periods when management practices were not available, we relied on surveys collected from U.S. producers, which were also integrated in the USDA COMET-Farm™ tool (a public tool for on-farm greenhouse gas and carbon accounting; ). The calibration used adequate biomass measurements (both C & N) from at least one study of each SC. Sites with merely N O and sites with fewer biomass data would be validation sites. The biomass of remaining studies and all N O emission measurements were used to validate the model performance of each SC. Available biomass N was calibrated and validated at the same locations where biomass C calibration and validation were performed. We calibrated minimal group of parameters (i.e., crop-specific) that are less sensitive to locations, which can avoid the situation of overfitting too many parameters in the first place followed by recalibration when applied to other sites. Furthermore, calibrations similar to this study will be relatively more stable and useful for large-scale simulations with finer tuning for site parameters if necessary ( ). For calibration, crop/tree phenological and physiological parameters were calibrated, including parameters regarding radiation use efficiency, temperature effects on growth, minimum and maximum C/N ratio of different organs, C allocation fraction of different organs, leaf area index effect on biomass production, growing degree days, lignin content of different organs, N reallocation fraction at harvest. Given the lack of information, characteristics from species-level but not cultivar-level (within a species) were parameterized in the calibration. Full lists of crop/tree parameters and definitions including calibrated ones are available in the Supplement. Other parameters regarding mineralization, nitrification and denitrification, were kept at default settings. Orchard and vineyard were configured by the Orchard tree fraction (OTRF) option in the DayCent, and 90 % fertilizer was assumed available for the tree/vine row. Representative area for interrow cover crops was assumed for respective orchard/vineyard types unless it was specified in the study. DayCent simulations for biomass (including different organs) were evaluated for accuracy and goodness of fit in calibration and validation datasets over treatments and sites of each SC, using mean relative error (MRE) in Eq. , relative root mean square error (RRMSE) in Eq. , Index of Agreement (IA) in Eq. , and the coefficient of determination (R ) in Eq. . The closer the MRE and RRMSE values are to 0, the lower the bias and error of the simulations compared to the measurements, respectively. IA ranges from 0 (no agreement) to 1 (perfect fit), and empirical goodness of fit indicated by IA was suggested by some studies at 0.7 to 0.8 for biomass, below and above which appears poor and strong, respectively ( ; ). However, for more variable emission data, 0.5 to 0.6 was also deemed as acceptable ( ; ; ). Combination of these criteria to evaluate a model efficacy is recommended since no single criteria performs ideally ( ). Relative standard deviations (RSD) of measurements [Eq. ] were also calculated among studies for each SC. All N O emissions (seasonal or annual) were also validated using the same metrics. represent the mean of the observed value. The IPCC Tier 1 method of the 2019 Refinement to the 2006 IPCC Guidelines was used to calculate seasonal and annual N O emissions [Eq. (6)], in which direct N O_N emissions are the product of N inputs [synthetic fertilizers (N ), organic additions (N ), and crop residue (N )] and EF [0.016 (0.013 to 0.019) for synthetic fertilizers, 0.006 (0.001 to 0.011) for other N inputs, and 0.005 (0.000 to 0.011) for N applied in dry climates] ( ; ). All the locations of orchards and vineyards in this study were in dry climates and low-volume irrigation ( ; ; ). Aboveground crop residue N was estimated from the average residue proportion and average aboveground N from the available lettuce, broccoli, cauliflower measurements. Emissions contributed from belowground residue N and residues in tomato fields, orchards, and vineyards were not included in the calculation due to unavailable measurements ( ; ). This study selected California, USA for regional simulations of N O emissions using DayCent, because California encompasses vast land longitudinally, including arid, temperate, and cold climates, as well as Mediterranean conditions. California is a significant producer and exporter of the SCs globally and accounts for 63.8 to 100 % of the eight abovementioned SCs in the United States ( ). About 500 sites were randomly sampled for each SCs in major production regions according to California Statewide Crop Mapping ( ) and then counties with at least 5 sample sites were selected to represent the major production regions in this study. Spinup periods were simulated for each site based on historical management records (also used in the COMET-Farm), followed by a 14-year establishment period. Representative inputs (fertilization and irrigation) and management schedules were the same for sites of the same SC, which were based on California Cost & Return Studies ( ) and a management survey to California specialty crop growers in 2016–2017. Based on the growing season ( ), vegetable systems were assumed to be monocropping with two rotations annually, except for tomatoes, which has one rotation. Residues of vegetables from yield harvests were assumed incorporated into soil (conventional practice). To estimate the current N O emissions of SC systems in California, each site was simulated for twenty years (2015–2035) using the above calibrated crop parameters, USDA Soil Survey databases [R package: soilDB ( )], and historical weather data from Daymet (1980–2022) in United States ( ). As aggregation of individual sites can reduce uncertainties ( ), county-level averages of annual yield-scaled N O emissions (the twenty years) were calculated. Locational and temporal coefficient of variations (CV) of annual yield-scaled N O emissions were calculated for each SC regions. Additional outputs and inputs of the DayCent, including fertilization and irrigation, were in the Supplement. Computations and graphics were generated using the R software (v. 4.3) and ArcMap (v. 10.7). The field studies simulated in this study included eight SCs across arid, temperate, and cold major climates ( ) ( ). Calibration on crop/tree parameters of DayCent can improve model performance for biomass C, N, and N O emission projections ( ). Across all datasets, crop calibration in DayCent significantly improved model fit (e.g., IA = 0.976 and 0.730) and reduced bias and error for both biomass C and N simulations, compared with using the original crop/tree parameterization (e.g., IA = 0.865 and 0.366), respectively. Accordingly, there were also better model fit and reductions in bias and error for simulations of N O emissions (MRE =1.226, RRMSE =0.971, R = 0.407, IA = 0.772), compared with using original parameterization (MRE = 1.454, RRMSE = 1.102, R = 0.326, IA = 0.728). The original and crop-calibrated DayCent both outperformed the 2019 refined IPCC Tier 1 method (IA = 0.448), which had overall greater overestimation (MRE = 4.085) and error (RRMSE =1.835) for N O emissions. Crop-calibrated DayCent simulated biomass C, N, and N O emissions in SC systems with good model performance over treatments and locations. Here, detailed calibration and validation performances of biomass C and N O emissions over treatments & locations were presented for each cropping system ( ). Biomass N was evaluated (not available for walnut and grape) in the locations where biomass C and/or N O emissions were also measured (Fig. S1-S6). In vineyards, there was strong performance of production simulations under different tillage and cover crop treatments, and different compost rates (IA = 0.81, MRE = 0.36, R = 0.50; A). DayCent was relatively less sensitive to yearly differences of the compost impact. For example, some grape production under compost was underestimated only in the first year of a 2-year experiment. The grape production was well validated at different irrigation rates (0.06 MRE, 0.75 IA, R = 0.91). The N O emissions were 63 % overestimated on average, but well fitted by DayCent with a 0.82 IA ( B). There was 60 % of the N O emission variation from sites and treatments explained by DayCent. Almond production was 19 and 46 % overestimated on average in calibration and validation studies, respectively ( A). Model fit was strong in calibration but poor in the validation of two biomass observations. There was a good model fit of N O emissions across different irrigation types and inorganic N treatments (IA = 0.78, MRE = 0.12, R = 0.47; B). Peach production was well fitted for calibration and validation studies (IA = 0.82–0.94; A) with relatively low bias (MRE = −0.32 to 0.14). DayCent explained about 77–79 % variability of production. Model fit was relatively weak for N O emissions (IA = 0.47; B). Nevertheless, the high overestimation and errors (2.12 MRE, 1.09 RRMSE, 0.35 R ) were significantly reduced from the original results (3.12 MRE, 1.41 RRMSE, 0.09 R ; Table S1). The model errors of biomass and N O emissions were greatly attributed to simulations for fertilizers with nitrification inhibitors, and N O absorption (negative values) in soils of Aitona, Lleida, Spain, which cannot be simulated in DayCent. As an exception, walnut's yield, woody organ biomass, and N O emission were simulated separately in different locations due to the limited available data ( A). The calibrated biomass was minimally biased, with a strong model fit (IA = 0.92). The model errors were mainly from the yields of DayCent insensitive to different irrigation rates. An independent study without biomass data was used to validate N O emission simulations, and resulted in a 26 % underestimation but strong model fit with 98 % of variability explained by the model (0.80 IA, 0.98 R ; B). Average lettuce production was fairly fitted (IA = 0.70–0.75, MRE = −0.09-0.31; A). Only 24 % of production variability was explained in the validation, which indicated model's weakness in certain treatments (e.g., organic N treatments). The model fit was good in the N O emissions (0.75 IA; B), but bias and error were high averaged over locations and treatments (1.17 MRE, 1.08 RRMSE, 0.45 R ), which however, was improved from original results (1.21 MRE and 1.15 RRMSE, 0.40 R ; Table S1). The broccoli production was well calibrated over studies with inorganic and organic N treatments (−0.05 MRE, 0.99 IA) but performed relatively weakly for the validation studies (0.19 MRE, 0.38 IA) ( A). Model fit was good for N O emissions (0.70 IA; B), but the bias and error were high (2.74 MRE and 1.72 RRMSE), which were reduced from the original results (3.72 MRE and 2.54 RRMSE; Table S1). DayCent still explained 60 % variability of N O emissions. The errors might be due to unknown residue amounts from previous rotation, and model's imperfect N representation for the interactions of inorganic N with organic inputs and/or nitrification inhibitors (Table S2), the processes of which have not yet been fully deciphered ( ). DayCent simulated the effects of N treatments well on the production of cauliflower in Stuttgart, Germany (silty clay loam; continental climate), while higher errors occurred in N treatments in Victoria, Australia (clay loam; temperate climate), which was partly attributed to the unknown harvested proportion (−0.72 MRE; A). The harvest index was currently based on the calibration study that measured both biomass C and N ( A, S5). N O emissions were 74 % overestimated but had a good model fit across treatments of studies with 64 % variability explained (0.76 IA; B). Tomato production was moderately to fairly fitted (IA = 0.68–0.79, MRE = −0.10 to 0.25, 0.26 RRMSE; A). N O emissions had a good model fit across studies (IA = 0.72; B) but were 109 % overestimated overall. Only 33 % of N O emission variation was explained by the model. This could be attributed to overestimation of soil ammonium (Fig. S10). There was a spatial distribution in the 20-year mean of yield-scaled N O emissions among major production regions for each SC in California, with the range of N O emissions high in vegetable systems and low in orchards and vineyards ( ). Yield-scaled annual N O emissions averaged over the major production areas were 0.45, 0.18, 0.28, 0.46 kg N O_N MgC yr for vineyards, almond, peach, and walnut orchards, and 1.02, 1.41, 1.18, and 1.37 kg N O_N MgC yr for lettuce, broccoli, cauliflower, and tomato cropping systems, respectively. According to the California Agricultural Districts ( ), the high N O emission region for vineyards was San Joaquin Valley, especially Kings and Kern County. Sacramento Valley was the high emission region for both almond and walnut orchards. Lake and San Joaquin County were also high emission areas for walnut orchards. Sacramento Valley and San Joaquin Valley, especially Kern County, were high emission regions for peach orchards. Central Coast region as well as Santa Barbara and Ventura County were high emission areas for broccoli and cauliflower cropping systems. Imperial County was high emission area for both lettuce and broccoli fields, and Riverside County was also a high emission area for lettuce fields. Kings and Sutter County were high emission areas for tomatoes. The spatial variability in yield-scaled N2O emissions were mainly driven by the interactions between SC growth potential, soils, climates, and management practices that could differ by SCs (e.g., fertilization and irrigation rates). Allocating more production resources of a SC from its high-emission counties to lower-emission counties could serve as a viable mitigation option for policymakers. There was uncertainty of yield-scaled N O emissions from year and location. The N O emission locational variation was mainly caused by soil variability ( ), and annual variation was mainly driven by annual weather variability ( ). The ranges of the spatial CV was higher in grape, almond, and tomato growing counties, while the temporal CV was higher in grape and lettuce growing counties. Both spatial and temporal CV were lower in walnut and peach orchards, and cauliflower fields (0–20 %), and higher in vineyards. This study is one of the most comprehensive for calibrating and validating both SC biomass (not only yields but other organs) and N O emissions with the largest coverage to date for SC species, geographic regions, soil and climatic types, and management practices ( ; ) ( ; ). While there is a high range of variability contributed by diverse pedological and climatic conditions in the field studies, this study tested the robustness of DayCent by validations using data that are outside the calibrated dataset. The crop parameterization in DayCent improved N O emission simulations in all aspects of model performances, including reducing RRMSE from 110 to 97 % ( ), which was closer to the RRMSE range (42–96 %) from a model ensemble of 24 models for global grassland, wheat, and maize experimental sites ( ). On the other hand, using empirical methods, such as the IPCC Tier 1 EF, to estimate N O emissions resulted in higher errors and bias (1.835 RRMSE, 4.085 MSE). The low IA and R also indicated the inefficacy of the EF method to project N O emissions across diverse conditions, especially for less-studied SC systems ( ). This study indicates that calibrating parameters only regarding crop biomass dynamics can generally improve model performance for both biomass and N O emission projections. Thus, it is valuable to collect biomass data for model calibrations of N O emissions given the relative scarcity of N O measurements for SCs. By both uptaking soil N and returning to soil C and N, SC biomass C and N life cycles are interwoven with soil C and N cycling, and consequently impact N O emissions ( ). Therefore, capturing biomass C and N dynamics can improve model simulations of C and N cycling in the systems and thus simulations of N O emissions. However, C and N of harvested/removed biomass are not always measured for N O studies due to the major emphasis on N inputs and soil characteristics ( ). This impedes same-site calibrations and validations of SCs biomass and N O emissions. This study uniquely improved N O emission simulations via synchronous parameterization for SC biomass C and N, which was often not accounted for before N O emission simulations at different scales ( ; ). The calibrated crops can be applied to site-level C and greenhouse gas accounting that are based on Century and DayCent methods, such as updating parameters in COMET-Farm. The discrepancy between simulations and measurements can be attributed to a lack of treatment effects theoretically represented in the model (e.g., fertilization methods of sidedress vs. subsurface fertigation; A), a need for site-specific calibrations, uncertain cultivar-specific parameters [e.g., unknown C and N removal fractions; ], lack of measurements (e.g., nearly no vegetable belowground biomass C or N measurements taken in N O emission studies), model structure uncertainty, i.e., variations of a treatment group (e.g., irrigation rates in A vs. A), or any combinations of the above. For example, the DayCent model version used in the study has the frequency and amount settings for both irrigation and fertilization. Irrigation can also be scheduled to fill up certain percent of the soil pores. However, the location of the fertilization or heterogeneous irrigation (e.g., furrow or drip irrigation) cannot be specified, which warrants future development. A model calibrated for certain management treatments may have higher errors when it is applied to relevant treatments with complex management (involving additional treatment factors), which may benefit from further site- and treatment-specific tuning with detailed information ( ). The DayCent simulations reasonably captured N₂O emissions across diverse sites, including those with multiple seasons and years, demonstrating the model's capability to handle varied temporal and spatial conditions in general. To be noted, the reported standard deviation of one data point captured the replicate variability of a certain treatment but did not include the uncertainty associated with sampling methods, timing, and sampling numbers ( ; ). In this study, model performance was relatively high in treatments applied in walnut and almond, but lower in other species, which can be attributed to certain treatment groups (e.g., treatments combining organic amendments, and/or uncertain residue amount and quality) and site variability (e.g., uncertain soil C and N contents, soil texture [e.g., clay], and climate conditions [e.g., wet and warm temperate]) (Table S2-S4). This calls for further research (as discussed in the next paragraph) to support data-driven improvements to the model, beyond the crop-specific parameterization conducted in this study. Nevertheless, using DayCent simulations to compare complex scenarios/ treatment effects may be more informative than absolute value predictions for individual practices. For example, although DayCent highly overestimated N O emissions from some inorganic N treatments with/without residue removal or straw addition in broccoli and cauliflower cropping fields ( B, B), the treatment effect rankings in simulations were similar to measurements (data not shown). Apart from the variable nature of N O emissions, there is a high variability of impacts from organic amendment (e.g., compost and manure) and nitrification inhibitors (e.g., B, B, B), which generally agreed with findings from a meta-analysis of global SC N O emissions ( ). The variability can be attributed to environmental factors (e.g., climate, soil characteristics, soil N levels), and variable compositions and releasing rates ( ; ; ; ; ; ). However, such detailed information, especially organic amendment compositions and nitrification inhibitor releasing rates, is often unavailable in the studies measuring N O emission. Additionally, the type (e.g., C/N ratio), heterogeneous placement, timing and decomposition of cover crops may increase or decrease N O emissions compared to no cover cropping ( ; ; ; ). The inconclusive treatment effect may be attributed to complexities that were not observed during the relatively short study periods (usually <2 years; ). Thus, it is demanding to synchronously document the details for accurate model inputs, including residue quality and amount, organic amendment compositions (e.g., C:N ratios) and their releasing rates, as well as the effect curves of different nitrification inhibitors on organic and inorganic N inputs ( ; ; ; ; ). Also, we will need more consistent data supported by long-term investigations to refine the respective mechanisms of impacts of crop residues, organic amendment, and nitrification inhibitors on N O emissions from SC systems. More comprehensive measurements of biomass, soil N₂O emission, mineral N, organic C, and water content and temperature, along with the availability of these raw data in publications, will help fill knowledge gaps and support the calibration and validation of N₂O emission models (Fig. S7–10). Comprehensive collection of the abovementioned data is critical for discovering key processes accounting for the varying treatment effects on N O emissions, and improving the N O emission pathway in process-based models ( ). In California, vegetable cropping fields generally emitted more yield-scaled N O than vineyards and orchards, which was mainly due to the low biomass absorbing N species, higher fertilization, more frequent tillage and crop rotations, and low C:N in residues of vegetables. The area-scaled annual N O emissions averaged over the major production regions for vineyards, almond, and peach orchards (0.62, 1.28, 1.10 kg N ha yr ), and lettuce, broccoli, cauliflower, and tomato cropping systems (2.43, 1.84, 1.73, 2.23 kg N ha yr ) generally agreed with the California state-level baseline emissions (0.92, 0.87, 0.80, 2.55, 3.94, 2.73, 2.67 kg N ha yr ) estimated by DNDC, respectively ( ). Moreover, this study highlighted the high N O emission areas for each SC, which can be emphasized for the adoption of effective mitigation practices, such as residue management, reduced tillage, and optimized efficiency in fertilization and irrigation ( ; ; ). This study compiled a comprehensive dataset of SC systems (vineyard, orchard, and vegetable cropping) and showcased that DayCent can simulate biomass and N O emissions from a variety of SC systems over a wide breadth of geographic and climatic regions. Crop calibration in DayCent significantly improved model performance for simulating biomass C (MRE = 0.148, RRMSE = 0.368, R = 0.913, IA = 0.976) and N (MRE = 0.549, RRMSE = 0.711, R = 0.298, IA = 0.730), compared with using the original crop/tree parameterization for biomass C (MRE = 0.309, RRMSE = 0.847, R = 0.589, IA = 0.865), and N (MRE = 1.327, RRMSE = 0.996, R = 0.001, IA = 0.366). Accordingly, there were greater model fit and reductions in bias and error for simulations of N O emissions (MRE =1.226, RRMSE =0.971, R = 0.407, IA = 0.772) than using original parameterization (MRE = 1.454, RRMSE = 1.102, R = 0.326, IA = 0.728), which also well outperformed the 2019 refined IPCC Tier 1 method (MRE = 4.085, RRMSE = 1.835, R = 0.031, IA = 0.448). Thus, it's both plausible and feasible to improve N O emission simulations after merely crop-specific parameterization. The spatial and temporal N O emission variations estimated in California highlighted the importance of simulating emissions at the site to county levels, and pinpointed high emission regions for SCs that should prioritize the adoption of mitigation practices. In the future, there are needs of long-term (> 2 years) investigations of the impacts of cover crop, organic amendment, and nitrification inhibitors on N O emissions from SC systems, and the synchronous measurements of residue quality and amount (both above- and belowground), organic amendment compositions (e.g., C:N ratios) and their N releasing rates, and effect curves of different nitrification inhibitors on organic and inorganic N inputs. Upon more data (detailed inputs, process knowledge, and measurements) becoming available for each SC, the model parameterization may be further improved by optimization algorithms such as Bayesian methods ( ; ; ) with the challenge of calibrating many sensitive parameters (e.g., about 20 tree parameters in DayCent). Writing – review & editing, Writing – original draft, Visualization, Validation, Supervision, Software, Project administration, Methodology, Investigation, Formal analysis, Data curation, Conceptualization. Writing – review & editing, Software, Methodology, Conceptualization. Writing – review & editing, Investigation, Data curation. Writing – review & editing, Supervision, Funding acquisition, Conceptualization.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Used\n",
      "* **AIモデルの判断:** Not Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 149 / DOI: 10.1016/j.surfin.2025.106134  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Ibuprofen removal using coconut husk activated Biomass\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Synthesis and adsorption efficiency of magnetically separable sawdust-based activated carbon for ibuprofen removal\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "Pharmaceutical emerging contaminants, such as ibuprofen (IBP)—a widely used non-steroidal anti-inflammatory drug (NSAID) for treating pain, fever, and inflammation—have raised significant environmental and public health concerns [ ]. These contaminants are persistently detected in various water sources, including surface water, groundwater, and even drinking water. They enter aquatic ecosystems primarily through wastewater effluents, agricultural runoff, and improper disposal of medications [ ]. Their continuous presence in water bodies can lead to serious environmental issues, such as endocrine disruption, antibiotic resistance, and toxicity to aquatic organisms [ ]. As a result, developing effective removal strategies is essential to mitigate these environmental risks. Conventional wastewater treatment plants (WWTPs) can remove many pharmaceutical contaminants, including IBP [ ]. However, it is essential to recognize that these facilities are not specifically designed to target and eliminate such persistent compounds [ ]. This has prompted the need for supplementary treatment processes, such as advanced oxidation processes (AOPs) [ ], adsorption [ ], and membrane filtration [ ], to achieve more comprehensive and effective removal of pharmaceuticals from wastewater. Among them, adsorption is a well-known process that has gained considerable attention due to its simplicity, cost-effectiveness, and ability to handle a wide range of contaminants [ ]. Nano-adsorbent materials such as iron oxides, CeO , TiO , Al O , ZnO, and ZnS, have been widely studied for various applications, including the removal of pharmaceuticals from aqueous solutions [ ]. To enhance sustainability, efficiency, and cost-effectiveness, these nanoparticles have been combined with biopolymers such as cellulose, alginate, and chitosan [ ], and carbonaceous materials like biochar and activated carbon [ ]. However, the choice of carbon precursor plays a crucial role in developing adsorbents with superior physical and chemical properties, as well as enhanced adsorption capacity. Sawdust is the most abundant bio-waste in the world. This material is generated mainly from forestry operations, wood factories, and woodworking shops. Its easy availability and large production create environmental challenges. This issue inspired researchers to find solutions for waste management by fabricating many products for multiple applications [ , ]. In literature, activated carbon (AC) with ZnCl derived from sawdust materials was reported to be an efficient adsorbent for removing numerous contaminants [ , ]. In essence, ZnCl chemical is commonly used as dehydrating agent for the chemical activation of lignocellulosic materials [ ]. In instance, ZnCl agent was used for N-doping activated carbon fabrication derived from sawdust to adsorb acid brown 14 dye in water. This adsorbent with a surface area of 281.84 m .g reached a value of 909.1 mg.g [ ]. Notably, activation with the ZnCl reagent enhances pore development in the carbon structure, leading to high yields of carbon due to the effective action of the chemical [ ]. Regardless, limited papers have been published on treating contaminants including IBP utilizing sawdust carbon activated with ZnCl chemical. ACs often suffer from some limitations such as reusability and difficulty in separation after treatment. Accordingly, the magnetization technique for adsorbent materials modification was reported to overcome this shortage [ ]. FeCl and FeSO chemicals are commonly used to prepare magnetic-based materials due to their availability at low prices and the strong chemical bonds formed [ ]. Magnetic materials typically combine conventional adsorbents' high surface area and porosity with easy magnetic separation and efficient recovery from treated water. These magnetic properties particularly contribute to reducing the complexity and cost of post-treatment processes [ ] . As far as we know, activated carbon derived from sawdust biomass through ZnCl chemical activation before pyrolysis, particularly with the incorporation of magnetic properties, has not been hitherto studied for the adsorption of ibuprofen. Hence, this study aims to address this gap by developing a novel magnetic sawdust-based activated carbon (named MSAC). The sawdust biomass was chemically modified using ZnCl and subjected to direct pyrolysis at 600 °C, which produced the initial activated carbon (SAC). This was followed by magnetization using iron oxide precursors (FeCl and FeSO ) to improve IBP removal efficiency and facilitate adsorbent separation from solutions. The physicochemical properties and surface characteristics of the developed materials were systematically characterized using a range of analytical techniques. The adsorption performance was assessed under various conditions in a batch system, investigating the effects of pH, stirring time, temperature, and initial IBP concentration to elucidate adsorption behaviors and mechanisms and optimize the process. Sawdust residues were collected from a local carpentry workshop in Constantine, Algeria. Iron chloride (FeCl •6H O, ≥99%) and iron sulfate (FeSO , ≥99%) were acquired from Biochem Chemopharma, and zinc chloride (ZnCl , ≥99%) was purchased from Sigma–Aldrich. All further reagents used were of analytical grade. The sawdust materials were flashed multiple times in distilled water, dried, ground, and sifted into a 150-mesh sieve. After that, around 20 g of the precursor was immersed in 250 mL distilled water containing 20 g ZnCl and agitated for a long period (24 hours) at room temperature to ensure a homogeneous distribution of the ZnCl compounds on the sawdust that facilitated the activation operation. This step was followed by drying the impregnated mass in the oven at 105 °C. It was then subjected to direct pyrolysis in a closed crucible at 600 °C for 1 h in an oxygen-limited environment. This produced activated char powder is referred to as SAC. Magnetically separable activated carbon (MSAC) was synthesized using a co-precipitation method, following the procedure outlined by Anyika et al. [ ]. In brief, 60 mL of FeSO (8.0 g) was added to 520 mL of FeCl (7.2 g) and stirred for 120 min at 60–70 °C. Separately, 20 g SAC powder was suspended in 200 mL of distilled water and then combined with the iron oxide solution under slow agitation at room temperature for 30 min to ensure uniform mixing. A 10-M NaOH solution was then added dropwise to the suspension, resulting in a color change from dark brown at pH 6.0 to black at pH 10.0. The magnetized material (MSAC) was collected by filtration, thoroughly washed, dried at 105 °C, and stored for further use. The functional groups present on the surface of the materials were identified using Fourier transform infrared (FTIR) spectroscopy (Shimadzu), with spectra collected in the range of 4000 to 400 cm . Scanning electron microscopy (SEM) (Bruker Nano, Berlin, Germany), coupled with energy dispersive X−ray spectroscopy (EDX), was employed to analyze the morphology, elemental composition, and particle sizes of the samples. Textural parameters of the prepared materials were studied using Quantachrome Autosorb−iQ3 equipment by the N gas adsorption-desorption isotherms at 77 K. Magnetic properties of the samples were examined at room temperature using a vibrating sample magnetometer (VSM, MicroSence−EZ9). The point of zero charge (pH ) was determined following the method reported by Nouioua et al. [ ]. A stock solution of IBP (20 mg.L ) was prepared by dissolving 20-mg IBP in 1000 mL deionized water, then diluted to 1.0−10 mg.L for calibration standards. Adsorption experiments were performed with varying adsorbent dosages (0.1−2.0 g.L ), pH values (2.0−12), IBP concentrations (5.0−20 mg.L ), and temperatures (25−45 °C). The suspensions were agitated for 24 h, filtered, and analyzed using a UV−VIS spectrophotometer (SHIMADZU UV−160A) at 222 nm. The impact of NaCl and KCl on MSAC adsorption performance was assessed. IBP adsorption efficiency ( %) was computed via , with adsorption capacity of MSAC determined using for time−dependent ( , mg.g ) and for equilibrium ( , mg.g ). , , and are IBP concentrations (mg. L ) at: initial, equilibrium, and at time ( ; min). denotes the used volume of the IBP solution (L), and is the dry mass of adsorbent used (g). For adsorption modeling, kinetic and isotherm data were analyzed using different non-linear models to describe the IBP adsorption process. The selected kinetic models include pseudo-first-order (PFO), pseudo-second-order (PSO), intraparticle diffusion (IPD) or Weber-Morris model, Avrami, and Elovich represented by – , respectively. and (mg. g ), represent the adsorption capacities of the IBP at equilibrium and at a specific time, respectively. (min) is the contact time between the adsorbent and the adsorbate. represents the first-order rate constant (min ) and (g.mg min ) is the pseudo-second-order rate constant. (mg. g .min ) is the intra-particle diffusion rate constant and is the intercept at stage I, II, and III. (min ) represents the Avrami kinetic constant and is a fractional adsorption order corresponding to adsorption mechanism. (mg.g .min ) represents the initial adsorption rate and (g.mg ) is associated with the surface coverage extent and the activation energy of chemisorption The adsorption isotherm was evaluated using Langmuir, Freundlich, Sips, Redlich–Peterson, and Temkin models, provided in − − , respectively. (mg. g ) represents the maximum adsorption capacity and (L.mg ) is the Langmuir constant. (L. mg ) is the Freundlich constant, while reflects the adsorption intensity. L.mg is the Sips constant associated with adsorption energy, while the parameter characterizes the heterogeneity of the adsorption system. (L.g ) and ((mg.L ) ) are the Redlich–Peterson constants, is a dimensionless exponent that must lie between 0 and 1 ((L.mg ) is the equilibrium binding constant, (J.mol ) represents the adsorption energy, (J.mol .K )is the universal gas constant, and (K) is the temperature. The adjusted coefficient of determination ( ) and the standard deviation ( ) were chosen to recognize the most suitable model for both kinetic and isotherm studies. These parameters were automatically obtained using the statistical software Origin. illustrates the isotherm adsorption-desorption of N for the two developed materials. Based on IUPAC classification, the isotherm curves for SAC and MSAC correspond to Type II with the apparition of H3−type and Type V with the apparition of H4−type, respectively [ ]. This indicates that SAC is a microporous adsorbent (pore widths < 2.0 nm), while MSAC is a mesoporous adsorbent (pore widths 2.0–50 nm). Their pore diameters are centered at 1.53 nm (for SAC) and 8.77 nm for MSAC ( ). The increase in pore diameter can be attributed to gaps formed between iron particles and the SAC matrix, this finding aligns with previous work [ ]. The and values of the SAC material are 1425.0 m .g and 1.166 cm .g , respectively ( ). After the addition of the magnetic properties to SAC, the characteristic parameters of the MSAC material were significantly enhanced, with increasing to 1547.5 m .g and to 29.17 cm .g . According to Ben Salem et al., adsorbent materials with a high are superior to those with lower in the adsorption capacity and solute diffusion rate [ ]. However, another mechanism may be involved in the adsorption of pharmaceutical pollutants onto carbon-based adsorbents, such as hydrogen bonding, electrostatic interactions, π-π interactions, or van der Waals forces [ ]. The Dubinin-Radushkevich equation ( ) was applied to confirm the category of the prepared adsorbents by calculating the adsorption energy ( ) [ ]. As expected, the calculated value for SAC is 18.46 kJ.mol (superior to 16 kJ/mol), confirming its microporous nature. Meanwhile, MSAC is confirmed to be mesoporous, with an value of 12.63 kJ.mol (inferior to 16 kJ.mol ). Therefore, Considering the molecular dimensions of ibuprofen shown in , it is evident that this adsorbate can access the micropores of both solids. The textural characteristics of SAC and MSAC materials are comparable to those of previously prepared adsorbents, as shown in . The powder X−ray diffraction (XRD) patterns of SAC and MSAC samples are presented in . In general, the XRD profile of activated carbon derived from lignocellulosic biomass treated with ZnCl (including SAC material) primarily reflects characteristics of amorphous carbon materials as demonstrated in the broad peaks at around 2θ = 25° and 43 [ ]. Some possible crystalline phases include zinc oxide (ZnO) was formed as a byproduct due to the activation process at 600 °C appears at the following 2θ angles (with corresponding planes): 2θ = 31.82 (100), 34.47 (002), 36.30 (101), 47.58 (102), 56.65 (110), 62.91 (103), 66.41 (200), 67.98 (112), and 69.08 (201) (JCPDS: 79−2205; b). This finding aligns with previous results [ , ]. Following the magnetization process, distinct sharp peaks appear in the XRD patterns of the MSAC sample ( c). The presence of magnetite (Fe O ) nanoparticles is confirmed and loaded in MSAC material, with peak positions aligning with JCPDS: 19–0629 as shown in d, at 2θ = 30.05° (220), 35.28° (311), 42.97° (400), 53.17° (422), 56.7° (511), 61.96° (440), and 73.96° (533) [ , , ]. Furthermore, these peaks closely overlap with those of spinel zinc ferrite (ZnFe O ), as indicated by JCPDS: 22–1012 ( e), at the same 2θ values of the pure Fe O . This suggests the successful incorporation of zinc from the initial ZnCl activation into the iron oxide matrix formed during the co-precipitation process involving FeCl and FeSO chemicals. Similar works are published elsewhere [ , ]. The scanning electron microscopy (SEM) analysis of both SAC and MSAC, as shown in , reveals significant morphological differences that influence their adsorption capabilities. The SEM image ( a) of SAC suggests that the activated carbons are constituted by an interconnected channel framework, likely due to the fibrous structure of the sawdust precursor [ ]. This framework, with its rough surfaces and varying pore sizes, enhances the surface area, facilitating the efficient adsorption of contaminants. EDX spectrum ( c) shows that SAC is primarily composed of 73.91% carbon, 11.14% oxygen, 13.17% zinc, 1.57% sodium and minor quantities of potassium (0.2%), reflecting its well-developed porosity and functional groups. In contrast, the SEM image ( b) of MSAC displays a more compact yet highly porous structure, with various sizes of cavities and holes resulting from the magnetization process. MSAC retains well-defined pores, with an elemental composition ( d) of 54.65% carbon, 24.04% oxygen, 13.47% iron, 3.31% sodium and 6.64% zinc, indicating successful magnetization [ ]. This finding is match the XRD discussion. Thus, this structural modification enhances magnetic responsiveness, facilitating easier separation of MSAC in practical applications, though it may slightly reduce adsorption capacity due to the increased density of the structure. presents the FTIR spectra of the synthesized adsorbents, SAC and MSAC, highlighting the presence of various surface functional groups. The results display bands in the 3669 and 3676 cm⁻ range, suggesting the presence of O–H groups, likely due to water adsorption [ ]. The bands detected at 2895 and 2915 cm are ascribed to the stretching vibrations of aliphatic C–H groups [ ]. Furthermore, the peaks at 1689 and 1757 cm suggest the presence of C=O groups [ ], potentially from ketones, while the band at 1400 cm indicates C=C bonds in aromatic rings [ ]. The peaks at 1079 and 1086 cm are associated with C–O groups [ ]. These functional groups were distinctly observed in both FTIR spectra (SAC and MSAC). Additionally, a new band around 744 cm⁻ in the MSAC spectrum confirms the presence of iron oxides (Fe–O group) confirming structural modifications from magnetization [ , ]. A vibrating sample magnetometer (VSM) was employed to measure the magnetic saturation of iron oxide [ ] in MSAC. The magnetic properties of MSAC were evaluated through the generation of magnetic hysteresis curves, as depicted in . The analysis revealed that MSAC exhibited ferromagnetic characteristics, with a saturation magnetization ( ) of 5.02 emu.g . However, based on the coercivity ( ) of 21.9, remanence ( ) of 0.072, and the remanence-to-saturation magnetization ratio ( / ) of 0.014, it is evident that the synthesized MSAC demonstrates superparamagnetic behavior at room temperature. The low / ​ ratio, well below the threshold of 25% [ ], strongly supports this conclusion, indicating minimal hysteresis— a hallmark of superparamagnetic materials. Similar superparamagnetic behavior has been reported in other magnetic-activated carbons (MACs), such as those derived from almond shells [ ], oak pericarp [ ], and coconut shells [ ]. These findings suggest that MSAC is well-suited for applications requiring easy magnetization and demagnetization, further supporting its potential utility in magnetic separation and adsorption processes. In the literature, solution pH and ionic strength are critical factors influencing adsorption by affecting the surface charge of the adsorbent and the adsorbate [ ]. The charge surface of SAC and MSAC is always changing with the solution pH ( a). At pH<pH of adsorbent (6.8 for SAC and 6.5 for MSAC), the two adsorbent surfaces will be predominantly positively (+) charged and vice versa. As a result, the amount of IBP adsorbed ( ) is increased with decreasing pH value from 10.0 to 2.0, and the two adsorbents reached their maximum adsorption capacity at pH 2.0 ( b). In essence, At pH 2.0, ibuprofen primarily exists in its non-ionized (neutral) form, as indicated by its pKa value of 4.91, which further facilitates adsorption [ ]. Conversely, at higher pH values (above 4.91), ibuprofen becomes negatively charged, and since the SAC and MSAC surfaces also become more negatively charged due to reduced protonation, electrostatic repulsion occurs, thereby weakening adsorption efficiency [ ]. Thus, pH significantly affects both the functional groups on the adsorbent surface and the ionization degree of the adsorbate molecules [ ]. The addition of NaCl or KCl ions to the IBP solution with different initial concentrations (0–40 g.L ; c) showed that the adsorption capacity of SAC and MSAC significantly increased with the superiority of the KCl solution. This improvement in adsorption capacity may be attributed to two possible mechanisms: the \"salting out effect,\" where increased salt concentration reduces the solubility of the pharmaceutical, thereby increasing the availability of active surface sites on the adsorbent, and the \"screening effect,\" where the electrically charged anions and cations shield the adsorbent's charged surface, reducing repulsive forces and enhancing adsorption capacity [ ]. A similar matching finding was reported in another study for the removal of IBP using magnetic-activated carbon [ ]. d illustrates the effect of adsorbent dose on both the adsorption capacity ( ) and the removal efficiency ( %) of Ibuprofen (IBP), which is a crucial parameter in adsorption studies due to its direct influence on adsorption performance. The results indicate that as the adsorbent dose increased from 0.1 to 1.0 g.L , the removal efficiency of IBP improved significantly, rising from approximately 69.2% to 98.8% for SAC, and from 55.5% to 98.6% for MSAC. However, the adsorption capacity ( ) exhibited a decreasing trend with increasing adsorbent dose. This phenomenon can be mainly attributed to the greater availability of sorption sites at higher adsorbent dosages, leading to the saturation of all active sites at equilibrium when the adsorbent amount reached 0.2 g.L , identified as the optimal dose [ ]. Moreover, the introduction of a large amount of adsorbent can lead to particle agglomeration, which reduces the total surface area available for adsorption [ ]. Consequently, this agglomeration decreases the amount of IBP adsorbed per unit mass of the adsorbent. The increase in adsorbent dosage results in a higher adsorbent-to-adsorbate ratio, making the optimal dosage dependent on the initial concentration of the pharmaceutical compound, with higher initial concentrations necessitating greater carbon dosages [ ]. Exploring the optimal time (equilibrium time) for the adsorption process is crucial to analyzing the cost of the operation. Therefore, illustrates the variation in the amount of IBP adsorbed ( ) onto SAC and MSAC adsorbents over different contact times (5–240 min) at varying initial concentrations ( ) of 5, 10, and 20 mg.L . The results demonstrate that the adsorption capacity ( ) of the adsorbents increased with increasing contact time and initial IBP concentration. Remarkably, in the early stages of the adsorption process, IBP removal occurred rapidly due to the abundance of available adsorption sites. As increased from 5 to 20 mg.L , increased from 19.1 to 77.0 mg.g for SAC and from 20.0 to 82.4 mg.g for MSAC. These results indicate that MSAC was more effective in the adsorption process than SAC. However, as contact time progressed, the removal rate ( %) stabilized, indicating that the number of available adsorption sites was becoming limited due to steric obstructions [ ]. The adsorption process of IBP reached its optimal time at around 120 min for both adsorbents. For kinetic modeling, the adsorption data for both adsorbents showed a strong fit with the Avrami kinetic model based on a and values ( ). However, the PSO model demonstrated an even higher level of agreement, particularly in the alignment between the calculated adsorption capacity of model ( ) and the experimental data ( ) across concentrations of 5, 10, and 20 mg.L . Particularly, for SAC, the values of 21.1, 46.7, and 85.8 mg.g closely matched the values of 21.1, 48.8, and 92.0 mg.g , respectively. Likewise, for MSAC, the PFO model exhibited a similar degree of agreement. Its values 20.6, 46.3, and 84.0 mg.g closely matched the experimental values 20.6, 47.1, and 86.0 mg.g , respectively. In this study, the intraparticle diffusion model was confirmed through data fitting ( ), revealing a multilinear plot with three regions. Region I showed rapid surface diffusion, Region II indicated slower pore diffusion, and Region III represented equilibrium [ ]. The slope of each segment reflects the adsorption rate, with steeper slopes indicating faster processes [ ]. The rate constant for the three IBP concentrations was significantly higher than and for both SAC and MSAC, suggesting intraparticle diffusion as the dominant step. In this study, Equilibrium adsorption experiments were carried out using the batch method, maintaining a solid/liquid ratio of 0.2 g.L . As shown in a, b, increasing the initial IBP concentration from 2 to 20 mg.L led to a corresponding rise in adsorption capacity. Specifically, SAC displayed an increase from 13.9 mg.g to 82.4 mg.g , while MSAC increased from 9 mg.g to 53.3 mg.g , indicating the superior efficiency of SAC in IBP removal. This trend suggests that as IBP concentration increases, available adsorption sites on the adsorbents become progressively occupied, resulting in a higher adsorption capacity nearing saturation. The adsorption isotherm observed was L–shaped, following the classification by Giles et al., and lacked a clear saturation region across the tested concentration range [ ]. The curves for the equilibrium models were plotted in a, b, with the corresponding parameters and coefficients listed in . The best-fit isotherm model was determined based on the highest adjusted correlation coefficients ( ) and lowest root mean square error ( ). For SAC, the Langmuir model provided the best fit, with the following order of fitting: Langmuir ( = 0.986, =3.507) >Redlich Peterson ( = 0.983, =3.783) > Sips ( = 0.983, =3.789) > Freundlich ( = 0.969, =4.364) > Temkin ( = 0.968, =5.226). This result indicates that IBP–adsorption on SAC follows a monolayer-type process on homogeneous surfaces. In contrast, for MSAC, the Sips model was the most accurate, with the order of fitting: Sips ( = 0.997, =1.007) >Redlich Peterson ( = 0.995, =1.202) > Temkin ( = 0.995, =1.332) >Langmuir ( = 0.994, =1.423) > Freundlich ( = 0.966, =2.937), suggesting that adsorption on MSAC is a multilayer phenomenon occurring on heterogeneous surfaces. It is noteworthy that for both adsorbents, the values for the Langmuir and Sips models were close, indicating that the Sips isotherm approaches the Langmuir isotherm at high concentrations. Activated carbon from sawdust (SAC) exhibited a maximum adsorption capacity ( ) of 210.53 mg. g for IBP removal, surpassing several adsorbents reported in the literature, as presented in , compared to maximum adsorption capacity (93.81 mg. g ) of magnetic activated carbon from sawdust (MSAC). The impact of temperature on IBP (10 mg.L ) adsorption onto SAC and MSAC was studied at 25, 35, and 45 °C ( c). At 25 °C, SAC exhibited a higher adsorption capacity ( =55.82 mg.g ) compared to MSAC ( =34.85 mg.g ). However, with increasing temperature, SAC adsorption capacity decreased, reaching =19.09 mg.g at 35 °C and further reducing to =17.49 mg.g at 45 °C. In contrast, MSAC adsorption capacity steadily increased with temperature, surpassing SAC at 35 °C ( =42.80 mg.g ) and reaching its maximum of =47.29 mg.g at 45 °C. Thermodynamic parameters, including changes in enthalpy (Δ °), entropy (Δ °), and Gibbs free energy (Δ °) ( − ), are crucial for understanding the temperature influence on the adsorption process. IBP adsorption onto SAC and MSAC was analyzed by computing these parameters using the van't Hoff plot (ln vs. 1/ ) ( ). Δ ° and Δ ° were obtained from the slope and intercept, and Δ ° was calculated accordingly. The thermodynamic parameters are summarized in . (8.314 J.mol . K ) represents the universal gas constant, is the equilibrium constant, and (K) is the absolute temperature of the solution. The results revealed that Δ ° values are positive for SAC, suggesting a non-spontaneous and thermodynamically unfavorable process and negative Δ ° values for MSAC indicate spontaneity, even though the adsorption became less favorable at higher temperatures. The negative Δ ° for SAC (–61.36 kJ.mol ) confirms an exothermic process, while the positive Δ ° for MSAC (79.86 kJ.mol ) indicates endothermic adsorption. The Δ ° values above 10 kJ/mol suggest chemical adsorption (for MSAC), whereas lower values indicate physical adsorption (for SAC) [ ]. The negative Δ ° for SAC (–186 J.mol .K ) implies decreased randomness at the SAC/IBP interface, meanwhile, the positive Δ ° for MSAC (288 J.mol .K ) suggests increased randomness and strong IBP affinity, indicative of structural changes at the interface. The reusability of adsorbents is economically essential, reducing costs and environmental impacts by minimizing waste. Studies indicate that adsorbents for ibuprofen removal can be regenerated using acids, bases, or organic solvents like methanol, ethanol, and acetone, which desorb ibuprofen molecules and restore adsorption capacity. The choice of desorbing agent depends on factors such as the adsorbent's properties, surface groups, bonding interactions, and the adsorption mechanism [ ]. In this study, pure ethanol was used for the chemical regeneration of the laden SAC and MSAC to assess their reusability in the adsorption of 20 mg.L IBP from water. Before desorption, both adsorbents underwent consecutive adsorption cycles ( a). The results show that both SAC and MSAC exhibited a decline in removal efficiency (R%) after the first cycle. Initially, SAC and MSAC achieved removal efficiencies of 92.7% and 86.5%, respectively, which decreased to 30.6% and 32.7% in the second cycle. This reduction suggests that the active sites become progressively occupied by IBP molecules, limiting further adsorption. After desorption with 250 mL of ethanol agent ( b), both SAC and MSAC regained their adsorption efficiency. SAC retained a higher efficiency, decreasing from 84.2% in the first cycle to 52.7% in the third, while MSAC started at 78.2% and decreased to 43.5% in the third cycle. Hence, ethanol can be considered an effective desorption agent for restoring the adsorption capacity of SAC and MSAC toward IBP. SAC and MSAC, as recyclable adsorbents, are promising materials for treating water contaminated with IBP pharmaceuticals. The production cost of SAC and MSAC is key to their competitive pricing in increasing their marketability and large-scale viability. In our study, the developed adsorbents were prepared using abundant sawdust feedstock and available chemicals under simple synthesis methods with limited workers that can reduce expenses. However, the production process requires covering the cost of additional operations such as energy (electricity) consumption and transport. Therefore, the total production cost ( ) for SAC and MSAC production was calculated using . ) represents the feedstock and chemicals costs; the cost of electricity consumption ( ) covers the drying and pyrolysis costs; and the other cost ( ) describes the exploited water and transport costs. The total production cost of SAC and MSAC materials is significantly impacted by the choice of chemicals, including their purity, brand, grade (analytical research or industrial), and the timing of purchase. As shown in , the estimated production cost for SAC ranges from 1.07 to 2.1 US$.kg and for MSAC from 1.86 to 3.89 US$.kg when using industrial-grade chemicals, making the process relatively affordable. However, when using analytical-grade chemicals, the cost rises dramatically to 117.36 US$.kg for SAC and 220.33 US$.kg for MSAC. In essence, low-priced chemicals often available as industry grade are typically employed to minimize the expenses of the production process. Hence, the total production charge for MSAC adsorbent is expected to remain sustainable within the range of 1.86 to 3.89 US$.kg . Sawdust biomass prepared activated carbon (SAC) by the direct pyrolysis method at 600 °C. Magnetic activated carbon (MSAC) with easily separable properties was then yielded from SAC via a simple co-precipitation technique using iron oxides FeCl and FeSO chemicals. The adsorption capacity obtained from the Langmuir model of the prepared adsorbents for ibuprofen was defined to be 210.53 mg.g for SAC and 93.81 mg.g for MSAC at 25 °C. The adsorption process of SAC was described to be non-spontaneous and exothermic while MSAC is spontaneous and endothermic. SAC and MSAC exhibit better durability and reusability for ibuprofen adsorption, making them more suitable for applications requiring multiple regeneration cycles. The SAC and MSAC materials proved their applicability as cost-effective adsorbents with promising avenues for IBP elimination from solutions. Writing – original draft, Visualization, Validation, Methodology, Investigation, Formal analysis, Data curation. Writing – review & editing, Supervision, Resources, Project administration, Methodology, Funding acquisition, Conceptualization. Writing – review & editing, Writing – original draft, Visualization, Validation, Formal analysis. Software, Project administration, Conceptualization. Software. Investigation. Investigation. Resources.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Used\n",
      "* **AIモデルの判断:** Not Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 153 / DOI: 10.1016/j.scitotenv.2024.177626  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Data supporting the spectrophotometric method for the estimation of catalase activity\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Effects of sublethal concentration of thiamethoxam formulation on the wild stingless bee, Partamona helleri Friese (Hymenoptera: Apidae): Histopathology, oxidative stress and behavioral changes\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "Approximately 90 % of plants are pollinated by animals ( ), highlighting the importance of pollination services for the maintenance of biodiversity and agricultural production ( ; ). Among pollinators, bees are responsible for approximately 50 % of pollination services in cultivated plants ( ). Stingless bees (Hymenoptera: Meliponini), with >500 species, occur in tropical and subtropical regions ( ), pollinating 90 % of the native plants ( ). Friese (Hymenoptera: Apidae) is a Neotropical stingless bee with eusocial behavior, forming perennial colonies with thousands of workers ( ). This bee pollinates several botanical families of economic importance, including representative of Poaceae, Fabaceae, Cucurbitaceae, Liliaceae, Myrtaceae, and Solanaceae ( ). Despite their importance, bee populations have declining trends ( ; ), with a 25 % decrease in species diversity since 1990 ( ). This decline can be attributed to multiple factors, including inadequate management, nutritional deficiencies, climate change, habitat loss, parasites, pathogens, and excessive use of pesticides ( ; ). Among these, the use of pesticides has been identified as the major cause for bee population decline ( ; ). The widespread use of these chemicals may contaminate soil, water, and air, in addition to poisoning the non-target organisms ( ). During foraging, bees may undergo multiple contacts with pesticides ( ; ), through exposure to airborne droplets, inhalation during or after spraying, or ingestion of contaminated pollen grains, nectar, honey, and water ( ; , ). Within insecticides, neonicotinoids have been widely used, with a systemic mode of action, being transported throughout the plant and showing efficacy against pests, even at small concentrations ( ). Neonicotinoids act on the nervous system of insects as competitive modulators of the nicotinic acetylcholine receptors (nAChRs) in neurons, causing hyperexcitation, lethargy, paralysis, and death ( ; ; ). Despite low toxicity to mammals ( ; ), these insecticides cause several damages in non-target animals, including bees ( ; ). Thiamethoxam is a second-generation neonicotinoid used in foliar applications and seed treatments ( ) to control aphids, whiteflies, leafhoppers, beetles and caterpillars ( ). Thiamethoxam residues were found in nectar (ranging from 0.01 to 11.0 ng/g) and pollen grains (ranging from 0.01 to 95.5 ng/g) of some plants ( ). After ingestion, insecticide molecules reach the digestive tract and may affect the midgut epithelium, the organ responsible for food digestion and absorption ( ). The midgut has a single layered epithelium with digestive, endocrine and regenerative cells ( ). However, insecticides may cross the gut epithelium, distributed through the hemolymph and reaching to other organs ( ; ; ; ; ), affecting its structure and function. In bees exposed to neurotoxic insecticides, such as neonicotinoids and pyrethroids, midgut damage has been reported, with increased cytoplasmic vacuolization and apocrine secretion, mitochondrial damage, brush border and peritrophic matrix disorganization ( ; ; ; ). Furthermore, insecticides activate signaling pathways of cell death by autophagy, mediated by LC3, and by apoptosis, mediated by caspases, in the midgut of bees ( ; ; ). In addition to morphological damage in some organs, pesticides may induce oxidative stress ( ; ) and behavioral changes ( ; ) in insects. Oxidative stress results in the production of reactive oxygen species, oxidizing biomolecules, and compromising cellular and tissue homeostasis ( ). In bees, it has been reported that insecticides, after ingestion, affect their behavior, learning, communication, and locomotion ( ; ), which impair foraging activity and pollination. This study investigated whether workers are affected by chronic exposure to thiamethoxam at a residual concentration. Specifically, we assessed mortality, midgut epithelial damage, oxidative stress, and locomotion behavior in . Forager adult workers of with corbiculae loaded with pollen grains were collected at the nest entrance using Erlenmeyer flasks in three colonies at the Central Apiary of the Federal University of Viçosa, Minas Gerais, Brazil (20° 75′ S, 42° 86′ W). The bees were acclimated for 15 min and then kept in the laboratory at 29 ± 2 °C and 80 % relative humidity ( ). All bees were collected from September to December 2023. The commercial formulation of thiamethoxam, Cruiser 350 Fs (Syngenta, Basel, Switzerland; 350 gL of active ingredient and 830 gL of other ingredients) was purchased from the market in 2023. To obtain the concentration of 0.09 ng/g, closely to the average found in pollen grains ( ), 10 mL of the commercial product was diluted in 990 mL of distilled water (solution 1), followed by a further dilution of 5 μL solution 1 in 1 L distilled water (solution 2) and, finally, 5 μL solution 2 was diluted in 1 L 50 % aqueous sucrose solution. Bees were kept in 250 mL plastic cages (11 cm in diameter and 8 cm in height) with ventilation holes and 1.5 mL perforated plastic tubes as feeders ( ), with 10 bees per cage, in triplicate. Food was supplied for seven days and changed daily to avoid contamination by fungi and bacteria. Bees in the control group received only 50 % sucrose solution for the same period. Survival analysis was performed using 30 bees from three colonies, with 10 bees per cage, in triplicate, as aforementioned. Mortality was assessed after 12 h and then every 24 h for seven days of chronic exposure to thiamethoxam. Dead bees were removed and discarded. After oral exposure for seven days, 12 bees from the control group and 12 from the treated group were transferred to Petri dishes (9 cm in diameter and 2 cm in height) covered with transparent PVC plastic perforated for ventilation and acclimated for 15 min. For each of the three colonies, four control bees were used per Petri dish and four treated bees in another dish. The insects were observed for 10 min using a digital video camera in a room with artificial light at 25 ± 3 °C and 80 ± 5 % relative humidity ( ). The videos were analyzed using Ethoflow® software to calculate the distance traveled (cm) and the average walking speed (cm/s) ( ). The Ethoflow® software allows the evaluation of different individuals in the same Petri dish, maintaining individual identity ( ). Workers of from the control group ( = 10) and fed with the approximate concentration of thiamethoxam found in the pollen grain (n = 10), as described above, were evaluated after seven days. Ten bees from control and 10 from the treatment were cryoanesthetized in a freezer at −5 °C for 3 min, dissected with the aid of tweezers in a Petri dish containing 125 mM NaCl, and the midguts transferred to vials containing Zamboni's fixative solution ( ) for 48 h. Then, the samples were dehydrated in a series of graded ethanol (70 %, 80 %, 90 % and 95 %) for 10 min each and embedded in historesin (Leica Biosystem Nussloch GmbH, Wetzlar, Germany) according to the manufacturer's instructions. After resin polymerization, the samples were sectioned at 3 μm slices with a Leica RM 2245 rotary microtome. The slices were added onto glass slides and stained with hematoxylin for 15 min and eosin for 30 s, and analyzed and photographed using an Olympus BX60 light microscope. The height of the brush border of the midgut epithelium was evaluated in the histopathological sections aforementioned. For this purpose, 10 sections of the midgut of different bees were used, 10 insects for the control group and 10 for the treatment, totaling 20 bees and 200 sections. The histological sections were randomly selected and photographed using a 20× objective lens, numerical aperture of 0.40 and the measurements were obtained with the aid of the Image J computer program. Five random sections of each midguts from the control (n = 10) and treatment (n = 10) groups were analyzed using a 40×, 0.75 numerical aperture objective lens to determine the midgut epithelium injury index according to . Briefly, the sum of histological changes in the midgut was calculated for each bee with the equation = ( × ), where is the organ injury index, is the sum of the changes, is the score value, and is the importance factor. The scores ( ) of the histological alterations were classified as i) no occurrence = 0, ii) mild occurrence = 1, iii) moderate occurrence = 2 or iv) severe occurrence = 3. The importance factors ( ) were classified as: i) reversible damage (cell fragments released into the gut lumen and/or apocrine secretion) = 1, ii) generally reversible damage (cytoplasmic vacuolization) = 2 or iii) irreversible damage (nuclear pyknosis and/or regenerative cells damage) = 3 ( ). Some sets of unstained histological sections of the midgut of workers obtained as aforementioned were randomly selected and submitted to mercuric bromophenol blue (BPB) and periodic acid –Schiff (PAS) histochemical tests ( ) for detection of proteins and carbohydrates, respectively, and analyzed and photographed using an Olympus BX 60 light microscope. Unstained sections were incubated in mercury bromophenol blue (BPB) solution (100 mL 2 % acetic acid; 0.05 g bromophenol blue; 1.5 g chloride mercury II) for 2 h and 15 min, washed with 0.5 % acetic acid for 10 min and in running water for 15 min, mounted and analyzed. Unstained midgut sections were incubated in 0.4 % periodic acid for 30 min, washed in distilled water and transferred to Schiff reagent ( ) for 1 h in darkness, washed in running water for 30 min, mounted and analyzed. Protein and carbohydrate quantification were performed using histochemical tests aforementioned. Quantification was performed using the computer program Image J. Images were obtained using a 40× objective, 0.75 numerical aperture lens with the same illumination parameters in an Olympus BX-60 light microscope. The intensities of positive reactions to mercuric bromophenol blue (BPB) and periodic acid–Schiff (PAS) were obtained analyzing the intensity of gray values in the images after converting RGB (red-green-blue) colors to grayscale ( ). Areas of 32 × 32 pixels were used to measure the pixel intensity in the images in 10 random sections of each midgut of the control and treated bees ( ), totaling 200 histological sections. To identify autophagy and apoptosis after 7 days of exposure, the midgut of the workers from the control group ( = 20) and fed on thiamethoxam (n = 20) were dissected in 125 mM NaCl. The midguts were transferred to vials containing Zamboni fixative solution for 30 min and washed five times in 0.1 M sodium phosphate buffer, pH 7.2 (PBS). Then, the samples were transferred to PBS with 1 % Triton-X 100 (PBST) for 1 h and 30 min. Ten midguts from the control group and 10 from the treated group were individually incubated for 72 h at 4 °C in 1:500 anti-LC3/AB antibody (#3868 T; Cell Signaling Technology, Inc., Beverly, MA, USA) for detection of autophagy, and the other 10 midguts from the control and 10 from the treated group were incubated for 72 h at 4 °C in 1:400 anti-cleaved drICE antibody (Asp230, #9478S; Cell Signaling Technology, Inc., Beverly, MA, USA) for detection of apoptosis, totaling 40 midguts. After incubation, the samples were washed in PBS and individually incubated with 1:500 FITC-conjugated anti-rabbit IgG secondary antibody (Sigma-Aldrich Corp., St Louis, MO, USA) for 72 h at 4 °C in the dark. Then, the samples were washed again in PBS and individually transferred to 2 μg/mL 4,6-diamidine-2-phenylindole (DAPI; Polysciences, Inc., Hayward, CA, USA) for nuclei staining for 30 min. Finally, they were once again washed individually with PBS and mounted on glass slides with 30 % sucrose solution. For the negative control, five midguts from each treatment were prepared in a similar way, but without incubation with primary antibody. The samples were analyzed using an Evos M5000 fluorescence microscope (Thermo Fisher Scientific, Carlsbad, USA). The immunolabeled cells were quantified throughout the organ using the 20× objective lens with 0.50 numerical aperture. The quantification evaluated the number of positive cells in all field depths ( ). Workers of from the control ( = 10) and fed on thiamethoxam (n = 10) for 7 days were dissected and the midguts transferred to 2.5 % glutaraldehyde in 0.1 M sodium cacodylate buffer, pH 7.2 with 2 % sucrose for 24 h. After washing with sodium cacodylate buffer, the samples were post-fixed in 1 % osmium tetroxide in the same buffer for 2 h. Then, the samples were washed in the buffer and dehydrated in a graded ethanol series (70 %, 80 %, 90 %, and 95 %). Finally, the samples were embedded in LR-White resin (London Resin Company Ltd.) according to the manufacturer's instructions. The ultrathin sections obtained with an RMC ultramicrotome Boeckellers were stained with 2 % aqueous uranyl acetate for 10 min and lead citrate ( ) for 25 min, examined and photographed in a Zeiss EM 109 transmission electron microscope at the Center of Microscopy and Microanalysis of the Federal University of Viçosa, Brazil. For oxidative stress analyses, abdomens of workers from the control group ( = 20) and those fed on thiamethoxam for seven days (n = 20) were dissected and homogenized in PBS using a Tissue Master 125 homogenizer (Omni International, Kennesaw, USA). The samples were centrifuged at 15,000 × for 15 min at 4 °C, and the supernatant was used for subsequent tests. Protein concentration was determined according to using bovine serum albumin as a standard. Oxidative stress was analyzed based on the activity of the enzymes superoxide dismutase (SOD), catalase (CAT), and glutathione-S-transferase (GST), total antioxidant capacity (TAC), and carbonylated protein (CP). SOD activity was evaluated by quantifying the superoxide anion radical produced by the autoxidation of pyrogallol ( ). CAT activity was determined by the reaction method of undecomposed hydrogen peroxide with ammonium molybdate ( ). GST activity was measured by thioester production using 1-chloro-2,4-dinitrobenzene as substrate ( ). TAC was measured by the ferric reducing antioxidant power (FRAP) method ( ). Protein oxidation was analyzed by quantification of protein carbonyls by the DNPH (2,4-dinitrophenylhydrazine) method ( ). The absorbance for SOD, CAT, GST, and TAC were measured in a Multiskan GO microplate spectrophotometer (Thermo Scientific, Vantaa, Finland) at 320, 374, 340, and 593 nm wavelengths, respectively. The results for SOD, CAT, and GST were expressed in units per milligram of protein (U/mg). For TAC, the amount of μM Fe was used. Protein carbonylation was expressed in nmol/mg of protein. The effect of thiamethoxam concentration on bee survival was analyzed using the Kaplan-Meier estimator. The similarities between the survival curves were assessed by the log-rank chi-square test. After verifying the normality and homogeneity of variances by the Shapiro-Wilk and Levene test, the means of the midgut injury index were compared by the UMann-Whitney test. The data of the brush border height, pixel intensity values from histochemical tests, and oxidative stress were submitted to the t-Student's test. The numbers of cells undergoing autophagy and apoptosis were analyzed using a generalized linear model (GLM) adjusted with a Poisson error distribution. For behavioral data (walked distance and average walking speed), a linear mixed model (LMM) was adjusted. Since bees from the same colony were monitored together in the same Petri dish, the colony was considered a variable effect. Residuals were verified in all models to check the distributions. All analyses were performed using R software (R Core Team, version 4.3.2, 2023) with 5 % significance. The approximate concentration of thiamethoxam found in pollen grains caused a significant reduction in the survival of workers after 7 days of exposure, being reduced from 97% in the control to 56% in the treatment (log-Rank: χ2 = 13.2, df = 1, p < 0.001; ) . Workers exposed for seven days to the approximate concentration of thiamethoxam found in pollen grains showed changes in locomotion behavior compared to the control ( A; Supplementary ), with a decrease in the distance traveled (χ2 = 4.22, df = 1, = 0.03; B, C) and in the average walking speed (χ2 = 5.74, df = 1, = 0.01; C). Workers exposed for seven days to the approximate concentration of thiamethoxam found in pollen grains showed changes in locomotion behavior compared to the control ( A; Supplementary Videos S1 and S2), with a decrease in the distance traveled (χ2 = 4.22, df = 1, = 0.03; B, C) and in the average walking speed (χ2 = 5.74, df = 1, = 0.01; C). The midgut of workers in the control group presented the epithelium with columnar digestive cells and a well-developed apical brush border, nuclei with predominance of decondensed chromatin, regenerative cells organized in nests in the basal region and evident peritrophic matrix in the lumen ( A ). In contrast, the midgut of workers treated with thiamethoxam showed epithelial disorganization, characterized by apical protrusions, cytoplasmic vacuolization and release of cell fragments into the gut lumen ( B). Bees exposed to thiamethoxam had a shorter brush border ( = 7.632, df = 18, < 0.0001; C) and higher lesion index (U = 2.471, p < 0.0001; D, Supplementary Table S1) than the control bees. The midgut of workers in the control group presented the epithelium and a well-developed apical brush border ( A, C ), and the midgut of workers treated with thiamethoxam showed epithelial disorganization ( B, D). Semiquantitative analyses obtained by pixel intensity of histochemical reactions indicate damage to the midgut due to weaker positive reaction for proteins in the peritrophic matrix region ( = 3.059, df = 18, = 0.006) and in the epithelium ( = 7.546, df = 18, p < 0.0001) for bees exposed to thiamethoxam compared to the control ( E). Similarly, the PAS (periodic acid-Schiff) test showed a weaker positive reaction in the thiamethoxam treatment than in the control in the peritrophic matrix ( = 5.760, df = 18, p < 0.0001) and in the epithelium ( = 19.948, df = 18, p < 0.0001) ( F), also indicating damage to the organ. Immunofluorescence revealed an increase in the amount of cells undergoing autophagy (4.14 ± 1.34 cells, χ2 = 8.39, df = 12, = 0.0037) ( A, C, D ) and apoptosis (9.71 ± 1.97 cells, χ2 = 34.64, df = 12, < 0.001) ( B, C, D) in the midgut epithelium of workers after exposure to thiamethoxam compared to control. The midgut of workers from the control group presented digestive cells with an apical surface containing well-developed microvilli and cytoplasm rich in mitochondria ( A ) and rough endoplasmic reticulum ( B). The median nucleus showed decondensed chromatin and evident nucleolus ( C). The basal region of the digestive cells showed plasma membrane infoldings forming a well-developed basal labyrinth associated with mitochondria ( D). The midgut of fed on thiamethoxam presented digestive cells with an apical surface containing some protrusions that were released as fragments ( A ) into the lumen and apocrine secretion ( B). The median cell region showed the nucleus with decondensed chromatin and cytoplasm rich in spherocrystals ( C) and vesicular rough endoplasmic reticulum ( D). The basal region showed an enlarged basal labyrinth ( A ) and mitochondria with dilated and irregular cristae ( B). Furthermore, some cells presented many autophagic vacuoles ( C). workers exposed to thiamethoxam presented an increase in the activity of superoxide dismutase ( = 3.646, df = 8; p = 0.006; A ) and glutathione-S-transferase ( = 2.661, df = 8, = 0.028; B), whereas catalase ( = 1.208, df = 8, = 0.261; C) and total antioxidant capacity ( = 0.347, df = 8, p = 0. 737; D) were similar to the control. The amount of carbonylated proteins increased after exposure to thiamethoxam ( = 3.033, df = 8; = 0.016; E). Our results show that 0.09 ng/g concentration of thiamethoxam reduces the survival of workers after seven days of exposure by >40 %. The contamination of non-target insects by neonicotinoids concerned the scientific community, with increase number of studies proving that this insecticide class has negative effects on these organisms ( ; ; ), including pollinators ( ; ), contributing to ban the use of some neonicotinoids in European Union countries (EFSA, 2015). Thiamethoxam is 192-folds more toxic to the European honeybee Linnaeus (Hymenoptera: Apidae) than other neonicotinoids, such as acetamiprid and thiacloprid ( ). The concentration of 37.5 g of the active ingredient thiamethoxam, recommended for pest control in citrus crops in Brazil, causes mortality of in <4 h after exposure ( ; ), in addition to affecting larval development, reducing body size and damaging neurons ( ; ; ). Furthermore, thiamethoxam is reported to affect the behavior and fat body cells of the stingless bee ( ), the Malpighian tubules and midgut of the stingless bee ( ) and the Malpighian tubules of the stingless bee . The survival results of the wild bee obtained here with chronic exposure to thiamethoxam at sublethal approximate concentrations found in pollen grains suggest that this stingless bee is more susceptible to thiamethoxam than the bee that is used for insecticide risk assessment. The reduction in average walking speed and distance by bees exposed to thiamethoxam is similar to that reported for insects exposed to acetylcholinesterase-inhibiting insecticides that degraded the neurotransmitter acetylcholine ( ). Thiamethoxam acts on the nervous system of insects by altering the opening of ion channels in nicotinic acetylcholine receptors. Therefore, it is plausible to suggest that the decreases in the walking distance and average speed of bees observed herein, may be due to the action of thiamethoxam on the nervous system. Furthermore, damage to the midgut epithelium may impairs energy-obtaining processes affecting ATP (Adenosine triphosphate) availability, similar to reported in workers exposed to the fungicide fluazinam ( ). Low ATP availability may compromise food collection, reducing flower visitation ( ; ). Analysis of the midgut of workers fed on thiamethoxam for seven days revealed an increase in the index of organ damage compared to control bees. Similar damage was reported in the midgut of after acute exposure to lethal concentrations of the insecticides lambda-cyhalothrin ( ) and imidacloprid ( ), fungicides iprodione ( ) and azoxystrobin ( ), acaricides spiromesifen ( ) and cyflumetofen ( ) and the herbicide pedimentalin ( ). Furthermore, similar damage was reported in exposed to lambda cyhalothrin ( ), fipronil ( ), spinosad ( ) and a mixture containing the herbicides mesotrione and atrazine ( ). These results demonstrate that, even at sublethal concentrations, thiamethoxam compromises nutrient absorption in the midgut of . The increased release of apocrine secretion in the midgut of workers exposed to thiamethoxam compared to the control may be a detoxification response of digestive cells, such as that found in the predator (Dallas) (Heteroptera: Pentatomidae) after exposure to spinosad ( ) and after exposure to the acaricide cyflumetofen ( ). Proteins of the cytochrome P450 superfamily play a role in detoxification by metabolizing xenobiotics, converting them into water-soluble compounds, facilitating excretion that can occur through apocrine secretion ( ; ). Increased apocrine secretion may also be due to increased secretion of peritrophic matrix components ( ; ), as a metabolic response to mitigate the chronic effects of thiamethoxam. The midgut epithelium of bees exposed to thiamethoxam has a shorter brush border compared to the control group, in addition to some disorganization, which may affect digestibility, since there is a decrease in the cell surface area for secretion, nutrient absorption, and ion transportation ( ; ). In bees exposed to thiamethoxam, there is dilation and disorganization of the basal labyrinth of digestive cells in relation to the control group, in addition to the presence of mitochondria with dilated and irregular cristae, similar to found in acutely exposed to imidacloprid ( ), lambda-cyhalothrin ( ), and cyflumetofen ( ). The dilation of the basal labyrinth, may be a detoxification response, through the high transport of substances between the cytosol and hemolymph. Mitochondrial damage, on the other hand, may cause loss of membrane potential and release of components that induce apoptosis mediated by caspases ( ; ), compromising the cell survival. Digestive cells of workers exposed to thiamethoxam presented vesicular rough endoplasmic reticulum, possibly indicating endoplasmic reticulum stress, such as reported for human cells exposed to heavy metals ( ; ), which can trigger increased levels of oxidative stress, autophagy and cell death ( ). Digestive cells also presented an abundance of spherocrystals after exposure to thiamethoxam in workers, similar to that reported in these cells of exposed to the insecticide teflubenzuron, indicating a possible response of the digestive cells of to thiamethoxam ( ). Histochemical tests using mercuric bromophenol blue and periodic acid-Schiff indicate low amounts of proteins and carbohydrates, respectively, in the intestinal epithelium and peritrophic matrix of workers exposed to thiamethoxam compared to the control group. Due to its functions in food digestion and nutrient absorption ( ), the midgut has digestive cells with well-developed rough endoplasmic reticulum and high secretory activity, responsible for the production of digestive enzymes ( ; ) and peritrophic matrix glycoproteins ( ; ; ; ), in addition to microvilli containing abundant glycoproteins and glycolipids (glycocalyx; ) that increase surface contact with food and aid in mechanical protection ( ; ). Our histochemical findings support the histopathological data, indicating that the glycocalyx was not an effective barrier against thiamethoxam, which affects proteins and carbohydrates of the epithelium and peritrophic matrix, impairing nutrient absorption and protein release for protection against xenobiotics. The increase in the number of cells undergoing apoptosis and autophagy in the midgut of bees exposed to thiamethoxam indicates possible removal of damaged cells. In apoptosis mediated by the caspase pathway, there is disorganization of the nuclear lamina that support the nuclear membrane and depolymerization of cytoplasm microtubules with fragmentation of the Golgi complex and endoplasmic reticulum, increasing cytoplasmic vacuolization ( ), which was found in the midgut epithelium of treated bees. Furthermore, other characteristics common to the apoptosis were observed in transmission electron microscopy, including the increase in the intercellular spaces, apical protrusions and microvilli disorganization ( ; ), affecting the nutrient absorption and disrupting the gut epithelium ( , ). The LC3/AB-positive cells indicate the occurrence of autophagy, which was evidenced by the presence of autophagic vacuoles, which may be due the cell protection mechanism involving the degradation of thiamethoxam, since autophagy has a cytoprotective function ( ). Similar results were reported in the midgut epithelium of exposed to spinosad ( ). Autophagy mitigates oxidative stress and acts in the removal of misfolded proteins and damaged organelles ( ). However, despite playing a crucial role in cell homeostasis, autophagy, at high levels, may cause cell death, including in the midgut of insects exposed to pesticides ( ; ; ; ). Thus, it is plausible to suggest that the autophagy induced by thiamethoxam has the potential to damage the midgut of . In this study we found thiamethoxam induced an increase in superoxide dismutase activity in the abdomen of workers, similar to that reported for the insecticides fipronil ( ) and lambda-cyhalothrin ( ) for this bee, suggesting a first defense against superoxides after exposure to thiamethoxam. Since this enzyme catalyzes the dismutation of superoxide anions to water and hydrogen peroxide, which is then dissociated into water and oxygen by catalase ( ). However, catalase activity did not differ from control bees, suggesting that the levels of this enzyme are sufficient to neutralize excess superoxides derived from the action of superoxide dismutase. Furthermore, the observed increase in glutathione-S-transferase activity in bees exposed to thiamethoxam may be compensated for catalase activity, as reported for other pesticides ( ; ), since glutathione-S-transferase acts in detoxification through the harmful metabolites conjugation with glutathione, producing fewer toxic compounds to be eliminated from the cell ( ; ). These results suggest that in , glutathione-S-transferase plays a fundamental role in protecting against reactive oxygen species induced by thiamethoxam. Total antioxidant capacity is not affected in bees exposed to sublethal concentrations of thiamethoxam. This test measures all antioxidant intracellular molecules, mainly those originating from food, which was similar (50 % sucrose solution) in the control and treatment. However, workers treated with thiamethoxam presented an increase in the amount of carbonylated proteins, indicating oxidative damage ( ). Protein carbonylation is an irreversible oxidative process often used as a biomarker of oxidative stress (Fedorova et al., 2014; ). Oxidative stress may activate cell death pathways, such as apoptosis and autophagy ( ; ), which was observed in this study, suggesting that the increase in cells undergoing cell death in the midgut of workers exposed to thiamethoxam may has been triggered by oxidative stress due to chronic ingestion of the insecticide at sublethal concentration. The mortality caused by insecticides in non-target organisms is an important factor, but other sublethal effects, such as those found here in bees in this study, remain neglected. Therefore, evaluating the sublethal effects of insecticides at residual concentrations found in the field is necessary to understand how they affect organisms, since in the long-term, these changes compromise survival with possible population decline. Our results indicate that long-term consumption of food with approximate thiamethoxam concentration found in pollen grains is harmful to adult workers of , jeopardizing digestion, inducing cell death in the midgut and oxidative stress, changing antioxidant enzymes activities, and affecting the locomotion with the potential to compromise the whole colony. The following are the supplementary data related to this article. Supplementary data to this article can be found online at . Table. S1. Data used for the calculation of the midgut lesion index. Writing – review & editing, Writing – original draft, Validation, Investigation, Conceptualization. Formal analysis, Investigation, Validation, Visualization, Writing – original draft. Methodology, Investigation. Methodology, Investigation. Methodology, Investigation. Methodology. Methodology. Methodology, Investigation. Validation, Investigation, Formal analysis. Writing – original draft, Validation. Writing – review & editing, Writing – original draft, Validation, Supervision, Investigation, Funding acquisition, Conceptualization.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Not Used\n",
      "* **AIモデルの判断:** Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 154 / DOI: 10.1016/j.ygeno.2021.09.013  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "An ATAC-seq atlas of chromatin accessibility in mouse tissues\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Differential analysis of chromatin accessibility and gene expression profiles identifies cis-regulatory elements in rat adipose and muscle\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "Chromatin accessibility across the genome defines cis-regulatory elements that dynamically control gene expression via interactions with transcription factors (TFs), co-factors, and chromatin regulators [ ]. To understand how the chromatin accessibility landscape can shape tissue-specific gene expression, we used white adipose tissue (WAT) and skeletal muscle, which both play critical yet contrasting roles in the regulation of whole-body energy metabolism. Muscle is a complex and highly organized tissue composed of structural, contractile, and regulatory proteins [ ]. Its remarkable plasticity is illustrated by its ability to undergo adaptive changes, e.g. in response to exercise, inactivity [ ], or with aging [ ]. By contrast, WAT mainly consists of large adipocytes containing a lipid droplet [ ] and storing energy as fatty acids. Both muscle and adipose tissue exert endocrine effects on other tissues, including on each other, via the secretion of myokines and adipokines, respectively [ ]. Exercise training induces adaptations in both muscle and adipose. In muscle, these adaptations include muscle growth, an increase in glucose uptake and mitochondrial activity [ ], and alterations in myokine secretion. In adipose, changes induced by exercise include a reduction in adipocyte size and in their lipid content, an increase in mitochondrial activity, and an altered profile of secreted adipokines [ ]. Altogether, muscle and adipose adaptations to regular exercise contribute to improving metabolic health and lowering the risk of metabolic disorders including obesity and type 2 diabetes [ , ]. Transcriptome profiling of muscle and adipose in rat has identified gene expression changes associated with exercise, diet/fasting, aging, and metabolic disorders [ ]. Gene expression is generally regulated by TFs that interact with cis-regulatory DNA elements. Assessing genome-wide chromatin accessibility can facilitate the identification of such functional genomic regions [ ] and provide insights into gene regulatory mechanisms and transcriptional networks [ ]. The Assay for Transposase-Accessible Chromatin using sequencing (ATAC-seq), is a low-input, relatively fast method, which consequently has been widely used for revealing cis-regulatory elements and predicting TF binding sites [ ]. Despite the availability of the Omni-ATAC protocol for profiling chromatin accessibility from frozen tissues [ ], due to the technical difficulties posed by the high lipid content of adipocytes [ ], there are few ATAC-seq studies in adipose or tissue-derived adipocytes [ ]. Additionally, there have been ATAC-seq data quality-related issues, i.e. low reproducibility between replicates. Particularly, variability in Irreproducible Discovery Rate (IDR) peaks and in transcription start site (TSS) enrichment scores across tissues has been reported [ ]. To date, there has been no integrated analysis of ATAC-seq and RNA-seq data in adipose tissue. Only one such study has been reported in human muscle [ ]. Here, we aimed to investigate the influence of genome-wide cis-regulatory regions on gene expression in rat adipose and muscle tissues. Because chromatin accessibility profiling in adipose tissue has been technically difficult, we developed a modified ATAC-seq method, ruptor-ATAC, that generates high-quality data from both tissues and is suitable for high throughput studies. To uncover and compare the cis-regulatory circuits of these two key metabolic tissues, we analyzed their chromatin accessibility and transcriptome landscapes. Integration of the ATAC-seq and RNA-seq data allowed us to identify tissue-specific chromatin accessible regions and map them to differentially expressed (DE) genes. Additionally, we determined the TF binding sites that were highly enriched in tissue-specific -regulatory regions, and established correlations between enriched motifs and the relative expression levels of TFs that are known to recognize these motifs. We further substantiated the functional relevance of some TFs by showing a positive relationship between differential chromatin accessibility at the cognate cis-motifs and differential expression of the corresponding genes. We sought to interrogate genome-wide chromatin accessibility in snap-frozen white adipose and skeletal muscle tissue samples obtained from male rats. While the Omni-ATAC protocol was previously developed to enable the profiling of chromatin accessibility from frozen tissues [ ], its generalization to various tissue types, adipose tissue in particular, has been limited. Notably, the fact that tissue homogenization, which precedes nuclei isolation, and nuclei count are not amenable to automation introduces a certain degree of variability to the experimental procedure. In particular, Dounce homogenization involves a defined number of strokes with the pestles (e.g. 10 with the loose pestle, followed by 20 with the tight pestle), which can vary between tissue types (e.g. soft vs. hard tissues), and the number of extracted nuclei may fluctuate significantly between tissue types, adding more sample-to-sample variation [ ]. To circumvent these issues and achieve high data reproducibility, we modified the Omni-ATAC protocol to include an optimized bead-based tissue homogenization process, ruptor-ATAC, that can be adapted to various tissue types, including adipose and muscle tissues (see Materials and Methods and Supplementary Fig. S1). Integration of this homogenization step to the remainder of the Omni-ATAC protocol led to improved ATAC-seq data quality, with an overall adherence to the ENCODE quality standards for ATAC-seq data ( [ ]), as detailed below. Adipose and muscle samples obtained from five individual male rats were independently subjected to ruptor-ATAC. The ATAC-seq libraries generated from the five biological replicates of each tissue were sequenced to an average of ~158 and ~203 million reads per adipose and muscle sample, respectively (Supplementary Table S1), which was previously proven to be sufficient for the detection of accessible regions [ ]. Both adipose and muscle ATAC-seq libraries showed expected fragment size and clear nucleosome phasing (Supplementary Fig. S2A, B and 2D, E). Reproducible peaks ranged from 44,673 to 54,458 in adipose, and from 84,195 to 112,272 in muscle samples (IDR peaks, Supplementary Table S1). TSS enrichment values were above the cutoff (>9; ), and the fraction of reads in called peak regions (FRiP) scores were generally greater than 0.2, indicating the high quality of the ATAC-seq data. Comparison of our ATAC-seq metrics with those obtained in the same tissue types in mouse using the Omni-ATAC protocol [ ] showed significantly higher percentages of usable reads and IDR peaks in our samples relative to mouse, and dramatically higher TSS enrichment values in our muscle samples, thus supporting the advantage of our protocol. When comparing our data quality statistics (e.g. FRiP, TSS enrichment, Usable Reads) with other ATAC-seq studies conducted in animal fat and muscle samples [ , ], numbers were within similar ranges, and an inter-tissue variability was observed across all studies, most likely reflecting differences in tissue cellular composition. Overall, this concurrence between quality control values supported the validity of our technology. To further evaluate the reproducibility of biological replicates, we performed Pearson correlation analyses on the ATAC-seq data obtained across the whole genome as well as in each genomic region. Heatmap clustering of Pearson correlation coefficients on genome-wide ATAC-seq peaks and on promoter peaks identified across tissue samples revealed significant correlations between all replicates (Supplementary Fig. S3). Correlations were also found for other genomic regions (Supplementary Fig. S3), thus corroborating the reproducibility of the ATAC-seq data. As anticipated, there was a strong enrichment of ATAC-seq reads around transcription start sites (TSS) in both tissues (Supplementary Table S1 and Supplementary Fig. S2C, F). We next examined the genomic distribution of open chromatin peaks across all adipose and muscle tissue samples. The majority of peaks mapped to intergenic regions, followed by intronic and promoter regions ( A ), the latter being defined as 3 kb upstream and downstream of the TSS (<1.0 kb = 80.7%, 1–2 kb = 10.5%, 2–3 kb = 8.9%). The peak frequency at these three major genomic regions of open chromatin was comparable across samples from the same tissue, confirming the reproducibility between replicates. Shown in B–D are ATAC-seq tracks at characteristic gene loci, following the alignment of peak sets for the different samples and determination of a consensus set of peaks. Loci correspond to the common housekeeping gene actin, beta ( ) the adipose-enriched perilipin 1 gene ( [ ]), and the muscle-specific myogenic differentiation 1 gene ( ; [ ]). Note the presence of promoter, intron, and intergenic peaks. To investigate the relationship between chromatin accessibility and gene expression profiles, we conducted RNA-seq analysis in adipose and skeletal muscle samples obtained from ten individual male rats. Quality assessment of the RNA-seq data included an examination of GC content, % of ribosomal RNA (rRNA), % of reads uniquely mapped to the genome, and % of mRNA (Supplementary Fig. S4A). We found similar GC content levels (~50%) across the 20 samples. The proportions of rRNA were less than 1% across all samples, while the proportions of mRNA ranged between 72.7 and 83.1%. The percentage of reads that were uniquely mapped to genomic regions was around 85% across all samples. Additionally, the Pearson correlation between the expression profiles of all biological replicates demonstrated a high level of measurement consistency and reproducibility among tissue replicates (Supplementary Fig. S4B). While examining the relationship between transcript levels and accessibility levels at each genomic region, we found the strongest correlation at the gene promoter in both tissues ( A, B ). These results were in agreement with previous studies in various tissue types reporting a positive correlation between gene expression and ATAC-seq signal at the promoter [ ]. To characterize the genes showing similar patterns of promoter accessibility and gene expression, we ranked genes based on their cumulative promoter ATAC-seq signal z-score and their RNA count z-score in each tissue and plotted RNA-seq scores against ATAC-seq z-scores ( B). Similar calculations were done for other genomic regions ( B and Supplementary Fig. S5A). As expected, RNA-seq and promoter ATAC-seq signal z-scores showed a fair correlation in each tissue, yet a significant number of genes had divergent z-scores, e.g. low accessibility and medium expression ( B). In contrast, there was no correlation between RNA-seq and ATAC-seq z-scores at other genomic regions ( B and Supplementary Fig. S5A). When next plotted the z-score differences between promoter ATAC-seq and RNA-seq in muscle against those in adipose tissue ( C). Genes with values close to 0 in both tissues (center of the plot) represented genes with similar levels of promoter accessibility and RNA expression in each tissue (see Supplementary Table S2). Additionally, we detected: i) two gene subsets that displayed opposite patterns of promoter accessibility and expression in both tissues, namely high promoter accessibility/low expression (HA-LE; top-right of the plot), and low promoter accessibility/high expression (LA-HE; bottom-left of the plot); ii) “outlying” genes showing divergent z-score differences in one tissue compared to the other, that is, genes showing differential promoter accessibility, differential expression, or both. Predictably, among the genes showing similar patterns of promoter accessibility and RNA expression in both tissues, those with either high accessibility/high expression (HA-HE) or medium accessibility/medium expression (MA-ME) represented genes involved in general biological processes (Supplementary Fig. S5B). Taken as a whole, these findings indicate that promoter accessibility is not always associated with transcriptional activity, and that reduced promoter accessibility does not necessarily correlate with low gene expression [ ]. Conceivably, HA-LE genes were repressed through the binding of transcriptional repressors to the promoter [ ], whereas promoters of LA-HE genes were potentially occupied by large regulatory complexes that blocked transposase accessibility [ ]. Moreover, additional mechanisms, including accessibility at non-promoter/enhancer regions, histone modifications, and DNA methylation, may contribute to activation of gene transcription. While chromatin accessibility is a requisite for promoter or enhancer activity, it is not sufficient. A combination of histone modification marks influences chromatin accessibility and can predict promoter or enhancer activity [ , , ]. To further classify active promoters or putative enhancers at intronic and intergenic regions, we sought to integrate our ATAC-seq data with publicly available ChIP-seq datasets of histone modifications in rat adipose or skeletal muscle. While only mouse ChIP-seq datasets were available for adipose, we identified one rat skeletal muscle ChIP-seq dataset of H3K27ac, a histone modification that marks active promoters and enhancers [ ]. A shows heat maps of the genome-wide H3K27ac and ATAC-seq signals. ChIP-seq peak intensity was highest in a subset of intergenic regions, at promoters, and at introns, and a comparable pattern was observed for ATAC-seq peaks ( B). When examining the relationship between ChIP-seq peak distance to TSS and peak intensity, we identified two major populations, one located in the vicinity of the TSS (0 to ~10^3 bp), the other corresponding to the intronic and intergenic regions (above 10^3 to 10^6 bp; C, lanes 1 and 2). Similar populations were identified in the muscle ATAC-seq data ( C, lanes 3 and 4). A pairwise comparison of H3K27ac peaks vs. ATAC-seq peaks within a ± 10 kb from TSS window showed that the vast majority of ATAC-seq peaks pairing with H3K27ac peaks (4997 out of 5596 paired peaks) were within 1 kb of each other, thus demonstrating that the two epigenetic signals were highly correlated within promoter and gene body regions ( D). Furthermore, overlapping of the ATAC-seq peaks with the ChIP-seq peaks of H3K27ac showed that a large proportion of these histone modification marks were enriched at open chromatin regions annotated to promoters, intronic, and intergenic regions ( E). Sequencing tracks annotated to two muscle-enriched genes are presented in F, illustrating the H3K27ac/ATAC-seq overlap. Thus, our analysis supports the enrichment of H3K27ac at open promoters and enhancer regions in muscle. Changes in chromatin structure are known to play a crucial role in the regulation of gene transcription [ ]. To evaluate the influence of tissue-specific chromatin accessibility on gene expression, our first step was to identify tissue-restricted ATAC-seq peaks using DiffBind [ ], with the following statistical cutoff: log2 Fold Change (L2FC) > 1 and FDR < 0.01. Among the 742,326 open chromatin regions identified in both tissues, 40,372 were differentially accessible and referred to as DARs; 20,194 DARs were adipose-specific, and 20,178 were muscle-specific. A illustrates the top 5000 tissue-specific DARs. Globally, tissue-specific DARs localized farther from TSS in adipose tissue compared to muscle ( B). When comparing the number of DARs at a given genomic region vs. the number of genes harboring DARs in that same region, the ratio of DARs/genes was approximately equal to 1 at the promoter, 2 at the intron, and 3 at the intergenic region ( C), suggesting the presence of multiple cis-regulatory elements in those two non-promoter regions. Comparison of the genomic distribution of tissue-specific DARs in adipose vs. muscle tissue revealed that the proportion of intergenic DARs was higher in adipose relative to muscle (59.2% vs. 37.6%, respectively), whereas the percentage of promoter DARs was higher in muscle (36.7% vs. 13.1%, respectively; D, E). Divergence of the proportions of intergenic/distal vs. proximal DARs in the two tissues was coherent with the overall positioning of DARs farther from TSS in adipose tissue ( B). Tissue differences were also observed in the proportions of genes harboring the major categories of DARs, with a higher percentage of genes holding intergenic DARs in adipose (44.1% vs. 26.7%; F, G), and a higher percentage of genes carrying promoter DARs in muscle (49.5% vs. 23.2%). Along with D, E, the number of genes harboring DARs in the different genomic regions in adipose vs. muscle ( F, G) was overall consistent with the ratios in C. Remarkably, a significant portion of genes holding tissue-specific DARs in one of the major genomic regions of open chromatin (intergenic, intron, or promoter region) concomitantly harbored DARs in either of the other two regions or both ( H, I). For instance, in muscle tissue, while 4262 genes had exclusively promoter DARs, 1329 genes concurrently had DARs in both promoter and intergenic regions, and 473 genes had DARs in all three regions. In either tissue, one third to a half of the genes bearing promoter DARs also had DARs in either of the other two regions or both. Similarly, a third to a half of the genes associated with intergenic DARs concurrently had DARs in either of the other two regions or both. Over half of genes bearing intron DARs also had DARs in the other two regions. Thus, a large proportion of genes had open chromatin regions at the promoters along with intergenic or/and intronic regions, suggesting an interplay between promoter and more distant cis-regulatory regions for the regulation of gene expression. Presumably, intergenic/distal and intronic genomic regions harbor active cis-regulatory elements such as enhancers, insulators, or silencers [ , ]. In order to examine the relationship between DARs and differential gene expression in adipose tissue vs. muscle, we initially mapped tissue-specific DARs to their adjacent genes. DARs were assigned to three categories of genes: i) genes showing a significantly higher expression (HE) in one tissue compared to the other, ii) genes exhibiting a significantly lower expression (LE) in one tissue compared to the other, iii) non-DE genes. While about half of DARs mapped to non-DE genes (48.9% [ = 9875] in adipose, 49.5% [ = 9988] in muscle; A ), more than a third of DARs were mapped to HE genes (39.4% [ = 7956] in adipose, 35.0% [ = 7062] in muscle), and a small percentage was mapped to LE genes (11.7% [ = 2363] in adipose, 15.5% [ = 3128] in muscle). We performed a functional enrichment analysis using Metascape [ ] to determine which biological functions were enriched in DARs-associated HE genes (HA-HE) and in DARs-associated LE genes (HA-LE) in each tissue. Adipose-specific HA-HE genes were enriched for processes that have been involved in the regulation of adipose tissue expansion or differentiation [ ], while muscle-specific HA-HE genes were enriched for muscle biology-related terms, as detailed in the Discussion ( B). On the other hand, adipose-specific HA-LE genes were enriched for muscle-related processes, while muscle-specific HA-LE genes showed enrichment in adipose tissue-related processes. This supported the hypothesis that ATAC-seq peaks in both HA-LE groups may correspond to either negative regulatory elements or actively repressed regions (see Discussion). To validate the differential expression of genes in adipose vs. muscle, we randomly selected 12 genes from the list of DE genes identified in the RNA-seq data analysis (i.e. 6 adipose-specific HA-HE genes and 6 muscle-specific HA-HE genes), and carried out quantitative real-time PCR (qPCR) experiments on the RNA samples from each tissue (5 samples per tissue). As illustrated in C, the 12 DE genes were replicated by qPCR. To assess the impact of DARs and their genomic location on gene regulation, we examined the correlation between differential gene expression and differential accessibility at each genomic region ( A–G ). Each correlation plot was divided into four quadrants corresponding to distinct functional groups of DARs. The NE and SW quadrants represented adipose- and muscle-specific DARs, respectively, associated with HE (DARs-HE). By contrast, the SE and NW quadrants comprised muscle- and adipose-specific DARs, respectively, associated with LE (DARs-LE). Consistent with the positive correlation between gene expression and promoter accessibility in both adipose and muscle tissues (see A, B), we observed a significant positive correlation between differential gene expression and differential ATAC-seq signal at the promoter region ( = 0.637; A). There was a high correlation between differential gene expression and chromatin accessibility at the 5′UTR ( = 0.505; B), a moderate correlation at the intron, exon, downstream, and 3′UTR (r ranging from 0.424 to 0.317; C–F), and a low correlation at the intergenic region ( = 0.291; G). To determine which genomic region(s) most contributed to each functional group of DARs, we combined same-group DARs across all genomic regions (i.e. from promoter through intergenic region; see A–G), and determined the fraction of DARs localizing to each genomic region ( K). As illustrated in K, intergenic DARs were predominant in the adipose-specific DARs-HE group, followed by intron and promoter DARs. By contrast, promoter and intergenic DARs were in similar proportions in the muscle-specific DARs-HE group. These results were consistent with the presence of dramatically more intergenic ATAC-seq peaks in adipose than in muscle ( D, E), and the comparable number of promoter vs. intergenic peaks in muscle ( E). In both tissue-specific DARs-LE groups, intergenic DARs were the most prevalent. The fraction of promoter DARs was markedly decreased in DARs-LE groups compared to DARs-HE groups, likely reflecting the weak or lack of promoter activity of lowly expressed genes. Considering the interplay between different classes of regulatory elements, namely promoters, enhancers, silencers, and insulators in orchestrating gene transcription [ ], accessible intergenic and intron regions in DARs-HE groups presumably represent active enhancers [ ] forming interactions with active promoters. By contrast, within the DARs-LE groups, we speculate that intergenic peaks may comprise negative regulatory elements, e.g. silencers, enhancer-blocking insulators, primed or repressed enhancers. To further investigate the contributions of promoter and non-promoter genomic regions to gene regulation, we examined the relationship between differential gene expression and the number of DARs at either the gene promoter, intron, or intergenic region ( H-J). Although the correlation coefficients were non-significant for all three regions, there appeared to be a trend where the higher the number of DARs, the lower the proportion of DE genes, suggesting that genes with a higher number of DARs were less likely to be differentially expressed. Tissue-specific DARs likely harbored motifs allowing for the binding of transcriptional activators or repressors. Using HOMER [ ], we performed motif enrichment analysis on tissue-specific DARs to predict the sets of TFs that may be involved in the regulation of gene expression in each tissue. In general, DARs that were mapped to HE genes showed very distinct motif enrichment patterns in adipose tissue vs. muscle, and the same was true for DARs mapped to LE genes ( A ). Motif over-representation in intergenic and intron DARs in addition to promoter DARs supported a functional role for non-promoter cis-acting regulatory elements in orchestrating tissue-specific gene activation or gene repression. In the adipose-specific DARs-HE group, top enriched motifs included binding sites for IRF1, IRF8, PU.1, ELF4 and ELF5, a canonical binding sequence for IRFs (ISRE), and a c-Jun-CRE composite motif. IRFs, PU.1, ELF4, and CREB have all been implicated in the regulation of adipogenesis [ ]. By contrast, the adipose-specific DARs-LE group was highly enriched in binding motifs for SCL, CEBP, NFY, IRF3, CLOCK, PPARα, ELF4, ERRγ, CHR, GLI2, and ZNF7. The muscle-specific DARs-HE group was enriched in binding sites for members of the muscle-enriched MEF2 TF family [ , ], KLF TFs, YY1, FOSL1 and FOSL2 (alias FRA-1 and FRA-2), SMAD3, and for a composite Jun-AP1 element. KLFs have emerged as important regulators of muscle biology [ , ], and previous studies demonstrated the involvement of YY1, Fosl2, AP-1, and Smad3 in the regulation of muscle regeneration and/or differentiation [ ]. In the muscle-specific DARs-LE group, top enriched cis-elements included KLF binding sites and motifs for ATF1, ELK1, SIX1 and SIX2, MYOG, and YY1. Along with SIX factors, members of the myogenic regulatory factor family of TFs such as MYOG are critical for muscle differentiation [ ]. Thus, within the same tissue, there was some overlap in the motifs enriched in the DARs-HE vs. DARs-LE group, suggesting the possibility that the same TFs: i) may act either as transcriptional activators or as repressors, depending upon the genomic context, the interacting co-factors, and the TF concentration, ii) may be inhibited by non-DNA binding repressors [ ]. We next sought to establish a direct link between binding motifs that were over-represented in tissue-specific DARs groups and the relative mRNA expression levels of TFs known to recognize these cis-motifs in adipose vs. muscle. As illustrated in B, , the majority of TF transcripts were expressed in both tissues. However, some of the transcripts were highly expressed in adipose, while others showed higher expression in muscle ( B, ). We found that the genes encoding SPI1 (alias PU.1), IRF1, IRF3, IRF8, ELF4, ELF5, TAL1, CEBPA, NFYA, GLI2 were highly expressed in adipose tissue relative to muscle, which was consistent with the enrichment of the corresponding binding sites in adipose-specific DARs. In muscle, the genes encoding MEF2c and MEF2d, SIX1/SIX2, MYOG, SMAD3, HLF, and KLF5 were up-regulated in muscle relative to adipose tissue, which was consonant with over-representation of the corresponding binding motifs in muscle-specific DARs. Despite a few discrepancies between motif enrichment patterns and transcriptomic data, e.g. with PPARα and ERRγ motif enrichment in adipose DARs vs. increased expression levels in muscle, there was a moderate, albeit significant correlation between differential expression of TF-encoding genes and differential TF motif enrichment at DARs (Spearman correlation, = 0.380; Supplementary Fig. S6). The overall higher abundance of TF-encoding transcripts in adipose than in muscle ( B, ) might explain, at least in part, the enrichment of adipose DARs at intergenic regions. Altogether, our results suggest that distinct sets of TFs regulate tissue-specific gene expression in adipose vs. muscle, and in each tissue these TFs may function as either transcriptional activators or repressors depending upon the gene context. To further substantiate the function of some tissue-enriched TFs as potential transcriptional activators and to re-evaluate which genomic region(s) they may primarily bind to, we analyzed the relationship between differential chromatin accessibility at the TF binding sites and differential expression of the genes harboring these DARs. Differential chromatin accessibility was inspected independently in each of the three major DAR subsets (promoter, intron, and intergenic). The analysis is illustrated for IRF8 and BCL11a, both of which were up-regulated in adipose tissue and whose binding motifs were over-represented in adipose-specific DARs ( A ), as well as for muscle-enriched YY1 and MEF2c TFs, whose binding motifs were over-represented in muscle-specific DARs ( B). Consistent with the enrichment of the IRF8 motif in all three subsets of adipose-specific DARs-HE and its elevated adipose expression, we found a predominance of ATAC-seq peaks in the NE quadrant corresponding to adipose-specific DARs-HE. Similarly, for the BCL11a motif, there was a prevalence of ATAC-seq peaks at the NE quadrant in all three genomic regions. With respect to the muscle-enriched YY1, we found predominant promoter peaks in the SW quadrant, which was in line with its motif enrichment in the promoter subset of muscle-specific DARs-HE. For MEF2c TF, intron and intergenic ATAC-seq peaks were prevalent in the SW quadrant (i.e. muscle-specific DARs-HE), in agreement with the motif enrichment analysis. Finally, we provide examples of DARs mapping to genes that are either DE in adipose vs. muscle or exclusively expressed in one of the two tissues (Supplementary Figs. S7–S8). While some of the adipose-enriched genes harbor an adipose-specific DAR at the promoter (e.g. , ), others hold adipose-specific DARs that are located outside of the promoter (gene body for ; intergenic for ; Supplementary Fig. S7). As for muscle-enriched genes ( , , , , , , , and ), most of them hold a muscle-specific DAR at the promoter, along with DARs in gene body or/and intergenic regions (Supplementary Fig. S8). Moreover, several TF binding motifs are highly predicted at these promoter and non-promoter DARs (Supplementary Figs. S7–S8). These examples underscore that tissue-specific DARs are not restricted to the promoter region, and that an interplay between promoter and non-promoter cis-regulatory regions is likely involved in the control of tissue-specific gene expression. Moreover, the detection of DARs in the absence of gene expression suggests they may function as negative regulatory elements. Altogether, our data highlight the value of ATAC-seq and RNA-seq data for identifying -motifs in tissue-specific regulatory regions. In the present study, we carried out an integrated analysis of genome-wide chromatin accessibility and the transcriptome to characterize the tissue-specific cis-regulatory regions controlling gene expression in rat white adipose and skeletal muscle tissues. We also compared our chromatin accessibility data in muscle with ChIP-seq H3K27 acetylation data in the same tissue. The analysis results supported the enrichment of H3K27ac at open promoters and enhancer regions. While integration of the ATAC-seq and RNA-seq datasets showed that promoter accessibility positively correlated with gene expression in both tissues, differential analysis facilitated the identification of tissue-specific DARs and their mapping to HE and LE genes. DARs localized preferentially in intronic and intergenic genomic regions, suggesting the functional significance of non-promoter cis-regulatory elements in gene expression. Motif enrichment analysis on DARs allowed us to predict binding sites for TFs with possible tissue-specific functions. By correlating the enriched motifs with the relative RNA levels of TFs recognizing those motifs, we identified factors that likely play key roles in transcriptional activation and repression in each tissue. Altogether, our findings provide new insights into the tissue-specific regulatory mechanisms underlying gene expression in adipose and muscle. To our knowledge, no integrated analysis of ATAC-seq and RNA-seq has been reported thus far in adipose tissue. Importantly, our ATAC-seq and RNA-seq profiles both showed a much higher inter-tissue variability than inter-animal variability. Our results were consistent with ATAC-seq and RNA-seq atlases that were independently reported in rodent tissues [ , ], and with the fact that our biological samples corresponded to animals from the same inbred rat strain (i.e. showing low genetic variability). Other advantages in our study design included the absence of batch effect, as each sample was run independently for ATAC-seq [ ], and the development of a robust ruptor-ATAC protocol generating high signal-to-noise ratios in both adipose tissue and skeletal muscle. Our method has proven successful for generating ATAC-seq data from other frozen rat tissues such as liver and heart (unpublished results). Despite these benefits, our study has a few limitations. A major portion of DARs were mapped to non-DE genes, which could be explained by one of the following reasons: i) to be activated, these genes may require accessibility at numerous enhancers; ii) some enhancers may have not been assigned to the correct genes due to their very long distance from genes; iii) some TFs bound to open chromatin regions may require interactions with additional cofactors to transactivate genes. Interestingly, we observed a trend where genes showing higher differential expression between the two tissues have fewer DARs than genes showing either lower differential expression or no differential expression. Thus, we surmise that a number of DARs are not functionally active and that additional mechanisms such as histone modifications, DNA methylation, and/or transactivation by TFs are needed to render them fully functional. In this work, a significant percentage of the identified tissue-specific DARs were assigned to HE genes, whereas a smaller group was mapped to LE genes. Adipose-specific HA-HE genes showed strong enrichment for inflammatory response, blood vessel development, and regulation of cell adhesion, in accordance with the modulation of inflammation by adipose-secreted adipokines and the requirement of angiogenesis and extracellular matrix remodeling for adipose tissue expansion [ , , ]. By contrast, muscle-specific HA-HE genes were enriched for muscle structure development, catabolic processes, muscle contraction, and other muscle-related GO terms. Consistent with their transcriptional repression, adipose-specific HA-LE genes showed a strong enrichment for muscle-related processes, while muscle-specific HA-LE genes were enriched for adipose tissue-related processes such as vasculature development and regulation of cell adhesion. As previously reported, down-regulated gene expression is not necessarily associated with chromatin inaccessibility; it can also be coupled with high accessibility [ , , ]. Indeed, while ATAC-seq peaks are frequently linked to active regions harboring the H3K4me3 histone modification, enrichment of ATAC-seq peaks in repressed regions bearing the H3K27me3 mark has also been reported [ , , ]. Gene repression may involve TFs acting directly as repressors, or non-DNA binding corepressors or chromatin remodeling complexes that interact with TFs to repress transcription [ , , ]. For instance, the skeletal myogenesis regulator SIX1 [ ], which was highly expressed in our muscle samples and had their binding motifs enriched in DARs-LE, either activates or represses gene transcription depending upon the cofactor(s) it recruits [ ]. Similarly, PPARα, which is known to regulate lipid metabolism in skeletal muscle, and Mef2c, which plays a crucial role in skeletal muscle differentiation [ ], were highly expressed in our muscle samples and can either activate or repress gene transcription depending upon which cofactor (coactivator or corepressor) they interact with [ , ]. Additionally, several TFs that are primarily transcriptional activators (e.g. BATF, FRA-1, FRA-2, and KLF TFs) also display repressor activity [ , ], which may explain, at least in part, the overlap in the motifs enriched in DARs-HE vs. DARs-LE. Mechanistically, a recent report demonstrated that a transcriptional repressor increases accessibility at its binding site and immediately around it, whereas an activator increases accessibility at its binding site and over 100 bp beyond it [ ]. In agreement with this, we found fewer DARs mapping to LE genes than to HE genes. In future work, incorporation of additional datasets such as ChIP-seq of TFs and DNA methylation may allow in vivo identification of binding sites in DARs, providing more insight into the regulatory mechanisms underlying tissue-specific gene transcription. While tissues share the same genome within an organism, they show different gene expression profiles, which are driven by tissue-specific cis-regulatory DNA elements and the binding of acting factors [ ]. By examining the relative RNA expression levels of genes encoding TFs that are known to recognize the enriched -motifs in tissues-specific DARs, we could identify functionally relevant TFs in each tissue. Notably, PU.1, which was highly expressed in adipose, was previously shown to inhibit adipogenesis [ , ]. PU.1 is also known to interact with IRF8 and has been reported as a pioneer factor in hematopoietic lineage differentiation [ ]. Pioneer factors elicit chromatin opening [ ] and are typically involved in cell differentiation and tissue-specific gene activation [ ]. Among the muscle-enriched TFs, KLF5 was previously described as an essential regulator of skeletal muscle differentiation [ ]. HLF expression was previously reported in murine skeletal muscle [ ]. We also compared the TF transcripts that were tissue-enriched in our study with those reported in a mouse TF atlas based on proteome-wide profiling of activated TFs [ ]. Muscle-enriched MEF2c, MEF2d, SIX1, and SMAD3 were among the TFs significantly detected in skeletal muscle in the mouse proteome, with SIX1 being restricted to muscle tissue. Adipose-enriched IRF3, ELF4, CEBPA, TCF21, and NFYA were among those expressed in mouse WAT, with CEBPA and TCF21 being adipose tissue-restricted. ELF4 was previously reported to enhance adipocyte differentiation in bone marrow [ , ], CEBPA is critical for WAT differentiation [ ], TCF21 regulates adipogenesis [ ], and NF-Y is an adipogenic factor [ ]. Additionally, two members of the AP-1/ATF superfamily, Batf and Batf3, were WAT-restricted in our rat samples and in the mouse proteome, respectively, and two cAMP responsive element binding proteins, Creb5 and Creb3, were WAT-restricted in rat and mouse, respectively. In contrast with the mouse proteome data, we found no adipose-restricted members of the Hox family of homeodomain-containing TFs. Interestingly, one of the clusters identified in the mouse proteome data comprised TFs that were highly enriched in WAT and skeletal muscle, and whose main functions were vasculature development, cell adhesion, blood vessel development, muscle organ and skeletal system development. This was consistent with our finding of similar enriched GO terms in adipose HA-HE genes and in muscle HA-HE genes (see B). Overall, the similarities between rat and mouse support the validity of our findings, while inter-species differences may account for differences in TF profiling. Further investigation will be needed to elucidate the mode of action of these TFs in adipose and muscle tissue. Collectively, our data support a combinatorial interplay between promoter and non-promoter cis-regulatory elements, such as promoter-enhancer interactions, promoter-silencer interactions [ ], or other types of TF-mediated long-range interactions, in regulating tissue-specific transcriptional activity in adipose and muscle. Further characterization of adipose- and muscle-specific intergenic/distal cis-regulatory elements could have clinical implications for metabolic disorders such as T2D and obesity, as disease-associated SNPs are often enriched in enhancers and silencers [ ]. All animal procedures in this study were conducted in accordance with the guidelines of University of Florida for the care and use of laboratory animals (IACUC # 201709757). Fischer 344 (F344) rats [ ] were obtained from the National Institute on Aging colony at 8 months of age. They were allowed 2 weeks to acclimate to a reverse light cycle (lights off at 8 am: lights on at 8 pm). For tissue collection, rats were anesthetized via isoflurane, secondary euthanasia was performed via decapitation, and gastrocnemius muscle (skeletal muscle) and abdominal WAT (adipose tissue) were dissected out, quickly frozen in liquid nitrogen, and stored at −80 °C until nuclei extraction. Tissue samples were transferred to 1.1 ml micronic tubes (Micronic, USA) and resuspended in 600 μl of homogenization buffer (320 mM Sucrose, 10 mM Tris-HCl pH 7.8, 100 nM EDTA, 5 mM CaCl2, 3 mM Mg(Ac)2, 167 μM β-mercaptoethanol, 1× protease inhibitor). Three, 2.8 mm ceramic beads (Omni International, Kennesaw, GA, USA) were added to each micronic tube. Tubes were vortexed briefly and kept on ice. The tissue samples were homogenized in a Bead Ruptor Elite bead mill homogenizer with Omni BR Cryo cooling unit (Omni International) using 2 cycles of the following settings: speed: 1.0, time: 20 s and speed: 2.1, time: 20 s, dwell time: 20 s at both speeds, while cooled by liquid nitrogen to 10 °C. Following homogenization, each tissue homogenate was filtered through a mini 20 μm pluriStrainer (pluriSelect, Leipzig, Germany). The number of nuclei in the filtrate was counted using an Automated Cell Counter (Cellometer K2 Image cytometer, Nexcelom) following staining with the Propidium Iodide (PI) dye. Approximately 75,000 nuclei were transferred into a fresh tube and diluted in 1 ml of ATAC-seq resuspension buffer (ATAC-seq RSB (Corces et al., 2017) + 0.1% Tween 20, Sigma, St. Louis, MO, USA). Nuclei were centrifuged at 3000 rpm using an Eppendorff 5418R centrifuge for 10 min at 4 °C, and the supernatant was carefully aspirated. Nuclei were incubated in 50 μl transposition reaction mixture containing 25 μl of 2× TD buffer (Illumina, Inc., San Diego, CA, United States), 16.5 μl of PBS, 0.5 μl of 1% digitonin (Sigma), 0.5 μl of 10% Tween-20, 1.5 μl of Tn5 transposase (Illumina, 0.8 U/μl), and 6 μl of nuclease-free water using a thermomixer (USA Scientific, Ocala, FL, USA) set at 37 °C for 30 min with a speed of 1000 rpm. Transposed DNA was purified using a DNA clean and concentrator kit (Zymo Research, Irvine, CA, USA), according to the manufacturer's protocol. Following PCR amplification, libraries were purified using Agencourt Ampure XP beads (Beckman Coulter, Indianapolis, IN, USA) double-size selection (0.5×:1.3×). Library quality was assessed using the Agilent Bioanalyzer High-Sensitivity DNA kit and the Qubit dsDNA high sensitivity assay kit (Thermo Fisher). All libraries were pooled and sequenced with 100-bp paired-end reads on the Illumina Novaseq 6000 at the New York Genome Center. The ATAC-seq data were processed (trimmed, aligned, filtered, and quality controlled) using the ENCODE ATAC-seq pipeline v1.7.0 ( ). For all samples, read quality was assessed using FastQC [ ]. Trimmomatic [ ] was used to remove adapters and low-quality base pairs and reads identified by FastQC. Reads for each sample were aligned to the rat genome (rn6.0) using Bowtie2 [ ]. After alignment, we removed reads mapping to the mitochondrial genome. As a general measure of sensitivity to Tn5 transposase fragmentation, ATAC-seq signal was defined as number of transposase cuts mapping to each bp (or a total count of cuts mapping to a selected genomic interval). The cuts were defined as 5′ ends of ATAC-seq reads, with additional shifting by +4 bp and − 5 bp for reads mapping to the plus and minus strands, respectively [ ]. Duplications were removed by Picard tools ( ). MACS2 [ ] was applied on each merged bam file to call peaks (with option –nomodel –extsize 200 –shift 100). We used HOMER to annotate ATAC-seq peaks to the various genomic regions (promoter, 5′ UTR, exon, intron, 3′ UTR, downstream, and intergenic), and assigned them to the nearest gene based on rat genome assembly rn6. Notably, promoters were defined as 3 kb upstream or downstream of the TSS. Downstream regions corresponded to the 3 kb DNA region located downstream of the transcription termination site. All sequencing tracks, bigWig files were viewed using the Integrated Genomic Viewer (IGV 2.3.61). Fold-change of ATAC-seq signal was calculated using DiffBind package (v1.2.4) in R [ ]. Interpretation was limited to peaks that exceeded a L2FC of 1 (FDR < 0.01) in either direction. Sequencing data have been deposited to GEO ( ). Skeletal muscle (10 mg) and white adipose (30 mg) samples were homogenized in a Bead Ruptor Elite bead mill homogenizer with Omni BR Cryo cooling unit (Omni International) twice for 20 s at a speed of 4.2 m/s, while cooled by liquid nitrogen to 10 °C. Tissue homogenates were processed for total RNA extraction using the Agencourt RNAdvance Tissue Kit (Beckman Coulter) on a BioMek FX Laboratory Automation Workstation (Beckman Coulter). Concentration and integrity (RIN) of isolated RNA were determined using Quant-iT™ RiboGreen™ RNA Assay Kit (Thermo Fisher) and an RNA Standard Sensitivity Kit (DNF-471, Agilent Technologies, Santa Clara, CA, USA) on a Fragment Analyzer Automated CE system (Agilent Technologies), respectively. Subsequently, cDNA libraries were constructed from 300 ng of total RNA using the Universal Plus mRNA-Seq kit (Tecan Genomics, San Carlos, CA, United States) in a Biomek i7 Automated Workstation (Beckman Coulter). Library concentration was assessed fluorometrically using the Qubit dsDNA HS Kit (Thermo Fisher), and quality was assessed with the Genomic DNA 50Kb Analysis Kit (DNF-467, Agilent Technologies). Preliminary sequencing of cDNA libraries (average read depth of 90,000 reads) was performed using a MiSeq system (Illumina) to confirm library quality and concentration. Deep sequencing was subsequently performed using an S2 flow cell in a NovaSeq sequencing system (Illumina) (average read depth ~30 million pairs of 2 × 50 bp reads) at New York Genome Center. Raw data were processed using bcl2fastq Conversion Software (Illumina) to obtain FASTQ files, and the FASTQ files were aligned to the rn6 genome using STAR [ ]. Picard tools ( ) and other custom scripts were used to assess the quality of RNA-seq data. Gene expression was quantified with featureCounts [ ]. The computation details and the custom scripts have been wrapped into a snakemake pipeline [ ] at . The raw FASTQ data and the final gene expression matrix have been deposited to GEO ( ). After filtering for low expression, RNA-Seq data were normalized using a voom correction and compared within the ten adipose vs. gastrocnemius tissues. Differential expression analysis was performed using Bioconductor [ ] package limma [ ] under R version 3.4.3 [ ]. Benjamini-Hochberg correction [ ] for multiple comparisons testing was applied, setting the false discovery rate (FDR) < 0.05. Using R package BioMart [ ] transcript Ensembl IDs were converted to gene names, when available, in order to allow for further annotation. Interpretation was limited to genes that exceeded a L2FC cutoff of 1 in either direction. Total RNA was extracted from each tissue as described in and reverse-transcribed using Superscript IV Reverse Transcriptase (ThermoFisher, Waltham, MA). SYBR Green qPCR assays were performed (40 cycles) in an Applied Biosystems 7900HT thermal cycler (Foster City, CA) as previously described [ ]. All biological samples were measured in triplicates, and comparative threshold (Ct) values, and Ct values of target genes were normalized to those of β-actin in subsequent analysis. Data were expressed as arbitrary units by using the formula, E = 2500 × 1.93 (β-actin Ct – target gene Ct), where E is the expression level in arbitrary units. Primer sequences are provided in Supplementary Table S3. H3K27ac inputs ChIP-seq fastq and bam files in rat skeletal muscle control samples were downloaded from the European Nucleotide Archive (accession number PRJEB41483) [ ]. Bam files were used to generate tag directories, which were employed to find and annotate peaks using HOMER by mapping to the rat rn6 genome assembly. Bam files were also converted to bedGraph files using BEDTools [ ], then bedGraph files were converted into bigWig files using bedGraphToBigWig (ENCODE). Heatmaps, average profiles, and overlap with ATAC-seq peaks were generated using the Easeq software [ ], and data were visualized using the bigWig files and the IGV software [ ]. Pearson correlation scores were calculated between the normalized RNA-seq read count for a given gene and the cumulative ATAC-seq signal from a specific genomic region (promoter, intron, exon, etc.) of that gene, and plotted using GraphPad Prism 5. We further ranked genes by their genomic region-specific ATAC-seq signal and by their normalized read count to learn which genomic regions showed the best correlation of large ATAC-seq signal with large gene expression. Comparing the difference in promoter-region ATAC-seq and RNA-seq ranks in muscle and adipose allowed for the identification of genes with discordant patterns of ATAC-seq and RNA-seq signal in both tissues. We refer to these populations as LA-HE (Low Accessibility-High Expression) genes and HA-LE (High Accessibility-Low Expression) genes. Comparing differential gene expression with differential chromatin accessibility allowed for stronger correlation between the ATAC-seq and RNA-seq signals. DARs were then labeled as DARs-HE if their corresponding genes had a L2FC RNA expression level greater than 1 in the same tissue, or they were labeled as DARs-LE if their corresponding genes had a L2FC RNA expression level less than −1 in the same tissue. If neither criterion was satisfied, the DARs were listed as DARs-non-DE genes. Gene ontology analysis was performed using Metascape [ ] on selected genes that contained peaks satisfying the criteria for HA-HE or HA-LE in either adipose or muscle tissue. The top seven pathway annotations were included for each set of genes. We conducted motif enrichment analysis on sets of peaks that satisfied the HA-HE and HA-LE criteria for adipose or muscle. Peak sets were further subdivided based on genomic region into promoter, intron, and intergenic sets. The analysis was carried out by findMotifsGenome.pl (HOMER v4.11.1). It was performed on the ±75 bp flanking regions of the peak summits. The search lengths of the motifs were 8, 10, and 12 bp. values for each TF in the HOMER rat database were calculated by comparing the enrichments within the target regions and those of a random set of regions (background) generated by HOMER. We also applied the -find flag to determine the presence of all known rat motifs in each DAR in the dataset generating a database of motifs attached to each DAR. With this information, we analyzed all target DARs for a given motif and compared their differential chromatin accessibility with their corresponding differential gene expression. All the raw and processed sequencing data generated in this study can be found in The National Center for Biotechnology Information (NCBI) under the accession numbers and .\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Not Used\n",
      "* **AIモデルの判断:** Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 169 / DOI: 10.1016/j.envres.2022.112901  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Viral metagenomes of Lake Soyang, the largest freshwater lake in South Korea\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Viral diversity and biogeochemical potential revealed in different prawn-culture sediments by virus-enriched metagenome analysis\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "Viruses were estimated population size of 10 viral particles, which are the most biological entities and virtually present in various ecosystems on the earth ( ; ; ). Viruses played vital roles in many microbial communities by functioning as vectors of horizontal gene transfer ( ), encoding auxiliary metabolic functions of benefit to their host ( ), and promoting dynamic co-evolutionary interactions ( ). By lysing their hosts, viruses released amounts of metabolic substance and assist microbes in driving biogeochemical cycles ( ; ), they also controlled host abundance and affect the structure of host communities. Importantly, auxiliary metabolic genes (AMGs) in viruses are presumed to augment viral-infected host metabolism and facilitate production of new viruses ( ; ). The roles of viruses in different ecosystems are increasingly being uncovered. To date, studies from a variety of environments have suggested that prokaryotic viruses act as key agents in natural ecosystems through a range of interactions with their microbial hosts, such as thawing permafrost ( ), mangroves soils ( ), rhizosphere soils ( ), freshwater ( ; ), the terrestrial subsurface ( , ), deep sea sediments ( ) and seawater ( ; ; ). Viral AMGs have also been proved to be involved in the biogeochemical cycle. AMGs of marine cyanophages involved in photosynthesis, carbon turnover, phosphate uptake and stress response ( ; ; ; ). Emerson et al. found that virus-encoded glycoside hydrolases played a role in complex carbon degradation in peatland soils of permafrost thaw gradient ( ). In freshwater lakes fed with sediment derived methane, the virus-encoded particulate methane monooxygenase might augment bacterial aerobic methane oxidation during infection ( ). Li et al. revealed that the virus-encoded AMGs, including genes related to carbon, sulfur, and nitrogen metabolism, might augment the metabolism and potentially altered biogeochemical processes mediated by cold seep microorganisms ( ). Given that virus infect and lyse microbial host are dynamic processes in environment, and microbes drive global biogeochemical cycles ( ), viral-encoded AMGs must play important roles in global biogeochemistry and microbial metabolic evolution. Aquaculture substantially contributes to global food and nutrition supply, which have become an indispensable food source for population growth and was rendered as one of the most rapidly growing food production sectors ( ; ). Aquaculture has grown rapidly during the last few decades due to research and developmental activities aiming at various aspects of aquaculture ( ; ). The use of microorganisms in aquaculture as environmental biomarkers, bioremediators, probiotics, and as a direct food source for the cultured species has expanded further in the last few decades ( ; ). Sediment is considered as an indispensable component of aquatic ecosystems ( ), which is an important location for matter exchange and nutrient cycling ( ). Sediment microbial communities involved in various of biogeochemical cycles are known to respond to environmental changes, which means that they can be a potential indicator for overall condition of aquatic ecosystems, particular in aquaculture ecosystems ( ). Indeed, sediment has multiple functions in controlling the basic biological and abiotic quality in pond bottoms, and has shown to be one of the most deterministic factors in the success of aquaculture activities ( ). The pond sediment quality parameters are significantly altered by different aquaculture patterns ( ). Moreover, sediment microbial community is closely related to the substantial shifts in environmental quality and host microbial composition ( ). These changes probably contribute to disease occurrence in shrimp and fish ( ; ). In addition, compared to water microbiota, the relationship between sediment and intestine was a little closer than that between water and intestine, and a large proportion of bacteria were shared between sediment and intestine ( ). That is to say, sediment is widely considered to be an important component of farming productivity. Therefore, appropriate pond sediment management is a breakthrough technology that promotes pond farming ( ). Viruses, as a vital component of microbial community, are being discovered in different aquatic environment (including sediment), and their roles are being revealed ( ). However, knowledge about the sediment viruses among different aquaculture patterns remains highly restricted, which limits the development of sediment management. As one of the largest aquaculture countries in the world, and mariculture is an important part of the aquaculture industry in China. The prawn is an important mariculture species along coastlines ecosystems due to its high nutritional value and significant economic benefits ( ). In recent years, prawn poly-culture with other species has become a popular mode of mariculture in China because of its environmental friendliness, economic benefits, and circular utilization of nutrition ( ). Zeng et al. revealed the distinctive of sediment microbiota between polyculture and monoculture patterns, which enlarged our understanding for the underlying mechanism of advances in the polyculture of shrimp and fish pattern from ecological perspective ( ). However, to the best of our knowledge, little information on the ecological roles of viruses in mariculture ecosystem was available, of which further investigation is required. Herein, for that many substances, including bacteria and viruses, have a tendency to concentrate in sediments ( ), viral diversity and their possible roles in sediments from four different prawn-mariculture ponds (monoculture of prawn and poly-culture of prawn with jellyfish, sea cucumber, and clam) were assessed by using a metagenomic approach with prior virus-like particles (VLPs) separation. In brief, a total of 1104 viral operational taxonomic units (vOTUs) were recovered following by the prediction of their hosts and identification the AMGs in them. Our findings can reveal the viral diversity and their roles in microbial ecology and biogeochemistry of prawn mariculture systems. These findings provided a fundamental understanding of the dissimilarities of sediment microbiome among mariculture patterns, and deepened insight to the viruses in mariculture sediments, which could help to improve appropriate farming management for sustainable mariculture. Sediments were collected from ponds with four different prawn-mariculture systems closed to Liaodong Bay, China. They are monoculture of prawn (P), polyculture of prawn and jellyfish (P.Jf), polyculture of prawn and sea cucumber (P.Sc), and polyculture of prawn, jellyfish and clam (P.Jf.C). Three sediment samples were collected from each pond and mixed as a representative of the sediment in that pond. Three mariculture ponds were selected from each prawn-mariculture system for sampling. Surface sediment samples from mariculture ponds were taken by a sediment collection equipment and shipped in a 50-mL polyethylene tube. There 12 sediment samples were obtained and stored in a portable ice box and transported to the laboratory within 24 h. Samples were sub-packed into 2 mL polyethylene tubes with 0.4 g samples each, and stored at −80 °C until DNA extraction. Viruses were purified from sediments by approaches reported by Calero-Caceres et al. with slight modifications ( ). Briefly, the equal mass samples from same pond sediments were mixed firstly. Then the mixed samples were mingled in a 1:10 (w/v) ratio with phosphate-buffered saline (PBS, pH = 7.4) and homogenized by magnetic stirring for 30 min at room temperature. The suspensions were centrifuged at 10,000 g for 30 min at 4 °C. The supernatants were subsequently filtered through 0.22 μm filters. Then, VLPs in the filtrate were enriched using 100 kDa centrifugal ultrafiltration tubes by centrifugation at 3,000 g following the manufacturer's instructions. The VLPs were recovered and treated with chloroform to rule out the presence of possible vesicles containing DNA. DNase treatment (100 units/mL of the phage DNA) was performed to eliminate any free DNA that might be present in the samples outside the VLPs. After that, DNase was heat inactivated at 80 °C for 10 min. Finally, the extracted VLP DNA was purified by using DNA Clean and Concentrator-column clean-up kit. Meanwhile, the total DNA of each sediment sample was extracted using the FastDNA Spin Kit for Soil following the manufacturer's instructions. The quality and concentration of all DNA extracts were evaluated by agarose gel electrophoresis and spectrophotometer analysis. All DNA were stored at −20 °C for further applications ( ). All extracted DNA were sequenced with an Illumina Hiseq2500 platform by applying the 150 bp paired-end strategy. Approximate sequencing depths of 12 Gbp per metagenome (12 total DNA metagenomes in total), each virome (4 VLP DNA viromes in total) was sequenced to a depth of approximately 6 Gbp. The quality of raw sequences was assessed using FastaQC v0.11.5. Low quality reads phred scores <30, containing ambiguous bases, and sequence lengths <150 bp) and adaptors were removed using the NGS QC Toolkit to generate clean data ( ). All sediment samples were individually assembled using metaSPAdes 3.14.1 ( ). Contigs less than 1 kb in length were discarded, and putative viral contigs were recovered from metagenome assemblies using VirSorter (’-db 1’ parameter) ( ) and VirFinder v1.1 ( ). Contigs ≥ 5 kb and circular contigs (≥1 kb) that were sorted as VirSorter categories 1–6 and/or VirFinder score ≥0.7 and p < 0.05 were retained. Of these, contigs following the criteria were then identified as viral sequence [1]: (1) VirSorter categories 1, 2, 4 and 5; (2) VirFinder score ≥0.9 and p < 0.05; (3) both identified by VirSorter categories 1–6 and VirFinder score ≥0.7 and p < 0.05. The remaining contigs were run through CAT ( ) and those contigs with <40% (based on an average gene size of 1000 bp) of the genome classified as bacteria, archaea, or eukaryota were considered viral sequences. Total viral contigs (≥5 kb) were removed duplicates with CD-HIT v4.6 (parameters: c 0.95 -aS 0.85 -d 400 -T 20 -M 20,000 -n 5) ( ; ), and a final set of representative viruses as the virus operational taxonomic units (vOTUs) were obtained in all VLPs sediment samples, of these mainly are free viruses. The open reading frames (ORFs) of non-redundant contigs were predicted by Prodigal v2.6.2 ( ) with “-p meta”. Quality and completeness of viral genomes was estimated using the CheckV v0.8.1 pipeline ( ). Metagenomic clean data were assembled by metaSPAdes 3.14.1 ( ) individually. Genome reconstruction of metagenomic sequencing data was performed with the function modules of metaWRAP v1.3.2 ( ), which is a pipeline with numerous modules for analyzing metagenomic bins (or metagenome-assembled genomes, MAGs) and used for contigs binning with the binning module (parameters: -maxbin2 --metabat1 --metabat2). The default of the minimum length of contigs used for constructing bins with different binning modules respectively. Refinement of MAGs was performed by the bin_refinement module (parameters: c 50 -x 10) of metaWRAP v1.3.2 ( ). CheckM v1.0.12 ( ) was used to estimate the completeness and contamination of the bins to choose the best one of each MAG with the highest scoring value and then consolidated into final bins for each sample. dRep v3.2.2 ( ) with parameters (-sa 0.95 -comp 50 -con 10) was used for dereplication of these produced bin sets. MAGs were clustered into species-level genome bins at the threshold of 95% ANI using the ‘cluster’ program in dRep v3.2.2. A total of 38 representative MAGs were generated and then were classified using the Genome Taxonomy Database Toolkit (GTDB-tk v1.5.0) ( ) based on the Genome Taxonomy Database (GTDB, ) taxonomy release202 (GTDB-Tk reference data version r202) (the 38 MAGs and their taxonomy are in supplementary materials). Finally, the 38 MAGs and the unbinning contigs were all used to establish virus-host linkages. The unbinning metagenomic contigs were assigned taxonomy by the contig annotation tool CAT ( ). Family level taxonomic annotations were assigned to the identified viral contigs using Demovir script ( ) with default parameters and database ( ). This script performs search for amino acid sequence homologies between proteins encoded by contigs in question to a viral subset of TrEMBL database, then uses voting approach to decide the taxonomic assignment. To calculate relative abundances of vOTUs in each sample, clean reads of sediments were first non-deterministically mapped to the 1104 vOTUs using bowtie2 ( ). BamM v1.7.3 ‘filter’ ( ) with parameters (--percentage_id 0.95 --percentage_aln 0.75) was used to remove low-quality mappings. Filtered bam files were then passed to Bedtools genomecov ( ) to generate coverage profiles. BamM v1.7.3 ‘parse’ was used to calculate the average read depth (‘tpmean’ -minus the top and bottom 10% depths) across each vOTU. The average per base-pair coverage for each vOTU in each sample was retained. The large terminase subunit gene , as the signature genes of the genomes (or tailed dsDNA phages), was used to construct phylogenetic tree to assess the diversity of mariculture sediment viruses ( ; ). Pfam domain of TerL protein of viral contigs were searched against the Pfam 33.1 database ( ) using the hmmsearch program in the HMMER3 ( ) package (E-value cutoff 10 ) using the method described by . The TerL proteins of viral reference were download form NCBI Viruse database ( ) in February 2021. Protein sequences with TerL domains were aligned by MUSCLE v3.8.31 ( ) and trimmed gaps using trimAl v.1.4 (-gappyout) ( ). Phylogenetic reconstruction was performed using the IQ-TREE v1.6.12 ( ) program, with the evolutionary models selected by IQ-TREE (-m LG + R4). The tree was manually edited by iTOL ( ) finally. Where possible, hosts for mariculture sediment vOTUs were predicted using a variety of bioinformatic methods as previous work report ( ; ). Broadly, these methods ( ; ; ) were based on: (i) host CRISPR-spacers match, (ii) host transfer RNA (tRNA) genes, (iii) host tetranucleotide frequency, and (iv) nucleotide sequence homology. Two datasets were used to establish virus-host linkages, one is the RefSeq genomes of bacteria and archaea downloaded from NCBI database ( ) in December 2020, the other one is the assembled genomes or genomic fragments (i.e. MAGs and unbinning contigs) of the bulk metagenomes from the four mariculture ponds. CRISPR-spacers were predicted using CRISPR recognition tool (CRT) ( ) with default parameters. BLASTn was used to compare spacer sequences to the viral contigs with matches retained if they contained ≤1 mismatch over the complete length of the spacer and had an E-value of ≤10 . tRNA-scan ( ) was used to identify the tRNA genes from sequences. Virus-host pairs were required ≥90% length identity in ≥90% of the sequence by BLASTn ( ). Tetranucleotide frequency patterns were assessed independently for viruses and hosts, and for each virus, the host genome with the most similar tetranucleotide frequency patterns was identified as a putative host. VirHostMatcher v1.0 ( ) was run to predict virus-host pairs with default parameters based on shared oligonucleotide frequency pattern, and the d2* values < 0.2 being considered as a viral host match. Finally, BLASTn was used to link viruses to hosts with a cutoff of ≥75% coverage over the length of the viral contig, ≥ 70% minimum nucleotide identity, ≥ 50 bit score, and E-value threshold of 10 . To identify a single virus-host linkage for each vOTU, hosts were predicted using the ranked criteria which reported previously ( ): first, CRISPR-spacer match; second, nucleotide sequence homology or tRNA match; third, tetranucleotide frequency comparison. The host predicted by the highest ranking criterion was chosen. If multiple hosts for a vOTU were predicted by multiple approaches, the virus-linkages were retained. To place 1104 mariculture sediment vOTUs in broader context of known viruses, predicted proteins were clustered with predicted proteins from viral sequences in public database. Specifically, this study compared the viromes of the samples to the 9556 viral sequences of freshwater sediment and the 1290 viral sequences of marine sediment downloaded from the IMG/VR v3 database ( ) (Integrated Microbial Genomes with Virus, downloaded in December 2020). For each viral contig, ORFs were called using Prodigal and the predicted protein sequences were then used with Diamond ( ) for the protein-protein similarity search with an E-value threshold of 10 , following the protocol published in protocols.io ( ) for the application of vConTACT2 ( ). Viral RefSeq (v99) including 3465 bacterial and archaeal viral genomes was selected as the reference database, and Markov clustering algorithm (MCL) with an inflation of 2 was used to cluster genomes and/or contigs into viral clusters (VCs). Other parameters were set as default. The resulting network was visualized with Cytoscape v3.7.2 software ( ), using an edge-weighted spring embedded model, which places the genomes and/or contigs that share more protein clusters in closer proximity in the display ( ). Prior to identification of viral AMGs, CheckV was used to identify and remove host contamination based on gene content for individual sequences. We used DRAM-v (the viral mode of DRAM v.1.2.0) ( ) to annotate mariculture sediment vOTUs and recover their putative AMGs. Due to DRAM-v requires VirSorter2 output, firstly, 642 viral contigs with high viral scores (>0.5) were selected throuth VirSorter v2.0 (--prep-for-dramv), meanwhile the affi-contigs.tab file was produced as input for DRAM-v annotation. Next, the selected viral contigs were run through DRAM-v with default parameters, from the DRAM-v output, putative AMGs with auxiliary scores <4 (a low auxiliary score indicates a gene that is viral gene with confidence) and gene descriptions were retained. For comparison, we also used the VIBRANT ( ) with default parameters to perform the annotations on viral contigs. VIBRANT can predict AMGs from genomes based on KEGG, Pfam and VOG. For VIBRANT output, KEGG annotations categorized into “energy metabolism” were reported as potential AMGs ( ). Following the method reported by , we performed manual curation of the AMG annotation outputs of VIBRANT and DRAM-v to avoid false-positive results for improving the confidence of identificated AMGs based on their genomic contexts. As previously reported ( ; ), to ensure the reliability of viral AMGs, we did not consider the genes related to ribosomal proteins, nucleotide metabolism, organic nitrogen and glycosyltransferases. Furthermore, if a putative AMG was located between two viral hallmark genes or virus-like genes, or next to viral hallmark gene or virus-like gene, the gene was considered to be a high-confidence viral AMGs for the subsequent analysis ( ). Of these AMGs, conserved domains of viral AMGs were identified using NCBI CD-search tool ( ), the Phyre2 web portal were used to search the protein structural homology ( ). The annotations for each contig containing AMGs were visualized based on DRAM-v, VIBRANT and VirSorter2. To investigate the viral community structure and reveal the genetic and functional diversity of viruses in different prawn mariculture systems, we collected the sediments from four modes of mariculture ponds and extracted the encapsidated viral DNA with prior VLPs purified. Then the VLPs sequenced and yielded a range of 41–70 million clean reads ( ). Finally, this gave rise to 1176 viral contigs belonging 1104 vOTUs (i.e. the non-redundant virus sequences), each represented by contigs ≥ 5 kb in size and all distributed in different mariculture sediments ( ). With regard to the representive viral contigs (namely vOTUs), there 295 contigs were ≥10 kb, 2 viral contigs with length >100 kb, and the longest was 152,645 bp (see ). Among the mariculturial sediment viruses, one high-quatily complete genome AAI-based with 6286 bp length and one low-quality complete genome HMM-based with 13,682 bp were identified by CheckV ( ), details were in . Meanwhile, 11 viral contigs were predicted to be potential proviruses based on CheckV and the longest and shortest ones were ∼20 kb and ∼5 kb in length, respectively ( ). Taxonomically, 49.46% of 1104 vOTUs can be classified and most of them are assigned to the order (i.e. infecting bacteria and archaea) ( A, ). At the family level, was the most numerous, accounting for 12.41% (137) of all vOTUs, followed by with 9.87% (109) and with 7.16% (79). Other viral families identified at relatively high numerous included (8.24%, 91), (1.09%, 12) and (1.00%, 11). In addition, the single-stranded DNA (ssDNA) virus with assigned to family (1 virus), the double-stranded (ds) DNA virus of the family (1 virus, commonly known as virophages) and the large dsDNA viruses of the family (2 viruses) were also detected in these sediment samples. To deepen our understanding of the differences in viral population composition, we compared the abundance of viromes in different sediments at the family level ( B and ). There are variations in the viral community composition of sediments from different prawn mariculture ponds were observed. The relative abundance of major families varied among mariculture ponds. In the mono-culture system of prawn, was the dominant family, accounting for approx 20% relative abundance of the total, followed by , and . Compared to the poly-culture systems, could be detected but was not found in poly-culture systems. In contrast, viruses of and could be detected in poly-culture ponds except for mono-culture ponds. In the poly-species mariculture systems, viruses of family occupied the largest abundance in P.Sc ponds and P.Jf.C ponds, there were 30.05% and 17.16% respectively, followed by (13.12%) in P.Sc and (11.00%) in P.Jf.C. While, viruses of family occupied the largest abundance in P.Jf ponds (19.85%), followed by (7.62%) and (5.97%). In addition, the relative abundance of viruses only accounted for 3.54% in P.Jf ponds, the proportion is much lower than in P.Sc ponds and P.Jf.C ponds, and slightly lower than in mono-culture ponds (8.25% of prawn mono-cultural ponds). That is to say, when other species was added to prawn mono-cultural pond, virus families and their abundance would make changes. With the addition of sea cucumber, the relative abundance of increased greatly, from 8.25% to 30.05% and 4.90%–13.12%, while the abundance of decreased to a large extent (19.18%–7.55%). The same trend was also observed when clam were added to the P.Jf ponds, but the magnitude of the changes were not as dramatic. To sum up, there are variations of the viral compositions and abundance of mariculture sediments with the culture species adding to different mariculture systems. As were the most abundant viruses in mariculture pond samples and the gene is conserved of , phylogenetic analysis of TerL protein was adopted to assess the diversity and genetic distance of mariculture sediment viromes. A total of 66 unique vOTUs with TerL were used to construct phylogenetic tree. As shown in , mariculture sediment viruses were widely distributed throughout on the tree, with most of the sequences affiliated with , followed by and . Although are the well-studied group of viruses to date, there are still some phylogenetically distancing to the known reference sequence, and these sequences formed three groups within the . These sequences of clades illustrated previously uncharacterized diversity and could be viewed as novel branched of . The phylogenetically distance of viruses in one clade illustrated similarly evolutionary diversity for in four mariculture sediments. In addition, there were two clades, one clade including the NC 030931.1 reference viruses, and another one including NC 029119.1 reference viruses, with close phylogenetical distance from the viruses of mariculture sediments provided clues to determine the hosts of these viruses and the same clues are also applicable of the other unclassified . To investigate the relationship between mariculture sediment viruses and publicly available viruses from a broader diversity of ecosystems, viral cluster (VC) based on protein-ecoding gene-sharing networks, approximately the genus level, is the lower taxonomic resolution (i.e. a higher taxonomic rank) compared to the vOTU level, which can overcome the virome individuality and allow cohort comparisons ( ). VCs achieved by vConTACT2 ( ) were used to reveal shared vOTUs in the viromes across sediment ecosystems in this study. Viruses of mariculture sediment, marine sediment, freshwater sediment and viral RefSeq were grouped into 2276 VCs ( A and ). Only 16 VCs were shared amongst all sediment ecosystems (shown in B). The limited extent of clustering between viruses sampled from various ecosystems may reflect a high degree of habitat specificity. Among mariculture sediment viruses, 333 out of 1104 vOTUs were clustered into 177 VCs, with the majority (54.80%) not encountered in any other ecosystem. This suggests that most mariculture sediment viruses may be endemic to these four mariculture systems. Only 67 vOTUs clustered with freshwater sediment viruses, 30 vOTUs with marine sediment viruses, and very few (only 4 vOTUs) clustered with taxonomically known genomes from Viral RefSeq. The proportion is much lower in comparison with recent estimates in soil viruses and cold seep viruses using a similar approach ( ; ). The details can be viewed in and . In addition, VCs of four viromes were pairwise compared, resulting in that VCs (23 of 100) of P.Jf sediment and VCs (30 of 53) of P.Sc sediment have more similar VCs with P.Jf.C sediment compared to the P sediment ( ). The increase of mariculture species encourage the formation of more viral clusters and more complex viral ecosystems. However, pairwise comparisons of viromes between prawn monoculture and biculture systems have only a small number of common VCs. We next investigated whether mariculture sediment viruses, like some of their sediment counterparts, might affect ecosystem function by infecting microorganisms that drive biogeochemical cycles. By comparing sequence similarity, tenucleotide frequencies, tRNA sequences and CRISPR spacers, 371 viruses of the 1104 vOTUs were able to assign the putative host ( and ). Prediction of virus host, the use of the bulk metagenomes in the same mariculture pond could improve the rate of viral host prediction compared to the reference genomes of the bacteria and archaea only. Consistent with previous observations ( ; ; ), most of these viruses were predicted to have narrow host ranges, with 79 vOTUs potentially exhibiting a broader host range across several phyla or domains. 14 vOTUs were linked to both bacterial and archaeal hosts, 7 vOTUs were linked to both procaryotic and eukaryotic hosts, suggesting existence of viral infection across domains. Mariculture sediment viruses were linked to hosts from 25 bacterial and archaeal phyla ( ) and the most frequent predictions were (18.39% of virus-host pairs), followed by (1.99%), (1.81%) and (1.72%). A proportion (1.18%) of mariculture viruses were linked to archaea and most of them are , including members of , , , and . Moreover, some viruses are capable of infecting eukaryotes (3.71%) and 21 vOTUs of them were linked to host from 9 eukaryotic phyla, including 9 viruses (42.86%) of . Numerous viruses in marine and soil encoding AMGs that supplement host metabolism, thus affecting the biogeochemistry and facilitating host adaptation to the environment. We assessed biogeochemical influences of the viral AMGs of mariculture sediment viruses to deepen our understanding of the niche specialization and ecological functions of mariculture sediment viruses. Overall, there were 58 putative AMGs were predicted by DRAM-v ( ) and 99 putative AMGs by VIBRANT ( ) prior to manual curation. Based on VIRBANT annotations ( and ), the viral AMGs of cofactor/vitamin, amino acid metabolism and carbohydrate metabolism tended to be encoded in maricultrue sediment viruses as previously reported ( ; ), and genes related to photosynthesis, carbohydrate metabolism and sulfur cycling were also be identified in these viruses. In addition, viral AMGs for energy metabolism were also encoded with a significant portion, those served as highly credible candidates of viral AMGs. DRAM-v placed most of the predicted AMGs in the carbon metabolism, followed by nucleotide metabolism and the “Organic Nitrogen” class. The carbon metabolism of AMGs spanned 11 CAZy families in 18 vOTUs ( ), including glycoside hydrolases (GH113 and GH117), glycosyl transferases (GT10, GT2, GT25, GT27, GT32, GT4, GT60 and GT7), carbohydrate-binding modules (CBM47). The nucleotide metabolism genes mainly involved in viral replication, which were likely to play a role in viral replication. Referring to the methods and criteria provided by , we manually scanned annotations of DRAM-v and DRAM-v software to estimate whether the predicted AMGs were likely to be in bona fide viral regions of contigs. There were 12 viral AMGs with high confidence and these were involved in a variety of metabolic pathways, such as carbohydrate metabolism, sulfur metabolism, nitrogen metabolism, methane metabolism and photosynthesis ( ). Moreover, these 12 AMGs contained the conserved functional domains and they also had strongly supported (the lowest confidence was 97.8%) structure predictions ( ). Among them, there 5 high-confidence viral AMGs were involved in sulfur metabolism and 4 were involved in carbohydrate metabolism, and viral AMGs with high-confidence involved in nitrogen metabolism, methane metabolism and photosynthesis were also identified in mariculture sediment viruses ( , ). Taxonomic diversity analysis revealed that were the major viral group among the classified virus, and the accounted for the largest proportion. A global survey of ocean virus genomes indicated that the is third largest family among the known viruses ( ). Nucleocytoplasmic large DNA viruses (NCLDVs) (e.g. , , and ) were also identified in these mariculture sediments. Most of them accounted for a small proportion due to the size-fractionated method ( ). However, the relative abundances of , and showed differences in the four mariculture ponds, especially ( B). Besides, NCLDVs were also explored in soil which showed high levels of representation ( ; ), and the phylogenetic diversity of giant viruses were recently identified across Earth's ecosystems using cultivation-independent metagenomics ( ). More recently, many novel viruses have been uncovered on different ecosystems, such as sediment ( ), ocean ( ), freshwater ( ) and soil ( ; ), providing new fundamental insights into the viral diversity and functions present in the environmental ecosystems. Phylogenetic analysis was used to identify several novel clades of from soils ( ; ). Although have been studied to date, the vast majority of them are still unknown. Herein, phylogenetic tree and gene-sharing network demonstrated that in mariculture sediment are relatively uncharacterized and understudied. The three clades of , as potential new viruses in phylogeny, were also identified by using the gene. Based on gene-sharing network, the novelty was also confirmed at the approximate genera level. In addition, most of viruses identified in mariculture sediments remain unknown, and these viruses also represent a large proportion of abundance in different mariculture systems. That is to say, limitation of viruses in current database is the major obstacle to study viral ecology, and the large component of uncharacterized viruses typically labeled as ‘viral dark matter’ need to be further explored. Previous studies have revealed that viruses in marine sediments and in freshwater sediments have their own individual characteristics ( ; ), However, little research has been done on viruses of aquatic cultural sediments ( ; ), especially in mariculture sediments ( ). Consistently, we compared the viruses in this study with publicly available viruses (viruses from marine sediment and freshwater sediment respectively) based on gene-sharing network. Notably, unlike other sediment systems, culture species have remarkably high density in their culture environment compared to their natural state, which have significant impacts on their aquatic and sediment environments. Therefore, mariculture sediment viromes are possibly multiple-shaped or affected by different species. Interestingly, compared to marine sediment virus, mariculture sediment viruses have more common viral clusters with freshwater sediment viromes. The possible reason is that the mariculture area is near the coast and will be affected greatly by people's activities, like freshwater area, which indicated that people's activities have an essential effect on the viral communities in mariculture sediments. Deeper inspection of viral host revealed notable mariculture sediment signatures. Phages known to infect typical marine bacteria were widely present in the mariculture sediment samples. For example, phages infecting bacteria and were identified in mariculture sediments. Previous studies have reported that was a lethal pathogen of the freshwater culture crustaceans (crab, crayfish and shrimp) in China ( ; ). The phage identified from mariculture sediment was of great significance in the prevention and control of pathogenic bacteria in mariculture ( ). Species of could be found in freshwater or marine environments and have a wide range of metabolic capabilities ( ). The phages infecting are more likely to have metabolic capacities through horizontal gene transfer ( ; ), and participate in promoting the geobiochemical circulation. Thus far, viral AMGs have been proved to be involved in the biogeochemical cycle and have increasingly been identified from many environments, most of which are mainly involved in carbon metabolism to facilitate viral replication ( ). For example, viruses infecting C-cycling-related microorganisms like methanogens and methanotrophs have been identified in a thawing permafrost peatland soil ( ; ). Based on DRAM-v annotations, carbon metabolism of viruses were consistent with known AMGs ( ; ). As the degradation of polysaccharide is a complex process, abundant carbon processing related AMGs in numerous environments may boost host metabolism and promote viral propagation ( ). The nucleotide metabolism genes can participate in viral replication, which were consistent with viral replication needs. For example, the thymidylate synthase of four vOTUs can catalyse the conversion of deoxyuridine monophosphate (dUMP) to deoxythymidine monophosphate (dTMP) ( ). dUTPase can produce the dTTP precursor dUMP from dUTP ( ), these genes were in 19 viral genomes. These enzymes may facilitate the use of the cellular pool of nucleotides for the synthesis of viral DNA. In the “Organic Nitrogen” category of DRAM-v, the methyltransferases (n = 3), have been described previously in viral genomes ( ), presumably protects a viral genome from host cleavage by methylating adenine or cytosine bases in the viral genome ( ). The general category of “methyltransferase” was the single common annotation in hypersaline lake viromes ( ). The viral AMGs with central carbon metabolism and hydrocarbon degradation were identified and increasingly recovered as putative viral AMGs, which may contribute to nucleotide and energy production during infection ( ; ). Based on VIBRANT ( ) annotations, the AMGs had capacities for glucose, phosphogluconate, mannose and possibly cellulose cleavage. The assimilatory sulfate reduction genes (two genes) were also identified in mariculture sediment viruses, and found in viral sequences obtained from oxygen-deficient water columns ( ), rumen ( ), a deep freshwater lake ( ) and sulfidic mine tailings ( ). Other related enzymes in the assimilatory sulfate reduction pathway including adenylylsulfate kinase (cysC), cysteine desulfurase (iscS) and cysteine synthase (cysK) were also identified. These genes likely facilitate host utilization of reduced sulfur compounds during infection, providing viruses with some fitness advantage. After manual inspection, the high-confidence viral AMGs were selected (see ). There were five viral AMGs from DRAM-v prediction and almost all of them involved in carbon metabolism, with the exception of the gene related to organic nitrogen metabolism. Jin et al. reported that various auxiliary genes encoding carbohydrate-active enzymes in viruses might assist their host in polysaccharide degradation in mangrove sediments ( ). The other seven viral AMGs with high confidence were selected from the prediction of VIBRANT. Among them, five viral AMGs involved in sulfur metabolism (nrnA, fccA, cysC and two cysH), these enzymes can participate in assimilatory sulfate reduction ( ). The gene contain the ATP binding domain of carbamoyl-phosphate synthase L chain that assimilates the ATP-dependent synthesis of carbamyl-phosphate from glutamine or ammonia and bicarbonate. The genes in cyanophage genomes can assist with the photosynthesis of their hosts ( ). As most protein products of these viral AMGs contained high-confidence structure models, these viral AMGs were speculated to encode functionally active enzymes to drive biogeochemical cycling in mariculture sediments. Together, these results suggest that viral infections contribute to sediment ecosystem functioning and that further interrogation of viral communities will yield a more comprehensive understanding of complex functional networks and ecosystem processes in mariculture sediments. With the development of metagenomic approachs, viruses in different ecosystems could be investigated using powerful bioinformatic tools without cultivation. However, the procedure starting from sample collection, viral particle separation, sequencing, virus identification to bioinformatics analysis of the virome is experimental and informatics-challenging. In this study, there were limitations worth mentioning. Firstly, we performed prior virus-like particles separation from the mariculture sediments for viromic analysis, which could ensure the accuracy of the identified viruses and viral AMGs. Unfortunately, the viruses in the lysogenic cycle (either integrated into the host genome or as a plasmid) without viral particles and were usually neglected, resulting in the low recovery rate of viruses. Viruses with large particles (diameter >0.22 μm) ( ; ) or tightly attached to large particles, mainly nucleocytoplasmic large DNA viruses were filtered out and were also ignored. Then, the virus database was being extended with viruses identified and cultured in different environments, more and more advanced tools with high accuracy for viral identification was being developed. These may provide powerfull support for further research on the viral communities. In addition, considering the limitation of sample size and the lack of environmental parameters, we systemically explored the viral communities in mariculture sediments, mainly revealed extensive diversity of viruses and the various viral AMGs that potentially contribute to carbon, sulfur, and nitrogen metabolism in this study. These results provided novel insight into the composition of viral communities of sediments in different mariculture ponds. Whereas, the viruses in different mariculture ponds were merely compared and described on the compositions and the relative abundances. So, in the future researches, we will consider the role of biochemical parameters to analyze the influence of biochemical parameters on the viromes of sediments. As sediments with rich nutrition contain an abundant microbial community, maricultural sediments additionally represent oases of viruses and viral activity. The roles viruses play in influencing microbial mortality, ecology and evolution remains largely unexplored in different mariculture systems. In this study, we systemically explored the viral communities in different mariculture sediments with prior VLPs separation. The recovered mariculture sediment viruses revealed extensive diversity and novelty by phylogenetic analysis, and there was little resemblance to viruses in other water sediments and RefSeq based on gene-sharing network. Meanwhile, the addition of other maricultural species into prawn mariculture ponds could make variations in the viral community composition. Many of the putative microbial hosts for mariculture sediment viruses were predicted and belonged to taxonomic groups with no cultured representatives. In addition, through strict standards of identification, virus-encoded AMGs were identified, mainly including genes related glycosyl transferase, glycosyl hydrolases, sulfur and nitrogen metabolism. The viral AMGs may serve to enhance the ability of the host to respond to environmental gradients and to adopt a more efficient metabolic mode to survive better in their habitats. However, the contributions of viruses in aquaculture sediments and other aquatic environments still remain to be revealed, such as the function of massive unknown viruses and the role of horizontal gene transfer. With only a fraction of viruses identified here able to be classified, and small proportion of their hosts able to be predicted, there remain large gaps in understanding the microbiology of these environments. So, much work still remains to be done to understand the dynamics and biogeography of viral communities in the different ecosystems and their potential biogeochemical impacts. Therefore, the AMGs may serve to enhance the ability of the host to respond to environmental gradients and to adopt a more efficient metabolic mode to survive better in their habitats. The raw sequence files obtained from this research were submitted to the NCBI Sequence Read Archive (SRA) with accession number PRJNA523946 and PRJNA512384. Sequences of 1104 viral contigs and 38 MAGs with genome taxonomy can be found at supplementary, or can be download from All other data are available from the corresponding author upon request.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Not Used\n",
      "* **AIモデルの判断:** Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 175 / DOI: 10.1016/j.ejmech.2021.113921  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Synthesis and characterization of 3-[3-(1H-benzimidazol-2-ylsulfanyl)-3-phenyl propanoyl]-2H-chromen-2-one derivatives as potential biological agents\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Coumarin–benzimidazole hybrids: A review of developments in medicinal chemistry\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "According to the International Agency for Research on Cancer (IARC), there were 19.3 million new cancer cases and nearly 10.0 million cancer deaths occurred in 2020 worldwide. The global cancer burden is estimated to increase to 28.4 million cases in 2040 by a 41% rise from 2020 [ ]. However, drugs for curing cancer patients are still largely needed in the clinic, even though the most ever 18 new oncology drugs have been approved by the U.S. Food and Drug Administration (FDA) in 2020 [ ]. For some types of cancers with low 5-year relative survival rates (e.g., 9% of pancreatic cancer [ ]), targeted drugs with high efficiency are still highly demanded. Even for cancers such as chronic myeloid leukaemia that can be treated by targeted drugs such as imatinib with a high overall survival rate of 83.3% at 10 years, drug-resistant mutations including the T315I mutant remain to be causative for the unmet clinical need at the late stages of treatment [ , ]. It is thus of great importance to develop new anticancer drugs especially with new mechanisms of action. Bacterial resistance to antibiotics is developing into a global crisis which has already affect the world's health and economic well-being. Antimicrobial resistance has emerged to be one of the World Health Organization (WHO)'s top 10 global public health threats largely due to overuse and misuse of the antibiotics [ , ]. The lack of new antibiotics, particularly for Gram-negative bacteria leads to rise in mortality rate and thus emphasizes the necessity of developing new drugs with different modes of action against a wide range of bacteria and fungi [ ]. Among bacterial infections, tuberculosis (TB) caused by (Mtb) is also one of the top 10 causes of death according to the WHO's report [ ]. The global increase in drug-resistant strains of Mtb and serious side effects caused by the current drugs such as bedaquiline and delamanid urges the development of new and high efficacy antitubercular drugs [ ]. Vaccination remains the most effective way to treat viral infections. However, vaccines are available only for few viral diseases such as small pox, measles, [ ] Chemotherapeutic agents currently available for viral infections are associated with various side effects such as headache, fatigue, nausea, diarrhea, depression, hemolytic anemia and subjected to drug resistance [ ]. Thus, development of new classes of potent antiviral agents are essential. Similarly, new classes of antioxidant, anti-inflammatory and anthelminthics agents with different mechanisms of action are also demanded to overcoming the problems associated with the currently available drugs. To increase the success rate of drug discovery, one practical strategy is to begin with natural bioactive products from herbal medicine or other sources. Coumarins ( ), first isolated from tonka beans and melilot flowers [ ], have been widely investigated for their pharmacological applications including anticoagulant, anticancer, anti-inflammatory, antioxidant, antiviral, antimicrobial and other activities [ ]. A list of coumarin-containing drugs currently used in the clinic is presented in . One of the most well-known coumarin-derived drugs is warfarin. It is one of the original anticoagulant medications that has been approved for reducing the risks of blood clotting and preventing strokes from atrial fibrillation and/or cardiac valve replacement [ ]. Structural modification on the coumarin scaffold not only leads to new anticoagulants but also shifts the biological activities of new coumarin analogs to antibiotic, antispasmodic, antiproliferative, vasodilating, and chemoprotective activities. Derivatives of coumarins have been investigated for their anticancer potentials [ ]. For example, coumarins bearing hydrazide-hydrazone moiety have been evaluated against human drug-resistant pancreatic carcinoma cells and other cancer cells [ ]. However, the inhibitory activities of those derivatives were generally moderate. To increase the inhibitory potency of coumarins as well as to potentially improve the pharmacokinetic and pharmacodynamic properties and reduce the toxicity, one useful drug design strategy is molecular hybridization [ ]. It involves the combination of two or more pharmacophores with or without any linking group(s). The inclusion of additional pharmacophores to coumarin has been shown to improve its efficacy, activity, oral bioavailability and toxicity. The hybrid molecules can also minimize the risk of drug-drug interactions and potential drug resistance [ ]. Several review papers have recently summarized the progress of the development of coumarin hybrids as anticancer agents [ ]. For example, hybrids of coumarin with artemisinins, azoles, chalcones, furoxans, indoles/isatins, pyridine/pyrimidines have been reported with anticancer activity [ ]. Among those hybrids, coumarin–benzimidazole chimeric molecules have not been comprehensively reviewed to the best of our knowledge. The benzimidazole moiety is considered the biostere of purine base due to their structural similarity [ ]. It has been widely found in drugs with various therapeutic applications [ , ]. summarized the approved drugs bearing the benzimidazole scaffold for the treatment of cancer, infectious diseases, tropical diseases, stomach ulcers, inflammation, and hypertension. One of the most frequent modification sites on benzimidazole is the C2 position. Benzimidazole- 1 and its phenyl ring are also modification sites that affect the bioactivities and the pharmacological applications. Based on the hybridization type, coumarin–benzimidazole chimeric molecules can be categorized into three classes ( ): (1) merged hybrids, (2) fused hybrids and (3) spacer-linked hybrids [ ]. Depending on the substitution position, fused hybrids and spacer-linked hybrids can be further classified as coumarin-C3 hybrids, coumarin-C4 hybrids, and coumarin-C5/6/7/8 hybrids. In the present paper, we aim to provide an updated review of the coumarin–benzimidazole hybrids as potential anticancer agents. The other biological activities including antiviral activity, antioxidant activity, anti-inflammatory activity, antimicrobial activity, anthelmintic activity, etc. are also extensively included. Depending on the merging position, the merged coumarin–benzimidazole hybrids can contain at least three pairs of compounds: the 5,6-merged hybrids, the 6,7-merged hybrids and the 7,8-merged hybrids. Due to the different positions of the imidazole ring substituent, each pair can contain two regioisomers (i & ii), as illustrated by the substituted 6,7-merged hybrids in . However, the chemical synthesis and biological study of such merged coumarin–benzimidazole hybrids have not been widely reported. To the best of our knowledge, there is only one example of biological applications of 6,7-merged coumarin-benzimidazole hybrids has been reported. In an authorized patent, Wang et al. reported a group of phenyl substituted 6,7-merged hybrids tagged with F, which could be used in the early diagnosis of Alzheimer's disease by targeting the β-amyloid protein. The representative compound ( ) was observed accumulated in the rat brain by positron emission tomography (PET) scan [ ]. In 2015, She et al. conducted high-throughput screening of 55,000 compounds and identified coumarin-C3 benzimidazole hybrid ( ) being the most active (0.07–0.41 μM of growth inhibition (GI ) values against 13 different cancer cell lines) [ ]. The structure-activity relationship (SAR) study suggested that the , -diethylamino substituent at the coumarin-C7 position was essential for the anticancer activity [ ]. Mono methylation on benzimidazole-C5 position (compound ) retained the activity in low micromolar range, while dimethylation on benzimidazole-C5,6 positions weakened the activity [ ]. The mechanism of action of these compounds appeared to involve with the inhibition of PI3K/AKT/mTOR pathway and the induction of cell cycle arrest and caspase-dependent apoptosis [ , ]. The importance of the coumarin-C7 substituent was also demonstrated by the SAR study conducted by Paul et al. [ ]. The only change of coumarin-C7 substituent significantly affected the inhibitory activity of the coumarin-C3 benzimidazole hybrids against a complete panel of 60 human cancer cell lines. The ethanolamine-substituted compound ( ) was found to be the most active, in contrast to its analog bearing the ethylenediamine group being inactive in most of the cells. studies suggested that compound could inhibit DNA replication for its anticancer activity. The formation of hydrogen bonds between ethanolamine moiety and the target protein likely led to its high activity [ ]. The other positions of the coumarin ring may also affect the inhibitory activity against cancer cell lines. For example, coumarin-C6 substitution by chloro ( ) significantly improved the IC in human T-lymphocytes compared to the methyl substitution ( ) ( ) [ ]. Wang et al. developed an interesting coumarin-C3 benzimidazole hybrid by fusing quinolizine to the coumarin moiety [ ]. The new compound benzoquinolizine coumarin was found to be selective against human umbilical vein endothelial cells over the other cancer cells. Since coumarin-bearing compounds are well known for eliciting fluorescent signals [ ], compound is thus considered the promising lead for developing new inhibitors and fluorescence imaging probe for tumor angiogenesis [ ]. The derivatization on the benzimidazole- 1 position can also benefit the inhibitory activity against cancer cell proliferation . Some -acyl derivatives of the coumarin-C3 benzimidazole hybrid showed increased anti-proliferative activity by the MTT assay. The most actives chloroacetyl derivative ( ) and hydrazonyl chloride derivative ( ) ( ) were more potent against glioma cell line U87MG with IC values of 0.34 and 0.29 μM, respectively, compared to the positive control foretinib with an IC of 0.90 μM. However, compound bearing the reactive chloroacetyl moiety was very toxic. The toxicity of the hydrazone derivative was eliminated by blocking the potential Michael addition site [ ]. More interestingly, the phenylthiocarbamoyl derivative was found very much potent (similar to doxorubicin) while the simple -acetyl derivative and the C4, 1-fused analogs ( ) were drastically less active. However, a toxic effect was observed in normal lung fibroblast cells exposed to these compounds [ ]. Benzimidazole derivatives have been shown with antiviral activities such as anti-hepatitis C virus (HCV) activity [ ]. The coumarin moiety was also an essential pharmacophore in the previously reported -HCV agents [ ]. Thus, the synthesized coumarin-C3 benzimidazole hybrids by Shwu-Chen et al. were tested for their -HCV activity. It was interesting that the β- -ribofuranoside of the unsubstituted coumarin-C3 benzimidazole hybrid ( ) was tested active in the HCV genotype 1b subgenomic replication system in Huh 5-2 cells. It had an EC of 20 μM and a selectivity index of 6.4. Substituents on the benzimidazole moiety and the coumarin moiety were determinants for the -HCV activity of the corresponding hybrids. Incorporation of methyl substitutions on benzimidazole moiety was found to improve -HCV activity. The compound , bearing two methyl substituents on benzimidazole at C5 and C6-positions was most active with an EC of 3.0 μM. However, its selectivity index was only 7.9. The best candidate compound with fused benzene ring to the coumarin nucleus showed a much improved selectivity index of 14.0 while maintained its EC in the low micromolar concentration (5.5 μM) [ ]. Sukhen reported a preliminary screening of benzimidazole- 1 substituted imine derivatives against two Gram-positive and two Gram-negative bacterial strains. Compounds with hydroxy or nitro group at coumarin C7 position, such as – ( ) were tested positive to the strains at unknown concentrations. No further studies have been added to validate the result [ ]. Chronic inflammation is a significant denotation of various types of diseases including osteoarthritis, atherosclerosis, cancer, Alzheimer's disease and others [ , ]. It is important to develop anti-inflammatory agents to prevent or synergistically treat those diseases. Krishan et al. has evaluated the anti-inflammatory activity of coumarin-C3 benzimidazole hybrids in rat paw. Two most active compounds and ( ) exhibited maximum inhibition of paw oedema by 45.45% and 46.75%, respectively, after 6 h administration. Their inhibition profiles at each time slot were found similar to that of the positive control indomethacin, suggesting a similar mechanism of action of these compounds by inhibiting prostaglandins synthesis to that of indomethacin. The SAR study suggested that coumarin-C6 substitution by electron-withdrawing groups such as chloro ( ) and bromo ( ) could enhance the anti-inflammatory activity of the coumarin-C3 benzimidazole hybrids. The insertion of the amide linker group (e.g., , ) destroyed the anti-inflammatory activity of these compounds [ ]. Metallic anticancer drugs such as cisplatin, carboplatin and oxaliplatin have been widely used for the treatment of various types of cancer [ , ]. However, severe side effects associated with those drugs and the emerging drug resistance require the development of new generations of metallic anticancer compounds [ ]. Qin et al. have reported a series of metallic complexes of coumarin-C3 benzimidazole hybrids with platinium(II) ( ). The cytotoxicity of those platinum complexes against cancer cells was significantly higher than their corresponding ligands. Some compounds (e.g., – ) could potently overcome drug resistance in SK-OV-3 cells that was induced by cisplatin ( ). More importantly, all the Pt(II) complexes displayed remarkably low cytotoxicity to the normal HL-7702 hepatocyte cells, suggesting a potentially improved therapeutic index [ ]. Mechanism studies suggested that compound was mostly active to inhibit telomerase, illustrated by the reduction of transcription and expression of c-myc and human telomerase reverse transcriptase (hTERT) gene. It was more efficiently up-taken by the cisplatin-resistant cell SK-OV-3/DDP (6.52 ± 0.26 nmol of Pt/10 cells) than cisplatin (4.11 ± 0.18 nmol of Pt/10 cells) in a gradual accumulation mode over the time period of 24 h. The mitochondrion-mediated apoptosis in the SK-OV-3/DDP cells was also induced by compounds – . The percentage of apoptotic cells in the treated cells was in good agreement with their cytotoxicity but was all higher than that induced by cisplatin treatment [ , ]. The methoxy substitution position on the coumarin ring appears to be important for the cancer cell cytotoxicity. Compound ( ) bearing the coumarin-C7 methoxy substituent was 10 times less potent (IC : 10.3 ± 0.3 μM) than compound (IC : 1.01 ± 0.27 μM) with the same substituent at the C8 position in the cisplatin-resistant cell SK-OV-3/DDP. The incorporation of an additional platinum(II) to compound significantly increased the resulting compound 's cytotoxicity (IC : 0.5 ± 0.2 μM in SK-OV-3/DDP cells) while maintained no cytotoxic effect to the normal HL-7702 cells. Compound also potently targeted telomerase and induced G2/M phase cell cycle arrest and mitochondrial dysfunction and apoptosis [ ]. Due to the low toxicity and high water solubility of ruthenium(II) complexes [ ], coumarin–benzimidazole hybrids with Ru(II) are attractive for the discovery of new metallic anticancer drugs. The same research group expanded their research from the Pt(II) complexes to the coumarin-C3 benzimidazole hybrids with Ru(II). Three new Ru(II) complexes ( – , ) were synthesized and screened against a panel of human cell lines. While they all showed weaker cytotoxicity against human ovary adenocarcinoma cell SK-OV-3 compared to the Pt(II) complexes, comparable or slightly lower IC values of compounds and were observed against the human lung carcinoma cell NCI-H460 compared to cisplatin ( ). More strikingly, compound with the fluoro substituent on the coumarin-C7 position significantly enhanced cytotoxicity (0.30 ± 0.02 μM) by 52.8 times in NCI-H460 cells when compared to the unsubstituted compound (15.78 ± 1.02 μM). All three compounds showed no appreciable toxicity in the normal human-derived liver cells HL-7702 ( ) [ ]. A similar mechanism of action was observed in the NCI-H460 cells by the treatment of the Ru(II) complexes. The higher cellular uptake and telomerase inhibition of the most potent compound was in line with its cytotoxicity and with the induction of cell cycle arrest and DNA damage. Compound showed superior anticancer activity by inhibiting 61.3% tumor growth in the H460 xenograft mouse model in comparison with cisplatin (25.5%) and oxoaporphine Ru(II) complex (53.3%) [ ]. A series of coumarin-C4 benzimidazole hybrids ( , ) and its -methyl ( ) or -sulfonyl derivatives ( ) were synthesized for anticancer study. Unfortunately, the unsubstituted hybrids and the -methyl derivative were inactive in HeLa cells (cervical cancer cells) and HT29 cells (colon cancer cells). Only two -sulfonyl derivatives with electron donating substituents methyl ( ) and methoxy ( ) on coumarin-C6 position were weakly active with GI values of ∼35 μg/mL against HeLa cells [ ]. The same set of coumarin-C4 benzimidazole hybrids ( ) have also been evaluated for their antimicrobial activity. Compounds - with no substituents on the benzimidazole ring showed some activity against the tested bacterial strains; the coumarin-C6 methoxy substituted compound was highly active in Gram-positive compared to the standard drugs ciprofloxacin and nystatin. The -sulfonated derivatives such as - were shown with a broad-spectrum antimicrobial activity, while the -methylated derivatives were generally inactive except for compound . The SAR study suggested that methoxy, chloro and bromo substitutions on coumarin-C6 position could improve the antimicrobial activity. studies suggested that the compounds could inhibit the bacterial cell wall development suppression of 2,2-dialkylglycine decarboxylase functions [ ]. The spacer group (or termed linker) linking the two privileged scaffolds into one hybrid is sometimes important due to its potential contribution to the overall pharmacological activity. The linker can be either a metabolically stable group that may prevent the cleavage of parent pharmacophore and act as a multi-target agent, or a metabolically unstable group that releases two drugs reversibly at the targeted sites [ , ]. For the stable linker, the incorporation of the other active pharmacophores is one of the well-acceptable drug design strategies for improving the bioactivity of the hybridized compound. Benzylcoumarin has been shown to display good antitumor activity by inhibiting multiple targets such as MEK1 [ ], 17β-HSD3 [ ] and NQO1 [ ]. Molecular hybridization of benzylcoumarin with benzimidazole led to the synthesis of a series of methane-linked coumarin-C3 benzimidazole hybrids ( ). However, unsubstituted ( ), mono methyl substituted ( ) and dimethyl substituted ( ) benzimidazole compounds were found to be inactive against 5 human cancer cells. Interestingly, benzylation/naphthylation at the benzimidazole- 1 position formed the corresponding imidazole salts, some of which were found with moderate or high inhibitory activity. It may be due to its molecular structure, changed charge distribution and increased water solubility [ ]. The salt with naphthyl substituent ( ) showed higher inhibitory activity with IC values of 2.04 - 4.51 μM in the human cancer cells. The mechanism of action study suggested that compound caused cell cycle arrest at G0/G1 phase and induced apoptosis in the liver carcinoma cell SMMC-7721 [ ]. This was likely due to the augmentation of cytotoxicity caused by the imidazolium salt that has been observed in the other chemical scaffolds [ ]. A series of thiomethylene-linked coumarin-C3 benzimidazole hybrids ( ) and their -glucoside analogs has been synthesized by Hwu et al. However, none of these compounds were active in the cell lines against a panel of 15 viruses including HIV, HSV, RSV. In the Huh 5-2 replicon system [ ], compounds and were found effective to inhibit HCV subgenomic replicon replication with EC values of 3.4 and 4.1 μM, respectively. They both strongly inhibited HCV RNA replication at the concentration of 5.0 μM and showed nearly no toxic effect on cell proliferation in Huh 6, Huh 9-13 and Huh mono cells [ ]. The same research group focused the SAR study on the modification of the benzimidazole moiety. The replacement of benzimidazole with imidazopyridine, purine, benzoxazole and benzothiazole yielded the actives (imidazopyridine), (purine), (benzoxazole) with EC values of 6.8, 2.0, and 12 μM, respectively, in the Huh 5-2 replicon system [ ]. The SAR of this series of compounds has been established as follows (i) The introduction of halogens on coumarin (e.g., , & , ): exhibited better inhibitory activity than the unsubstituted derivatives. (ii) Incorporation of β- -glucose peracetate ( & ) and peracetyl 2-deoxy-β- -glucose ( ) displayed significant HCV inhibitory activity. (iii) The replacement of coumarin moiety with other aryl groups such as benzene or naphthalene or pyridine diminished the activity. (iv) The replacement of benzimidazole with purine ( ) enhanced the activity, whereas corresponding imidazopyridine and benzoxazole derivatives ( & ) displayed less inhibitory activity. The other group led by K. K. Kumar also synthesized a series of thiomethylene-linked coumarin-C3 benzimidazole hybrids as new antimicrobial agents. It was very interesting that the unsubstituted compound ( ) showed 100% inhibition against human bacterial pathogens and at the concentration of 50 μg/mL. The presence of electron withdrawing groups such as chloro and bromo groups at coumarin-C6 position, and nitro and methyl groups on benzimidazole-C6 also led to the potent inhibitors against the other plant pathogens ( ), ( ) and ( ). The compound with electron withdrawing groups on both coumarin-C6 as well as benzimidazole-C6 positions also showed 100% inhibition against human pathogen at 50 μg/mL [ ]. The analogs of thiomethylene-linked coumarin-C3 hybrids, featured by the incorporation of coumarin-C4 hydroxy group and the aromatic substitutions on the linker, were reported to be potent antimicrobial agents in the tested bacterial and fungal strains. The minimum inhibitory concentrations (MICs) of the two most active compounds and ( ) were comparable to the positive drugs streptomycin and fluconazole against various types of pathogens. The other compounds were mostly active in moderate or weak potency except for being completely inactive. No clear SAR was observed from the current study [ ]. The same research group extended the thiomethylene linker to the thiopropanoyl linker of the coumarin-C3 benzimidazole hybrids with similar aromatic substitutions. Most of the new group of compounds with no or simple substitutions on phenyl ring (e.g., , ) showed superior antibacterial activity against both Gram-positive and Gram-negative bacteria than the thiomethylene-linked compounds – . Their MICs (2.1–2.5 mg/mL) were comparable to that of the positive drug streptomycin (2.2–2.5 mg/mL) [ ]. A new group of coumarin-C3 benzimidazole hybrids bearing the linker of sulfonamide was evaluated against 4 strains of bacteria. The most active compound showed a superior zone of inhibition (ZOI) value against (13 mm) than the positive drug novobiocin (25 mm) at the concentration of 0.5 mg/mL [ ]. The other coumarin-C3 benzimidazole hybrids with the heterocycle linkers were also reported as antimicrobial agents. New hybrids with the methylenehydrazine linker ( , ) and thiazolidinone-NH linker ( ) were synthesized by Naganna and Sheshikant. Those hybrids were screened for antimicrobial activity against two Gram-positive and two Gram-negative bacteria and three fungal strains. The ZOI values of those compounds ranged from 13 to 16 mm, which were comparable to the drugs gentamicin and fluconazole in the bacteria and the fungi, respectively [ ]. Wagnat reported the thiophene-S-linked hybrid with good ZOI values against the tested bacterial strains but not the fungal strains [ ]. Abdel-Motaal and Raslan synthesized two new pyrimidine-thiomethylene-linked compounds , and one pyrimidine-methylsulfinyl-linked compound . They were moderately or weakly active to the tested 8 strains of bacteria and fungi [ ]. An interesting group of ethanimine-linked coumarin-C3-benzimidazole- 1 hybrids were investigated for their antibacterial activity. The SAR study revealed that the compounds with halogen atom on phenyl ring have good antibacterial activity. Specifically, chlorine atom at position on phenyl ring ( , ) showed remarkable inhibitory activity against a broad spectrum of tested strains including (MIC: 0.95 μg/mL), (1.56 μg/mL), (3.12 μg/mL) and (3.12 μg/mL) than the - or -chlorine substituted ones. The compound was found nontoxic even at a concentration 10 times greater than the MIC concentration in human erythrocytes. No inhibition of lactate dehydrogenase or cell viability and proliferation was observed in the normal fibroblast cells [ ]. Naganna et al. tested the anti-inflammatory activity of benzimidazole-coumarin hybrids with methylenehydrazine linker ( , ) and thiazolidinone-NH linker ( ). Compounds and have been shown to be moderate anti-inflammatory agents by the formalin-induced rat hind paw oedema method. Among the 4 compounds evaluated, the thiazolidinone-NH linker compound , with no substituent on coumarin-C4 showed 48% inhibition after 4 h administration, which was comparable to that of the positive control drug diclofenac sodium with 52% inhibition. The introduction of methyl substituent on coumarin-C4 position ( ) diminished the activity [ ]. Compounds – in were evaluated for their antioxidant activity. Of all the compounds that displayed considerable free radical scavenging activity, compound with the phenolic linker was the most active with an IC value of 9.82 ± 1.79 μg/mL at the maximum inhibition percentage of 74%. This was comparable to that of the standard compound butylated hydroxytoluene (BHT) with the IC value of 10.5 ± 1.06 μg/mL at the maximum inhibition percentage of 78% [ ]. The other compounds bearing a new linker of aromatic thiopropanoylene (e.g., compound ) were much less active in the same assay, suggesting the importance of the coumarin-C4 hydroxy group and the linker phenolic group for maintaining the antioxidant activity in these compounds [ ]. A series of coumarin-C3 benzimidazole hybrids bearing the amide linker ( ) was synthesized and tested as new antioxidants. They showed superior EC values than the positive control compound BHT except for the methoxy substituted analog . This suggested that the electron-withdrawing groups (such as halogen and nitro) rather than the electron-donating groups (such as methoxy) enhanced the antioxidant activity in this scaffold. The analogs without the amide linker (e.g., and , ) were much less active, highlighting the necessity of the amide linker being the radical scavenging pharmacophore of the hybrids [ ]. Compound ( ) and its analogs were shown to have the power of cleaving form I and/or form II DNA [ ]. Based on their previous results on the chromone scaffold, Huang et al. found that the flexible methylene groups of the thioether and thiomethyl linkers were essential for the anti-proliferative activity of those derivatives [ , ]. However, this linker did not gain the anti-proliferative activity for coumarin-C4 benzimidazole hybrids (e.g., , ) against a panel of 6 human cancer cell lines. Replacement of benzimidazole with benzothiazole and benzoxazole failed to improve the activity [ ]. 1,2,3-Triazole has emerged to be one attractive linking group due to its easy synthesis via copper-catalyzed “click chemistry” and excellent biocompatibility and stability [ ]. Raic-Malic et al. reported the synthesis of several coumarin-C4 benzimidazole hybrids with the 1,2,3-triazole linker. Among those compounds, compound bearing methyl substitutions on both coumarin-C7 and benzimidazole-C2 positions showed the highest cytotoxicity with an IC value of 0.90 μM against human hepatocyte HepG2 cells. However, this compound was toxic to normal fibroblasts cells 3T3 and WI38, which would prevent it from being developed as the lead compound [ ]. The corresponding 7-hydroxycoumarin and simple benzimidazole derivatives were found to be 2 – 4-fold weaker than in antiproliferation activity ( ). Thus, the presence of methyl group on both coumarin and benzimidazole were essential for the activity. The mechanism study suggested that compound potently inhibited 5-lipoxygenase, the enzyme which catalyzes the production of inflammatory mediator leukotrienes and contributes to the progression of cancer [ , ], and induced cell death due to early apoptosis [ ]. The methylene-linked coumarin-C4 benzimidazolium hybrids were also synthesized against human ovarian cells A2780 and prostate cancer cells PC-3. With all the benzimidazolium hybrids showing high cytotoxicity against the two cell lines, the most active ones appeared to be and in A2780 cells and PC-3 cells, respectively. However, similar to the methylene-linked coumarin-C3 benzimidazole hybrids, they showed significant inhibition of cell viability after 3 days of treatment indicating their high toxic potential [ ]. A new amine-linked coumarin-C4 benzimidazole hybrid ( , ) was reported with good inhibitory activity against a panel of bacteria and fungi with the ZOI ranging from 12 to 23 mm. The maximum inhibition of compound was observed to the Gram-positive bacterial strain . Its ZOI was 23 mm which is comparable to that of the positive control drug tetracycline (25 mm) [ ]. The coumarin-C4 benzimidazole hybrids with the 1,2,3-triazole linker (e.g., in ) were also screened for their antimicrobial activity. However, they were considerably weaker in the inhibition of 11 Gram-positive and Gram-negative bacterial strains [ , ]. Coumarin-C4 benzimidazole hybrids with the sulfonylmethylene linker were reported by Jeyachandran et al. with antitubercular activity. The most active compound ( ) had a MIC of 1.61 μg/mL against H Rv, which is nearly equipotent to the positive drug ethambutol (MIC: 1.56 μg/mL) and 4 times higher potent than pyrazinamide (MIC: 6.25 μg/mL). Methoxy substituents on the coumarin-C7/8 positions decreased the inhibitory activity of compound [ ]. The other two hybrids and with the linking groups containing the 1,2,3-triazole moiety and dimethyl substitution on the coumarin-C5/7/8 showed excellent activity against Mtb H Rv strain. Their MICs were 3.8 μM, which is 2.5-, 2.8-, and 6.6-fold lower than the standard drugs ciprofloxacin, streptomycin and pyrazinamide, respectively. Modifications on the coumarin ring such as replacing the methyl groups with other substituents like chloro, hydroxyl, methoxy and fusing into the coumarin phenyl ring all decreased the compound inhibitory activity. Molecular docking study suggested that the DprE1 (decaprenylphosphoryl-β- -ribose 2′-epimerase) enzyme essential for Mtb cell wall synthesis might be a potential target for these compounds [ ]. Helminthiasis, caused by three parasites cestodes (tapeworms), nematodes (roundworms) and trematodes (flukes), can be infected through the parasitic contamination of food and water [ ]. The introduction of the benzimidazole-derived drug thiabendazole leads to the beginning of the new era of antiparasitic drug discovery [ ]. It as well as the other benzimidazole-derived antihelminthics including albendazole and mebendazole targets the β-tubulin and inhibits microtubule polymerization [ ]. However, the relatively low efficacy and the emerging drug resistance demands the development of new antihelminthics [ ]. A series of coumarin-C4 benzimidazole hybrids with the methylene-amide linker was reported to be potent anthelmintic agents. The most active compound ( ) with no substitution on coumarin showed comparable potency to the positive drug albendazole regarding the paralytic activity and mortality against the earthworms at the concentration of 0.1% w/v. The substitution such as methyl, nitro and hydroxy groups on the coumarin ring did not enhance the compound activity [ ]. A class of coumarin-C4 benzimidazole hybrids with the methylene linker was evaluated for their anti-inflammatory activity. All compounds showed moderate to good inhibitory activity with the maximum 36–45% inhibition when compared to indomethacin after 3 h administration. The best compounds were the unsubstituted compound and its methyl analog ( ). The changes of the substituents on the coumarin ring generally diminished the potency by introducing hydroxy or nitro groups [ ]. Madhura et al. reported an interesting series of coumarin-C4 benzimidazole hybrids with the fused linker as anti-inflammatory agents. The maximum activity was observed after 5 h administration at the dose of 200 mg/kg body weight in mice. Compound was the most active with the maximum inhibition of 52.6%. The replacement of the methoxy group on the phenyl ring by the methyl group was tolerated. However, the replacement by the chloro substituent resulted in the loss of anti-inflammatory activity of the derivative compound [ ]. The same series of coumarin-C4 benzimidazole hybrids with the methylene and the methylene-amide linker were also evaluated for their antioxidant activity. The best actives were the coumarin-C8 nitro derivatives and ( ). Their EC values were 0.17 and 0.08 μM which are significantly lower than the standard antioxidant BHT (23.4 μM) but relatively higher than the other antioxidant ascorbic acid (0.03 μM). The comparison of the antioxidant activity of the two classes of hybrids clearly shown that the scaffold bearing the methylene-amide moiety is superior to the one with the methylene linker [ ]. Considering the antioxidant role of paraoxonase-I (PON-1) enzyme in the progress of multiple diseases including atherosclerosis, diabetes, hypercholesterolemia and parasite infection [ ], inhibitors targeting PON-1 may be useful drug candidates for the relevant clinical applications. Karatas et al. reported a series of water-soluble coumarin-C4 benzimidazolium chloride salts with the methylene linker as potential PON-1 inhibitors. The most active compound was ( ) bearing the hydrophobic substituent 2,3,4,5,6-pentamethylbenzyl group. It seems that increased lipophilicity of the inhibitor compound could lead to the increased activity of PON-1 inhibition. The substituent on the coumarin ring also plays an important role in regulating the inhibition activity of derivatives against PON-1 [ ]. Carbonic anhydrases are a group of zinc(II) metalloenzymes responsible for the catalytic conversion of CO to bicarbonate and a proton in prokaryotes and eukaryotes [ ]. The disturbance of the function of carbonic anhydrases caused multiple pathological consequences such as oedema, glaucoma, obesity, cancer, epilepsy and osteoporosis [ ]. It is thus important to develop carbonic anhydrase inhibitors especially with good selectivity for the treatment of diseases with less toxicity [ ]. Karatas et al. evaluated the same set of coumarin-C4 benzimidazolium chloride salts as potent human carbonic anhydrase (hCA) I and II inhibitors. Most of the compounds derived from coumarin-C7,8-di-OH ( ), coumarin-C7-OH ( ), and coumarin-C6,8-di-Me ( ) ( ) showed moderate inhibitory activity against hCA I and II. The bisbenzimidazolium chloride salts ( ) did not improve the activity. The optimization of the substituents on the benzimidazolium ring led to the most active compounds and . Their IC values were 5.34 and 4.99 μM for hCA I, and 6.01 and 6.01 μM for hCA II, respectively. However, they both were less potent than the standard compound acetazolamide with the IC values of 3.30 and 2.40 μM for hCA I and hCA II, respectively [ ]. The same set of synthesized compounds were also evaluated for their anticonvulsant activity in mice. After administration by intraperitoneal injection with two different doses (30 and 100 mg/kg), compound ( ) appeared to be the best candidate showing good anticonvulsant activity with no observed neurotoxicity [ , ]. A series of silver(I) complexes coordinated with -heterocyclic carbenes (NHC, here as coumarin-C4 benzimidazole carbene) was synthesized and evaluated for their cytotoxicity against human lung cancer cells A549 and H1975. While the -NHC coordinated complexes (e.g., , ) were moderately cytotoxic in the assays, the -NHC complexes (e.g., ) showed increased activity by coordinating the other coumarin-C4 benzimidazole carbene ligand. The corresponding uncoordinated ligands (e.g., ) were totally inactive. The substitution on the benzimidazole- 3 position by increasing the length of the carbon chain or the replacement with the benzyl group did not affect the cell cytotoxicity. All the active compounds also showed similar IC values against the normal human skin fibroblast cells Hs68, suggesting a general cytotoxic mechanism with no cancer cell selectivity [ ]. Silver -heterocyclic carbenes (Ag-NHCs) have been reported to be stable organometallic compounds in the presence of air or moisture with antimicrobial agents [ ]. Thus, the silver(I) complexes coordinated with coumarin-C4 benzimidazole carbenes were synthesized and evaluated for their antibacterial activity against two Gram-positive and two Gram-negative strains. None of the NHC precursors showed inhibitory activity in the assay. However, the -NHC silver(I) complexes were found to be moderately or highly active, with ZOI values of 7–28 mm and MICs of 8–128 μg/mL against all the strains. The ZOI values of the positive drug ampicillin ranged from 6 to 30 mm, suggesting the good potency of the -NHC silver(I) complexes [ ]. The replacement of coumarin-C6 chloro with methyl group enhanced the antibacterial activity of these compounds. For example, compounds and in showed excellent inhibitory activity against , with MICs of 4 and 2 μg/mL, equally potent or slightly better compared to ampicillin (MIC: 4 μg/mL) [ ]. The following studies suggested that substitution on the coumarin ring and the benzimidazolium ring could affect the antibacterial activity however with no clear SAR [ ]. The simple hybridization of coumarin and benzimidazole with the straight alkoxy linkers led to a series of coumarin-C7 benzimidazole hybrids with antiviral activity against the spring viraemia of carp virus (SVCV), which causes high mortality in the cultivation of cyprinid fishes. The best compounds bearing alkoxy linker with 3 ( , ) or 4 carbon atoms ( ) had IC values of 2.2 and 0.56 μg/mL, respectively. After treatment, both compounds protected the infected zebrafish with an increased survival rate, possibly via the effective inhibition of SVCV-induced cell death by activating the Nrf-2 pathway and the ability to maintain the cell morphology and to keep the oxidative balance [ , ]. The same group further studied the antiviral activity of several alkoxy-linked coumarin-C7 benzimidazole hybrids against infectious hematopoietic necrosis virus (IHNV), which causes a variety of acute infectious diseases in livestock [ ]. Among the compounds, with 6 carbon atoms in the alkoxy linker exhibited the highest activity with an IC value of 2.96 μM. This is due to its ability of blocking virion replication. Compound ( ) also exhibited good antiviral activity against two other rhabdoviruses such as SVCV and micropterus salmoides rhabdovirus (MSRV) with IC values of 1.68 and 2.12 μM, respectively [ ]. The same type of coumarin-C7 benzimidazole hybrids with the alkoxy linker was investigated for their antibacterial activity. The SAR study suggested that the alkoxy linker with 8 carbon atoms resulted in the best inhibition against , , and , whereas the 6-carbon linker showed the best activity against . However, the compounds with the benzimidazole ring (e.g., , ) were less active than the compounds with the imidazole moiety [ ]. A panel of azole-based drugs (e.g., fluconazole, itraconazole, voriconazole and posaconazole) have been approved for the antimicrobial application [ ]. It is thus a promising strategy to hybrid different azole moieties such as triazole, benzotriazole, benzimidazole and thio-benzimidazole into the privileged pharmacophore coumarin. Damu et al. reported a series of coumarin-C5/7 azole hybrids (e.g., and , ) and found that simple benzimidazole did not show significant antimicrobial activity. Low logP values were suggested by the authors to favour the antimicrobial activity of these compounds [ ]. The simple combination of coumarin and benzimidazole with the alkoxy linker also led to the identification of potential anthelminthic agents against , which causes the parasitic infection of fish. Benzimidazole hybrids were found more active than various imidazole derivatives. The best compound ( ) (EC value: 0.85 mg/L) required the appropriate linker of 6 carbons for the best activity. However, this compound was found toxic in goldfish. The toxicity profile appears to be associated with the linker length, where 6 carbons displayed high toxicity in every series of hybrid compounds. Further study with toxicity reduction of these compounds is needed for their application in aquaculture production [ ]. Coumarin and benzimidazole are the two scaffolds that have been widely used in drug discovery and development. Molecular hybridization of the two privileged scaffolds has led to extensive studies of coumarin–benzimidazole chimeric compounds with various biological applications. Based on the hybridization type, we reviewed the coumarin–benzimidazole hybrids in three major classes: merged hybrids, fused hybrids, and spacer-linked hybrids. Depending on the linking position on the coumarin moiety, those hybrids were further sub-grouped into coumarin-C3 benzimidazole hybrids, coumarin-C4 benzimidazole hybrids, and coumarin-C5/6/7/8 benzimidazole hybrids. Metallic coumarin–benzimidazole hybrids with biological activities were also included in the present work. In this review, we have focused on the SAR studies and the mechanism of action of coumarin–benzimidazole hybrids with their anticancer, antiviral, antimicrobial, anthelmintic, anti-inflammatory, and antioxidant activities. Some compounds such as , , showed strong inhibitory activity against bacteria with comparable or even superior when compared to the positive control drugs. The compound exhibited excellent anticancer activity whereas compound exhibited prominent anticancer activity. A few compounds (such as , ) were evaluated with good anti-inflammatory activity. Although great progress has been made with the medicinal chemistry study of coumarin–benzimidazole hybrids, research opportunities that remain to be pursued for future study are considered as follows: (1) The complete SAR study of coumarin–benzimidazole hybrids is needed, since many studies failed to report the detailed study of structural modification on the lead compounds. The detailed SAR study can probably lead to more active compounds that may be suitable for studies. (2) The evaluation of coumarin–benzimidazole hybrids is needed for assessing the overall pharmacological properties of such compounds in relevant animal models. (3) The target identification study of potent coumarin–benzimidazole hybrids is needed for optimization of such compounds using rational drug design strategies such as computer-aided drug design. (4) The detailed study of mechanism of action is needed for the comprehensive understanding of the molecular basis for the biological functions of such compounds. Overall, we here provide a comprehensive review on the progress of medicinal chemistry study of coumarin–benzimidazole hybrids. The compounds were categorized based on their structural features with the focus on the SAR and mechanism of action studies. The anticancer activity as well as the other biological activities including antiviral activity, antimicrobial activity, etc. were included for a full understanding of the pharmacological potentials of these coumarin–benzimidazole hybrids.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Used\n",
      "* **AIモデルの判断:** Not Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 178 / DOI: 10.1016/j.pmatsci.2017.02.001  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Data on photo-nanofiller models for self-cleaning foul release coating of ship hulls\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Recent progress in marine foul-release polymeric nanocomposite coatings\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "Nanocomposites constitute a class of materials that exhibit advanced properties at low nanoparticle (NP) concentrations in comparison with conventional filler contents in coatings . Selected NPs in coating formulations can enhance various properties, including antifouling (AF), mechanical and optical characteristics, permeability, and wettability . Biofouling is a complex issue that causes serious economic casualties and unfavorable ecological impacts on maritime ecosystems. Marine fouling costs about US$ 150 billion every year in transportation . As such, fouling control through ecofriendly and economic AF paint solutions is of great importance because the annual consumption reaches 80,000 tons . Fouling layers increase friction drag and hydrodynamic weight and thus decrease shipping speed and maneuverability . For this reason, high fuel consumption is necessary to maintain the required speed and navigation setting; as a consequence, financial costs are increased and harmful compounds are emitted into the environment. This action also requires personnel resources, entails machinery effort, consumes time, and generates wastes that pose harmful effects on health and environment . Therefore, these problems have prompted researchers to develop coatings that prevent ship hull biofouling caused by commercial biocidal AF coatings . With environmental toxicity issues related to the use of toxic AF coatings, modern research has focused on environmentally friendly alternatives, such as foul-release (FR) technology . This review discusses the advanced ecofriendly technologies for AF coatings and focuses on the modern streams achieved in FR coatings for maritime applications. Non-stick FR coatings, which include fluoropolymers and silicon compounds, offer an ecofriendly alternative that can be used for prolonged periods . FR coating technology acts by preventing fouling settlements and providing extremely smooth self-cleaning surfaces . Organo-silicone polymers, particularly polydimethylsiloxane (PDMS), are more efficient than fluoropolymers and thus are considered the most promising FR coating systems . As a non-stick FR layer, PDMS with a Si-O backbone and a CH side chain possesses excellent properties, such as high level of smoothness, hydrophobicity, and mobile molecular structure; however, surface tension and porosity are low . In addition, PDMS exhibits excellent heat resistance, anti-oxidation and anti-ozone properties, and durability against ultraviolet (UV) irradiation . Although PDMS coatings show inherently superior FR attributes, its combination with inorganic nano-additives has become necessary. An enhanced matrix-nanofiller interaction provides a cost-effective method and improves some characteristics of nanomaterials. Several characteristics can influence the nanocomposite FR features, such as filler type, size, shape, dispersion percentage, and compatibility with matrix segments . Reduced surface tension and increased contact angle (CA) are also major factors that diminish fouling settlements and bacterial cohesion . Coating techniques with noble metal NPs and metal oxides are possibly effective methods to prevent fouling because of their stability against microbial attacks . Among these materials, Cu O, ZnO, TiO , and Ag NPs are easily prepared from low-cost, highly secured natural sources and thus are applicable to feasibility and advanced studies . A series of uniform in situ, ex situ, and sol‐gel FR polymer nanocomposites with varying degrees of hydrophobicity and hydrophilicity has been investigated in micro- and macro-biofouling assays and in field tests . In addition to surface chemistry, surface free energy and surface topologies affect adhesion mechanism, biofouling, and self-cleaning performance . Considering the high costs associated with fouling problems, toxic AF paints, and mitigation programs worldwide, we understand that the proper utilization of modeled non-toxic FR nanocoatings may provide an environmentally friendly alternative. In the following sections, the main achievements in FR nano-coatings are summarized and arguments are provided to support our conclusion that this nano-coating technology is a well-established platform for the development of ecofriendly coatings that hinder fouling in maritime activities. In this context, in-depth research programs for the design of green and cost-effective fouling preventive measures have been implemented. Alternative solutions have also been investigated. FR coatings composed of nanostructured hydrophilic/hydrophobic surfaces have also been widely explored. Marine biofouling refers to the undesirable colonization of man-made structures immersed in seawater by biotic and abiotic dissolved compounds, microorganisms, plants, and animals . Fouling can be considered a noise to various economic and environmental issues . Numerous fouling organisms may be divided into two groups on the basis of their size: (1) microorganisms, also known as biofilm, slime, or microfouling and (2) macrofouling ( ). Fouling organisms undergo five main stages of succession, namely, adsorption, immobilization, consolidation, microfouling, and macrofouling . A biofilm can be described as a collection of microbial strains embedded in a natural polymer matrix secreted by microorganisms referred to as extracellular polymeric substances . Bacteria that form slime films are present after the attached bacterial cells reproduce by dividing into daughter cells. Each of the resulting daughter cells then grows to normal size and divides again . Microbial strains must form biofilms, which protect them from natural stress factors, such as pH shift, UV radiation, osmotic shock, and desiccation and provide a system with high immunity against biocidal and antibacterial attack . Biofilms are controlled by physical forces to form a complex structure composed of cells, aggregates, and bacterial microcolonies . Moreover, they are randomly located in a highly hydrated polymeric matrix (80–95% water) with pores or voids, through which nutrient flow is measured. Biofilms are further characterized by their heterogeneity in coverage and thickness, and their sizes range from several micrometers to several centimeters. Fouling is inevitable for almost any material immersed in seawater. Fouling development can be divided into three steps . Molecular fouling is the primary cause of the adsorption of organic and inorganic macromolecules, such as polysaccharides and proteins from the marine biosphere. Microfouling involves two steps, namely, primary and secondary colonization . In primary colonization, primary colonizers, including bacteria and diatoms, adsorb onto a conditioning layer via physical forces; this adsorption is reversible because bacteria can be easily removed . Numerous primary colonizers can create extracellular polysaccharides and irreversibly attach to a conditioning film through adhesion. In secondary colonization, secondary colonizers consist of macroalgal spores and protozoa, which are considered parts of a microfilm. An interesting feature of microfouling is that the microfilm mass progresses nonlinearly. Macrofouling is of greater concern than microfouling because of the larger contribution to weight or hydrodynamic loading of the former than the latter. Macrofouling may also be caused by plants or animals and can be classified into hard and soft fouling. As such, ship hull fouling has been extensively explored . Marine biofouling causes environmental disasters and consumes billions of dollars every year in the shipping industry ( ) . Shipping is a prime facilitator of global trade and economic growth because it represents more than 90% of the international commerce . Surface smoothness and drag are decreased when organisms attach to vessel hulls; as a consequence, hydrodynamic weight is increased, top speed is subsequently reduced, and maneuverability is lost . Therefore, fouling increases fuel consumption by up to 40% and promotes the emission of harmful compounds, such as SO and NO , which cause acid rains and soil damage; other toxic substances, such as CO and atmospheric pollutants, are also released and pose serious environmental problems . Approximately 60,000 people lose their lives and €200 billion is lost every year because of the harmful emissions from global maritime navigation . Such an increase in fuel consumption triggered by fouling can increase the total cruise expenses by approximately 77% and can shorten the dry-docking intervals. AF coating technologies have been developed because of adverse ecological and economic impacts of biofouling and defects in nanocoating techniques. Fouling settlements are conventionally inhibited by toxicants released from the applied biocidal AF paints, but their toxicity severely affects non-objective organisms such as dolphins . This market is worth billions of dollars annually worldwide, but the assessment of total costs is difficult as this problem involves numerous industrial factors; the poor efficiency of AF efforts also requires frequent and continuous counter measures . Fouling costs are related to several major effects, including physical material damage, mechanical interference, biological competition for resources, such as food and space, environmental modification caused by the colonization of culture infrastructure, and increased friction and drag resistance on equipment. Equipment maintenance and loss directly contribute to production costs; in particular, biofouling increases capital cost and accounts for approximately 20% of overall fouling in energy generation . In addition to interferences in structural functions, the corrosion of metallic surfaces may be accelerated or paint coatings protecting the surfaces from rusting may become damaged. Furthermore, the treatment of wastewater contaminated with biocidal AF additives is an emerging cost factor because the amount of released biocides increases; as such, these additives should be removed . Full-scale ship tests have been conducted to determine the effect of fouling on the drag of AF coatings and have revealed that even slime can significantly increase drag to a dangerous extent. Therefore, technologies should be improved to protect ships from fouling, to extend the period between dockings, and to save time and costs However, statistical data on the volumes and value of AF paints have yet to be provided. According to a report from industrial representatives who provided at least 56 data validated by European Chemicals Agency in 2009, the global market consumed 904 million litres of AF paints costing approximately €3.5 billion in 2012 . Market analysis has forecasted that the global market of marine coatings will increase twofold by 2018 and exhibit an annual growth rate of >11% from 2013 to 2018 despite the ongoing severe crisis of global shipping; for example, German ship owners, who are the largest stakeholders in the worldwide container shipping trade with 1600 container vessels, reduced the national merchant fleet by 200 vessels between 2012 and 2013 . This phenomenon also occurred because ship owners focused on cost reduction to address the historically high costs of oil-derived fuel. Another issue involves the economic sustainability of elastomeric coatings. When these coatings are applied to large vessels that consume tens of thousands of US dollars of fuel per day, the large return on investments associated with energy savings largely justifies the high capital expense to apply new silicone-derived coatings on dry dock. For instance, ePaint, which is a small US company, manufactures 10,000 gallons per year of ZnO-based (5% as an active biocide) photocatalytic AF formulated paints and SeaNine 211 to repel hard-shelled organisms through the formation of a thin H O layer around the hull . The development of fouling prevention solutions is closely associated with increased marine transportation and shipping of people and cargo. The harmful effects of fouling on ship performance have been documented, and attempts to utilize AF systems began in the 7th century BCE ( ). Early written testimonies on the first AF attempts were provided by Phoenicians, Carthaginians, Greeks, and Romans, which were ancient Mediterranean civilizations . A lead-sheathed timber was first used in Phoenician galleys in 7th century BCE. In 5th century BCE, arsenic and sulfur-and-oil mixture coatings were utilized to prevent fouling . In the 3rd century, lead sheathing was secreted with copper nails by Romans and Greeks . Several centuries later, the Vikings reportedly used “seal tar.” In the 13–15th centuries, pitch was widely utilized to prevent fouling and usually mixed with oils or tallow as used by Christopher Columbus during his expedition to the New World . Hide sheathing was utilized in the 14th century. In the 16th century, ships were sheathed with woods placed above a coated film of animals' tar . Although lead sheathings were the most commonly used among AF systems, their use resulted in severe rusting of steel materials, such as ship rudders; thus, the use of lead sheathings was banned by the British in the 17th century . Wooden sheathing was used again, coated with many blends, such as iron and copper spike fillers in tar or grease. This approach was followed by copper sheathing, which was an important stage in the development of AF. In the first patent published in 1625 by William Beale, a British man, who based his formula on cement admixture including iron and copper compositions, copper compounds were used as AF agents . In 1758, AF sheathing with copper was investigated in England and yielded more efficient results than lead sheathing did; therefore, the technology had been extensively utilized by the governmental fleet since 1758 and soon became the preferred approach to protect wooden ships hulls . However, the advent of steel ships and the subsequent corrosion problems associated with metallic sheathing have prompted ship owners to stop the application of this technology and focused on the use of AF compositions. In the mid-1800s, William John Hay isolated iron hull from metallic toxicants by using a non-conductive varnish and copper oxide, mercury oxide, and arsenic dispersed in high amounts of resins . Moreover, Mallet recorded a patent of AF coating, a blend of varnish and toxicants with high solubility, but the developed system suffered from abrasion and uncontrollable solution rate . In 1860, hot-plastic paint was developed by using copper compounds embedded in rosin admixture as a second layer after an anticorrosive layer was applied. Nevertheless, these coatings entailed high cost, yielded a comparatively inefficient performance, and provided a short life span . Paint for the bottom part of a ship was first manufactured by the Americans in 1908 after a spirit varnish paint was successfully developed; many paint developments were based on mercury oxide dispersed in graded grain alcohol and turpentine; as a result, the lifetime of the manufactured paint was approximately 9 months . According to a research study conducted by the US Navy in the early 20th century, rosin demonstrated advantageous properties, such as low cost, abundant source, and successful replacement for the rare and costly shellac. Hence, rosin was blended with poisonous compositions of mercury or copper for enhanced performance; for instance, rosin was combined with synthetic petroleum-derived resins, which enhanced their mechanical characteristics but caused concerns on safety and health . Hot-plastic paints are difficult to apply; thus, cold-plastic paints were developed for convenient application in ship sites. These paints resisted fouling, and the period between dry dockings was doubled. In the mid-20th century, the exacerbated poisoning of organotin compounds, especially tributyltin (TBT), was successfully recorded by van der kerk's team, which presented excellent AF properties of the TBT moiety. Kiil et al. modeled the working mechanisms of these paints; in this study, a TBT biocidal copolymer hydrolyzes and erodes the toxicants in the marine environment. TBT compounds were initially utilized in advanced copper AF coatings and progressively applied as a whole AF system, which underwent “free association form” inside the matrix. TBT self-polishing coatings (TBT–SPC), which are composed of TBT acrylate esters as AF coatings, were developed in 1958 James patented a SPC based on copolymers of TBT acrylate and methyl methacrylate in 1964 . In general, TBT–SPC technology addressed the rapid leaching of TBT-bearing compounds incorporated in free-associated form; thus, long-lasting AF protection was provided ( ) . TBT–SPC provides various advantages, such as smooth hydrodynamic profile during sailing, decreased fuel consumption, and constant biocide release rates over time. The hulls of more than 70% of the worldwide fleet of ships were painted with TBT–SPCs because of the success of this technology in the 1990s. Unfortunately, the degradation rate of TBT-bearing biocides was slow in the sea water column; as a consequence, lipophilic compounds have become readily bioavailable to non-target species . Once these compounds are partitioned into cellular membranes, they disrupt essential metabolic and enzymatic functions, such as energy production through oxidative phosphorylation; as a result, organisms die . At considerably low concentrations, these compounds cause a series of sublethal effects, such as endocrine disruption leading to sexual disorders, including imposex . Moreover, the International Maritime Organization (IMO) reported that these compounds accumulate in mammals and debilitate immunological defenses in fishes . The IMO prohibited the application of organo-tin-based AF coatings on all vessels on January 1, 2003 ( ) . Copper-based tin-free AF paints were developed, but the use of these paints has been under scrutiny in several countries and has been subjected to environmental restrictions in terms of copper release rates. Despite the high toxicity of copper to numerous maritime communities, common algae exhibit a high degree of tolerance and can easily attach to the surface. Thus, booster biocides were added to the copper AF coatings to overcome these problems and afford highly biocidal coatings, such as Irgarol 1051 and diuron . These paints often contain herbicides, which slow down the growth rate of photosynthetic organisms. As such, legislation in certain countries has been implemented to regulate the application of these booster toxicants because of their exacerbated poisoning effect even at small percentages and non-biodegradability in water . The erosion of AF vessel coatings causes surface damages, such as rusting and discoloration, complete leaching of ingredients, and short dry-docking intervals. With the growing concern on the harmful side effects of biocidal coatings on the environment, studies have been conducted to develop non-biocidal and non-stick FR coatings . The functionalities of polymer brushes enabled their potentials in AF paints, especially for the shipping industry. Marine coatings based on polymer brush technology have been developed for FR applications with eco-friendly effects . These polymer brushes inhibit marine biofouling with insertion of specific functional groups, thereby enhancing the anti-adhesion and anti-bacterial non-toxic efficiencies. Three techniques have been utilized in designing AF polymers: (1) degrading or killing of biofoulant; (2) fouling resistance (preventing biofoulant attachments); and (3) fouling release (reducing the strength of biofoulant adhesion) . Prevention of the adsorption protein, cells, or microorganisms is largely affected by controlling polymer structures and coating process techniques. The criteria for non-fouling polymers include the existence of hydrogen bond acceptor and polar functional groups, and the absence of net charge or hydrogen bond donor groups . In this section, we focused on nano-structured functionalities and fidelities, and surface properties of synthetic polymers and their derived nanocomposites that display eco-friendly non-fouling performance. The surface properties are significantly affected by the brushy structure of polymeric coaters and AF operating conditions. Amphiphilic polymer nanocomposites are dominant fouling-resistant polymer coatings. These materials possess low polymer–water interfacial energies. The high hydration degree increases the energetic penalty of removing water when bio-foulants attach to a surface; as a result, the surface becomes resistant to protein adsorption and to settlement of fouling organisms . Several hydrophilic polymer nanocomposite coatings, such as polyethylene glycol (PEG), hydrogel, zwitterionic, and hyperbranched polymers, have been developed as marine AF coatings. PEGylated materials have been applied because of their strong AF tendency against cell and protein cohesion. PEG is non-toxic, highly hydrophilic, and neutrally charged. PEG presents weak basic ether linkage and reduced interfacial energy with water (5 mJ/m ), and these characteristics facilitate its good AF performance . Maximizing the surface hydrophilicity and minimizing the attraction forces (caused by formation of hydrogen bonds with water) with fouling community are the mechanistic key issues of PEG . The AF efficiency of PEG with long chains is higher than that of oligo(ethylene–glycol) (OEG) coatings. PEG prevents spore and larval attachment, and OEG-modified surfaces reduce the adhesion between spores and surfaces; as a result, PEG is easily released through minor hydrodynamic forces ( ) . PEG coatings suffer from rapid auto-oxidation in the presence of oxygen and transition metal ions, thereby leading to the decomposition of coatings. However, the formation of PEG nanocomposites significantly overcomes this drawback . For example, PEG-ZnO nanocomposites reduce the protein adsorption by 30%. This FR nanocoating is more effective than the silicone hydrogel coating with Ag-polyvinylpyrrolidone that exhibits 28.2% reduction in protein adsorption. Furthermore, most bacteria attached on ZnO-PEG nano-surfaces are eliminated after 4 h of incubation, whereas those on Ag-polyvinylpyrrolidone ones are effectively eliminated after 8 h . PEG-grafted multiwall carbon nanotubes (MWCNTs) nanocomposites have also been synthesized and used as an AF agent enriched with nanofillers to prepare nanohybrid polyethersulfone (PSf) surfaces. Therefore, PEG- -MWCNTs/PSf nanohybrid systems with effective hydrophilicity and AF performances can be applied to water purification technologies . These nanocomposites, especially with 1.5% PEG- -MWCNTs, are also promising materials because of their enhanced hydrophilicity, surface area of nanofillers, AF capability, and mechanical strength . Hydrogels are composed of hydrophilic polymer networks and are distinguished from solid materials by their high water composition. These hydrogels are porous, three-dimensional network structures that contain 80% water; they are also non-toxic, highly elastic, and inert against bio-macromolecule adhesion that may resist the irreversible protein fouling . These key factors inhibit coupling formation between the cement protein and the hydrogel surface, thereby prohibiting fouling attachment. Hydrogels with long PEG chains outperform coatings with short PEG chains because of their capability to prevent the attachment of fouling organisms. Thiol-ene click reaction has been facilitated to prepare PEG hydrogel coatings with different structural compositions, various PEG lengths, vinyl end groups, and thiol cross-linkers . Although hydrogels provide several advantages, such as efficient mass transfer, hydrophilicity, and stimulus- and cell-induced responsiveness, their widespread use is hampered because of their poor mechanical properties and brittleness upon dehydration . Different strategies have also been used to solve these drawbacks; forming hydrogel nanocomposites, such as nanoclay in hydrogel polymers, is the optimum option to create mechanically robust hydrogels ( ) . Nanoclay, which is composed of inorganic silicate NPs, offers a high interfacial area and improved mechanical and rheological properties because of the reinforced polymer matrix . A hydrogel nanocomposite of carboxybetaine methacrylamide and 2-hydroxyethyl methacrylate and clay NPs exhibits high AF and mechanical characteristics . Furthermore, nanocomposite hydrogels with interpenetrating polymer network structures based on PEG methyl ether methacrylate-modified ZnO NPs and 4-azidobenzoic agarose exhibit excellent mechanical AF performance with negligible cytotoxicity . These hydrogels manifest high resistance to protein adsorption, cell cohesion, and bacterial settlement; therefore, they present high non-fouling behavior. Zwitterionic polymers have been widely explored as a new generation of fouling-resistant materials. These polymers comprise positive and negative charges, which produce more potent and stabilized ionic bonds with water molecules than those created from other hydrophilic materials . These zwitterionic polymers are promising fouling-resistant materials because of their excellent hydration capacity with strong hydrophilicity. Advanced easy-cleaning system has also been designed successfully by tethering a zwitterionic poly(4-(2-sulfoethyl)-1-(4-vinylbenzyl) pyridinium betaine) (PSVBP) onto polyamide (PA) surfaces . A PA- -PSVBP exhibits a prominent short-lived fouling prevention and salt-responsive property and becomes unsuitable for long periods. These surfaces can regain self-cleaning abilities by rinsing them with brine . Zwitterionic nanocomposites with controlled NP dispersion and without agglomerations also enhance the fouling resistance of hydrophobic materials ( ). Zwitterionic polymer brushes are attached to indium tin oxide (ITO) substrates. Photochemically grafted hydroxyl-terminated organic layers serve as an excellent platform to initiate fouling attachment and exhibit excellent AF properties ( B) . Exfoliated montmorillonite nanocomposites with catechol/zwitterionic quaternized polymer were developed, and they present AF properties and resistance against physical damage . Furthermore, the modification of zwitterionic coating films with SiO NPs can significantly enhance the AF performance . Eco-friendly silver-zwitterion nanocomposites were fabricated, and they exhibit considerable antimicrobial activity and surfaces with anti-adhesion characteristics . Highly branched coating matrixes with hydrophilic terminals have been extensively investigated to provide biofouling resistance to surfaces. Hyperbranched polymers are likewise advantageous for FR applications because of numerous terminal units, branching density, high solubility, low viscosity, and low volatile organic compound (VOC) . These polymers can form extremely hydrophilic surfaces resembling a hydrogel surface during water contact. Other advantages of these polymers are the simple preparation through single-pot technique and purification; as such, the resulting polymer is a low-cost material . Hyperbranched polyethyleneimine with significant surface topology characteristics shows excellent AF performance and high resistance against non-specific protein adsorption . Consequently, hyperbranched based nanocomposites provide surfaces with compositional and topographical complexities that hamper any favorable interactions with adhesive biomacromolecular segments secreted by marine organisms. For instance, sericin is a natural hydrophilic protein and contain polar –OH, C=O, and –NH– side chains . This protein is applied as an FR film. AF tests have demonstrated that sericin thin-film composite (TMC) shows a more efficient fouling resistance than commercial composite surfaces do ( ). This superior performance inevitably results in an increased electrostatic repellency by TMC of sercine toward fouling organisms; as a consequence, foulant molecules are adsorbed on membrane surfaces to a low extent . Deka et al. demonstrated that hyperbranched polyurethane incorporated with Ag nanocomposites exhibits higher AF properties than linear analogs. The Ag particles cause cellular damage through production of reactive oxygen species. Nanocomposites of catecholic hyperbranched polyglycerol with TiO NPs were developed, and they exhibit significant AF performance . By applying this design, the protein adsorption deceases with the increase in catechol functionality and with good dispersion of TiO NPs along polymer matrices. Non-stick FR coatings are successful alternatives that inhibit the attachment of marine organisms by providing surfaces with minimum drag resistance and maximum smooth topology . FR coating exhibits promising characteristics as follows ( ) : Other advantages include copper-free composition, lower weight than standard AFs, and high solid content . In the early 1970s, FR coatings were manufactured simultaneously with TBT–SPCs. However, the development of FR technology has been limited in few research laboratories because of the efficacy and commercial benefits offered by SPCs. In the 1980s, FR systems were commercially developed because the demands for the rapid delivery of passengers and cargoes have increased and vessels with enhanced design speeds have been developed. This initiative also coincided with the move to ban TBT AF paints. In March 1996, FR coating was first used for a full fast ferry application on a 33-knot aluminum catamaran . With FR coating, the performance of the catamaran improved, with an increase in speed of 2–3 knots under all weather conditions compared with the performance of a newly applied low-copper TBT-free AF system, which was also widely used in those days . The commercialization of micro-topological surfaces for large ships is limited mainly by painting costs and impractical applications . Following the ban of organo-tin compounds by the IMO in 2003, the total sales of FR coatings have increased significantly. A hypothetical complete conversion to FR coatings is estimated to save 70,000,000 tons of copper-based biocides, 6,000,000 tons of booster biocides, and 20,000,000 L every year globally . FR coatings possess high volume solid content (70%) compared with SPC (40–50%) and CPD coatings (50–60%); thus, FR coatings exhibit low VOC . FR coatings also require one top layer whereas biocidal AF coatings require two or three layers. This requirement of FR coatings lowers the paint consumption, saves cost, and reduces docking time. FR coatings possess long life efficiency (5–10 years) and do not release biocides such that they are unaffected by biocidal legislation. The potential ultra-smooth surface of FR coatings compared with other technologies result in low drag resistance and maximum fuel savings . The fuel consumption of tin-free SPC coatings and FR coatings were compared through application on the hulls of two vessels by Corbett et al. . The results illustrated that FR coatings decrease speed-adjusted fuel oil consumption by 22%. If similar fuel efficacies are achieved in the international fleet, then the annual fuel consumption, fuel expenditure, and carbon dioxide emission are expected to decrease by 16,000,000 metric tons, $8.800,000,000, and nearly 49,000,000 metric tons, respectively . Data on AF system prices are not easily accessible because coating manufacturers are unwilling to openly share such information. The total cost accompanied with biocidal or FR coatings depends on the liquid paint, off-hire for the vessel, dry-dock hire, hull cleaning, surface preparation, and roughness costs . Roughness increases by 10%, 4.8%, and 1.4% for hybrid SPC, SPC, and FRC, respectively, after a 5-year service life because of fouling. Eliasson indicated that FR coatings are less costly than biocidal AF paints even after 5 or 10 years. FR terminology, which is awarded by fluoro and silicon polymers, is also applied by minimizing the adhesion strength between material surfaces and fouling organisms. Thus, organisms can be easily removed through mechanical cleaning or hydrodynamic stress during shipping. Fluoropolymers form non-porous and smooth surfaces with minimized surface tension and good anti-adhesion performance toward fouling organisms . These substances also create a weak interface with marine adhesives, and fouling attachment can be simply disrupted by shear stress. Fluoropolymers are hard, glassy materials with glass transition temperature ( ) that exceeds room temperature (RT). These polymers are applied to thin layers (75 µm) to produce non-stick FR coatings ( ). However, these materials exhibit limited mobility because fluorine atoms form stiff molecular structure and thus inhibit bending at the back bone chain . In addition, high critical stress is necessary to induce failure on the adhesive–substrate joint because fluoropolymers yield a higher bulk modulus than elastomers do . Therefore, fouling organisms that accumulate on surfaces are not easily released. Thin coatings require very high removal forces because stress and force are inversely related to thickness. As an attempt for modification, fluorine compounds can be mixed as a polymer moiety or as curing agent to the silicone coatings. These compounds possess lower surface tension than silicone; this condition allows an interfacial immigration of fluorinated moiety to the surface and reduction of the coating surface tension, thereby providing high chemical stability in water . However, fluorinated silicone coating induces fragile layers, which are prone to splits and cracks. The drawbacks in applying fluoropolymers as FR coatings have prompted modern research toward elastomeric silicone FR coatings. Silicone derived its name because of the structural resemblance between R SiO and R CO of ketones. Structural studies on bond lengths have indicated that the lengths of the siloxane backbone bond and carbon–carbon bond are 1.64 and 1.53 Å, respectively; thus, the longer siloxane bond rotates simply. The siloxane angles are 143° and 110° that correspond to Si–O–Si and O–Si–O, respectively; these angles are higher than the tetrahedral bond angle of 109.28° in hydrocarbon chains ( ) . The rotational energy for free rotation of Si–O and Si–CH in PDMS is lower than that for C–O and C–CH in hydrocarbon polymers. This condition allows high structural mobility and low for the silicone backbone ( ). The PDMS bond angles and the existence of an alpha-helix configuration form an open structure and flexible bonds that rotate through a 180° state and a low rotational barrier. As a result, isotropic protection that accounts for the lowest (around −120 °C) of any other polymer and low interfacial free energy (around 20–24 mJ/m ) is provided . Another major factor that contributes to the increased flexibility of PDMS is that O atoms in the skeletal siloxane bonds are not encumbered by any side chains. Moreover, all silicone backbone bonds yield a very low torsional barrier for free rotation, which accounts for the mobile configuration. The linear PDMS is liquid at RT because its Tg is less than its RT. Methyl groups (–CH ) are characterized by very intense σ bonds; as such, these groups protect the –Si–O– groups, which are reactive because of their polar nature . Organo-silicon polymers, particularly PDMS, represent a marked niche among specialty (co)polymers. These advantages provide PDMS with superior fouling anti-adhesion characteristics, surface inertness, hydrophobicity, high heat resistance, and excellent insulation property ( ) . PDMS also presents viable options in several hazardous and polluting industries because of their environmental-friendly nature. With regard to market value, silicone products reached US$ 13.5 billion in 2010 and are expected to reach US$ 17.2 billion in 2017 because of their extraordinary properties . PDMS can inhibit the attachment of fouling functional units and release their attachments even at low sailing speed . Kohl and Singer tested the resistance of silicone coatings to marine fouling and found that fouling capability is correlated with the square root of the result of multiplying surface tension ( ) in the modulus of elasticity ( ) ( ). The relative adhesion forces decrease with the decrease in either surface tension or elastic modulus. Minimum adhesion properties coinciding with the lowest have also been investigated; results indicate that the is also a major factor in silicone FR surfaces . Although PDMS has been considered non-biodegradable by fouling organisms, its biodegradability data are insufficient and the biodegradation by living organisms have been poorly studied . PDMS biodegradability mechanism gives rise to the formation of dimethylsilanediol, carbon dioxide, and inorganic silicate as illustrated in . Another major factor is that increasing the PDMS molecular weight reduces the interfacial tension of the surface and thus enhances hydrophobicity and self-cleaning performance of the FR paints . Innovation of advanced materials, especially the functional hybrid nanocomposites, has become an urgent need to meet the promise of improved and often unparalleled characteristics at a reduced cost. Hybrid organic–inorganic nanocomposites provide the opportunity to combine the properties of both materials and generate advanced chemical, physical, and biological properties. These engineered nanomaterials have resulted in a technological leap in various fields, such as electronics, biological applications, pharmaceuticals, renewable energies, and coatings . The availability of structure–property relationships in AF coatings through good dispersion of controlled NPs in polymer matrices can ameliorate bio-static, thermal, anticorrosive, self-cleaning, and AF properties . After mixing NPs with large polymers, necklace-like structures of trapped NPs into polymer coils are obtained with altered rheological behavior . At high NP concentrations, a few NPs settle in the coil periphery and form bridge aggregates through sharing; these aggregates can weaken the coil junctions . This problem is due to the electrostatic repulsion between the adsorbed and free particles. The opposite situation, in case of well dispersion, is reported . PDMS-based nanocomposites have been extensively investigated because of their facile preparation and environmental stability. Inorganic nanofillers are one of the promising solutions for superior PDMS-based nanocomposite coatings. Various NPs, such as SiO , Al O , Fe O , ZrO , and TiO , have been introduced to modify FR membranes based on reinforcing the physical, chemical, and mechanical performance . However, an occurrence of a swap between self-cleaning of PDMS paints of vessel bottom surfaces and durability controls the morphology, size, and percentage of nanofiller insertion. Biocidal AF paints function by possessing an acid binder and dissolved biocides, which undergo leaching and releasing into sea water to kill the fouling organisms ( ). In turn, silicone-based FR paints exhibit non-leachant property and are thus environmentally controversial. These silicone-based FR paints reinforce the bonds of fouling attachments, thereby releasing the attached organisms back to the sea without killing them ( ). Bactericide-hydrophobic nanofillers, such as MWCNT, natural sepiolite, and organo-modified montmorillonite, have also been incorporated in PDMS FR coatings . Insertion of hydrophobic nanofillers in PDMS increases CA, decreases roughness, reduces the surface free energy, minimizes elastic modulus, and improves the self-cleaning characteristics. Incorporating a small amount of MWCNT (less than 0.1 wt.%) within the silicone coatings enhances the fouling release performance against fouling organisms without considerably changing the bulk properties . Insertion of natural sepiolite Si O Mg (OH) (H O) ·8H O NPs in commercial vinyl terminated PDMS (Dow corning Sylgard® 184) reduces the modulus of elasticity and the relative adhesion of fouling communities. The high surface area of sepiolite is attributed to its micropores, channels, fine particle size (5–10 nm in diameter), and fibrous nature . This clay structurally forms blocks and channels extending in the fiber direction. Some Si–O–Si groups in the silica tetrahedral sheets are present as silanol groups (Si–OH) as a result of imperfections. These groups also facilitate the covalent modification of these particles. However, the FR performance of the filled nanocomposites only affected anti-adhesion of definite biofouling organisms such as zoospores and Ulva . Although, the initial hydrophobicity was changed and the adhesive tendency of adult barnacles was reduced by 50% after addition of 0.05% MWCNTs, the tailored PDMS/MWCNT nanocomposites did not affect the durability characteristics . The strong CH–π interactions between silicone methyl side chains, which render mobile molecular configuration and the π-electron-rich surface of MWCNT, are responsible for the unexpected behavior. Modeling of fluorinated carbon nanotubes (CNTs) for silicone-based FR coatings is a modern strategy for high FR performance and improved self-cleaning capability . Irani et al. investigated PDMSs filled with MWCNTs and fluorinated MWCNTs at different concentrations of 0.05, 0.1, and 0.2 wt.%, and they found that these materials can be successfully fabricated and used for FR treatment. Both structural types of nanotubes alter the nanocomposite surface characteristics; however, the fluorinated MWCNTs/silicone composite model reveals superior FR performance in ship hull. These studies indicated that the fluorinated MWCNTs/silicone composite may decrease the strengths of pseudo-barnacle attachments by 67% compared with pure MWCNTs/silicone nanocomposite models, which exhibit reduction of only 47% . However, single-wall CNT, graphene, and their modified models are promising in the reinforcement of self-cleaning, fouling anti-adhesion and mechanical characteristics of the designed FR-based PDMS nanocomposites . Other terminology involves insertion of low surface tension fluid such as oils which undergo air interference migration and create weak surface bonds resulting in anti-adhesion capability . The additive concentrations should be studied carefully because brittleness, cracks, and low durability result when some of these materials are added at high concentrations . Lowered hydrophobicity is regained after immersion in water, indicating that the silicon-based compounds have renewable properties. Webster et al. extensively investigated self-stratified cross-linked polysiloxane–polyurethane coatings and reported a considerable increase in the critical stress removal of some species of acorn barnacles . PDMS-poly(caprolactone) surfaces were reported to decrease the free energy of the modified coatings from 21 mJ/m to 16 mJ/m which improve biofouling detachments. Several nanocomposites possess superhydrohobic characteristics i.e. a CA of more than 150° is applicable for FR coatings of ship hulls . The superhydrophobic characteristics of three polysiloxane-reinforced silica fumed NPs composite paints with different structural compositions were tested by Scardino et al. . The three nanocomposite coatings had different roughness measurements where an overlapped micro-nano roughness was recorded for the first and second coating while the third coating records only with nano-roughness ( ). Three attachments of different biofouling organisms; motile ( ), subsequently motile ( ) and non-motile ( ) were assessed after considering the same CA for the three coatings. Results clarified the superior FR performance of the third coating. The study reflected that not only the surface hydrophobicity is the dominating factor for fouling prevention but also the topology of the surface, roughness length scale, and the percentage of entrapped air at the coatings. Fabricating superhydrophilic/superhydrophobic patterns without toxicants is very challenging. TiO −PDMS composite films have been fabricated by sol-gel method followed by curing with UV light. These composite films could recover their initial wettability. Superhydrophobic TiO nanowire coatings have been reported using the low-surface energy PDMS modification which can resist photocatalytic breakdown caused by the embedded TiO NPs . Sankar et al. reported the FR performance of the CuO, cetyl trimethyl ammonium bromide (CTAB)-CuO and ZnO nanofillers when incorporated in PDMS coatings . The superior AF properties were approved for CTAB-CuO with low concentration (0.1 wt.%) NPs because of the improved hydrophobicity, nanofiller dispersion, surface smoothness, and antibacterial properties . Several parameters can influence the AF performance of the utilized NPs such as utilization of capping agents, ultrasonication, nanofiller dispersion, particle size, and morphology . Two-dimensional ZnO-containing porous boron-carbon-nitrogen sheets dispersed in PDMS form a stable coating over aluminum alloys with water CA of 157.6°, which demonstrates oil fouling-resistant superhydrophobicity and self-cleaning. Previous studies indicated that the coating remains stable under high humidity at a thickness of 30–40 μm without cracking when PDMS matrix is incorporated with SiO NPs . A series of non-toxic nanostructured coatings based on PDMS-polyurea/nanoclay segmented copolymers has been investigated, and these coatings exhibit high mechanical strength coupled with enhanced hydrophobicity, self-cleaning, and FR characteristics . Several metal oxide NPs of TiO , Cu O, and Ag exhibit unique bactericidal activities caused by their chemical properties, quantum confinement effects, and large surface area-to-volume ratios. Selim et al. investigated the real AF applicability of the ecofriendly silicone filled Ag, PDMS/Cu O, and PDMS/TiO nanocomposites with different concentrations ranging from 0.01% to 5% nanofillers. PDMS/Ag (0.1%) and PDMS/Cu O (0.1%) nanocomposites showed enhanced properties caused by the good dispersion and complete disagglomeration, which improved surface roughness, and hydrophobic wettability . The PDMS/Ag nanocomposites (0.1% nanofillers) exerted the most profound self-cleaning and FR effects after comparison with many approved systems; (1) the hydrosilated cured Sylgard® 184 (Dow Corning Co.) and the condensed cured RTV11 (Dow Corning Co.) ; (2) Sylgard 184/MWCNT ; and (3) PDMS nano-filled with 0.1% cubic Cu O coating . Among the four FR models, PDMS filled with 0.1% Ag achieved the maximum hydrophobic characteristics and the minimum roughness and interfacial tension . These properties were confirmed by applying the nanocomposites in natural waters of the Red Sea to be a promising environment-friendly material ( ). The exposure of Ag NPs to the aqueous environment promotes the significant release of Ag ions . Selim et al. developed smart photo-induced silicone/spherical single crystal TiO nanocomposites as a novel terminology for solar light boosted FR paints of vessel bottoms. They developed photo-smart silicone nanocomposites aimed to enhance the durability and self-cleaning AF performance through green pathway based on the perfect usage of solar light irradiation. The results indicated the considerable distribution and complete disagglomeration of TiO NPs in the silicone matrix at controlled low concentrations of up to 0.5% loadings . Incorporating spherical single-crystal TiO NPs remarkably influences the coating roughness, wettability, and FR performance when these materials are pulsed by UV–vis irradiation. Self-cleaning through increased superhydrophilicity is one pathway for FR coating ( ) . We compared the static CA measurements for Sylgard®184 silicone film as a control sample , PDMS/nanorod TiO composites , PDMS/anatase spherical TiO nanocomposite , and our newly designed PDMS/spherical single crystal rutile TiO nanocomposite with its superior concentration (0.5% TiO NPs). We found that the performance of FR is superior to that of the tailored nanocomposites. Comparative results based on CA varied after pulsation by UV irradiation; CA remains unchanged after UV irradiation for unfilled Sylgard®184 but decreases from 104° for Sylgard®184 to 77° for PDMS/nanorods TiO composites and to 80° for PDMS/anatase spherical TiO nanocomposite . However, the CA in our developed model (0.5%) decreased to 10° after UV irradiation, indicating superhydropilicity and increased self-cleaning capability and thus higher AF performance ( c). These results were confirmed by the free energy of hydration (Δ ), which exhibits superhydrophilicity (−144.49 mJ/m ) and inhibits fouling cohesion on the ship hull surface. These results were confirmed by biological assays and field exposure tests, which revealed the improved durability and self-fouling-releasing performance. The tailored nanocoatings are ecofriendly and are possible economic long term solutions which could replace the harmful biocidal AF materials. Future efforts should be spent to design novel FR nanocomposite coatings for economic savings and prolonging longevity. CA (θ) is commonly used to express the wetting behavior of a solid surface and is highly significant in all solid-liquid-fluid interfacial phenomena. Wettability is also expressed by the surface free energy of solid surfaces. The FR technology is based on binary eco-friendly modes of actions as follows: In 1805, Young first described the intrinsic CA of a liquid droplet on an ideally flat and homogenous solid surface ( a). Young’s equation for computing interfacial surface tension of a solid is as follows: is Young’s CA. , and refer to the interfacial surface tensions with , , and as vapor, liquid, and solid, respectively. The attachment of marine community has been historically related to critical surface free energy , as introduced by Fox–Zisman’s theory . This theory is used to determine for any substance with surface energy of less than 72 mJ/m , by using CA data. This approach is suitable for polymer–liquid systems. This approach is related to the surface tension of the used liquid , where is obtained through determining the CA of different liquids on a solid substrate. A plot between the angle cosine and of the liquids assuming θ = 0° (the best linear fit) . only is equal to for total polar surfaces and liquids; these issues limit its application . According to Fox–Zisman’s theory and using Owens–Wendt–Rabel–Kaelble’s method (also called the geometric mean method), the total surface free energy ( of the prepared films and nanocomposites can be determined. This approach suggests that the is equal to the sum of a dispersive and a polar components. can be calculated from the CA measurements of different solvents, such as H O and diiodomethane . The following equations were used: and γ represent the CAs and free energy determinations of the utilized solvents, respectively. and represent the dispersion surface tension of the testing liquids and surfaces, and and correspond to the polar surface tension of the testing liquids and surfaces, respectively. Baier investigated the mechanism of microfouling attachment and was first reported in 1968 based on the relationship between surface tension and the strength of attachments. Baier's curve exhibits the minimum attachment strength from 20 mJ/m to 25 mJ/m , which is caused by forming weak boundary layers between the coating surface and the adhesives of fouling strains ( ). A lower tendency of fouling strains to attach to ship hulls was reported for hydrophobic coatings with minimum surface tension. The weakly attached fouling organisms can be easily eliminated through the movement of vessels . Three non-stick FR nanocoating techniques toward fouling organisms and their environmental relevance were extensively addressed in this review; these approaches are (i) superhydrophobic, (ii) UV-induced superhydrophilic, and (iii) amphiphilic modes. Owing to the unique properties of graphene-based materials, we introduced a new prospective avenue toward their application in self-cleaning FR nanocomposites for maritime navigation below. With their wide applications and unparalleled easy-cleaning, non-stick, and ecofriendly characteristics, water repellent (superhydrophobic or lotus effect) surfaces gained worldwide attention . The lotus effect is common in natural environments, such as lotus plants, rice plants, and butterfly wings, which are characterized by their high hydrophobicity and self-cleaning surfaces . Ultra-water-repellent surfaces with water-CA of >150° and where water droplets continuously roll at low-contact-angle hysteresis of <5°, have been extensively investigated. These surfaces are related to wettability mechanism and correlated with surface roughness and surface free energy ( ). Methyl side chains and the fluorine atoms are not the requirements for lotus effect of natural surfaces and subsequently the very low free energy is unnecessary for water repellency in nature. However, controlling the surface morphology and topology from micro- to nanoscale is the key factor for these natural superhydrophobic characteristics. Lotus effect surfaces can be produced via two facile strategies: (1) designing ultra-smooth surfaces by using low surface tension materials; and (2) designing rough surfaces modified with low-surface-energy materials. Superhydrophobic surfaces have been investigated in terms of biofouling prevention, decreasing friction resistance, anti-corrosion performance, coating easy-cleaning, and snow anti-sticking . Several synthetic techniques such as sol-gel technique, self-assembly, electrochemical deposition, and plasma treatment were used to design self-cleaning materials with lotus effect performance . These surfaces possess surface free energies near the Baier's minimum. These energies decrease bacterial attachment on ship hulls. Furthermore, ship movement results in the elimination of the weakly bonded foulers by shear stress. Hence, employing hydrophobic FR surfaces of PDMS and fluorinated polymers with minimum surface tension presents potential use for highly-active ships and fast ferries. Numerous techniques were used to produce these potential surfaces since the first synthetic superhydrophobic surface was introduced in the mid-1990s . Many researchers studied the chemistry of non-stick and anti-adhesion coatings based on Cassie-Wenzel hypothesis . Recent advances in the development of water repellent nanocomposites for fouling prevention have been reported . Superhydrophobic coatings prepared on various substrates have been reported using two different approaches: (i) application of suspension of suitable NPs in PDMS over a smooth surface, and (ii) infusing a lubricant liquid into a porous rough topology called slippery liquid-infused porous surfaces (SLIPS) . These processes can result in the formation of a rough hierarchical binary microstructure and nanostructure with low surface energy and high water-repellent properties. Superhydrophobic PDMS-silica nanocomposite coating can become oil repellent (olephobic) through surface modification by using fluoroalkylsilane (FAS) via a combination of their simple and scalable deposition processes, low surface energy, and roughness characteristics of aggregates . Incorporating hydrophobic NPs into a polymer matrix is a practical method to artificially mimic lotus leaves. Extensive studies have been performed to fabricate superhydrophobic surfaces based on inorganic fibers, such as SiO , ZnO, TiO , and Fe O -filled carbon nanofibers. This technique provides a versatile method to coat substrates of various sizes, geometries, and surface chemistries. Zhang et al. fabricated several ZnO superhydrophobic surfaces by utilizing oxidation reaction between Zn powder and H O to produce multi-scale structures, from microstructure to nanostructure. PDMS coatings incorporated with ZnO/SiO NPs exhibit outstanding superhydrophobicity and lotus effect characteristics . Superhydrophobic surfaces with a water CA of 158.3° are readily prepared by mixing silicone resin, aminopropyltriethoxysilane, and Fe O NPs . Wen et al. prepared superhydrophobic coating by modifying PDMS with micro-CaCO /nano-SiO composite particles with hexamethyldisilazane and further blending the modified particles with mixed PDMS and -aminopropyltriethoxysilane. Li et al. prepared superhydrophobic coatings from organosilane-modified nano SiO particles. Xu et al. combined perfluorooctanoic acid-modified TiO NPs with polystyrene (PS), while Harton et al. blended PDMS-modified SiO with PS to achieve the superhydrophobic coatings as reported in the literature . Immobilizing the photocatalytic activity of TiO by SiO NPs is one strategy to prevent the release of photocatalysts from coatings. Other superhydrophobic coatings were prepared by directly modifying PDMS resin with unmodified magnetite (Fe O ) NPs. One-dimensional (1D) nanomaterials with high surface area and unique morphology are particularly appealing in the fabrication of highly efficient self-cleaning coating applications. Surfaces coated with 1D nanostructure, such as aligned nanotubes, nano-fibres, nanorods, and nanowires, have exhibited excellent superhydrophobicity, with a CA larger than 150° . CNT forest functionalized by poly(tetrafluoroethylene) is also composed of a superhydrophobic surface. Aligned silicon carbide nanowires show excellent superhydrophobic property after they are functionalized with perfluoroalkysilane. The superhydrophobicity characteristic of the composite film of CNTs and silicon carbon nanowires film could be tunable by controlling the ratio and different morphology . Fluoro-compounds and resins have also been widely explored because of their low surface tension. Superhydrophobicity is induced by roughening the fluorinated surface with low-surface-energy materials . Aizenberg et al. demonstrated the performance of SLIPS. The slippery materials were obtained by infusing a viscous liquid (perfluorinated lubricants or silicone oil ) into porous or textured substrates. The infused liquid was maintained as a thin dynamic layer at the substrate to form hydrophobic interface. The created interface allows other fluids to slip through the surface with a sliding angle of <2° . Microporous or micro-structured surfaces, such as poly(teraflouro ethylene) (PTFE) and fluoro-silanized surfaces, were used as SPLS substrates. SLIPS shows fouling resistance against various fouling communities . Applying PTFE obtains higher AF performance than applying PEG-modified surfaces . This finding is related to the high mobility on the slippery interfaces and low surface energy of the interfused liquid. However, SLIPS may be a promising candidate when using fluorinated lubricant PTFE in maritime ecosystem. Zhang et al. reported a simple pathway to form superhydrophobic surfaces by extending a film of the waxy PTFE with the addition of fibrous crystals of a large fraction of void space. In summary, the blending technique is an essential avenue to create outstanding superhydrophobic surfaces because of the limited solubility of fluoropolymers. The utilization of UV and sun rays for reinforcing the AF performance on ship hulls has driven worldwide concern because of economic and ecological considerations . Changing the surface wettability of a designed nanocomposite to a superhydrophilic character ( ⩽ 10°) is the main function achieved by UV light irradiation. Photocatalytic activity of definite nanofillers could be utilized for decreasing fouling settlement and maintenance costs by reducing surface free energy for costly self-cleaning operations ( ) . TiO NPs have gained considerable attention because of their unique physico-chemical and mechanical characteristics, availability, and potential AF capability. Nano-TiO particles, as semiconductive materials, semiconductive materials perhaps the most widely used among photocatalysts because of their stability, low cost, strong oxidizing agent, and promising UV light sensitivity nature . When TiO NPs are exposed by UV illumination from the solar light, two types of generated carriers (electrons and holes) are obtained ( A). These carriers are produced by photon energy ( ) ⩾ the energy band gap ( ); their values are usually 3.0 eV for rutile or 3.2 eV for anatase at a wavelength of 388 nm. The photo-generated electrons ( ) transfer from the valence band to the conduction band, while the holes ( ) remain in the valence band. The e in the conduction band can reduce oxygen to superoxide anion radicals, while the strong oxidizing capability of h+ can oxidize water to produce a hydroxyl radical. As a consequence, this radical can remove adsorbed organic pollutants and easily clean nanocomposite surfaces . The utilization of dispersed TiO NPs for designing self-cleaning and non-stick FR nanocomposite surfaces had been reported . The non-toxic FR behavior of well-dispersed and spherical single crystal TiO in PDMS nanocomposites presents evidence of the formation of non-toxic FR behavior. This design inspiration was used to improve coating smoothness, weak fouling attachments, surface stability against biodegradation, and superhydrophilic character induced by UV irradiation . However, TiO NPs can only absorb UV light because of their wide band gap that constitutes 4% of the sun rays . Thus, enhancing the photocatalytic activity of TiO nanocomposite coatings to cause absorption in the visible region is an attractive route for pollutant degradation and fouling elimination ( and ). The application of noble-metal/TiO photocatalysts for superhydrophilic self-cleaning coating achieves important economic and ecological returns. Doping with selected noble metal, such as Au or Ag, is another strategy to enhance the photocatalytic activity of TiO NPs . For example, Au–TiO NPs show a high photo-activity toward superhydrophilic self-cleaning coating designs . Chen and Carroll synthesized Ag@TiO , which showed an excellent photocatalytic performance by degrading Rhodamine B compared with the commercial TiO . The photocatalytic performance was enhanced by completely degrading Indigo carmine with Ag-supported TiO for 2 h under visible light irradiation . Ag NPs are very chemically reactive and thus are oxidized upon direct contact with TiO Therefore, using SiO or Al O NPs as interlayer between TiO and silver NPs may prevent the oxidation of Ag to modulate the distance between Ag and the TiO NPs . Ag-SiO -TiO photocatalysts have been developed to obviate silver NPs oxidation. The self-cleaning phenomenon of an Ag-SiO -TiO nanocomposite coating was proven to have superior photocatalytic properties . Surfactants such as n-octylamine added to the mesoporous nanocomposites can improve the photocatalytic performance by coarsening the pore structure . However, incorporating TiO photoactive NPs in organic layers is promising for self-cleaning applications; a drawback of oxidizing the polymeric resin should be considered . Consequently, changes in photo-induced wettability in TiO NPs increased the difficulty of developing water repellent properties compared with water-loving superhydrophilic surfaces. Adsorption of the water-repellent flouroalkysilane polymer on TiO nano-surface was studied and could enhance durability and superhydrophilicity . FR nanocomposites based on mixed TiO /noble metal photocatalyst with hydrophobic polymers have been rarely studied, and future studies on this state of art technology are thus promising. The complex biochemical phenomenon was confirmed by studying the mechanism of biofouling. For example, water repellent adhesive protein binds barnacles, whereas diatoms bind through hydrophilic proteins. Hydrophilic coatings minimize fouling attachments and adsorption of protein and thus provide potential options for utilization as FR coatings . Combining the properties of hydrophilic and hydrophobic materials results in effective new FR terminology called amphiphilic surfaces ( ). These surfaces with prominent nanoscale hydrophilic and hydrophobic domains are new generation for non-biocidal coatings that possess surface properties which prevent the binding of biological macromolecules and improve fouling-prevention characteristics . Fluoropolymer-modified silicone nanocomposites enjoy amphiphilic surfaces and block copolymer structure, providing both protein adsorption resistance and low surface energy characteristics . A methoxy-terminated PEG conjugated with dihydroxyphenylalanine shows prominent amphiphilic FR performance over a virgin silicone coating . Based on its water solubility, neutral nature, ingrained hydrophilicity in AF performance and flexible chains, PEG gained unparalleled capability to be hydrated with water. The coordination capability of PEG as well as steric exclusion effects are essential factors for fouling prevention and protein adsorption resistance . PEG monolayers, multilayers and PEGylated hydrogel paints chains, have been evaluated in marine AF because of their biofouling anti-adhesion characteristics . Hyperbranched polyglycerol (HPG) was also introduced and confirmed to have superior AF properties. HPG ensures an AF performance equivalent to PEG, but HPG provides more benefits, such as increased stability against oxidation and high functionality. Moreover, HPG activation and bioconjugation can be accomplished easily through a one-pot reaction; thus, the number of preparative steps and time consumption are reduced ( ). An effective FR paints were reported based on combination between hyperbranched amphiphilic fluoropolymers and PEG linear polymers and approved superior anti-adhesion of green sporelings compared with silicone paints . Also amphiphilic block copolymer coatings based on grafted ethoxylated fluoroalkyl groups were studied and approved to release fouling organisms easily. The results suggested that the tailored amphiphilic coatings can prevent algae settlements . Studies have been performed to develop a triblock amphiphilic copolymer coatings that combine resistance to cell adhesion and low surface energy by randomly incorporating discrete PEG contain fluorine groups with polystyrene-block-poly(ethylene) blockpolyisoprene. The xerogel coatings exhibit anti-adhesion and FR effects toward various microogranisms. Barnacle cyprids show reduced settlement on hydrophobic xerogel coatings than on hydrophilic coatings, but the removal percentage of diatoms increases as surface wettability is increased . Phosphorylcholine (PC) copolymer films can be formed from hydrophilic PC block and hydrophobic lauryl block networks. A PC film used without pre-treatment promotes the adhesion of because of the presence of hydrophobic lauryl groups with good AF/FR properties at the film/water interface. The frequently observed protein resistance of some nanopatterned, amphiphilic diblock copolymers is due to the intrinsic high density of surface interfacial boundaries . Modeling innovative and non-toxic FR nanomaterials for vessel bottoms is a challenge . Carbon nanostructured materials have also been extensively investigated . Graphene materials, including graphene, graphene oxide (GO), reduced graphene oxide (rGO), and graphene-embedded polymers are the most widely used nanomaterial in the field of biotechnology and environmental technique . Graphene materials possess characteristics such as economical savings, biocompatibility anticorrosive performance, and environmental security, thus presenting potentials for various applications . These carbon-based compounds are 2D nanomaterials with -hybridized carbon atoms in a hexagonal lattice model and are featured with supreme mechanical properties and large surface area . Great efforts have been paid to incorporate graphene nanomaterials in polymer matrix for developing AF coatings for the next generation . Studies on the AF performance of Graphene, GO, and rGO have demonstrated superior properties of rGO to other materials because of the high surface area of the rGO nanomaterials . Modern studies have investigated the anti-adhesion performance of nanomaterials based on graphene compounds which inhibit protein adsorption and biofilm formation . Nowadays, graphene-related materials are introduced as eminent catalyst supports in photocatalytic systems as they have the following characteristics : Thus, modern studies have reported that anchoring well defined photo-catalysts on the surface of graphene compounds introduces nanocomposites with superior photocatalytic behavior for various applications . Many graphene-based nanocomposite nanomaterials, like TiO –GO, ZrO –GO, ZnS–rGO, and CdS–graphene–TiO have been demonstrated and obtained higher photocatalytic performance than the unanchored metal oxides . Also, He reported the development of inexpensive (ZnO–rGO) nanocomposite films with improved photo-induced and superhydrophilic characteristics for self-cleaning applications. This phenomenon is promising for solar light-boosted self-cleaning applications. Selim et al. reported eco-friendly FR coatings based on utilization of UV-irradiation for boosting self-cleaning behavior of PDMS/spherical single crystal TiO nanocomposites. The utilization of solar light boosting for smart FR coatings based on rGO anchored with single crystal photocatalyst materials is a future trend for superior FR and durability performance ( ) which has yet to be examined. Fouling represents a major disaster in the naval industry and exacerbates fuel drain, drag resistance, maintenance costs, and negative effects on the environment. The prevalent utilization of biocidal AF coatings has caused numerous environmental repercussions and resulted in strict legislations in the shipping industry. Non-stick silicone FR technology prevents the attachments of colonizing species and weakens their cohesion strength by providing low-friction and ultra-smooth surfaces. These surfaces are non-leachant and are fairly stable in water. FR paints based on PDMS elastomers exhibit excellent self-cleaning and AF performance; however, filler reinforcement has become necessary to reduce costs and overcome the mechanical disadvantages. Nano-engineered composites have a great potential for application as durable and eco-friendly alternatives to toxic AF paints in addition to minimizing fuel consumption and environmental pollution emissions. Consequently, many newly designed FR coatings are also based on organic–inorganic hybrid nanocomposite materials. Superior criteria for marine FR coating composites are urgently needed, as follows: With the capability to tailor the FR performance, new series of hydrophobic, hydrophilic, and amphiphilic FR nanocomposites are designed as a modern stream for self-cleaning applications in maritime navigation. Techniques to controlling AF composition systems including fluorinated, silicone, PU, PCL, and PEG based FR coatings, in addition to the utilizations of hyperbranched and linear polymers and copolymers, show the greatest contributions to the improved FR properties. The fundamental functions of size-and shape-controlled hybrid NPs can tailor the controlled hydrophobicity/hydrophilicity and surface free energy that produce ultra-smooth surfaces. To a high extent, GO-, rGO-, MWCNT-, and SWCNT-coated metal, metal oxide NPs, and multifunctional hybrid NPs are good models to provide an important guidance on the influence of hydrophobicity/hydrophilicity in designing high performance FR surfaces. In this filler design, the particle size, shape, and surface characteristic or well-dispersion of metal and metal oxide NPs in nanocomposite AF systems promote the physico-mechanical and wetting behavior. These two key factors effectively prevent agglomeration problems and improve the activity of silicone nanocomposite FR system. The application of a wide range of recent FR technologies in the marine industry is being considered to satisfy ecological and economic considerations. Some of these technologies have converted to commercial products, and others remain at the laboratory–development stage. Several surface factors, such as free energy, CA, elastic modulus, topology, and structure–property relationship, are essential to inhibit bonding of marine organisms. This review discusses the future requirements of nano-FR innovations applicable to marine environment. A new class of nano-FR technology is continually progressing in for future ecological and economic applications. Synthetic nanomaterials with control over the structural morphology, geometry, tunable particle size and surface functionality, and desired sites can promote their capability as potential candidates for the production of eco-friendly AF nano-coatings. Organic-inorganic nanomaterial hybrids can also aid in the improvement and development of economically viable high-performance nano-FR composite coating. It is marine environmental security that underpins less of a potential hazard and thus a more attractive route is the development of innovative nano-FR materials, which possess a wide range of compositions, structural features, and surface properties such as: These characteristics of NPs are expected to save billions of dollars annually for the treatment of fouling problems. With the diversity of these organic and inorganic hybrid nanocomposites, innovative trends may be promising in minimizing fuel consumption, maintenance costs, and docking intervals. The FR models summarized in this comprehensive review are relevant examples of how materials technology can help solve global environmental concerns caused by outdated or previously applied chemical technologies. The results of this study provide new insights into the manufacturing of scalable, cost-effective, and reliable FR nano-based formulation coatings. Furthermore, the utilization of renewable solar light in engineering FR coatings and in protecting aquatic environments can be feasible.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Used\n",
      "* **AIモデルの判断:** Not Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 181 / DOI: 10.1016/j.media.2024.103201  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "MedMNIST v2 - A large-scale lightweight benchmark for 2D and 3D biomedical image classification\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "A comprehensive survey on deep active learning in medical image analysis\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "Medical imaging visualizes anatomical structures and pathological processes. It also offers crucial information in lesion detection, diagnosis, treatment planning, and surgical intervention. In recent years, the rise of artificial intelligence (AI) has led to significant success in medical image analysis. The AI-powered systems for medical image analysis have approached the performance of human experts in certain clinical tasks. Notable examples include skin cancer classification ( ), lung cancer screening with CT ( ), polyp detection during colonoscopy ( ), and prostate cancer detection in whole-slide images ( ). Therefore, these AI-powered systems can be integrated into existing clinical workflows, which helps to improve diagnostic accuracy for clinical experts ( ) and support less-experienced clinicians ( ). Deep learning (DL) models serve as the core of these AI-powered systems for learning complex patterns from raw images and generalizing them to more unseen cases. Leveraging their robust feature extraction and generalization capabilities, DL models have also achieved remarkable success in the field of medical image analysis ( ). The success of DL often relies on large-scale human-annotated datasets. For example, the ImageNet dataset ( ) contains tens of millions of labeled images, and it is widely used in developing DL models for computer vision (CV). The size of medical image datasets keeps expanding, but it is still relatively smaller than that of natural image datasets. For example, the brain tumor segmentation dataset BraTS consists of multi-sequence 3D MRI scans. The BraTS dataset expanded from 65 patients in 2013 ( ) to over 1200 in 2021 ( ). The latter is equivalent to more than 700,000 annotated 2D images. However, the high annotation cost limits the construction of large-scale medical image datasets, mainly reflected in the following two aspects: In clinical practice, automatic segmentation helps clinicians outline different anatomical structures and lesions more accurately. However, training such a segmentation model requires pixel-wise annotation, which is extremely tedious ( ). Another case is in digital pathology. Pathologists usually require detailed examinations and interpretations of pathological tissue slices under high-magnification microscopes. Due to the complex tissue structures, pathologists must continuously adjust the microscope’s magnification. As a result, it usually takes 15 to 30 min to examine a single slide ( ). Making accurate annotations is even more challenging for pathologists. In conclusion, the annotation process in medical image analysis demands a considerable investment of time and labor. In CV, tasks like object detection and segmentation also require fine-grained annotations. However, the widespread use of crowdsourcing platforms has significantly reduced the cost of obtaining high-quality annotations in these tasks ( ). However, crowdsourcing platforms have certain limitations in medical image annotation. Firstly, annotating medical images demands both medical knowledge and clinical expertise. Complex cases even require discussions among multiple senior experts. Secondly, even in relatively simple tasks, crowdsourcing workers tend to provide annotations of poorer quality than professional annotators in medical image analysis. For example, results in supported the conclusion above in annotating the segmentation mask of surgical instruments. Crowdsourcing platforms could also raise privacy concerns ( ). Nevertheless, we will face new challenges when the annotators change from crowdsourcing workers to clinical experts. First of all, recruiting doctors for annotation is very expensive. For instance, a radiologist usually takes about 60 min to manually segment brain tumors per patient in its multi-sequence MRI volumes ( ). And the median hourly rate of a radiologist is $221 in the US. Besides, to minimize individual bias for certain scenarios, it is common to have a doctor annotate the same case multiple times or have multiple doctors annotate it. Multiple annotation rounds and annotators introduce intra- and inter-annotator variability and handling such variabilities leads to additional annotation costs ( ). In summary, high-quality annotations often require the involvement of experienced doctors, which inherently increases the annotation cost of medical images. The high annotation cost is one of the major bottlenecks of DL in medical image analysis. Active learning (AL) is considered one of the most effective solutions for reducing annotation costs. The main idea of AL is to select the most informative samples for annotation and then train a model with these samples in a supervised way. In the general practice of AL, annotating a part of the dataset could reach comparable performance of annotating all samples. As a result, AL saves the annotation costs by querying as few informative samples for annotation as possible. The process of AL is illustrated in , which we will detail in Section . Specifically, we refer to the AL works focusing on training a deep model as deep active learning. Reviewing AL works in medical image analysis is essential for reducing annotation costs. investigated the role of humans in developing and deploying DL in medical image analysis, where AL is considered a vital part of this process. In , AL was one of the solutions for training high-performance medical image segmentation models with imperfect annotation. As one of the methods in label-efficient deep learning for medical image analysis, summarized AL methods from model and data uncertainty. There are also several surveys on AL in machine learning or CV. provided a general introduction and comprehensive review of AL works in the machine learning era. After the advent of DL, reviewed the development of deep active learning and its applications in CV and natural language processing. summarized the model-driven and data-driven sample selectors in deep active learning. reimplemented high-impact works in deep active learning with fair comparisons. reviewed recent developments of deep active learning in CV and its industrial applications. However, the surveys mentioned above have certain limitations. Firstly, new ideas and methods are constantly emerging with the rapid development of deep active learning. Thus, a more comprehensive survey of AL is needed to cover the latest advancements. Secondly, a recent trend is combining AL with other label-efficient techniques, which is also highlighted as a future direction by related surveys ( ). However, existing surveys still lack summaries and discussions on this topic. Thirdly, limited surveys have evaluated the performance of different AL methods on the medical imaging dataset, indicating a near absence of such efforts. Finally, the high annotation cost emphasizes the increased significance of AL in medical image analysis, yet related reviews still lack comprehensiveness in this regard. This survey comprehensively reviews AL for medical image analysis, including core methods, integration with other label-efficient techniques, and AL works tailored to medical image analysis. We first searched relevant papers on Google Scholar and arXiv using the keyword “Active Learning” and expanded the search scope through citations. The included papers mainly belong to medical image analysis. It should be noted that some important AL works in the general CV field are also included since the development of AL in medical image analysis is influenced by the advance of AL in CV. Ignoring these works would flaw the logic and taxonomy of this survey. To balance the AL works of different fields, we first present the seminal works in each subsection, which may include works in the general CV field. Then, we provide a detailed review of the AL papers related to medical image analysis within this category. Additionally, most works in this survey are published in top-tier journals (e.g., TPAMI, MedIA, TMI, TBME, JBHI, etc.) and conferences (e.g., MICCAI, ISBI, MIDL, CVPR, ICCV, ECCV, ICML, ICLR, NeurIPS, etc.). As a result, this survey involves nearly 164 relevant AL works with 234 references. The contributions of this survey are summarized as follows: The rest of this survey is organized as follows: Section introduces problem settings and mathematical formulation of AL, Section discusses the core methods of AL, including evaluation of informativeness (Sections & ) and sampling strategies (Section ), Section reviews the integration of AL with other label-efficient techniques, Section summarizes AL works tailored to medical image analysis. The experimental settings, results, and analysis are in Section . We discuss existing challenges and future directions of AL in Section and conclude the whole paper in Section . The overall framework of this survey is shown in . Due to the rapid development of AL, many related works are not covered in this survey. We refer readers to our constantly updated website for the latest progress of AL in medical image analysis. AL generally involves three problem settings: membership query synthesis, stream-based selective sampling, and pool-based active learning ( ). In the case of membership query synthesis, we can continuously query any samples in the input space for annotation, including synthetic samples produced by generative models ( ). We also refer to this setting as generative active learning in this survey. Membership query synthesis is typically suitable for low-dimensional input spaces. However, when expanded to high-dimensional spaces (e.g., images), the queried samples produced by generative models could be unidentifiable for human labelers. The recent advances of deep generative models have shown great promise in synthesizing realistic medical images, and we further discuss its combination with AL in Section . Stream-based selective sampling assumes that samples arrive one by one in a continuous stream, and we need to decide whether or not to request annotation for incoming samples ( ). This setting is suitable for scenarios with limited memory, such as edge computing, but it neglects sample correlations. Most AL works follow pool-based active learning, which draw samples from a large pool of unlabeled data and requests oracle (e.g., doctors) for annotations. Moreover, if multiple samples are selected for labeling at once, we can further call this setting “batch-mode”. Deep active learning is in batch-mode by default since retraining the model every time a sample is labeled is impractical. Also, one labeled sample does not necessarily result in significant performance improvement. Therefore, unless otherwise specified, all works in this survey follow the setting of batch-mode pool-based active learning. The flowchart of active learning is illustrated in . Assuming a total of annotation rounds, active learning primarily consists of the following steps: In the th round of annotation, , an informativeness function is used to evaluate the informativeness of each sample in the unlabeled pool . Then, a batch of samples is selected with a certain sampling strategy . In medical image analysis, active learning selects a batch of images most of the time (i.e., image-wise selection). In this survey, the sampling unit of AL selection is an image (could be 2D or 3D) unless specifically stated. However, with the development of AL, region-wise (Sections , ) or slice-wise annotations (Section ) are adopted in AL. Please refer to these sections for more details. Specifically, the queried dataset of th round is constructed as follow: represents sample in the dataset, and are unlabeled and queried dataset in round , respectively. and represent the deep model and its parameters from the previous round, respectively. The annotation budget is the number of queried samples for each round, far less than the total count of unlabeled samples, i.e., . After sample selection, the queried set is sent to oracle (e.g., doctors) for annotation, and newly labeled samples are added into the labeled dataset . The update of is as follow: represents the label of , and and denote the labeled sets for round and the previous round, respectively. Besides, the queried samples should be removed from the unlabeled set : It is worth noting that some current works combine active learning with interactive segmentation. In interactive segmentation, the model assists experts in annotation, thereby reducing the difficulty of the annotation process. For more details, please refer to Section . After oracle annotation, we train the deep model using the labeled set of this round in a fully supervised manner. The deep model is trained on to obtain the optimal parameters for round . The mathematical formulation is as follows: represents the loss function and it could be rewritten as for simplicity. Recently, some works adopted the one-shot fashion in active learning, which performed sample selection without multiple rounds. Please refer to Section . It is worth noting that the model needs proper initialization to start the AL process. If the initial model is randomly initialized, it could only produce meaningless informativeness. To address this issue, most AL works randomly choose a set of samples as initially labeled dataset and train upon . For more details on better initialization of AL using pre-trained models, please refer to Section . In this survey, we consider the evaluation of informativeness and sampling strategy as the core methods of AL. Informativeness represents the value of annotating each sample. Higher informativeness often indicates a higher priority to request these samples for labeling. Typical metrics of informativeness include uncertainty and representativeness. Based on the informativeness scores, a certain sampling strategy is used to select a small number of unlabeled samples for annotation. Most AL works simply ranked these samples by their informativeness metrics and selected the highest ones according to the annotation budget (i.e., top-k selection). However, current informativeness scores are more or less flawed and they may cause issues like redundancy or class imbalance among the queried samples. Therefore, we need more advanced sampling strategies to mitigate these issues arising from imperfect informativeness metrics. In this section, we reviewed two major informativeness metrics, including uncertainty (Section ) and representativeness (Section ), and sampling strategy (Section ). As a unique contribution of this survey, we, for the first time, explicitly define sampling strategies as core methods of AL and review how to design a better sampling strategy in AL. Additionally, we provide a summarization of all the cited AL works in this survey. Methods and basic metrics of uncertainty or representativeness and sampling strategies are detailed in . Despite great progress has been made in medical image analysis, safety and interpretability are still unsolved problems for deploying DL models in real-world clinical practice. Due to the high variability of medical images and the limited training data, the predictions of DL models are not reliable and trusted. Correctly assessing and quantifying uncertainty in medical image analysis would allow models to alert ambiguities, artifacts, and unseen patterns in the data ( ). Such properties of uncertainty are helpful in AL since novel patterns in the unlabeled samples can be identified by the uncertainty. Therefore, uncertainty is frequently used in active learning as an informative metric. In the AL query, samples with higher uncertainty are considered hard and more likely to be misclassified by the current model. Annotating and training on these samples helps the model learn new patterns and improve performance. Tracing back to the cause of uncertain predictions, the uncertainty can be mainly separated into two types: aleatoric uncertainty (AU) and epistemic uncertainty (EU) ( ). AU (i.e., data uncertainty) captures the noisy observations in data, such as motion artifacts of MRI or metal artifacts of CT in medical image analysis. AU cannot be reduced by acquiring more data. A high EU (i.e., model uncertainty) indicates that the samples contain knowledge that has not yet been mastered by the model. Therefore, the EU can be reduced by involving more data. However, most of the AL works did not consider the separation of AU and EU. So, the terminology of uncertainty in AL mainly refers to prediction uncertainty, which is the composition of AU and EU. This is because explicitly separating AU and EU is usually very difficult and may not bring much benefit in the practice of AL ( ). In this survey, unless explicitly stated otherwise, all uncertainties refer to predictive uncertainties, meaning that no separation between AU and EU has been performed. The most straightforward uncertainty metrics in deep AL are based on prediction probabilities with a single forward pass. These metrics have been widely used in AL since the machine learning era, and their formulations are detailed in . However, directly transferring them to deep AL would be challenging due to the notorious issue of over-confidence in deep neural networks ( ). Over-confidence refers to the model having excessively high confidence in its predictions, even though they might not be correct. It could result in high confidence (e.g., 0.99) of the wrong class for misclassified samples. For uncertain samples, it leads to extreme confidence (e.g., 0.99 or 0.01) instead of normal one (e.g., 0.6 or 0.4) as it should. As a result, over-confidence distorts uncertainty estimation since it affects the predicted probabilities for all classes. This section divides the uncertainty-based AL into multiple inference, gradient-based uncertainty, performance estimation, uncertainty-aware models, and adversarial-based uncertainty. The taxonomy of uncertainty-based AL is shown in . To mitigate over-confidence, a common strategy for uncertainty-based AL is to run the model multiple times under perturbations. The main idea is to reduce the bias introduced by network architectures or training data. These biases often contribute to the over-confidence issue. Two approaches are often used to utilize multiple inference results for AL. The first is to calculate the classic uncertainty metrics with the average probability of multiple inferences. Averaging the prediction probabilities of multiple inferences helps to reduce individual bias that causes over-confidence. The other approach takes the disagreement between different prediction results as uncertainty quantification. Samples with higher disagreement indicate higher uncertainty and are suitable for annotation in AL. In this section, we will introduce four types of methods for AL with multiple inferences: Monte Carlo dropout (MC dropout), model ensemble, model disagreement, and data disagreement. The first two used the average probability of results of multiple inferences to calculate the uncertainty metrics, like entropy and margin. The last two are based on disagreement. For the source of perturbation, the first three perturb the model parameters while the last perturb the input data. randomly discards certain neurons in the deep model during each inference ( ). With MC dropout enabled, the model runs multiple times to get different predictions. was the pioneering work of deep AL. They were the first to use MC dropout in computing uncertainty metrics like entropy, standard deviation, and Bayesian active learning disagreement (BALD) ( ). Results showed that MC Dropout could significantly improve the performance of uncertainty-based deep AL. Besides, they were also among the first to apply deep AL in medical image analysis. In the skin lesion analysis dataset ISIC 2016, they found that BALD consistently outperformed the random baseline. In brain cell type classification, calculated entropy using the average probability of multiple MC dropout runs. adopted the variance of multiple MC dropout runs as the uncertainty metric in the classification of confocal endomicroscopy and gastrointestinal endoscopy. trains multiple models to get numerous predictions during inference. conducted a detailed comparison of model ensemble and MC dropout in uncertainty-based AL. Results in the standard datasets demonstrated that the model ensemble performs better. For AL in diagnosing diabetic retinopathy, the proposed method achieved significant improvement compared to the random baseline. However, model ensemble requires significant training overhead in DL. To reduce the computational costs, snapshot ensemble ( ) obtained multiple models in a single run with cyclic learning rate decay. An early attempt in showed that snapshot ensemble leads to worse performance than model ensemble. improved the snapshot ensemble by maintaining the same optimization trajectory in different AL rounds, along with parameter regularization. Results showed that the improved snapshot ensemble outperforms the model ensemble. Besides, employed stein variational gradient descent to train an ensemble of models, aiming to ensure diversity. Their proposed method showed advantages to other competitors in segmenting the pancreas and tumor on CT and hippocampus on MRI. We can utilize the disagreement between the outputs of different models, which can be also referred to as Query-by-Committee (QBC) ( ). This type of method was widely used in AL for medical image analysis. Suggestive annotation (SA) is the pioneering work of AL for medical image analysis ( ). They trained multiple segmentation networks with bootstrapping. Variance among these models is used as the disagreement metric. SA demonstrated superior performance in segmenting glands on pathological images and lymph nodes on ultrasound images. In abdominal multi-organ segmentation, trained three different segmentation models and adopted the variance between their predictions. In carotid intima-media segmentation for ultrasound images, selected samples with the highest Kullback–Leibler (KL) divergence between the predictions of teacher and student models for annotation. In polyp segmentation of capsule colonoscopy, trained multiple decoders using class activation maps (CAMs) ( ) generated by a classification network. They further proposed model disagreement and CAM disagreement for sample selection. Model disagreement included entropy of prediction probabilities and Dice between outputs of different decoders, while CAM disagreement measured the Dice between CAMs and outputs of all decoders. This method selected samples with high model disagreement and CAM disagreement for annotation. However, samples with low model disagreement but high CAM disagreement were treated as pseudo-labels for semi-supervised training. In rib fracture detection, adopted Hausdorff distance to measure the disagreements between different CAMs. Besides, adopted vote entropy between different MC dropout inferences as the disagreement metric. Since training multiple models can be computationally expensive, measuring the disagreements between different perturbations of input data is also helpful in AL. KL divergence is a commonly used metric for quantifying disagreement. In COVID diagnosis, computed KL divergence between different versions of augmentations as the disagreement measure to select informative CT scans for annotation. calculated the KL divergence between predictions of different viewpoints in 3D scenes to select informative regions for AL. Additionally, recent works have adopted alternative metrics to calculate disagreement. proposed input-end committee, which randomly augmented the input data to get multiple predictions. They further measured the classification and localization disagreements between different predictions with cross-entropy and variance, respectively. interpolated the unlabeled samples and labeled prototypes in the feature space. If the interpolated sample’s prediction disagrees with the corresponding prototype’s label, it indicates that the unlabeled samples introduce new features. Thus, these unlabeled samples should be sent for annotation. Results showed advancements across various datasets and settings. Gradient-based optimization is the cornerstone of DL-based medical image analysis. The gradient of each sample reflects its contribution to the change of model parameters. A larger gradient length indicates a tremendous change of parameters by the sample, thus implying high uncertainty. Furthermore, gradients are independent of predictive probabilities, which makes them less susceptible to over-confidence. Three metrics that are frequently used as gradient-based uncertainty: gradients, Fisher information, and influence functions. It should be noted that the gradient computation in this section did not use the ground truth labels which are unavailable for unlabeled samples. Instead, the corresponding methods either used supervised loss with pseudo-labels (e.g. cross-entropy loss with pseudo-labels) or unsupervised loss (e.g. entropy loss), thereby making the gradient computation independent of the true labels. A larger gradient norm (i.e., gradient length) denotes a greater influence on model parameters, indicating higher uncertainty in AL. As an early attempt, adopted the classic expected gradient length ( ) to select valuable samples for annotation in exudate classification of eye fundus images. As a popular and pioneering work in the DL era, proposed batch active learning by diverse gradient embeddings (BADGE). They calculated the gradients only for the parameters of the network’s final layer, with the most confident classes as pseudo labels in gradient computation. Then, k-Means++ is performed on gradient embeddings for sample selection. Results showed competitive performances of BADGE across diverse datasets, network architectures, and hyperparameter settings. Gradient has been widely used in active learning of medical image analysis. extended the BADGE framework into semantic segmentation of laparoscopic surgical images. proved mathematically that a larger gradient norm corresponds to a lower upper bound of test loss. Thus, they employed expected empirical loss and entropy loss for gradient computation, which both obviate the necessity for labels. The former is the weighted sum of the losses of each class and the class probabilities, which are as follows: is the label of class . The entropy loss is solely based on the probabilities of all classes, which are as follows: proposed a new gradients-based active learning method in MRI brain tumor segmentation. They first trained a variational autoencoder (VAE) ( ) to learn the data manifold. Then, they trained a segmentation model and calculated gradients of Dice loss using available labeled data. The sample selection was guided by the gradient projected onto the data manifold. Their extended work ( ) further demonstrated superior performance in MRI whole brain segmentation. is effective in AL of machine learning models ( ). Fisher information (FI) reflects the overall uncertainty of model parameters according to data distribution. FI is defined as the expectation of the squared gradients with respect to the model parameter, the formulation is as follows: is the notation of Fisher information. The trace of the inverse of FI often serves as the objective for AL: , the selected samples could help the model converge faster toward optimal parameters. However, the computation cost of FI-based methods grows quadratically with the increase of model parameters, which is unacceptable for deep active learning. and their extended work ( ) were the first to incorporate FI into deep active learning of medical image analysis. They used the average gradients of each layer to calculate the FI matrix, thus reducing the computation cost. Due to the absence of ground truth labels, they adopt the expected empirical loss (i.e., Eq. ) for gradient computation. This method outperformed competitors in brain extraction across different age groups and pathological conditions. Additionally, only computed the FI matrix for the network’s last layer. The gradient computation of this work is the same as that of BADGE ( ). employed influence function ( ) to select samples that bring the most positive impact on model performance. The influence function of an unlabeled sample is defined as follows : is the Hessian matrix of the labeled set, . In Eq. , the gradients of the first (i.e., the sum of gradients of labeled samples) and the second term (i.e., the Hessian matrix) can be derived with the ground truth label. For the third term, they replaced the true gradients with the gradients of the expected empirical loss due to the unavailability of the ground truth label. In this section, uncertainty metrics are estimations of the current task’s performance. There are two types of such metrics: test loss or task-specific evaluation metrics. These metrics reflect the level of prediction error. For instance, a low Dice score in tumor segmentation for a patient suggests the model failed to produce accurate segmentation. Request annotations for these samples would be beneficial for improving the model’s performance. However, due to the unavailability of ground truth labels, we can only estimate these metrics instead of calculating them precisely. There are primarily two methods for estimating performance: surrogate metrics and learnable performance estimation. are widely used in active learning of medical image analysis. For example, these metrics could be upper or lower bounds for loss or task-specific evaluation metrics. In breast cancer segmentation on immunohistochemistry images, calculated the intersection over union (IoU) of all predictions by MC dropout. They found a strong linear correlation between this IoU and the real Dice coefficient. In skin lesion and X-ray hand bone segmentation, calculated the average Dice coefficient between the predictions of the intermediate layers and final layer through deep supervision. They found a linear correlation between this average Dice and the real Dice coefficient. Besides, found that within limited training iterations, the loss of a sample is bounded by the norm of the difference between the initial and final network outputs. Inspired by this, they proposed cyclic output discrepancy (COD) as the difference in model output between two consecutive annotation rounds. Results indicated that a higher COD is associated with higher loss. Therefore, they opted for samples with high COD. They also demonstrate a linear correlation with the evaluation metrics with post-hoc validation. We can train auxiliary neural network modules to predict the performance metrics. As one of the most representative works in this line of research, learning loss for active learning (LLAL) ( ) trained an additional module to predict the loss value of a sample without its label. Since loss indicates the quality of network predictions, the predicted loss is a natural uncertainty metric for sample selection. Results showed that predicted and actual losses are strongly correlated. The proposed method also outperformed several AL baselines. In lung nodule detection with CT scans, built upon LLAL to predict the loss of each sample and bounding box. In COVID diagnosis, adopted both the predicted loss and the disagreements between different predictions for sample selection. further combined the loss prediction and sample diversity in the federated active learning of COVID diagnosis and colonoscopy polyp analysis. Since AL focuses only on uncertainty ranking of the unlabeled samples, relaxed the loss regression to loss ranking prediction. Thus, they replaced the loss regressor in LLAL with the ranker in RankCGAN ( ). Results showed that loss ranking prediction outperforms the actual loss regression in LLAL. and their subsequent work ( ) introduced a quality assessment module to provide a predicted average IoU score for each slice. They interactively selected slices with the lowest scores in each volume for annotation. In the above sections, uncertainty is derived based on the commonly used deterministic model in DL. However, some models can inherently capture uncertainty, such as VAE or probabilistic U-Net for medical image analysis ( ). In this way, they no longer output a point estimate but instead a distribution of possible predictions, thus mitigating over-confidence. We refer to them as uncertainty-aware models in this survey. They only require a single forward pass of the deep model, thus significantly reducing computational and time costs during inference. Evidential deep learning (EDL) and mixture density networks (MDN) are often used for uncertainty-aware models in AL. replaces the Softmax distribution with a Dirichlet distribution ( ). The network’s output is interpreted as the parameters of a Dirichlet distribution, so the predictions followed the Dirichlet distribution. The Dirichlet distribution will be sharp if the model is confident about the predictions. Otherwise, it will be flat. Another advantage brought by EDL is that AU and EU are easy to obtain with a Dirichlet distribution. proposed a federated AL method based on EDL for medical image analysis. Following the federated AL setting, they kept a global model across all clients and local models for each client. AUs of the global and local models and the EUs of the global models were used for sample selection. In chest X-ray classification, modified the EDL-based AL to accommodate the multi-label setting. Specifically, they transformed the Dirichlet distribution in EDL into multiple Beta distributions, each corresponding to one class label. They then calculated the entropy of the Beta distributions as the AU for annotation. introduced a model evidence head to scale the parameters of the Dirichlet distribution adaptively in object detection, which enhanced training stability. They first calculated the EU for each detection box. Then, the sample-level uncertainty was obtained through hierarchical uncertainty aggregation. Besides, introduced EDL into active domain adaptation. Samples with high distribution and data uncertainties are selected for annotation, which are both based on EDL. transformed the classification and localization heads in object detection networks to the architecture of MDN ( ). Besides the coordinates and class predictions of each bounding box, the MDN heads produced the variance of classification and localization. They used the variances as uncertainty metrics for sample selection. Results showed that this method is competitive with MC dropout and model ensemble while significantly reducing the inference time and model size. Uncertainty in AL can also be estimated adversarially, including adversarial samples and adversarial training. help measure the sample’s distance to the decision boundary implicitly, while a higher distance indicates higher uncertainty. By attacking the deep models, adding carefully designed perturbations to original samples results in adversarial samples ( ). The differences between adversarial and original samples are nearly indiscernible to the human eye. However, deep models would produce extremely confident but wrong predictions for adversarial samples. The reason is that adversarial attacks push the original samples to the other side of the decision boundary with minimal cost, resulting in visually negligible changes but significantly different predictions. From this perspective, the strength of adversarial attacks reflects the sample’s distance to the decision boundary ( ). A small perturbation indicates that the sample is closer to the decision boundary and, thus, is considered more uncertain. adopted the DeepFool algorithm ( ) for adversarial attacks. Samples with small adversarial perturbations are requested for labeling. attacked the deep model by maximizing the KL divergence between predictions of adversarial and original samples while the strength of perturbation is limited. alternates between training feature extractors and classifiers with conflicting objectives, aiming to expose uncertain samples by increasing classifier disagreements. and their extended work ( ) implemented this with two classifiers on labeled and unlabeled datasets, first tuning classifiers while fixing the feature extractor to reveal more uncertain samples, then adjusting the feature extractor against fixed classifiers to minimize the gap between labeled and unlabeled samples. After several rounds, samples with the greatest disagreements are annotated. While uncertainty-based methods play a crucial role in deep AL, they still face certain challenges: The goal of using uncertainty in AL is to improve performance by querying hard samples of the current model. However, these methods could also select outliers that harm the model training ( ). This happens mainly because uncertainty-based methods often ignore the intrinsic characteristics of the sample itself. In the feature space, uncertain samples are often located near the decision boundary ( ). Therefore, the distribution of samples selected by uncertainty-based methods is usually different from the overall data distribution. This discrepancy introduces dataset bias and leads to a performance drop. This challenge could be alleviated if the relationship between different samples is carefully considered during the AL query. In summary, uncertainty-based AL lacks exploration of the visual information carried in each sample and the relationship between different samples. These challenges above call for a new informativeness metric in AL. Representativeness is adopted in AL to overcome challenges brought by uncertainty. Representativeness-based AL aims to select a subset of samples that can represent the entire dataset. Specifically, representative samples should be visually distinctive in properties like imaging style or visual content. In medical image analysis, images are often high-dimensional and thus computation-intensive for the DL model. Besides, important information like lesions or tissues is not always directly visible or easily distinguishable. A good feature representation greatly reduces the image dimensionality and also extracts anatomical, histological, pathological, or even functional information in medical images. Therefore, the query process of representativeness-based AL is often conducted in the feature space. Besides, representative samples should also be widely distributed across the data distribution rather than concentrated in a specific region. In other words, these samples should be diverse. This is to minimize redundancy in the query result and try to keep the original data distribution as much as possible. Therefore, proper metrics of sample-wise or distribution-wise distance and sample density are needed to assess the landscape of a dataset. Besides, the uniqueness of medical images may require different distance metrics than that of natural images. This section introduces four types of representativeness-based AL: clustering-based, cover-based, discrepancy-based, and density-based representativeness AL. The taxonomy of these methods is shown in . With the advancement of feature extraction in medical image analysis, images with similar appearances tend to group together in the feature space ( ). Therefore, a straightforward approach involves clustering on the data embeddings to select representative samples. This type of method grouped the data into several clusters and then selected the centroid samples of each cluster. It leveraged the inherent structure within the data for insightful groupings and was also very easy to implement. K-Means was the most popular choice in clustering-based active learning. performed k-Means on the off-the-shelf self-supervised features, then selected cluster centers for annotation. Based on the self-supervised feature, adopted the k-Means++ for clustering and the silhouette coefficient to determine the optimal number of clusters. Their proposed method achieved commendable performance in lung segmentation of chest X-rays and lesion segmentation of dermoscopic images. In nuclei segmentation, performed coarse-level and fine-level clustering using K-Means, aiming to select informative patches from pathological images. In connectomics, proposed two-stream clustering for active selection. They first predicted the semantic mask for each unlabeled sample and simplified the AL task to judge the correctness of each predicted ROI. Besides, they trained two feature extractors of VAE with segmentation masks and unlabeled images, respectively. For two-stream clustering, they first applied mask-level clustering with mask features to group ROIs with similar appearances. Within each mask cluster, image-level clustering is further performed. This method achieved excellent performance in synapse detection and mitochondria segmentation. Results also showed that two-stream clustering outperforms clustering with concatenated mask and image features by preventing the image feature from dominating the results. We can formulate representativeness-based AL as a covering problem. A classic example of the covering problem is facility location, such as covering all the city’s streets with billboards ( ). Likewise, cover-based AL uses a few samples to cover the entire dataset, which is analogous to using several spheres to cover all the samples in the feature space, with the center of each sphere being the selected sample. Ideally, these samples should be representative and contain information on other samples. These methods usually involve two settings: set cover and maximum cover. Both settings are NP-hard, meaning they cannot be optimally solved in polynomial time. However, near-optimal solutions could be achieved in linear time using greedy algorithms, which iteratively select samples that cover most of the other samples for annotation ( ). These two variants are slightly different in the problem setting. The set cover is constrained by complete coverage, which means that it cannot omit any sample in the dataset. To achieve this goal, the radius of its covering spheres may be very large, and when there are very few samples selected, outliers might be chosen as the centers of the spheres ( ). The goal of max cover is to cover as much of the entire dataset as possible, breaking the constraint of set cover, and thus avoiding the issue of outlier selection. Core-Set ( ) followed the setting of k-Center location ( ), which is also a variant of the set cover problem. They employed farthest-first traversal to solve the k-Center problem for selecting representative samples. The L2 distance of deep features is used to measure the similarity between different samples. introduced contextual diversity for AL, a metric that fused uncertainty and diversity of samples spatially and semantically. They replaced the L2 distance with contextual diversity and adopted the same farthest-first traversal for sample selection as . adopted graph convolutional networks (GCN) to model the relationships between labeled and unlabeled samples. GCNs improved the feature representation of unlabeled samples with the labeled dataset. Enhanced feature representation was further used for Core-Set sampling. As a pioneering work of AL in medical image analysis, SA ( ) stands out as one of the initial endeavors to introduce the concept of representativeness into AL. SA first selected highly uncertain samples and then chose representative samples for annotation. The formulation of the representativeness part in SA followed the setting of maximum cover. The representativeness metric was based on the cosine similarity of deep features. Specifically, sample is represented by the most similar sample from queried dataset is the representativeness of sample with respect to and represents cosine similarity. Besides, representativeness between and the unlabeled set is as follow: indicates that better represents . It should be noted that SA is a generalization of the maximum cover problem since the cosine similarity ranges from 0 to 1. But they still employed a greedy algorithm to find sample that maximizing . Many subsequent AL works built their framework of cover-based AL on SA, especially in the field of medical image analysis. quantized the segmentation networks in SA and found that it improved the accuracy of gland segmentation while significantly reducing memory usage. proposed representative annotation (RA), which omits the uncertainty query in SA. RA trained a VAE for feature extraction and partitioned the feature space using hierarchical clustering. They selected representative samples in each cluster using a similar strategy to SA. RA achieved superior performance in gland segmentation on histological images, fungus segmentation on electron microscopy images, and whole heart segmentation on MRI. In breast cancer segmentation on immunohistochemistry images, changed the similarity measure in SA from to , which enhanced the diversity of the selected samples. Additionally, some works follow different formulations than those of SA in maximum cover. In keypoint detection of medical images, proposed a representative method to select template images for few-shot learning. First, they trained a feature extractor using self-supervised learning and applied the scale-invariant feature transform descriptor for initial keypoint detection. Next, they calculated the average cosine similarity between template images and the entire dataset. Finally, they picked the template combination with the highest similarity for annotation. found that Core-Set ( ), which followed the setting of the set cover, tends to select outliers, especially when the annotation budget is low. To address this issue, they proposed ProbCover, which changed the setting from set cover to maximum cover. With the help of self-supervised deep features and a graph-based greedy algorithm, ProbCover effectively avoided outlier selection in cover-based AL. Both set cover and maximum cover can be formulated from the perspective of submodular set functions ( ). These functions show diminishing returns. Specifically, given two sets and , , for every element that not in , a submodular set function has that . This property makes submodular set functions suitable for AL. Suppose the informativeness function is submodular. It means that each newly queried sample brings less informativeness gain than the previous one, which indicates that highly informative samples should be queried first. Besides, if we can formulate optimization problems in terms of monotonic and submodular functions, we can use a greedy algorithm to get near-optimal solutions in linear time. For AL, if is submodular and monotonic, it means that we could greedily select the samples that maximize . In cover-based AL, methods like SA and RA followed the setting of submodular functions, but the authors did not present their methods from this perspective. Introducing submodular functions would extend the formulation of AL and ensure the selected samples are both representative and diverse. Typical steps for this type of method involve calculating sample similarities, constructing a submodular optimization problem, and solving it using a greedy algorithm ( ). introduced an AL framework based on submodular information measures, effectively addressing issues such as scarcity of rare class, redundancy, and out-of-distribution data. In object detection, focused on samples of minority classes. They first constructed a reference dataset containing samples of certain classes of interest. Then, unlabeled samples similar to the reference set for annotation through submodular mutual information (SMI). SMI is used to measure the similarity between two sets. Suppose two sets , and a submodular function , the SMI is defined as . Please refer to for more detailed definitions of SMI. In discrepancy-based AL, unlabeled samples farthest from the labeled set are considered the most representative. The main idea is that if we queried such samples for multiple rounds, the discrepancy between the distributions of labeled and unlabeled sets would be significantly reduced. Therefore, a small set of samples could well represent the entire dataset. The key to these methods is measuring the discrepancy (i.e., distance) between two high-dimensional distributions. In this section, we present four discrepancies between probability distributions: similarity-based discrepancy, H-divergence, Wasserstein distance, and maximum mean discrepancy (MMD). As a practical and easy-to-implement metric, we can approximate the distance between distributions based on sample similarity. In gland and MRI infant brain segmentation, adopted the average cosine similarity as the distance between two datasets. They selected samples far from the labeled set and close to the unlabeled set. proposed UncertainGCN, which employed GCN to model the relationship between labeled and unlabeled samples. They selected the unlabeled samples with the lowest similarity to the labeled set. In object detection, constructed prototypes with sample features and prediction entropy. They selected unlabeled samples that were far from the labeled prototype. estimates the distance of distribution with the help of the discriminator from generative adversarial networks (GAN) ( ). More specifically, the discriminator tries to distinguish between labeled and unlabeled samples, and there is a close relationship between H-divergence and the discriminator’s output ( ). Variational adversarial active learning (VAAL) ( ) combined VAE with a discriminator for discrepancy-based AL. In VAAL, the VAE mapped samples to a latent space while the discriminator distinguished whether samples were labeled. These two are mutually influenced by adversarial training. VAE tried to fool the discriminator into judging all samples as labeled while the discriminator attempted to differentiate between labeled and unlabeled samples correctly. After multiple rounds of adversarial training, VAAL selected samples that the discriminator deemed most likely to be unlabeled for annotation. VAAL inspired many subsequent works. adopted multimodal information to improve VAAL. For multimodal medical images, they modified the VAE to reconstruct images of both modalities using the latent code of only one modality. The proposed method was evaluated on brain tumor segmentation, classification, and chest X-ray classification. trained the discriminator without adversarial training. replaced the discriminator’s binary label with sample uncertainty. They also combined features of VAE with features from the supervised model. adopted a neural network module for sample selection. To train such a module, they added another discriminator on top of VAAL, which aimed to differentiate between the real and VAE-reconstructed features for unlabeled samples. After adversarial training of both discriminators, the module selected uncertain and representative samples. combined learning loss for active learning with VAAL, feeding both loss ranking predictions and VAE features into the discriminator. is widely used for computing distribution distances. indicated that H-divergence compromises the diversity of sample selection, while Wasserstein distance ensures the queried samples are representative and diverse. They further proposed Wasserstein adversarial active learning (WAAL), which built upon VAAL and adopted an additional module for sample selection. They trained this module by minimizing the Wasserstein distance between labeled and unlabeled sets. WAAL selected samples that are highly uncertain and most likely to be unlabeled for annotation. formulated AL as an optimal transport problem. They aimed at minimizing the Wasserstein distance between the labeled and unlabeled sets with self-supervised features. They further adopted mixed-integer programming that guarantees global convergence for diverse sample selection. Moreover, considered the candidates as continuously optimizable variables based on self-supervised features. They randomly initialized the candidate samples at first. Then, they maximized the similarity between candidates and their nearest neighbors while minimizing the similarity between candidates and labeled samples. Finally, they selected the nearest neighbors of the final candidates for annotation. They proved the objective is equivalent to minimizing the Wasserstein distance between the labeled and unlabeled samples. measures the distance of two distributions as the distances between their mean features with kernel trick ( ). In active domain adaptation (will be detailed in Section ), adopted MMD to measure the distance between the source and target domain. Then, MMD was used to select representative and diverse samples in the target domain. It should be noted that the Wasserstein distance belongs to the family of integral probability metrics (IPM), while MMD simultaneously falls into the range of IPM and previously mentioned H-divergence. Please refer to for a more detailed taxonomy of the discrepancy between probability distributions. Density-based active learning tends to select samples from the most densely populated area of the data distribution. It employs density estimation to characterize the data distribution in a high-dimensional feature space. The likelihood is the estimated density of the data distribution, and a more densely populated area indicates a higher likelihood. In this case, representative samples are samples with high likelihood. However, such methods can easily cause redundancy in sample selection. As a result, techniques like clustering are frequently used to improve diversity in sample selection. Density-based AL directly estimates the data distribution, which prevents the need to solve complex optimization problems. In shoulder MRI musculoskeletal segmentation, adopted infoVAE ( ) to estimate the density of each sample in the labeled dataset and unlabeled pool. Specifically, MMD replaced the KL divergence as the regularization term in the training of infoVAE. The posterior probability by the encoder was used as the density metric. Samples with higher density regarding the unlabeled pool and lower density regarding the labeled dataset were selected for annotation. TypiClust ( ) projected samples to a high-dimensional feature space via a self-supervised encoder. The density of a sample was defined as the reciprocal of the L2 distances to its k-nearest neighbors. Additionally, TypiClust performed clustering beforehand to ensure the diversity of selected samples. proposed two variants of density-based AL. The first variant fixed the feature representation. The process was similar to TypiClust, but they maximized the distances between selected samples to ensure diversity. The other variant was in an end-to-end fashion. Feature representation and sample selection were trained simultaneously. This variant used a learnable k-Means clustering to jointly optimize cluster assignment and feature representation with a local smoothness constraint. It is worth noting that cover-based and density-based AL differ in both concept and methodology. In concept, samples in cover-based AL tend to cover the entire dataset. However, they do not have to lie in the densest area of the data distribution. For example, showed that Core-Set ( ), a popular cover-based method, tends to select outliers in the low-budget regime. In this case, cover-based AL is opposite to density-based AL, which also indicates that density-based AL may be a better choice in the low-budget regime. From the perspective of methodology, cover-based AL needs to solve an NP-hard problem in linear time with a greedy algorithm. Although this algorithm results in acceptable solutions, it is almost impossible to know how the AL performance would be if the optimal solutions could be achieved. For density-based AL, the NP-hard problem is replaced with density estimation, which is more computation-efficient. With a well-developed informativeness metric, most deep AL works simply adopted top-k to select samples with the highest informativeness for annotation. However, existing informativeness metrics face several issues, such as redundancy and class imbalance. These issues are exacerbated due to the unique characteristics of medical images. Despite the high variability, medical images of the same region-of-interest (ROI) could be classified into several groups, and images within each group share a high similarity ( ). Also, class imbalance is notorious in medical image analysis since the healthy objects often outnumber the diseased ones. Instead of proposing a better informativeness metric, we can improve the sampling strategy upon the top-k selection to effectively resolve these issues above. Besides, specific sampling strategies can also be used for combining multiple informativeness metrics. Furthermore, with the recent development of deep AL, more and more studies directly employ neural networks for sample selection. In this context, we no longer evaluate informativeness but directly choose informative samples with neural networks. Regrettably, despite the importance of sampling strategies in AL, prior works or surveys have seldom discussed their specific attributes. As one of the contributions of this survey, we systematically summarize different sampling strategies in AL, including diversity sampling, class-balanced sampling, hybrid sampling, and learnable sampling. The taxonomy of different sampling strategies in AL is shown in . Diversity strategies aim to reduce sampling redundancy in active learning, meaning certain selected samples are highly similar to each other. The lack of diversity leads to the waste of the annotation budget. Besides, redundancy in the training set causes the deep models to overfit to limited training samples, thus leading to a performance drop. Therefore, many AL methods employ diversity sampling to mitigate the redundancy in selected samples. In this section, we discuss four strategies of diversity sampling, including clustering, farthest-first traversal, determinantal point process (DPP), and specific strategies tailored to certain informativeness metrics. is one of the most commonly used strategies of diversity sampling. This strategy improves the coverage of the entire feature space, thereby easily boosting diversity. employed k-Means++ clustering on gradient embeddings to select diverse uncertain samples. Besides, boosted margin-based uncertainty sampling with hierarchical clustering. They selected samples with the smallest margins within each cluster. When the number of queries exceeded the number of clusters, samples from smaller clusters were prioritized. This method can scale to a huge annotation budget (e.g., one million). incorporated hierarchical clustering with cover-based AL. In their experiments, clustering showed consistent performance improvement in multiple medical imaging datasets, which demonstrated that clustering does improve the sampling diversity. It is important to highlight that clustering in this section is different from that of in Section . To ensure that the selected samples are sufficiently representative, clustering-based AL generally chooses samples that are closest to the cluster centers. However, when clustering is used to enhance diversity, we can not only select samples closest to the cluster centers, but also select samples with the highest uncertainty, or even randomly select within each cluster. Therefore, clustering can serve as a plug-and-play technique to conveniently enhance the sampling diversity in AL. is a stochastic probability model for selecting subsets from a larger set. DPP reduces the probability of sampling similar elements to ensure diversity in the results. employed two DPPs for sample selection: Uncertainty DPP is based on uncertainty scores, while Exploration DPP aims to find samples near decision boundaries. Then, sampling results from both DPPs were sent for expert annotation. However, DPP is more computationally intensive compared to clustering. compared the performance and time cost of using k-Means++ and k-DPP. Results showed that their performance is similar, but the time cost for k-Means++ is significantly lower than that for k-DPP. Besides, adopted DPP in AL for medical image reconstruction, please refer to Section for details. could also be used for better diversity. In prostate segmentation of MRI, randomly partitioned the entire dataset into different batches, which were referred to as ‘stochastic batches’. Batches with the highest uncertainty scores were selected for annotation. Experimental results showed that the stochastic batches consistently improved the performance of various uncertainty-based AL methods under an extremely low budget. Their extended works ( ) further illustrated the effectiveness of stochastic batches on the anterior and posterior hippocampus segmentation. is also a widely used strategy for diverse queries, which was first adopted by . This strategy requires the distance between sampling points to be as large as possible, which leads to a more uniform distribution of selected samples in the feature space. adopted a farthest-first traversal strategy with cosine distance for a diverse initial labeled dataset. Experiments on breast ultrasound, liver CT, and chest X-ray segmentation showed significant effectiveness of the farthest-first traversal. Besides, and improved the diversity with farthest-first traversal with their proposed contextual diversity and GNN-augmented features, respectively. In uncertainty-based AL, BatchBALD ( ) extended BALD-based uncertainty AL to batch mode. Results showed that BatchBALD improved the sampling diversity compared to . FI-based methods formulated AL as a semi-definite programming (SDP) problem to improve sampling diversity and various methods were employed for solving SDP. used a commercial solver to solve SDP, while proposed a greedy algorithm to adapt to high-dimensional feature space. In skin lesion analysis, introduced image hashing for diversity sampling. In their proposed method, the first principal component of each image was used for feature representation. Then they mapped similar images into the same buckets using local sensitivity hashing. Samples were uniformly selected from each bucket for human annotation. Class imbalance is a common issue for DL in medical image analysis, where a small set of classes have many samples while the others only contain a few samples ( ). For example, long tail distribution of classes existed in almost all tasks of medical image classification, such as skin lesion classification and whole-slide image classification. Training on imbalanced datasets can lead to the overfitting of the majority classes and underfitting of the minority classes. Apart from dealing with class imbalance during training, AL mitigates class imbalance by avoiding over-annotation of the majority classes and enhancing the annotation of the minority classes during dataset construction. In a class-imbalanced COVID-19 dataset, evaluated multiple informativeness scores and sampling strategies. Results showed that diversity sampling is more favored for class-imbalance. assumed that samples closer to the tail of the distribution are more likely to belong to the minority classes. Thus, the tail probability is equivalent to the likelihood of minority classes. Specifically, they trained a VAE for feature extraction and adopted copula to estimate the tail probabilities upon VAE features. Finally, informative samples were selected with clustering and unequal probability sampling. The proposed method was validated on the ISIC 2020 dataset, which has a long-tailed distribution. used submodular mutual information to focus more on samples of minority classes. They achieved excellent results on medical classification datasets in five different modalities, including X-rays, pathology, and dermoscopy. In blood cell detection under microscopy, requested expert annotation of a sample whenever its classification probability of the minority class exceeded 0.2. Besides, directly estimated the probability of a classifier making a mistake for a given sample and decomposed it into three terms using Bayesian rules. First, they trained a VAE to estimate the likelihood of the data given a predicted class. Then, an additional classifier was trained upon VAE features to estimate class prior probabilities and the probability of mislabeling a specific class. By considering all three probabilities, they successfully mitigated class imbalance in AL. The proposed method achieved good performance on stepwise class-imbalanced CIFAR-10 and CIFAR-100 datasets. For uncertainty-based methods, introduced an optimization framework to maintain class balance. They compensated the query of minority classes with the most confident samples of that class, leading to a more balanced class distribution in the queried dataset. Due to certain AL methods selecting regions instead of the entire image for annotation, there is a need to ensure that the selected regions contain rare or small objects (e.g., optic chiasma or optic nerve in head and neck multi-organ segmentation). and both proposed class-balanced sampling strategies for such scenarios, as detailed in Section . In AL, more and more works use multiple informativeness metrics simultaneously. However, how to effectively integrate multiple metrics remains a critical issue. This issue is addressed by the hybrid sampling discussed in this section. Two approaches to hybrid sampling are often used, including multi-round sampling and metric fusion. first selects a subset of samples based on one particular informativeness metric and continues sample selection within this subset based on another informativeness metric. Multi-round sampling is widely used in AL for medical image analysis for its convenience ( ). For example, SA ( ) performed representativeness sampling based on uncertainty to reduce redundancy in the sampled set. Besides, employed an adaptive strategy that sets dynamic weights to adjust the budget of representativeness and uncertainty sampling. The weight of representativeness sampling is larger initially, while the situation is reversed in the latter phase. This is because representativeness methods can quickly spot typical data, while uncertainty methods continuously improve the model by querying samples with erroneous predictions. is another widely used approach of hybrid sampling. It directly combines different informativeness metrics. For example, one could directly sum up all metrics and select the samples with the highest values for annotation. Metrics fusion is also widely used in AL of the medical domain ( ). Besides, ranked batch-mode ( ) can adaptively fuse multiple metrics in AL. Previously mentioned AL methods typically follow a “two-step” paradigm, which first involves the evaluation of informativeness and then selects samples based on specific heuristics (i.e., sampling strategy). However, learnable sampling skips the informativeness evaluation and directly uses neural networks for sample selection. In this context, the neural network is known as a “neural selector”. One of the most common methods of learnable sampling is to formulate sample selection as a reinforcement learning (RL) problem, where the learner and the dataset are considered the environment, and the neural selector serves as the agent. The agent interacts with the environment by selecting a limited number of samples for annotation, and the environment returns a reward to train the neural selector. In medical image classification, employed an actor–critic framework where the critic network is used to evaluate the quality of the samples selected by the neural selector. This method has performed excellently in lung CT disease classification and diabetic retinopathy classification of fundus images. Besides, adopted a probabilistic policy network as the neural selector. The rewards returned by the environment encouraged the neural selector to choose diverse and representative samples. The neural selector is trained using the REINFORCE algorithm ( ). utilized contextual diversity as RL rewards and trains a bidirectional long short-term memory network as the neural selector. For more works on learnable sampling in AL, such as formulating AL as few-shot learning or training neural selectors by meta-learning, please refer to the survey of . As discussed in Section , the high annotation cost has severely dragged down the development of DL in medical image analysis. Despite the wide use of AL in medical image analysis, various methods have been proposed to reduce the large amount of labeled data required for training deep models, such as semi-supervised and self-supervised learning, etc. These methods, including active learning, are collectively called label-efficient deep learning ( ). Label-efficient learning is a broad concept that includes all related technologies designed to improve annotation efficiency. In the real-world practice of AL in medical image analysis, there is still room for higher label efficiency by integrating AL with other label-efficient techniques. For the example of AL in medical image segmentation, since many samples were left unlabeled in the cycle of AL, we could further include them to achieve better performance by integrating AL with semi-supervised learning. The rapid development of self-supervised learning in medical image analysis introduced many powerful pre-trained models ( ). These models are also valuable in AL of medical image analysis for their superior ability of feature extraction. For another circumstance, since the ROI in medical imaging is usually small, we could select and annotate the informative regions that contain the ROI in AL instead of annotating the whole image. As a result, integrating active learning with other label-efficient techniques holds significant potential to increase annotation efficiency. However, existing surveys have not yet systematically organized and categorized this line of research. Hence, as one of the main contributions of this survey, we comprehensively reviewed the integration of AL with other label-efficient techniques, including semi-supervised learning, self-supervised learning, domain adaptation, region-based annotation, and generative models. Additionally, how each surveyed work integrated with other label-efficient techniques is summarized in . Semi-supervised learning ( ) aims to boost performance by utilizing unlabeled data upon supervised training. The need for tedious human annotation can be further reduced by integrating AL and semi-supervised learning in medical image analysis. The reason is that AL and semi-supervised learning complement each other. Specifically, a large pool of unlabeled images should be collected from the hospital information systems to train a DL model for some clinical applications. With the help of AL, the DL model is trained on an optimal labeled dataset constructed with a certain AL method, which reduces the annotation workload for doctors. However, massive unlabeled samples sit idle during the model training in the AL cycle. By combining the AL with semi-supervised learning, the model can be trained on both labeled and unlabeled samples ( ). This section will introduce the integration of AL and semi-supervised learning from the perspectives of pseudo-labeling and consistency regularization. Pseudo-labeling ( ) is one of the most straightforward methods in semi-supervised learning. It uses the model’s predictions of unlabeled data as pseudo-labels and combines them with labeled data for supervised training. Although it is possible to assign pseudo-labels to all unlabeled samples for training, it could introduce noise. To mitigate this, proposed cost-effective active learning (CEAL), integrating pseudo-labeling with uncertainty-based AL. Specifically, CEAL sent the most uncertain samples for expert annotation and assigned pseudo-labels to the most confident samples. Many subsequent works have built upon the ideas of CEAL. adopted the CEAL framework in the melanoma segmentation and used the MC dropout for uncertainty estimation. In medical image segmentation, refined the pseudo-labels with dense conditional random fields. Additionally, proposed a new approach for selecting samples for oracle annotation and pseudo-labeling in the Gleason grading of prostate cancer with histopathology images. They employed curriculum learning to categorize all samples into hard and easy. Hard samples were all sent for oracle annotation. For the easy samples, they evaluated the presence of label noise based on the training loss. Easy samples with low training loss were used for pseudo-labels to assist training, whereas easy samples with high loss were considered noisy and excluded from training. Consistency regularization aims to enforce similar outputs under perturbations of input data or model parameters. Maximizing consistency serves as an unsupervised loss for unlabeled samples, which helps improve the robustness, reduces overfitting, and improves model performance. Many works integrated existing consistency-based semi-supervised methods into the training process of AL. In chest X-ray classification, incorporated several semi-supervised methods with AL to further reduce annotation costs, including MeanTeacher ( ), VAT ( ) and NoTeacher ( ). combined their proposed COD with MeanTeacher ( ), demonstrating superior performance. combined density-based AL with different existing semi-supervised methods. Results showed that the proposed method outperforms other active learning methods and excels in semi-supervised learning. Consistency could be also used for sample selection. introduced a semi-supervised active learning framework. Consistency here was used for both semi-supervised training and evaluating informativeness. In this framework, samples are fed into the model multiple times with random augmentations. The consistency loss of unlabeled samples was implemented by minimizing the variance between multiple outputs. They further selected less consistent samples for annotation. Results showed that combining AL with semi-supervised learning significantly improves performance. Besides, combined AL with both pseudo-labeling and consistency regularization. The unlabeled images first underwent both strong and weak data augmentations. When the confidence level of the weakly augmented images exceeded a certain threshold, they used these samples for semi-supervised training. Specifically, predictions of the weakly augmented images were assigned as pseudo-labels, and the outputs of the strongly augmented images were forced to be consistent with the pseudo-labels. However, when the confidence level was lower than the threshold, they used these samples for AL. A balanced uncertainty selector and an adversarial instability selector were used to select samples for oracle annotation. They validated the effectiveness of their proposed method in grading metastatic epidural spinal cord compression with MRI images. Integrating semi-supervised learning with AL has achieved successful applications. However, its effectiveness is constrained by the dataset size. This limitation is particularly evident for medical imaging datasets which are relatively small. In clinical practice, plenty of raw medical images are stored in hospital information systems without human annotation. Self-supervised learning ( ) could be a vital tool for mining information hidden in those raw images. Its idea is to train the model with the supervision of the data itself, thus allowing pre-training on a large unlabeled dataset. Many studies have shown self-supervised pre-trained models could achieve impressive performance by finetuning on a few randomly selected labeled samples in medical image analysis ( ). A natural expectation is to integrate active learning strategies with self-supervised learning, aiming for higher annotation efficiency over mere random sampling ( ). Besides, these models could also act as a powerful feature extractor, which provides good initialization for AL. In this section, we will first introduce how self-supervised models solve the cold-start problem in AL and then explore different ways of integrating AL with self-supervised learning. Current AL methods usually require an initial labeled dataset to train the model for start and ensure reliable informativeness evaluation. However, when the initial labeled set is small or even absent, the performance of these AL methods drops dramatically, sometimes even worse than random sampling ( ). Studies also showed that simply integrating self-supervised learning with AL baselines leads to inferior performance than random sampling ( ). This phenomenon is known as the cold-start problem which commonly exists in AL of various domains, including medical image analysis ( ). Tackling the cold-start problem is vital for improving the efficacy of AL, especially in the medical domains where annotation costs are extremely high. A key solution to the cold-start problem in AL is selecting the optimal set of initial labeled samples, which requires different strategies than the existing AL methods. Early attempts focused on utilizing the fully supervised pre-trained models to address the cold-start problem in AL. and their subsequent work ( ) used ImageNet pre-trained models to select samples for annotation from completely unlabeled datasets in medical image analysis. They combined entropy and disagreement as informativeness metrics, where the disagreement was the KL divergence of prediction probabilities between different patches of the same sample. They also introduced randomness to balance exploration and exploitation. Experiments on two colonoscopy datasets and a CT pulmonary embolism detection dataset showed superior performance than other competitors. Self-supervised pre-trained models offer a good initialization for effectively tackling the cold-start problem in AL. ALPS ( ) was the first to introduce the cold-start problem in AL and employed self-supervised pre-trained models to address this issue. Based on a contrastive learning feature extractor, CALR ( ) employed BIRCH clustering and chose the samples with maximum information density within each cluster for labeling. Compared to k-Means, BIRCH clustering is less sensitive to outliers and can further identify noisy samples. TypiClust ( ) theoretically proved that querying typical samples is more beneficial for a low annotation budget. Therefore, based on self-supervised features, TypiClust selected samples from high-density areas of each k-Means cluster. Beyond that, employed a graph-based greedy algorithm to select the optimal initial samples based on self-supervised features. In CT segmentation, proposed ProxyRank which designed new pretext tasks for self-supervised pre-training. The model was trained to learn the threshold segmentation by an abdominal soft-tissue window. Results indicated that the proposed method significantly outperforms random sampling in selecting initial samples. To benchmark the effectiveness of different cold-start AL methods in 3D medical image segmentation, reproduced ALPS, CALR, TypiClust, and ProxyRank on five MSD datasets ( ). Results showed that TypiClust stands out from the four competitors. However, no method consistently outperformed random selection on all five datasets, which calls for further exploration of cold-start AL in medical image analysis. The simplest way is leveraging the high-quality features of the self-supervised pre-trained models. Many studies are based on a powerful self-supervised feature extractor ( ). in self-supervised learning are designed to derive supervision directly from the data itself. Solving these pretext tasks on large-scale unlabeled data, the model acquires useful feature representations that reflect data characteristics. Different pretext tasks correspond to different pre-training paradigms, the typical ones including rotation prediction ( ), contrastive learning ( ), and masked image modeling ( ), etc. Related works generally employed the loss of pretext task for AL. In , the loss of contrastive learning was used to tackle cold-start problem for AL in medical image analysis. They assumed that samples with higher losses are more representative of the data distribution. Specifically, they pre-trained on the target dataset with momentum contrastive learning ( ), and then used k-Means clustering to partition the unlabeled data into multiple clusters, selecting the samples with the highest contrastive loss within each cluster for annotation. They then selected samples with the highest contrastive loss in each cluster for annotation. The proposed method addressed the class imbalance caused by the bias of traditional AL methods, and the failure to detect anomalies when the number of initially labeled datasets was limited. This method showed superior performance in PathMNIST, OrganMNIST, and BloodMNIST ( ). found a strong correlation between the loss of pretext tasks and the loss of downstream tasks. Thus, they initially focused on annotating samples with higher loss of pretext tasks and later shifted to those with lower loss. Results showed that rotation prediction performed the best among different pretext tasks. Furthermore, recent works leverage self-supervised learning in other ways for AL. introduced one-bit annotation into AL for classification tasks. In this setting, oracles only returned whether the prediction was right or wrong rather than its specific class label. Contrastive learning was adopted to pull the correct predictions closer and push away wrong predictions from their predicted classes. Results indicated that the proposed method outperforms other AL methods regarding bit information. integrated contrastive learning into AL to address the class distribution mismatch, where unlabeled data includes samples out of the class distribution of the labeled dataset. In this work, contrastive learning was adopted to filter samples of mismatched classes and highlight sample informativeness by carefully setting negative samples. Their extended work ( ) provided more theoretical analysis and experimental results and also integrated existing label information into the proposed framework. Most AL works require the oracle to label the full image in medical image analysis. However, labeling a full image can introduce redundancy in fine-grained tasks like segmentation or detection, resulting in an inefficient use of the annotation budget. For the example of abdomen multi-organ segmentation, large organs that are easy to segment (e.g., liver or spleen) do not need exhaustive annotation. Instead, those budgets would be better spent on small organs that are hard to segment, like the esophagus and adrenal glands. To address this issue, images could be divided into non-overlap regions for higher annotation efficiency, and experts can opt to annotate specific regions within an image, which is termed “region-based active learning”. This section introduces region-based active learning from the perspectives of patches and superpixels, which means that AL methods mentioned in this section selected either patches or superpixels within an image for annotation. Patches are most commonly used in region-based active learning, generally represented as square boxes. combined uncertainty and annotation cost to select the informative patches for annotation. In retinal blood vessels segmentation of fundus images, selected patches with the highest uncertainty for annotation. Furthermore, they utilized latent-space mixup to encourage linearization between labeled and unlabeled samples, thus leveraging unlabeled data to improve performance. employed deep reinforcement learning to automatically select informative patches for annotation. In gray matter and white matter segmentation of pathology images, first split the whole slide image into multiple patches. With the confidence (i.e., maximum predictive probability) of each patch, a mean filter of size 5 × 5 is used to aggregate the confidence of the neighboring patches. As a result, one aggregated metric corresponded to a region of 5 × 5 patches, and regions with the highest uncertainty were selected for annotation. Besides, adopted an adaptive region selection with non-square patches for whole-slide images. Instead of sampling square patches, they dynamically determined the size of each non-square patch by carefully locating an informative area on each slide. The proposed method demonstrated improvement in annotation efficiency and robustness to AL hyperparameters compared to the square patch baseline. Superpixels are also widely used in region-based active learning. Superpixel-based AL initially pre-segments the images with superpixel generation algorithms based on color and texture ( ), and then calculates the informativeness of each superpixel. The informativeness metric of each superpixel is the average of its constituent pixels. adopted uncertainty and disagreement between different viewpoints to select informative superpixels for annotation. In OCT segmentation, proposed edge-based entropy and divergence to select highly uncertain superpixels for annotation. Experiments on three datasets were conducted to illustrate the effectiveness of their method. For superpixel selection, proposed dominant labeling which is the majority class label of all pixels in the superpixel. They assigned the dominant labeling to every pixel within a superpixel, thus eliminating the need for detailed delineation. They further introduced a class-balanced sampling strategy to better select superpixels containing minority classes. Results showed that dominant labeling with superpixels significantly outperforms precise labeling with patches under the same number of labeling clicks. As a follow-up work, proposed to adaptively merge and split spatially adjacent, similar, and complex superpixels, respectively. This approach yielded better performance than with dominant labeling. utilized the superpixels to estimate the regional consistency which is the difference between the prediction and the dominant class of each superpixel. Combining other metrics like entropy and diversity, they selected the most uncertain foreground and background superpixel to reduce the annotation cost. In recent years, the advancement of deep generative models enabled high-quality generation and flexible conditional generation. For example, a trained model could generate the corresponding lung X-ray scan when conditioned on a lung mask. By integrating generative models, we can further improve the annotation efficiency of AL. In this section, we discuss how AL can be combined with generative models from two aspects: data augmentation and generative active learning. The simplest approach considers the synthetic sample produced by generative models as advanced data augmentation. These methods utilize label-conditioned generative models. As a result, it is guaranteed that all synthetic samples are correctly labeled since specifying the labels is a prerequisite for data generation. This method enables us to acquire more labeled samples without any additional annotations. argued that most synthetic samples produced by generative models are not highly informative. Therefore, they first adopted the BALD uncertainty to select samples for annotation, then trained a VAE-ACGAN on these labeled data to generate more informative synthetic samples. used conditional GANs to generate chest X-rays with varying diseases to augment the labeled dataset. Then, MC Dropout was used to select and annotate highly uncertain samples. With the help of AL and synthetic samples, they achieved performance near fully supervised using only 35% of the data. Training conditional generative models requires a large amount of labeled data, while the labeled dataset in AL is often relatively small. To address this issue, proposed a conditional SinGAN ( ) that only requires one pair of images and masks for training. The SinGAN improved the annotation efficiency for nuclei segmentation. integrated implicit semantic data augmentation (ISDA) ( ) into AL. They initially used ISDA to augment unlabeled samples, then selected samples with large diversity between different data augmentations for annotation. The model is trained on both the original data and its augmentations. trained a VAE for synthesizing informative and non-redundant samples. These samples are generated by first sampling in the latent space of VAE and feeding them to the VAE decoder. Besides, scores of label preservation and redundancy avoidance were adopted to pick the most informative synthetic samples. The proposed method was tested in chest X-ray classification and multiple toy datasets from MedMNIST ( ). Generative active learning selects synthetic samples produced by generative models for oracle annotation, thus without requiring a large unlabeled sample pool. The advantage of this approach lies in its ability to continuously search the data manifold through generative models. It is worth noting that works in this section follow the setting of membership query synthesis, while works in the last section follow the setting of pool-based active learning. This distinction arises because generative models in the last section were solely utilized to augment existing labeled datasets. attempted to generate uncertain samples with GAN for expert annotation. Unfortunately, the quality of the generated samples was low and included many samples with indistinguishable classes. Since experts find it difficult to annotate low-quality synthetic samples, alternative methods are needed to annotate these samples. first trained a bidirectional GAN to learn the data manifold. They then selected uncertain areas in the feature space and generated images within these regions using bidirectional GAN. Finally, they used physics-based simulation to provide labels for the generated samples. In calcification level prediction in aortic stenosis of CT, they improved annotation efficiency by up to 10 times compared to random generation. Domain Adaptation (DA) ( ) has wide applications in medical image analysis. It aims to transfer knowledge from the source to the target domain, thus minimizing annotation costs. Currently, the most common setting of DA is unsupervised domain adaptation (UDA), in which the source domain is labeled while the target domain is unlabeled. For the example of abdominal multi-organ segmentation, we can train a domain-adaptive segmentation model with labeled MR images alongside unlabeled CT images to achieve good performance on the CT domain ( ). However, the performance of UDA still lags behind fully supervised learning in the target domain. Selecting and annotating informative samples would be beneficial to bridge this gap. This setting is known as active domain adaptation (ADA). For better queries in ADA, one should consider both uncertainty and representativeness regarding the target domain. The latter is commonly referred to as domainness or targetness in ADA. This section reviews the image-wise and region-wise ADA. In this section, ADA methods performed an image-level selection, which involves most of the ADA works. was the first to introduce the concept of ADA and combined domain adversarial learning with AL. Through a domain discriminator and task model, they performed importance sampling to select target domain samples that are uncertain and highly different from the source domain. combined query-by-committee, uncertainty, and domainness in ADA. They adopted a domain discriminator to select samples with high domainness and employed Gaussian kernels to filter out anomalous and source-similar samples of the target domain. Random sampling was also used to improve diversity. performed k-Means clustering on target domain samples and selected cluster centers for annotation. The cluster centers were weighted by uncertainty, thus ensuring that selected samples were uncertain and diverse. In segmentation tasks, introduced the idea of anchors in ADA. They concatenated features of different classes from the source domain images. Cluster centers of these concatenations were referred to as anchors. They then computed the distance between each target sample and its nearest anchor. Target samples with the highest distance were requested for annotation. introduced the concept of energy ( ) into ADA. The energy is inversely proportional to the likelihood of the data distribution. In this work, the model trained on the source domain was used to calculate the energy of target domain samples. Samples with high energy were selected for annotation, which suggested they are representative of the target domain and substantially different from the source data. selected samples with high uncertainty and prediction inconsistency to their nearest prototypes. In the context of medical image analysis, tackled domain shifts in the setting of federated active learning. They proposed an EDL-based framework with a global model across all clients and local models for each client. In this work, the EU is related to domain shifts between the global model and local data. Therefore, the AUs of the global and local models were calibrated by the EU, thus improving performance. Results on multiple medical imaging datasets showed its effectiveness in reducing annotation costs. In nasopharyngeal carcinoma tumor segmentation, proposed a source-domain and target-domain dual-reference strategy to select informative samples for annotation. Specifically, the features of the source samples were clustered and the cluster centers were reference samples. Target samples with the highest and lowest similarity to the references are selected for annotation, which were treated as domain-invariant and domain-specific samples, respectively. To better utilize the annotation budget, some ADA works also selected patches or superpixels within an image for annotation. proposed LabOR, which first used a UDA pre-trained model to generate pseudo-labels for target samples, which was used to train two segmentation heads. They maximized the disagreements between the two heads and annotated regions that exhibited the most disagreement. LabOR achieved performance close to full supervision with only 2.2% of target domain annotations. In , uncertainty and regional impurity were used to select and annotate the most informative patches. Regional impurity measured the number of unique predicted classes within the neighborhood of a pixel, which presents the edge information. They used extremely small patches (e.g., size of 3 × 3) for annotation and achieved performance close to full supervision with only 5% of the annotation cost. proposed a density-based method to select the most representative superpixels in the target domain for annotation. They employed Gaussian mixture models (GMM) as density estimators for superpixels in both the source and target domains, aiming to select those with high density in the target domain and low density in the source domain. Due to the potential of significantly reducing annotation costs, AL is receiving increasing attention in medical image analysis. The unique traits of medical imaging require us to design specialized AL methods. Building on the foundation of the previous two sections, this section will focus on introducing AL works tailored to medical image analysis across different tasks, including classification, segmentation, and reconstruction. Additionally, in , we list all the AL works related to medical image analysis in this survey, providing the name of the used dataset, its modality, ROIs, and corresponding clinical and technical tasks. Common clinical tasks like disease diagnosis, cancer staging, and prognostic prediction can be formulated as medical image classification. Most AL works in medical imaging classification directly employ general methods, such as using class-balancing sampling in Section to mitigate the long-tail effect of medical imaging datasets. However, specialized design of AL algorithms is required for certain modalities of medical image classification. For example, the classification of chest X-rays often involves the idea of multi-label. Besides, classifying pathological whole-slide images typically needs to be formulated as a multiple-instance learning problem. This section will introduce AL works specifically targeted at classification problems in chest X-rays and pathological whole-slide images. Chest X-ray examinations are crucial for screening and diagnosing lung, cardiovascular, skeletal, and other thoracic diseases. Computer-aided diagnosis in this domain has been extensively researched, including AL works aimed at reducing annotation costs for physicians. introduced saliency maps to select informative samples for annotation. To aggregate the per-pixel saliency maps into a single scalar, they explored three different approaches, including computing the kurtosis of the saliency map, utilizing multivariate radiomic features, and combining deep features of autoencoders and clustering. Results demonstrated that the aggregation using deep features performs the best. introduced a gist-set to select samples near the decision boundary. Besides, uncertain samples with high entropy were sent for annotation, while the confident samples were assigned as pseudo-labels. Additionally, they adopted momentum updates to enhance the stability of the sample predictions. To handle the annotation noise, proposed a framework called ‘active label cleaning’. This framework ranked samples based on estimated label correctness and labeling difficulty. Experiments on the chest X-ray dataset showed that the proposed method improves performance by efficiently reducing label noise with fewer expert annotations compared to random selection. However, multiple diseases and abnormalities often coexist simultaneously in diagnosing chest X-rays. Therefore, multi-label classification has been introduced, allowing each sample to be categorized into multiple classes ( ). Consequently, AL algorithms for chest X-ray classification must adapt to the multi-label setting. Built upon saliency maps, further introduced GNN to model the inter-relationships between different labels. In this work, each class was treated as a node in a graph, with the relationships between classes represented as edges. They employed various techniques to aggregate information between different classes. As a follow-up work, further introduced graph multiset transformers ( ) for more powerful inter-label relationships than GNN. Compared to modalities like X-ray, CT, and MRI, pathological whole-slide images (WSIs) provide microscopic details at the cellular level, making them critically important for tasks such as cancer staging and prognostic prediction. However, WSIs are very large, with maximum resolutions reaching pixels. To handle these large images for deep learning, WSIs are usually divided into many small patches. Fully supervised methods require patch-level or even cell-level annotations, resulting in high annotation costs. AL can effectively improve annotation efficiency. For instance, in classifying breast pathological images, used entropy as the uncertainty metric. Uncertain patches were sent for annotation, whereas those with low entropy were given pseudo-labels to assist training. In AL of patch-level histological tissue classification, proposed category-wise curriculum querying to dynamically adjust the weight of uncertainty sampling of each class. They further proposed negative pre-training with wrong predictions to better distinguish the visually similar classes. To obtain fine-grained cellular annotation from WSI, proposed a human-augmenting AI-based labeling system with the help of AL. An active learner was used to select the next best patch for annotation and a classifier was trained for suggesting annotation. Specifically, Core-Set ( ) was used as the active learner. Experiments with pathologists demonstrate its ability to reduce workload by around 90% and slightly improve data quality across various cellular labeling tasks. Nevertheless, pathologists might only provide WSI-level annotations in real-world clinical scenarios. Consequently, a prevailing direction in research is to formulate WSI classification as the weakly-supervised multi-instance learning (MIL) ( ). In this framework, the entire WSI is viewed as a bag, and patches within each WSI are treated as instances within that bag. A well-trained MIL learner can automatically identify relevant patches based on WSI-level labels, thus significantly reducing annotation costs. For example, a trained MIL classifier can automatically spot related patches by annotating whether or not cancer metastasis is present in a WSI. Nonetheless, task-relevant patches are often outnumbered by irrelevant ones, making MIL convergence more challenging. In MIL-based pathological WSI classification, AL filters out irrelevant patches and selects informative patches for annotation. Based on attention-based MIL, adopted MC Dropout to estimate both attention and classification uncertainties of each patch, then sent the most uncertain patches in each WSI for expert annotation. found that in addition to patches related to the target (e.g., tumors, lymph nodes, and normal cells), WSIs contain many irrelevant patches (e.g., fat, stroma, and debris). Therefore, they adopted the open-set AL ( ), in which the unlabeled pool contained both target and non-target class samples. They combined feature distributions with prediction uncertainty to select informative and relevant patches of the target class for annotation. Segmentation is one of the most common tasks in medical image analysis, capable of precisely locating anatomical structures or pathological lesions. However, training a segmentation model requires pixel-level annotation, which is time-consuming and labor-intensive for doctors. Therefore, active learning has been widely used in medical image segmentation and has become an important method to reduce annotation costs. Based on the unique traits of medical imaging, this section will focus on specialized designs in AL for medical image segmentation, including slice-based annotation, one-shot annotation, and annotation cost. In 3D modalities like CT and MRI, adjacent 2D slices often exhibit significant semantic redundancy. Consequently, annotating only the key slices of each sample can reduce annotation costs. AL works mentioned in this section select 2D slices within a 3D volume for annotation. Representativeness-based methods have been widely applied in this line of work. For instance, utilized autoencoders to learn the semantic features of each slice, then selected and annotated key slices from axial, sagittal, and coronal planes with a strategy similar to RA ( ). Specifically, they initially trained three 2D segmentation networks and one 3D segmentation network, where the inputs for the 2D networks are slices from different planes. These segmentation networks were used to generate four sets of pseudo-labels and subsequently to train the final 3D segmentation network. Results showed that this slice-based strategy outperforms uniform sampling. Building upon this method, adopted a similar strategy in 3D knee cartilage and bone segmentation. Besides, incorporated a self-attention module into the autoencoder to enhance slice-level feature learning. Uncertainty methods have also been introduced for selecting key slices. introduced a quality assessment module to select slices with the highest predicted average IoU score. In muscle segmentation of CT images, selected key slices and key regions. This work adopted clustering to select key slices and further selected regions with high uncertainty within each key slice for annotation. In recent years, hybrid strategies combining both uncertainty and representativeness were proposed for slice-based annotation. In shoulder MRI musculoskeletal segmentation, adopted the variance of multiple MC dropout runs as the uncertainty metric. The posterior probability estimated by infoVAE ( ) was used as the representativeness metric. proposed a hybrid strategy to select informative slices in musculoskeletal segmentation of lower extremities, where uncertainty was estimated with a Bayesian U-net while the representativeness was based on cosine similarity. They further adopted mutual information to minimize the sample redundancy following . The proposed method achieved impressive performances on both the MRI and CT datasets. Currently, most AL works require multiple rounds of annotation. However, this setting could be impractical in medical image segmentation. Multi-round annotation requires physicians to be readily available for each round of labeling, which is unrealistic in practice. If physicians cannot complete the annotations on time, the AL process must be suspended. In contrast, one-shot annotation eliminates the need for multiple interactions with physicians. It also allows for selecting valuable samples in a single round, thus reducing time costs. Both one-shot annotation and cold-start AL aim to select the most optimal initial annotations. However, the former allows for a higher annotation budget and strictly limits the number of interactions with experts to just one. Most relevant works combine self-supervised features and specific sampling strategies to achieve one-shot annotation. For example, RA ( ) is one of the earliest works in one-shot AL for medical image segmentation. They applied the VAE feature and a representativeness strategy to select informative samples for annotation in one shot. RA performed excellently in gland segmentation of pathological images, whole-heart MRI images, and fungal of electron microscopic images. proposed a representativeness-based framework for selecting key slices for annotation in one shot. They adopted self-learning to learn the semantic representation of each slice and used it to propagate the expert annotation to different slices. combined features of contrastive learning with farthest-first sampling to achieve one-shot annotation. The proposed method demonstrated effectiveness on the ISIC 2018 and lung segmentation datasets. Additionally, utilized auto-encoding transformations for self-supervised feature learning. They selected and annotated samples with high density based on reachable distance. Current AL works often assume equal annotation costs for each sample. Yet, this is not the case in medical image segmentation, where the time to annotate different samples can differ greatly. AL techniques can better support physicians by considering annotation costs (e.g., annotation time). In detecting intracranial hemorrhage of CT scans, combined predictive disagreement with annotation time to select samples for annotation. Specifically, they adopted the Jensen–Shannon divergence to measure the disagreement between the outputs of multiple models. Annotation time for each sample was estimated by the length of the segmentation boundary and the number of connected components. In this work, AL was framed as a 0–1 knapsack problem, and dynamic programming is used to solve this problem for selecting informative samples. In brain tumor segmentation, derived the annotation cost of a slice based on the distance between the queried slices and the already-labeled slices. Specifically, lower distance represented lower annotation cost. The rationale is that the annotation cost of labeling a similar slice would be cheaper than that of the unfamiliar slices. In brain structure segmentation, further considered the spatial relationships between multiple regions of interest to more accurately estimate the annotation cost. Moreover, the average Dice coefficient of previous rounds was used to predict the average Dice for current segmentation results. They selected and annotated regions that can maximize the average Dice. Despite the success of automatic segmentation in medical imaging, there is still a potential for errors in clinical applications due to domain shifts or unseen ROIs. Interactive segmentation ( ) could produce real-time adjustment of the current segmentations based on the user inputs of clicks, bounding boxes, or scribbles. As a result, interactive segmentation could rapidly tune the model towards current clinical applications with the guidance of doctors. For the sake of flexibility, current interactive segmentation methods accept annotations for any position. However, such a paradigm would be more efficient when the model itself could suggest where to annotate, which is exactly what active learning is good at. Therefore, combining AL and interactive segmentation would further reduce the annotation cost. In this section, all the mentioned papers worked interactively with different labeling units. Before the DL era, had already integrated AL in the interactive cell segmentation. They selected the most informative superpixels for interactive annotation with expected prediction error. In MRI fetal brain segmentation, proposed an uncertainty-guided framework for interactive refinement. They developed a novel network architecture to produce multiple segmentation results simultaneously, and the variance between different predictions served as the uncertainty metric. Slices with the highest uncertainty were fetched for interactive refinement by human experts. In interactive segmentation of 3D medical images, proposed a quality predictor, which produced a predicted IoU score with the current segmentation for each slice. With the interactive segmentation network, the quality predictor suggested slices with lower scores for expert annotation which could be in the form of scribble, bounding box, or extreme clicking. In , the most informative foreground and background superpixels were selected for interactive annotation. AL can also be applied in medical image reconstruction. AL methods can help minimize the observations needed for modalities that require a long imaging time. This accelerates the imaging process and shortens the waiting period for patients. In this section, we will explore the application of AL in the reconstruction of MRI, CT, and electron microscopy. Please refer to for more detail. Deep learning has been applied to accelerate MRI acquisition and reconstruction. A common practice is to reduce k-space sampling through a fixed mask and use a deep model to reconstruct the undersampled MRI ( ). To further improve the imaging speed, learnable sampling in AL can be applied to select the next measurement locations in k-space. For example, adopted adversarial learning to train an evaluator for selecting the next row in k-space. utilized reinforcement learning to train a dual deep Q-network for active sampling in k-space. adopted policy gradient in reinforcement learning to train a policy network for adaptive sampling in k-space. The reward for the policy network was based on the improvement in structural similarity before and after the acquisition. Additionally, explored how to jointly optimize the reconstruction and acquisition networks. In addition to MRI imaging, AL has been employed in CT reconstruction as illustrated by . They adaptively chose the scanning angles tailored to individual patients, leading to a reduction in both radiation exposure and scanning duration. In electron microscopy, initially enhanced low-resolution images to high-resolution and then predicted the location of region-of-interest and reconstruction error. A weighted DPP based on reconstruction error was applied to select pixels that needed to be rescanned. Results showed that weighted DPP maintained both low reconstruction error and spatial diversity. In the field of medical image analysis, there is now an increasing amount of AL works. Despite its rapid development, AL for medical image analysis still faces several issues that limit its application in real-world clinical tasks. On one hand, there is a lack of comprehensive evaluation of AL methods on the medical imaging datasets. Most AL works conducted experiments on standard datasets, such as CIFAR-10, CIFAR-100, or MedMNIST. However, real-world medical imaging datasets often contain less available data and higher complexity for analysis ( ). Some AL works focused on a specific domain of medical imaging and achieved excellent performance, but their potential to generalize to a wider aspect of applications remains questionable. Beyond that, different AL methods exhibit inconsistent performance and may not necessarily outperform random sampling. In AL of classification tasks, highlighted the absence of a consistently outperforming AL method and the fact that random sampling performs relatively well. In Section , we have also mentioned that the AL methods are inferior to random sampling when the annotation budget is low. As a result, we are uncertain about which AL method works according to our requirements and whether this method could outperform the most straightforward baseline, random sampling. To clarify the aforementioned issues, we have conducted a comprehensive evaluation of different AL methods on multiple medical imaging datasets. The adopted three datasets are widely used by the entire medical imaging community. They also correspond to different modalities, organs, and tasks (e.g., classification and segmentation). We choose the most representative and popular AL methods for their evaluations of the medical imaging datasets. Besides, we provided the details of dataset splits, network architecture, and training hyperparameters for better reproducibility. Codes are also available on our accompanying website. In this survey, we chose three medical imaging datasets for the performance evaluation of the AL methods, including two classification datasets and one segmentation dataset. The descriptions and dataset split are presented as follows, where a summarized table of the dataset split is in . ( ): This dataset contains 100,000 patches from 86 hematoxylin & eosin (H&E) stained histological slides of human colorectal cancer and normal tissue. All the patches are 224 × 224 at 0. per pixel. The patches are grouped into nine classes of different tissues, including adipose (ADI), background (BACK), debris (DEB), lymphocytes (LYM), mucus (MUC), smooth muscle (MUS), normal colon mucosa (NORM), cancer-associated stroma (STR), and colorectal adenocarcinoma epithelium (TUM). For the dataset split, we divided the dataset into training and validation sets with a ratio of 9:1 and utilized an additional dataset CRC-VAL-HE-7K which is provided by the same authors as the testing set. CRC-VAL-HE-7K shares the same acquisition protocol and tissue classes with NCT-CRC-HE-100K but contains 7180 patches from 50 patients other than the patients of NCT-CRC-HE-100K. ( ): ISIC 2020 is composed of 33,126 dermoscopic images from over 2000 patients. Each image is labeled as benign or malignant by either doctors, long-term follow-up, or histopathology. ISIC 2020 contains 32,542 images of benign lesions and only 584 images of malignant lesions. We split this dataset of training, validation, and testing sets with a ratio of 6:1:3. ( ): This dataset contains short-axis cardiac cine-MR images from 100 patients. In this survey, we only adopted the end-diastolic frame of each patient for evaluating different AL methods, which resulted in a total of 100 scans. Each scan corresponds to a human-annotated segmentation mask of the left ventricle (LV), myocardium (MYO), and right ventricle (RV). We followed the split from , which contains 70, 10, and 20 scans in the training, validation, and testing sets, respectively. Due to the large spacing along the -axis, 2D segmentation is more appropriate compared to 3D segmentation. Therefore, we trained the segmentation model with 2D slices and evaluated it with 3D volumes following . Therefore, the training set is composed of 656 slices. We employed different evaluation metrics for the task of each dataset. For the multi-class classification of NCT-CRC-HE-100K, we adopted the accuracy (ACC) to evaluate the classification performance. Due to the heavy class-imbalance of the binary classification task of ISIC 2020, we adopted the area under the receiver operating characteristic curve (AUC) for evaluation. For the segmentation task of ACDC, two well-known metrics of Dice similarity coefficient (DSC) and average surface distance (ASD) are used. DCS ranges from 0% (non-overlap) to 100% (perfect segmentation), and the lower ASD indicates a better alignment between the segmentation prediction and ground truth. To evaluate the overall performance, we presented the mean DSC and ASD of LV, MYO, and RV. Evaluation metrics on the testing set were reported as the final results. In this study, we performed rounds of annotation. To investigate how the number of queried samples in each round (i.e., annotation budget) affects the performance of different AL methods, we set different levels of annotation budgets for each dataset. Following , high ( ) and low ( ) budgets were used for classification tasks of NCT-CRC-HE-100K and ISIC 2020. Following , we adopted a budget of 10 slices ( ) for ACDC segmentation. Considering the sizes of the involved datasets, the low budgets for classification ( ) and segmentation ( ) provide a chance to look into the performance of different AL methods in the low data regime. Before the active learning process started, we randomly selected the initial labeled pool to train an initial model. The size of the initial pool is equal to the annotation budget. The model training and sample selection are seeded. We ran each active learning method of a specific budget and dataset five times with different random seeds and reported the mean and standard deviation as the result. For fairness and reproducibility, we conducted the evaluation using the following methods: : the baseline of active learning, which randomly draws the unlabeled samples. ( ): these methods are all classic uncertainty-based AL methods, which calculated the confidence, entropy, and margin with the prediction probability as the uncertainty scores. Lower confidence, higher entropy, and lower margin indicate higher uncertainty. ( ): This method integrated entropy and MC dropout for better uncertainty estimation. During sample selection, the model runs multiple times with all dropout layers activated, and the average probability of all MC dropout runs is used for calculating entropy. ( ): This method calculated BALD as the uncertainty score, which aims to maximize the mutual information between predictions and model parameters. MC dropout was also used in this method. ( ): This method performed cover-based sampling using the feature embedding of each sample. For the balance of computation time and performance, we used the k-Center-Greedy for sample selection. To investigate how the distance metric affected the AL performance, we proposed a variant of Core-Set named “Core-Set-Cosine” which replaced the original L2 distance with the cosine distance. The original Core-Set was referred to as “Core-Set-L2” to avoid confusion. ( ): This method applied gradient as uncertainty estimation and utilized k-Means++ to improve diversity. Specifically, the gradient of cross-entropy loss was used in the classification task while the segmentation task used the gradient of the sum of the Dice loss and cross-entropy loss. It should be noted that uncertainty-based methods in segmentation are slightly different from those of the classification. Specifically, we first produced the pixel-wise scores and then utilized the averaged scores for sample selection in the segmentation task. For all the classification task, we used ResNet-18 ( ) as the backbone and the loss function is cross-entropy. We trained the model using stochastic gradient descent with momentum for 100 epochs with a batch size of 128. The learning rate and momentum were set as 0.01 and 0.9, respectively. Also, the cosine learning rate decay was adopted for smoother convergence. Data augmentations of the input image are different in the two classification datasets. In NCT-CRC-HE-100K, we only used the random horizontal flip. For ISIC 2020, we followed the data augmentation from which includes random crop, flip, rotation, affine transforms, and color jittering. We used a 5-level U-Net ( ) for segmentation. Each level of the encoder or decoder contains two blocks. Each block consists of a 2D convolution, dropout layer with a probability of 0.1, batch normalization, and leaky ReLU activation. The segmentation loss is the combination of cross-entropy loss and Dice loss. We trained the model using the Adam optimizer ( ) for 4000 iterations with a batch size of 32. The learning rate is 0.001 while decaying along the training iterations with the polynomial schedule. Data augmentation includes random flip, rotation of 90 degrees, and rotation of arbitrary degrees. Experiments were conducted on NVIDIA GeForce RTX 3090 and 4090 GPUs and the CUDA version is 11.3. Codes are implemented using Python (version 3.8.10) and the PyTorch framework (version 1.11.0). We first evaluated the active learning performance on the pathological tissue classification task. The results of the testing accuracy are shown in . Margin performed well in the low-budget scenario. The reason for that is this method exploits the information of the wrong predictions of a similar class, which is in line with the finding in . BADGE performs well in the low-budget scenarios largely due to the k-Means++ clustering. However, its performance drops in the high-budget scenario, which may indicate that the gradient embeddings are less suitable in AL when there is a distribution shift between the training and testing sets. The results of this section call for a more in-depth investigation of the generalizability to distribution shift of the AL methods. We have also conducted thorough evaluations on the ISIC 2020 dataset, which corresponds to a binary classification problem with severe class-imbalance. The AUC of the test split is shown in . It should be noted that Confidence and Margin are equivalent in the binary setting, so we only report the results of the former. In the low-budget scenario, Core-Set and its variant achieved better performances compared to the uncertainty-based methods. It indicates that representativeness-based methods or methods with improved diversity are more favored than uncertainty-based ones when the budget is low and the task is extremely difficult. Among all the uncertainty-based methods, BADGE stands out in certain rounds for its clustering operations that enhance diversity. For the high budget, the performance of the Core-Set variants is still competitive. However, the performance of the uncertainty-based methods improved. Results here demonstrated how the annotation budget affects the performance of the uncertainty-based and representativeness-based methods, in which the former fits a higher budget while the latter fits a lower budget. For segmentation, we evaluated different AL methods on the ACDC dataset. We reported the mean DSC and ASD of the segmentation results in . BADGE achieved the best or second-best performance in mean DSC for multiple rounds. Core-Set performed well on both the mean DSC and ASD in the early rounds of the lower budget scenario. Both two methods improve sampling diversity to some extent. However, for the later rounds, the performance of the uncertainty-based methods and random sampling improves in mean DSC and ASD. This result aligns with the findings in the previous section. Distance measurement plays an important role in AL which may significantly impact the performance of AL algorithms. In this section, we evaluated the performance of the two most popular distances in AL, which are L2 and cosine distances. These two distances are based on the feature embedding. Assume stands for the sample itself and its corresponding feature embedding is , is the feature dimension. Based on the feature embedding, the L2 distance between two images and is as follow: Performance comparisons between Core-Set-L2 and Core-Set-Cosine are illustrated in . Results on the NCT-CRC-HE-100K dataset showed no significant difference between the L2 and cosine distances across all budget levels. In ISIC 2020, the L2 distance tends to be better in the early rounds, indicating its ability to rapidly start the model, while Core-Set-Cosine significantly outperformed the Core-Set-L2 when the budget was high. On the ACDC dataset, Core-Set-L2 also outperformed Core-Set-Cosine in the early rounds but their performance after selecting more samples is similar. These results indicate that distance metrics play an important role in the performance of AL methods used in medical image analysis, and they should be carefully chosen according to the target tasks and the budgets. Generally speaking, L2 distance is more suitable for the low-budget scenario while the cosine distance might be a better choice when the budget is high. Currently, annotation scarcity is a significant bottleneck hindering the development of medical image analysis. AL improves annotation efficiency by selectively querying the most informative samples for annotation. This survey reviews the recent developments in deep active learning, focusing on the evaluation of informativeness, sampling strategies, integration with other label-efficient techniques, and the application of AL in medical image analysis. In this section, we will discuss the existing challenges faced by AL in medical image analysis and its future perspectives. In AL, uncertainty plays a pivotal role. However, it would be beneficial if the uncertainty more directly highlighted the model’s mistakes. We can enhance the model’s performance by querying samples with inaccurate predictions. Recently, many works have adopted learnable performance estimation for quality control of deep model outputs. For instance, the recently proposed segment anything model (SAM) ( ) provides IoU estimates for each mask to evaluate its quality. In medical image analysis, automated quality control is critical to ensure the reliability and safety of the deep model outputs ( ). For example, employed deep generative models for learnable quality control in cardiac MRI segmentation, where the predicted Dice scores showed a strong linear relationship with the real ones. Additionally, used an additional neural network to predict the Dice coefficient of brain tissue segmentation results. Overall, learnable performance estimation can accurately predict the quality of model outputs. Hence, delving deeper into their potential for uncertainty-based AL is crucial to effectively tackle the issue of over-confidence. Moreover, improving the probability calibration of model prediction is a promising way to mitigate the over-confidence issue. Calibration ( ) reflects the consistency between model prediction probabilities and the ground truth. A well-calibrated model should display a strong correlation between confidence and accuracy. For instance, if a perfect-calibrated polyp classifier gives an average confidence score of 0.9 on a dataset, it means that 90% of those samples should indeed have polyps. In reality, deep models generally suffer from the issue of over-confidence, which essentially means that they are not well-calibrated. Currently, only a few uncertainty-based AL works have considered probability calibration. For instance, found that the model ensemble has better calibration than MC Dropout. mitigated miscalibration by considering all possible prediction outcomes in the Dirichlet distribution. However, these methods are limited to proposing a better uncertainty metric and validating the calibration quality post-hoc. Existing calibration methods ( ) directly adjusted the distribution of prediction probabilities. However, these methods require an additional labeled dataset, thus limiting their practical applicability. Therefore, integrating probability calibration into uncertainty-based AL represents a valuable research direction worth exploring. Among all the mentioned methods in Section , adversarial-based uncertainty currently has limited applications in AL of medical image analysis. Since the adversarial samples tend to be close to the classification boundary, they can be regarded as uncertain samples, and selecting them for training can potentially improve the trained model’s robustness. Exploring such ideas in medical image analysis, especially in the federated learning scenario, could be an interesting topic for future work. Representativeness-based AL effectively utilizes feature representations and data distributions for sample selection. Cover-based and discrepancy-based AL methods implicitly capture the data distribution, whereas density-based AL explicitly estimates it. However, the latter requires supplementary strategies to ensure diversity. For discrepancy-based AL, we can opt for a better metric of the distance between two probability distributions ( ). Besides, discrepancy-based AL has limited applications in medical image analysis currently. Finding a proper metric for medical images considering their special characteristics could be a promising direction for the future development of AL in the medical imaging domain. As the core of density-based AL, density estimation in high- dimensional spaces has always been challenging. Popular density estimation methods, such as kernel density estimation and GMM, can encounter challenges when applied in high-dimensional spaces. In future research, we can consider introducing density estimators tailored to high-dimensional spaces. Advanced tools like normalizing flow ( ) could be an appropriate choice in density estimation in high-dimensional spaces. In Section , we discuss region-based active learning, which only requires region-level annotation of a sample. However, annotating all pixels within the region is still needed. Several existing works have incorporated weak annotations with AL to simplify the task for annotators. In object detection tasks, trained deep models with image-level annotation. They selected samples with box-in-box prediction results and annotated them with bounding boxes. Moreover, adopted disagreement to choose which objects are worth annotating. Rather than annotating all objects within the image, they only required box-level annotations for a subset of objects. In AL of instance segmentation, only required annotations for each object’s class label and bounding box, without the annotation of fine-grained segmentation masks. In future research, AL based on weak annotations is a direction worthy of in-depth exploration. In Section , we summarize the applications of generative models in AL. However, existing works have mainly focused on using GANs as sample generators. Recently, diffusion models ( ) have advanced in achieving state-of-the-art generative quality. Furthermore, text-to-image diffusion models, represented by Stable Diffusion ( ), have revolutionized the image generation domain. Their high-quality, text-guided generation results enable a more flexible image generation. With the use of ControlNet ( ), the diffusion models could learn to follow a more detailed condition like a sketch or segmentation mask. Exploring the potential of diffusion models in deep AL is a promising avenue for future research. With the rise of visual foundational models, such as contrastive language-image pre-training (CLIP) ( ) and SAM ( ), and large language models (LLMs) like GPT-4 ( ), deep learning in medical image analysis and computer vision is undergoing a paradigm shift. These foundational models ( ) offer new opportunities for the development of AL. AL is closely related to the training paradigms in deep learning of computer vision and medical image analysis. From the initial approach of train-from-scratch to the “pre-train-finetune” strategy using supervised or self-supervised pre-trained models, these paradigms usually require fine-tuning the entire network. Foundation models contain a wealth of knowledge. When combined with recently emerging parameter-efficient fine tuning (PEFT) or prompt tuning techniques ( ), we can tune only a minimal subset of model weights (e.g., 5%) for rapid transfer to downstream tasks. As the number of fine-tuned parameters decreases, AL has the potential to further reduce the number of required annotated samples. integrated prompt tuning with AL in liver tumor segmentation. A segmentation model trained on publicly available datasets was transferred to in-house datasets via a novel prompt updater. With a mixed AL strategy of uncertainty and diversity, the proposed method reached the comparable performance of fully supervised tuning using around 5% of samples and 6% of tunable parameters. Therefore, it is essential to investigate the applicability of existing AL under PEFT or prompt tuning and explore the most suitable AL strategies for PEFT. In natural language processing, LLMs have already taken a dominant role. Since most researchers cannot tune the LLMs, they rely on in-context learning, which provides LLMs with limited examples to transfer to downstream tasks. We believe that visual in-context learning will play a vital role in future research. Therefore, selecting the most suitable prompts for visual in-context learning will become an important research direction of AL. Active learning is important to deep learning in medical image analysis since it effectively reduces the annotation costs incurred by human experts. This survey comprehensively reviews the core methods in deep active learning, its integration with different label-efficient techniques, and active learning works tailored to medical image analysis. This survey also conducted performance analysis on different medical imaging datasets and tasks with experiments. We further discuss its current challenges and future perspectives. In summary, we believe that deep active learning and its application in medical image analysis hold important academic value and clinical potential, with ample room for further development. Writing – review & editing, Writing – original draft, Visualization, Software, Investigation, Formal analysis, Conceptualization. Writing – review & editing, Conceptualization. Writing – review & editing, Investigation. Writing – review & editing, Investigation. Writing – review & editing, Supervision, Project administration, Funding acquisition, Conceptualization. Writing – review & editing, Supervision, Resources, Project administration, Funding acquisition, Conceptualization.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Used\n",
      "* **AIモデルの判断:** Not Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 182 / DOI: 10.1016/j.jretconser.2024.103723  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Dataset on an extended technology acceptance model: A combined application of PLS-SEM and NCA\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Importance and performance in PLS-SEM and NCA: Introducing the combined importance-performance map analysis (cIPMA)\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "Comparing attributes' performance and importance in order to produce a given outcome has a long tradition in the management discipline ( ). Researchers routinely visualize this interplay in a two-dimensional plot, in which the attributes are normally grouped into four quadrants that combine low and high importance and performance values. This plot is usually referred to as an importance-performance grid ( ), importance-performance matrix ( ), a quality map ( ), or, more broadly, an importance-performance map analysis (IPMA, ). The IPMA has become a standard tool for understanding where managerial improvement efforts should be focused (e.g., ; ). Nevertheless, authors from different disciplines and methodological backgrounds have encountered challenges when using the toolset, which – among others – relate to understanding importance and performance, as well as the threshold levels' definition (see, for instance, ). The IPMA has also been used in the context of partial least squares structural equation modeling (PLS-SEM), a multivariate method for estimating complex interrelationships between constructs and their indicators ( , Chapter 1; , Chapter 2; ). In the past decades, the use of PLS-SEM evolved in marketing research ( ; ; ), including retailing and consumer services (e.g., ; ; ). PLS-SEM facilitates an IPMA, due to the way the method estimates model parameters. Specifically, PLS-SEM calculates indicator variables' composites to represent the constructs in a statistical model. Researchers have used these composite scores as representations of the constructs' performance and compared them with the total effects that the constructs exert on a specific target (e.g., ; ; ). The underlying IPMA draws on the average performance and importance scores and subsequently visualizes them in an importance-performance map. Numerous studies have used the IPMA in a PLS-SEM context. For example, in the context of retailing and consumer services, draw on the IPMA to investigate environmental stimuli's importance and performance for consumers' in-store purchase intentions via two consumer attitude constructs. In a technology acceptance model (TAM), whose use also features prominently in retailing and consumer services (e.g., ; ; ), the IPMA can be used to compare the total effect of a technology's ease of use in terms of its intended usage (its importance for the target construct) with its potential users' average perceived ease of use (its performance). An IPMA's results therefore allow the identification of a target construct's highly important antecedents, that, however, exhibit relatively low performance. This information is crucial, since it indicates how action should be prioritized to improve these antecedent constructs, which will increase the target construct substantially. For instance, ease of use's high importance, combined with a technology's poor ease of use rating (i.e., a poor performance), indicates a specific need to improve this technology's ease of use to ultimately increase its usage. Inherently, the IPMA's implementation follows an additive sufficiency logic according to which multiple antecedents could contribute to an outcome. Antecedent constructs that improve an outcome, namely those that show a low performance, but have a strong and significant total effect on an outcome, are prioritized, while those showing weak effects are not prioritized. However, it may well be that a certain antecedent construct with a relatively low importance (i.e., a weak effect regarding increasing the desired outcome) might still require particular attention when its absence prevents the outcome. For instance, an IPMA in the context of the TAM might ascertain that a technology's compatibility has relatively low importance for its use. Yet, if the compatibility doesn't meet a specific threshold, users would be unwilling to use the technology. Considering such interrelationships requires adopting a necessity perspective, which implies that an outcome – or a certain level of an outcome – could only be achieved if the necessary cause is in place or at a certain level. To incorporate such a necessity logic, researchers can draw on the necessary condition analysis (NCA; ; ), which has recently gained prominence in many business research fields (for a recent review of the topics analyzed with an NCA, see ), either as a stand-alone method or in combination with PLS-SEM and other regression-based methods ( ). The NCA seeks to identify necessary conditions representing constraints, bottlenecks or critical factors, which need to be solved or satisfied to achieve a certain outcome. Identifying such necessary conditions is highly relevant for management practice, since an outcome can only be achieved if the necessary condition is in place or is at a certain level ( ; ; ). Importantly, antecedent constructs showing weak and nonsignificant PLS-SEM effects might be necessary conditions; that is, without these antecedents a certain outcome cannot be achieved. Failure to consider these antecedent constructs could therefore lead to incomplete recommendations. For example, in the context of retailing and consumer services, highlights the impact of recession and quality risks as constant necessary conditions for holidaymakers' purchasing intentions. use the NCA to reveal that moral obligation, moral accountability, perceived risk, perceived risk, and cost knowledge are necessary antecedents for consumers' purchase of remanufactured products. The recently proposed combined use of NCA and PLS-SEM ( ) has been acknowledged as “a unique contribution by comparing and combining approaches, demonstrating how NCA (which focuses on necessary conditions) can [ ] be used in combination with PLS-SEM (which focuses on sufficiency) to create a previously unrecognized way of assessing causality” ( , p. 1842). In line with recent research work that adopted this multimethod approach (e.g., ; ; ; ), we argue that identifying necessary conditions could also enrich an IPMA by providing information about the antecedent constructs' necessity. This information provides researchers and practitioners with a more complete picture of the important and necessary antecedents, while also helping to complement an IPMA's results. By following the path model results of a technology's perceived usefulness, researchers assessing consumer acceptance of novel technologies might, for instance, find that these are of little importance, although they are a necessary condition for the technology's usage. In other words, the technology will not be used unless it is perceived as useful. While an antecedent construct with little importance according to its PLS-SEM results might receive little attention from a classic IPMA, it might represent a necessary condition, which would once again make it the focus of decision making when PLS-SEM is combined with an NCA. Against this background, we first contribute to the field by discussing how importance and performance are understood in the context of PLS-SEM and an NCA, as well as how the two approaches complement one another in importance-performance analyses. Building on prior research in each field (e.g., , ; ), we discuss the concepts of importance and performance in a PLS-SEM context – as implemented in an IPMA – and an NCA context. In addition, we present guidelines for a combined IPMA that builds on PLS-SEM and NCA results. By doing so, we extend the guidelines proposed in , who did not address the IPMA (see also ). Finally, we illustrate a combined IPMA by using an extended TAM and offering researchers implementing the approach a toolset, before concluding our research and outlining future research. PLS ( , Chapter 2; ) is a composite-based approach to SEM in that it represents constructs as weighted sums of indicators ( ; ). Building on this characteristic, researchers have used composite scores produced by PLS-SEM to develop indices such as the Swedish Customer Satisfaction Barometer ( ) and the American Customer Satisfaction Index ( ). In the computation of these indices, the composite scores are conceived as performance values, reflecting the respondents' satisfaction with certain features or with the overall construct. For example, if all respondents show a maximum level of overall satisfaction, this translates into a 100 percent performance of this construct. Starting with and , follow-up research has contrasted these scores with measurement or structural model weights representing the importance of individual indicators or constructs for improving a certain target, such as firm profit. Jointly, these two dimensions define an importance-performance map's axes, which represent the average importance and performance scores with regard to a certain target construct. With models' increasing complexity, which often span multiple layers of constructs, researchers usually draw on structural model total effects – that is, the sum of an antecedent construct's direct and all of its indirect effects on an outcome – to represent an IPMA's importance dimension ( , Chapter 4; ). That is, rather than restricting the analysis to a target's direct antecedents, the IPMA also considers indirectly related constructs (i.e., those that impact the outcome via one or more mediating constructs). In the importance-performance map, the antecedent constructs' importance values are plotted on the -axis and their performance values on the -axis. Using this illustration, researchers could, for example, identify antecedent constructs that are relatively important regarding explaining the key target constructs of interest (i.e., those with a strong total effect) with a relatively low performance (i.e., low average latent variable scores). Such constructs would, specifically, be highly prioritized in order to achieve improvement, because they are especially important for the target construct, although they simultaneously perform relatively poorly. To better illustrate the IPMA concept, consider the hypothetical path model in (Panel A) with the four constructs Y to Y . In this path model, Y is the target construct. Based on the path coefficients, the antecedent constructs Y , Y , and Y have direct effects on Y . In addition, Y and Y have indirect effects on Y via Y , which are due to the corresponding direct effects' product (for further details, see , Chapter 4). In this example, the indirect effects of Y on Y are computed as follows: Hence, the total indirect effect of Y on Y is 0.25 + 0.03125 + 0.0625 = 0.34. Adding the direct effect of Y on Y (i.e., 0.50), as well as the antecedent constructs' total indirect effect (i.e., 0.34), we not only obtain the total effect of 0.5 + 0.34 = 0.84, but also the importance of Y for the key target construct Y . In contrast, the construct scores' average values represent their performance. Here, researchers could use the standardized construct scores or the unstandardized construct scores. The are estimated on the basis of -transformed standardized indicator data. The resulting standardized construct scores always have a mean value of zero and a standard deviation of 1. Since reporting mean values of zero for all the constructs is less useful for the IPMA, the procedure refers to the unstandardized construct scores. The unstandardized construct score depend on the indicators' scales (e.g., 1 to 5 or 1 to 7). If all the indicators are measured on the same scale (e.g., all on a scale from 1 to 5), their interpretation on the construct level is straightforward. However, their interpretation becomes ambiguous if different scales are involved (e.g., ; , Chapter 4). For a better comparison of the performance levels, the unstandardized indicator data can be rescaled so that they all range between 0 and 100, with 0 representing the lowest and 100 the highest performance. The rescaling of an observation with respect to indicator proceeds via where x is the -th indicator in the PLS path model, E[.] represents indicator i's actual score of respondent , and the min[.] and the max[.] represent the indicator's minimum and maximum values ( ; , Chapter 4). The indicator's minimum and maximum can refer to the theoretical minimum and maximum values (e.g., 1 and 7 when using a seven-point scale) or the empirical minimum and maximum values resulting from the sample data (e.g., 2 and 7, because no respondent evaluated the indicators below 2). It is easy to determine the theoretical minimum and maximum of an interval scale (e.g., 1 and 7 when using a seven-point scale). The use of other scales might not have a theoretical minimum or maximum, and the researcher could be advised or even forced to use the data at hand's empirical minimum or maximum. The rescaled latent variable scores are a linear combination of the rescaled indicator data and the rescaled outer weights. To obtain the rescaled weights, we must first compute the unstandardized weights by dividing the standardized weights by the standard deviation of its respective indicator. These weights are normalized per measurement model so that the sum of these final rescaled outer weights equals one. These rescaled outer weights allow us to determine the unstandardized rescaled construct scores that range between 0 and 100 (i.e., by the linear combination of the indicator data on a scale from 0 to 100 and the rescaled outer weights per construct). The average of these values represents the construct's performance score referred to in the IPMA in PLS-SEM (see values in brackets in the constructs in , Panel A). (Panel B) shows how the IPMA combines these two aspects graphically for the target construct . Two grey dashed lines divide the importance-performance map into four quadrants. The vertical line represents the mean importance and the horizontal line corresponds to the constructs' mean performance, computed as follows ( , Panel A): Researchers using the IPMA could use these average scores and plot them on an importance-performance map into four separate quadrants. However, researchers might rely on a different logic when dividing the four quadrants, such as previous knowledge or expert assessment. The combination of high/low importance and high/low performance induces specific recommendations, which antecedent constructs target through managerial activities ( ). Unlike the classic IPMA, the NCA builds on a necessity logic to assess the antecedent constructs' importance and performance for an outcome. A necessity logic implies that an outcome can only be achieved if a specific condition is present ( , , Chapter 2). A necessary condition therefore represents a constraint or a bottleneck that must be overcome to allow the outcome to exist. In order to detect necessary conditions in data sets, NCA uses ceiling line techniques to separate areas in scatter plots without observations from those with observations. By dividing the ‘empty' space (also called the ceiling zone) by the entire area that includes observations (also called the scope), NCA calculates the necessity effect size , which could be 0 if there is no empty space, and 1 if the maximum possible space is empty. Consider, for example, a trichotomous case where the condition and the outcome each have three levels, namely , , and ( ). The effect size is calculated as d = C/S, with being the ceiling zone and the scope. The size of is calculated by counting the number of ‘empty' cells. could be calculated as follows: S = (q × r) − q − r + 1; where is the number of levels, and is the number of levels. In our example, S = (3 × 3) − 3 − 3 + 1 = 4. In Scenario A in , the effect size is d = 1/4 = 0.25; in Scenario B in , the effect size is d = 3/4 = 0.75 (for details on calculating the effect sizes see , Chapter 4). The effect size therefore depends on how many of the total cells are empty. If only one cell is empty, the effect size is 0.25 ( , Scenario A). If three cells are empty, the effect size is 0.75 ( , Scenario B). The effect size therefore specifies the extent to which is constrained by . As an arbitrary benchmark, 0 ≤ d < 0.1 could be considered a ‘small effect,' 0.1 ≤ d < 0.3 a ‘medium effect,' 0.3 ≤ d < 0.5 a ‘large effect,' and d ≥ 0.5 a ‘very large effect' ( ). However, in order to conclude that a necessary condition hypothesis could be accepted, should at least have a medium effect size (e.g., d ≥ 0.1) and a statistical significance (e.g., with a value lower than 0.05). Moreover, a necessary condition should always be theoretically supported ( , Chapter 2; ). In order to decide whether an identified effect size is satisfactory, researchers should always include contextual knowledge ( , Chapter 4). In fact, evaluating importance for an NCA could be perceived as less straightforward than for a PLS-SEM application. For example, to conclude that an effect size of 0.75 is more important than one of 0.25 might be misleading if a high level is always desired or needed from a practical perspective. In our trichotomous example, both effect sizes could be considered equally important, since both indicate that a specific level is needed to achieve a high (i.e., a medium level of in Scenario A and a high level of X in Scenario B). Conversely, an at least medium level and statistically significant effect size might not be important at all if it does not constrain the outcome's desired level. For example, in Scenario A, the outcome of is not constrained by if one only endeavors to achieve a medium level. In this case, the effect size of 0.25 is therefore of no practical importance. In contrast, in Scenario B, represents a necessary condition to achieve a medium Y outcome level. In the NCA context, performance should be understood in terms of how much of a specific condition's required level has already been achieved. This can be analyzed from a case or sample perspective. From a case-level perspective, we could determine whether one case performs better than another case in terms of achieving the necessary level. For example, in , Scenario B, a case with a level of = performs worse than a case with a level of = (although this is still not enough to achieve a high level). This case perspective could, for example, be particularly interesting if an organization wants to know how it compares to other organizations. From a sample perspective, we could analyze how many cases in a sample still need to achieve the condition's required level. This sample perspective could, for example, be applied if an organization wants to obtain insights into its customers or employees. PLS-SEM analyses and the interpretation of a combined importance performance map analysis's (cIPMA's) findings focus on samples (or subsamples, such as specific groups of customers in a dataset), which makes the latter perspective particularly relevant when combining PLS-SEM-based IPMA results with NCA insights. In the case of multiple necessary conditions, a comparison of the different performance levels should help identify the conditions that require particular attention. For example, if almost all the cases have already achieved the required level, this condition might not be as imperative as one where many cases have not done so ( , Chapter 4). Consequently, comparing different necessary conditions' performance could help prioritize actions. In sum, in order to assess the importance of different necessary conditions in the context of a cIPMA, we suggest that researchers should first evaluate whether a necessary condition is theoretically supported and whether it at least has a medium effect size and is statistically significant. Nevertheless, we do not recommend merely focusing on the effect size's magnitude in order to understand its practical importance in a specific research project's context. Instead, we propose defining a desired outcome (or target construct) level and assessing whether this level could only be achieved with a specific level of the condition. This could be done by using the information inherent in the ceiling line, which could also be illustrated in tabular form in a bottleneck table. The first column of such a table shows the outcome's different levels, while the next one displays the condition's corresponding critical levels; that is, the levels that need to be satisfied to achieve the outcome (as we will illustrate in the following sections). Consequently, the ultimate assessment of a cIPMA's importance depends on the outcome or target construct's desired level (i.e., the level), which the researcher specifies. In order to analyze performance, we suggest that cIPMA refers to the number of cases below the necessary levels, which will prioritize the necessary conditions' identification. provided guidelines to combine PLS-SEM with NCA, which we will extend to accommodate the cIPMA (see also ). shows our extended guidelines, whose additional steps (printed in bold) we will discuss in detail. These additional steps relate to the further requirements checks in Step 4, to running the cIPMA and transferring the latent variable scores in Step 5, to the specific settings to run the NCA in Step 6, and to an enriched interpretation of the findings in Step 8. Please note that we also update some of the steps outlined in (marked with * in ). The IPMA usually relies on the unstandardized latent variable scores, which are being rescaled on a scale from 0 to 100 to aid the interpretation of the constructs' performances. Using the rescaled latent variable scores in the NCA will provide the same necessity effect sizes as when applying the method to the (standardized or unstandardized) latent variable scores from a PLS-SEM analysis, as long as the indicators used to measure a construct are measured on the same scale. For this reason, we recommend applying the cIPMA to models where each construct relies on indicators that are measured on the same (theoretical) scale. That is, while one constructs' indicators may draw on a 7-point Likert scale only, another construct's indicators may rely on a 5-point rating scale only. Researchers must not use differently scaled indicators in the measurement model of a single construct when running the cIPMA. In the context of PLS-SEM, IPMA requires the outer weights estimates to be positive, regardless of whether the measurement model is formatively or reflectively specified. If the outer weights are negative, the (rescaled) latent variable scores will not fall within the 0–100 range (but would, e.g., be between −5 and 95) ( ). If an indicator's outer weight is unexpectedly negative and significant, researchers are advised to examine the indicator and its scale. The scale or question should not have a different direction (e.g., reverse-scale items) compared to the measurement model's other indicators. If the negative outer weights are nonsignificant, researchers should consider removing them. Finally, negative outer weights could be a result of high indicator collinearity, which the variance inflation factor values of 5 and higher indicate ( ). In this case, researchers should again consider removing the indicators. However, they should note that removing indicators from measurement models means taking more considerations into account, which , Chapter 5) explain in more detail. Researchers should rely on the minimum and maximum scores of the indicator scales when running the IPMA; for instance, 1 and 5 on a scale that ranges from 1 to 5. This way, the latent variable scores can be interpreted in the form of percentage values from 0 to 100. If a respondent, for instance, answered a 3 on a scale that ranges from 1 to 5, the rescaling indicates a mediocre performance of the trait that the indicator expressed. After running the IPMA, the rescaled latent variable scores need to be exported into a new data file that serves as input for the NCA. If the aim is to later extend the IPMA to the indicator level, we recommend that researchers also export the rescaled indicator data. In step 6, researchers should run the NCA in order to identify potential necessary conditions. The NCA can be conducted with a free R package ( ). Alternatively, the SmartPLS 4 software ( ) also supports the NCA (see also ; ; , Chapter 4; for basic guidelines on how to conduct and report NCA results, see also ). The NCA draws on the endogenous construct's (constructs') and their direct antecedent constructs' latent variable scores, as well as on the potential indicator scores if there are formative measurement models (as explained by ; ; , Chapter 4). The NCA could refer to the input data's empirical or theoretical scope. In the NCA context, the general recommendation is to use the empirical scope, as it produces a more conservative estimation of the effect size ( , Chapter 4) – we concur with this recommendation. That is, we use the rescaled latent variable scores from the IPMA in PLS-SEM as input data and refer to their empirical scope (which may not necessarily be from 0 to 100). Creating bottleneck tables helps researchers assess their necessary conditions' importance and performance. A bottleneck table is a tabular representation of the ceiling line. The first column of this table displays different levels of the outcome, while the next column or columns display the condition or conditions' corresponding levels. The levels of the outcome and the levels of the condition(s) can be displayed in different ways ( , Chapter 4.3). The NCA package in R produces (as a default) bottleneck tables in which, for example, both the outcome and the condition(s) are displayed as percentage ranges, with a level of 0% corresponding to the lowest observed values and a level of 100% to the highest observed values (that is, it refers to the values' empirical range). An alternative is to display the outcome and the condition(s) in terms of their actual values. Actual values are those used as input for the analysis; that is, if researchers use rescaled latent variable scores, these are the actual values. Note that the use of percentage ranges and actual values will provide the same results when researchers use rescaled latent variable scores with an actual minimum of 0 and a maximum of 100. By referring to these bottleneck levels, researchers can select a specific outcome level and determine which level of a condition is necessary for its achievement. The bottleneck table can also be used to determine the antecedent constructs' performance from a necessity logic perspective. Therefore, the bottleneck table needs to be displayed as percentiles of the antecedent constructs, which provide the percentage and number of cases that do not reach the required level for a corresponding outcome level. The NCA usually produces bottleneck tables with ten equidistant steps when, for instance, using percentage ranges from 0 to 100 (0, 10, 20 etc.). In order to offer a combined interpretation of the findings, we recommend specifying a performance level of interest for the dependent or target constructs in respect of which the antecedent constructs' levels will be evaluated in more detail. Researchers could determine the target construct's or constructs' desired performance level by building on expert knowledge, typical standards in the research context, or on ambitious aims forming the project's background. For instance, in job satisfaction studies, this could be employees' job satisfaction score of 70 (on a scale from 0 to 100) (as in ), while in a marketing project on customer satisfaction this could be a benchmark level of 75 or more, depending on the industry (on a scale from 0 to 100) ( ; ). Depending on these levels, researchers might need to change the presentation of the steps in their bottleneck tables to show the target constructs' selected performance level (e.g., increasing the number of steps from 10 to 20 will provide information on the 0, 5, 10, 15 etc. levels). The findings' interpretation leverages both the PLS-SEM results regarding the constructs' importance and performance values, and the NCA results regarding the corresponding necessity conditions. In order to facilitate the interpretation, researchers should produce a figure that clarifies the antecedent constructs' importance and average performance – in a standard PLS-SEM-based IPMA – as well as providing information on whether these antecedent constructs are necessary conditions for the selected outcome level and, if so, how they perform. shows an example of such a graphical illustration. The chart presents the PLS-SEM-based IPMA results; that is, the antecedent constructs' total effects on the target construct (importance) on the -axis and their average rescaled scores (performance) on the -axis. In addition, the chart distinguishes between necessary and not necessary constructs; that is, constructs that show at least a medium effect size and have statistical significance and those that do not. Constructs that are not necessary for achieving the target construct's desired level are displayed as black circles, while necessary constructs are displayed as white circles. In respect of the example shown in , we assume a desired level of the target construct of = 80. Next, building on the target construct's desired level, the chart uses the bottleneck table information to identify the percentage of cases remaining below the antecedent constructs' required levels (provided in the percentile display). The size of the bubbles of each of the antecedent constructs that are necessary conditions reflects this information. The larger the bubbles, the larger the percentage of cases that have not achieved the necessary condition's required level. Large bubbles therefore indicate that, from a necessity perspective, researchers should focus their attention on this aspect. Examining the results in , we find that is not a necessary condition for improving the target construct, while and are. It also becomes obvious that many cases have not achieved the required level. This makes highly relevant, since investments in other constructs will not increase the outcome in these cases unless the bottleneck is solved. Improving 's performance is therefore very important in this situation in order to enable the target construct 's desired performance. By integrating information from the NCA, specifically from the bottleneck tables, the cIPMA enables an enriched interpretation of the findings, which we characterize in . For example, constructs mapped in the upper right corner – despite their high importance – are often outside the researcher's focus, as they have already achieved a high performance level. However, if the NCA results suggest that these constructs have not achieved a sufficient necessity level, their improvement should still be prioritized. Similarly, constructs positioned in the lower left quadrant (i.e., those that have a low importance and performance) should not be written off as irrelevant per se. If these constructs do not achieve a desired necessity level, actions for improvement should be taken. The interpretation of the other cases in these quadrants (e.g., constructs achieving a satisfactory necessity level) and in the other two quadrants is analogous. While various models are suitable to serve as an example (e.g., ; ), we illustrate the application of a cIPMA by drawing on an extended TAM version ( ), which has served as a blueprint for researching consumer behavior in various marketing and consumer behavior contexts. introduced the conceptual model and the relevant theoretical arguments and hypotheses in their demonstration of PLS-SEM's and NCA's combined use. The data and PLS-SEM analysis are also well documented. Consequently, we do not present all the steps in detail, but concentrate on aspects that are relevant for the cIPMA. The TAM under consideration comprises two endogenous constructs: the to adopt a technology, which leads to the actual (e.g., ; ; ; ). It has four key exogenous constructs that precede behavioral intention and technology use: , which reports the innovation's fit with the customer's lifestyle and values, the perceived and perceived , and the , which measures whether customers enjoy or have positive feelings when using a product ( ). indicates the conceptual model. We used a sample of e-book reader adopters in France ( = 174) and collected responses via an online survey. We implemented a single item and a 7-point Likert scale to measure the target construct ; the remaining constructs used reflective measurement models with items measured on 5-point Likert scales (for more information on the sample and descriptive statistics, see in the Appendix and ). That is, the indicators used to measure a specific construct are all measured on the same scale. The dataset is freely accessible via . We used the data to estimate the extended TAM by means of the SmartPLS 4 software ( ) – for details on the model estimation, see . The measurement and structural models demonstrated appropriate statistical quality when evaluated by means of the standard assessment criteria (see in the Appendix and ). To finalize Step 4, we need to conduct an additional IPMA requirements check. We do so by evaluating whether the outer weights in our model are positive, which they are ( in the Appendix). We therefore continue with the analyses without making further changes. We run the IPMA in PLS-SEM, using the SmartPLS 4 software ( ). Since all the respondents made use of the entire scale range at the indicator level, the empirical and theoretical scales at the indicator level are identical. Hence, we do not need to adjust the scale's theoretical minimum and maximum values in the IPMA. Following our guideline, we specified the analysis such that it produces rescaled latent variable scores on a scale from 0 to 100. After running the IPMA, we create a new data file with the rescaled scores for our NCA. We use the rescaled latent variable scores to run the NCA on the construct scores generated in PLS-SEM. In the NCA, we use the empirical scope (to run the NCA in R, see ; please note that the NCA itself is a bivariate analysis not influenced by the other analyzed relationships). We create bottleneck tables to identify the antecedent constructs' necessary levels that need to be satisfied. Since we want to evaluate our findings based on an outcome level of technology use of 85 (see Steps 7 and 8), we increase the number of steps to 20 in order to produce bottleneck tables that display this outcome level. We create two bottleneck tables (based on the CE-FDH ceiling line technique), one with actual values , which provides the antecedent constructs' necessary levels that the different levels of technology use need to achieve, and one with percentiles for the conditions , which informs us about the number and percentage of cases that have not achieved the required antecedent construct levels for the corresponding levels of technology use (see next step). In this step, we first evaluate the findings from the PLS-SEM analysis, including the IPMA. Thereafter we evaluate the NCA's findings. We illustrate the evaluation of the target construct . Analyzing the standardized total effects from the PLS-SEM analysis ( ), we find that adoption intention has the strongest significant impact on technology use (0.437), followed by emotional value (0.362). The other constructs' total effects are not significant. The constructs' average importance is 0.225. The explained variance of technology use is R = 0.420. The IPMA results show the rescaled latent variables' average performance scores ( ). Focusing on the direct antecedent constructs of technology use, we find that compatibility has the lowest average performance (61.557), while ease of use has the highest (75.640). This construct's minimum (case-specific) performance score is 16.871, which suggests that no respondent evaluated all of the ease-of-use indicators at the lowest level. Taken jointly, the antecedent constructs have an average performance of 68.731. On examining the IPMA results in full, we learn that all constructs perform at fairly similar levels, with adoption intention and emotional value having the largest importance for technology use. Next, we turn to the NCA results. First, we inspect the scatter plots visually. We find that all the scatter plots show an empty space in the upper left corner, indicating potential necessary conditions (see in the Appendix). None of the scatter plots indicates single cases with a particular influence on the ceiling line (ceiling outliers) or the scope (scope outliers). Next, we consider the necessity effect sizes 's significance and size with regard to the target construct technology use. We followed and refer to the CE-FDH ceiling line ( ). We note that the selection of the ceiling (CE-FDH versus CR-FDH) line should be discussed based on (a) the data scaling (discrete versus continuous data), (b) the pattern of observations near the ceiling line (irregular versus linear), and (c) the potential theoretical ideas (indicating a straight ceiling line, see , Chapter 4). We find that all the effect sizes are significant at p < 0.05 and at least medium in size. Building on the scores presented in , we subsequently identify the value levels of our antecedent constructs that need to be satisfied for a desired level of technology use. In respect of our analysis, we assume a desired outcome level of technology use of 85, which is a rather conservative (i.e., high) benchmark. The corresponding levels are 75 for adoption intention, 50 for emotional value, 67 for ease of use, 66 for perceived usefulness, and 34 for compatibility. That is, to achieve a technology use score of 85 (on a scale from 0 to 100), we need to achieve a score of 75 for adoption intention (on a scale from 0 to 100), 50 for emotional value (on a scale from 0 to 100), etc. In addition, in we identify the percentage of cases that do not achieve the antecedent constructs' required levels. For example, 39.1% of all cases did not achieve the necessary level of adoption intention to enable a score of technology use of 85 (on a scale from 0 to 100). The corresponding percentages for the other antecedents are 5.7% for emotional value, 28.7% for ease of use, 47.1% for perceived usefulness, and 8.6% for compatibility. In the following, this information will be used to extend the initial IPMA results. In the following, we combine the results from to design a combined importance-performance map that depicts its importance in the form of PLS-SEM's total effects, and as well as its average performance in the form of the average rescaled latent variable scores of adoption intention, emotional value, ease of use, perceived usefulness, and compatibility as obtained from the PLS-SEM analyses. In addition, the combined importance-performance map shows whether the antecedent constructs are necessary conditions for technology use or not, and, if they are necessary, how many of the cases do not achieve the required levels. shows the combined importance-performance map. In line with our previous elaborations, we find that adoption intention, which is in the map's upper right corner, is highly important and already shows high performance. However, adoption intention still needs prioritization, as it is the necessary condition for the target construct, technology use. More specifically, 39% of the cases have not achieved adoption intention's required level. Moreover, perceived usefulness and ease of use, which are in the left quadrants, require prioritization even though they are of little importance and already perform relatively well. Both are necessary conditions, but many cases do not achieve the required levels (47% for perceived usefulness, 29% for ease of use). Failure to prioritize these constructs, which may seem negligible at first glance given the PLS-SEM results, would limit other activities' success regarding improving the technology use. These areas need to be addressed, because they are necessary conditions. Conversely, emotional value and compatibility are also necessary conditions, but only a few of these cases do not achieve the required levels (namely 6%, and 9%); consequently, these constructs are relatively less critical in a practical setting. In this paper, we introduced an enriched understanding of importance and performance that combines insights from PLS-SEM and NCA. Our paper makes several beneficial contributions for researchers and practitioners aiming to profit from PLS-SEM, NCA, and importance-performance analyses in general. Our overarching goal is to create awareness of a different logic for researchers and practitioners using PLS-SEM and IPMA; that is, the necessity logic, which offers several routes to complement findings from PLS-SEM's sufficiency thinking. First, it complements findings with a new perspective on importance. Constructs might have a low IPMA-based importance classification in PLS-SEM (i.e., when they show a relatively low association with a target construct and, therewith, less potential to induce an increase in the targeted outcome). However, such constructs might be highly important from a necessity perspective. If these constructs are bottlenecks, their desired level will not be achieved. Second, the cIPMA provides researchers with a deeper understanding of the relevant performance levels to be achieved, since the NCA's bottleneck information offers a fine-grained perspective on such information is actually needed to advance a target outcome to the next level. These insights benefit research investigating success's antecedents in various contexts (e.g., job satisfaction, team performance, and customer satisfaction to name but a few). In addition, such insights are of high practical value, as they allow actions to be clearly identified and prioritized and, moreover, provide information on how much to invest in certain improvements. Third, to further assess the performance achieved in a sample (which is the normal approach in the PLS-SEM context), we propose analyzing how many cases still need to achieve the condition's required level. In the case of multiple necessary conditions, this information could help prioritize actions. Furthermore, our study also contributes to the broader research on importance and performance analyses, which face the challenge of having to define importance and performance, as well as the critical performance levels. We believe that the cIPMA is a step forward in this direction. In addition to these contributions, there are also various areas on which future research could focus on. First, PLS-SEM and other methods that generate information on should-have factors, use the strength of the association between the constructs to determine the effect that an increase in the antecedent construct has on the target construct. These findings are mostly generated in respect of the full sample; that is, for all combinations of data involving low, medium, and high construct values. However, when combining PLS-SEM with NCA, it could be interesting to understand the association between the constructs involved when a specific bottleneck is bypassed. For instance, if the NCA points to a medium level antecedent construct being critical to achieve a desired high outcome level, it could be interesting to understand the association between the antecedent and the target construct of cases that have already achieved the required medium level. From a sufficiency logic perspective, this could ultimately represent an increase in the antecedent construct's further potential in the IPMA. Second, based on the PLS-SEM findings, the IPMA's importance refers to all of the involved constructs' total effects. In other words, they involve both the direct and indirect associations. These indirect or mediation effects are currently not explicitly considered in the proposed cIPMA. We therefore encourage future research to develop ideas for mediation relationships' analysis in the NCA context, which could ultimately be used in a cIPMA. Writing – review & editing, Writing – original draft, Visualization, Validation, Supervision, Methodology, Investigation, Formal analysis, Data curation, Conceptualization. Writing – review & editing, Writing – original draft, Visualization, Validation, Supervision, Project administration, Methodology, Investigation, Formal analysis, Data curation, Conceptualization. Writing – review & editing, Writing – original draft, Visualization, Validation, Methodology, Investigation, Conceptualization. Writing – review & editing, Writing – original draft, Validation, Software, Project administration, Methodology, Investigation, Formal analysis, Data curation, Conceptualization.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Used\n",
      "* **AIモデルの判断:** Not Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 186 / DOI: 10.1016/j.resconrec.2024.107934  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Harmonised global datasets of wind and solar farm locations and power\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Assessment of forest disturbance and soil erosion in wind farm project using satellite observations\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "Accelerating the penetration of renewable energy stands as a key pillar in efforts to limit global warming below 2 °C ( ). Wind power, as a low-cost, clean and efficient energy source, serves as a primary contributor to the global transition towards renewable energy ( ). Over the past decade, wind power generation has experienced exponential growth worldwide ( ). According to the 2022 Global Wind Report released by the Global Wind Energy Council (GEWC), the global installed capacity of the wind power market reached 837 GW by the end of 2021 ( ). Over 90 % of this installed capacity comes from onshore wind farms. By 2030, wind power is projected to meet 20 % of the global electricity demand [5]. Compared to the continuously increasing installation scale of wind turbines, the public acceptance of onshore wind farms is declining ( ). This decline is attributed to conflicts over land use in populated regions due to wind farm construction ( ), as well as negative impacts such as noise emissions, shadow flicker, and visual impact caused by turbine operations ( ). One potential approach to addressing the issue is deploying wind farms in regions far from residential areas, leading wind farm developers to focus on forested areas ( ). Apart from acquiring land at lower costs, advancements in wind turbine technology, such as taller hub height designs, make establishing wind farms in forested areas more technically and economically feasible ( ). When wind turbines are deployed on forested hilltops and ridgelines, electricity generation can be amplified by the topographical advantage ( ). Following the law of wind shear, higher altitude result in larger measured wind velocity. Additionally, compared to flat terrain, the steeper and longer the slope can create more obvious “Speed-up effect” ( ). In densely wooded areas, forest wind farms unlock significant new wind energy potential, leading to rapid expansion of wind power in forested areas like Northern Europe ( ) and Southern China ( ). In forests in the United States of America, statistics indicate that nearly 2000 wind turbines were operating in 2015 ( ). However, the construction of wind farms inevitably results in adverse impacts on forest ecosystems ( ). The deployment of wind farms requires space to accommodate wind turbines and equipment required for the installation. In addition, to mitigate forest roughness near the wind turbines and prevent forest edge effects, sufficient trees are typically felled by project developers to increase annual power generation and prolong the wind turbines' lifespan ( ). The construction of access roads is also a significant factor contributing to tree felling ( ). Maintaining a certain distance between wind turbines for enhanced power generation inadvertently extends the road length between turbines ( ), leading to broader forest disturbance. Large-scale construction projects result in forest fragmentation, leading to habitat loss, degradation, and displacement of wildlife ( ). The reduction in forest coverage near wind turbines is closely associated with observed declines in forest species density ( ). Moreover, the decrease in forest coverage exposes surface soil to strong winds and rainfall, exacerbating soil erosion ( ). Hence, spatially explicit monitoring of forest disturbance caused by wind farm construction is necessary to assess the environmental impact of wind energy development in forested areas. With the advancement of remote sensing technology, high spatiotemporal resolution remote sensing data can provide crucial information for monitoring forest disturbance caused by large-scale wind farm construction ( ). In recent years, significant progress has been made in monitoring the vegetation impact of wind farms using remote sensing data of various resolutions, focusing mainly on wind farms located in farmland and grassland areas ( ; ; ). Forest ecosystem plays a vital role in providing a variety of essential services and values to human society ( ). However, there has been limited research focused on monitoring the forest disturbance caused by forest wind farm construction. Addressing this research gap, Qin et al. analyzed numerous wind farm samples in the United States and found that wind farms built in the forests exhibited the greatest decrease in forest Normalized Difference Vegetation Index (NDVI) ( ). Given the small areas occupied by wind turbines and roads on wind farms, higher resolution remote sensing imagery is needed to study the impact of wind farm construction on forest. fused Landsat and MODIS data to assess the impact of forest wind farms on vegetation growth and soil erosion in the mountainous regions of Yunnan Province, China ( ). The study revealed a significant decline in vegetation growth due to wind turbine foundations and road construction, with post-construction soil erosion reaching 200−300 % or even 1000 % of pre-construction levels. However, this study primarily focused on forest wind farms in a single region. In many other regions, variations in terrain and climate can lead to different degrees of forest disturbance and soil erosion changes. A global understanding of the current status of forest wind farm-induced environmental impacts is urgently needed. This requires a robust and scalable approach that can be applied to diverse forest areas across the globe. However, previous research methods largely rely on existing data such as wind farm construction dates and internal road distribution ( ). Due to a lack of relevant data in other regions, especially internal road data, such approaches are not valid or viable for wide-scale application. Therefore, the utilization of publicly available remote sensing data to automatically identify the construction date of forest wind farms and extract the internal roads is an urgent direction to be addressed. This will aid in identifying the forest disturbance caused by the construction of forest wind farms. Based on the above background, this study aims to map forest disturbance caused by the construction of forest wind farms from the perspective of multi-source space-borne data, jointly utilizing the Landsat and Sentinel-2 imagery. A novel methodology was developed to automatically extract the construction date of forest wind farms and map internal road distribution. Forest disturbance and soil erosion changes due to forest wind farm construction were mapped using construction date and road distribution as spatial and temporal constraints. The methodology was applied to various regions worldwide to reveal the differences in the extent of forest disturbance and soil erosion change caused by forest wind farm construction. This study contributes to understanding the impact of wind energy development on forest vegetation in different regions, aiding in identifying potential environmental risks associated with such projects. The study areas selected for the survey include China, the United States, Canada, Sweden, and the United Kingdom, which led the way in wind power investments. As of 2021, the cumulative installed wind power capacity of each of the selected five countries exceeds 12,000 MW, securing positions within the top eleven globally ( ). Notably, China and the United States claim the first and second positions worldwide with installed capacities of 346,670 MW and 134,846 MW, respectively. Due to the extensive forest coverage, wind power deployment in the forested areas, of these five countries surpasses that of others ( ; ). In this study, six study areas concentrated on the development of forested wind power were selected from these five countries, with two from China. The location of the six study areas and the corresponding information are presented in and Table. S1. Climate, a crucial factor influencing land use ( ), has led to the inclusion of various climate classes in the selected study areas. The selection of diverse climate zones (e.g., tropical wet and dry climate, humid subtropical climate) serves to demonstrate the reliability of the applied research methodology on a global scale. Multiple datasets were jointly used to monitor forest disturbance and soil erosion of wind farms. These datasets comprise satellite observation data with spatial resolutions ranging from 30 m to 10 m, publicly accessible wind turbine spatial data from crowdsourced geospatial datasets, and environmental data utilized for soil erosion assessment. The multiple satellite observation data selected is based on a careful consideration of their strengths and limitations in both temporal and spatial information. Due to extensive temporal coverage and frequent observation frequency, Landsat images with a resolution of 30 m and a revisit frequency of about 8 days were used to track forest disturbances (Lu et al., 2022). We used all the available Landsat Tier 1 Surface Reflectance scenes from 2000 to 2022 that overlapped the six study areas. These data can be archived in Google Earth Engine (GEE), including images from the Landsat 5 Thematic Mapper (TM), Landsat 7 Enhanced Thematic Mapper Plus (ETM+), and Landsat 8 Operation Land Imager (OLI). Fig. S1 shows the annual distributions of all the available Landsat images by sensors during 2000–2020. Within each scene, the cloud-obscured pixels were masked using Quality Assessment bands. Sentinel-2 Multispectral Instrument (MSI) imagery was used to map the latest roads within the forest wind farm, which provided multispectral optical imagery with a resolution of 10 m and a revisit cycle of ∼5 days at the equator. The high spatial resolution of Sentinel-2 facilitates an enhanced precision in feature extraction, particularly for the delineation of infrastructure such as roads ( ; ). All the available Level-2A Sentinel-2 Surface Reflectance scenes can be acquired from GEE. Cloud-obscured pixels were masked using cloud probability images available on GEE, and a composite image was created using median selection from 2022 Sentinel-2 images. The wind turbine location data is acquired from the OpenStreetMap (OSM) infrastructure data. The data retrieval involves utilizing the Overpass API with \"Generator: source = Wind\" as the search key to capture target features ( ). We also conducted a visual interpretation to add the missing wind turbine locations to the data. The wind farm capacity data is sourced from the Global Power Plant Database, readily accessible on the GEE platform. The global soil erodibility (K-factor) data are sourced from the European Soil Data Centre ( ), with a spatial resolution of 1 km. In contrast to traditional K-factor estimations based on soil texture, this dataset incorporates the direct effects of surface runoff, infiltration, and drainage on soil erosion, integrating soil hydraulic properties into the K-factor estimation ( ). Global rainfall erosivity data (R-factor) are derived from the Global Rainfall Erosivity Database (GloREDa), providing monthly R-factor values at a 1 km resolution. This dataset is estimated using Ensemble Machine Learning method using station data based on 4000 global stations ( ). The global forest management categories originate from the Global Forest Management data created by , which categorizes global forests into six management categories with a spatial resolution of 100 m ( ). Global elevation data are derived from the Copernicus digital elevation model (DEM) with a spatial resolution of 30 m. Slope length and steepness (LS factor) data are derived through the calculation process using elevation data and the \"LS Factor\" tool within the SAGA GIS software ( ). An automated framework was developed to map forest disturbance caused by forest wind farm construction from multi-source space-borne data, by jointly exploiting two critical features of forest wind farms, i.e., spectral changes of vegetation during construction as observed in long-term Landsat time series, and spatial details of internal roads depicted in higher-resolution Sentinel-2 images. Furthermore, this framework allows for the precise extraction of main internal roads through a semi-automatic approach. Based on the delineated forest disturbance range, the soil erosion changes induced by wind farm construction were evaluated. The entire methodological process is illustrated in . In this assessment, the LandTrendr (Landsat-based detection of Trends in Disturbance and Recovery) algorithm was adapted to detect forest disturbance and the construction date of wind turbines. As a well-established spectral-temporal segmentation method, the LandTrendr algorithm is able to automatically capture forest disturbance recovery trends and identify the first year of disturbance changes ( ; ). The annual images used in the algorithm are derived from the median synthesis of annual time series images during the peak growing seasons (June—September). The selection of the peak growing season aims to eliminate the compounding effects of ice, snow, and soil, while maximizing the spectral changes after forest disturbances ( ). A total of five effective spectral indices representing vegetation greenness and structures were utilized, including two spectral bands (shortwave infrared I and II), normalized burn ratio (NBR), NDVI and tasseled cap wetness (TCW) ( ). For each spectral index, the LandTrendr algorithm described the start year of the disturbance event at the pixel level. Because wind turbine construction involves forest clearance and vegetation leveling, the disturbance year at the turbine location was chosen as the construction date of the wind turbine (Fig.S2). Its disturbance year is derived from the majority voting among the results of the five spectral indices. The deployment year of the wind farm was described by counting the construction date of all wind turbines. Subsequently, the obtained deployment year was utilized to delineate the prospective forest disturbance range in the corresponding period within the region, followed by more detailed differentiation and extraction in subsequent steps. As an indicator of the spatial distribution density of vegetation, the NDVI is applied to depict vegetation boundaries for identifying roads. Annual NDVI image was obtained through band operations and median synthesis based on Sentinel-2 images from 2022. Then, the NDVI image was processed through a Bottom-Hat morphological filter to accentuate road features and suppress other features [29]. A visual inspection of different regions indicated 60 as a suitable threshold for extracting potential road distributions. As road construction of forest wind farms can cause forest disturbances, an overlay analysis was conducted based on the forest disturbance generated from the previous step to further refine the road range ( a). Finally, the roads within the forest wind farm were automatically extracted by overlapping the extracted road range with wind turbine points. Simultaneously, a semi-automated process was developed to extract the main roads within the forest wind farm more accurately ( b), facilitating improved visualization and understanding of wind farm road distribution. For rasterized road distributions, erosion and dilation operations were performed to extract linear features and eliminate fragmented features. Subsequently, the skeleton axis extraction plugin in QGIS software was used for road line extraction. Redundant branching lines were automatically removed using a line trimming tool. Manual corrections were applied to rectify minor missing road segments and obtain precise main roads within the forest wind farm. Previous satellite observations and field work have demonstrated that the impact of wind turbines and road construction primarily occurred around turbines and roads [11, 18]. Consequently, we extracted forest disturbance patches intersecting with roads and wind turbine points based on potential forest disturbance distribution, facilitating the differentiation of forest disturbance caused by wind farm construction ( a). This approach considers that forest disturbances caused by wind farm construction typically occur around wind turbine platforms and roads ( ), while forest disturbances observed beyond these areas may result from forest management activities or detection errors manifested as salt-and-pepper noise. The forest disturbance intensity was calculated based on the area of the forest disturbance and the installed capacity ( ). To further distinguish whether these forest disturbances originated from wind turbine or road construction, measurements of the impact ranges of wind turbines and roads of forest wind farms were consulted ( ). Based on their findings, a wind turbine buffer zone with a radius of 60 m was established around wind turbine points, while a road buffer zone with a radius of 30 m was delineated around road lines beyond the wind turbine buffer zone. Using Euclidean distance, forest disturbance pixels closer to the wind turbine buffer zone were classified as influenced by wind turbine construction, while those closer to the road buffer zone were classified as influenced by road construction (Fig. S3). Additionally, the spatial accuracy of forest disturbance maps was assessed across six study areas to make sure they accurately represented disturbance events. For each region, 100 reference points were randomly generated within a 500 m buffer zone around wind turbine points. Two interpreters visually inspected high-resolution Google Earth imagery and time-series spectral data for each reference point, categorizing them into disturbance and no disturbance. This process resulted in a total of 600 reference points, including 184 forest disturbance points and 416 non-disturbance points. Based on the confusion matrix, commonly used overall accuracy (OA), producer's accuracy (PA), user's accuracy (UA) and the Kappa coefficient were calculated. Based on the delineated extent of forest disturbance, we proceeded to estimate soil erosion changes caused by the construction of wind turbine foundations and transportation roads ( c). This assessment aims to discern the impact of wind farm construction on forest ecosystem services related to soil conservation. Soil erosion in this context encompasses processes such as soil denudation, scouring, migration and accumulation, occurring under external forces like gravity, wind, and water flow ( ). The Revised Universal Soil Loss Equation (RUSLE) model was used to quantify soil erosion changes before and after the deployment of wind farms. The basic form of RUSLE is shown in . ·a); R represents the rainfall erosivity factor, MJ·mm/(hm ·h·a); K represents the soil erodibility factor, (t·h)/(MJ·mm); LS represents the slope length and steepness factor; C represents the vegetation cover factor; P represents the soil and water conservation measure factor. The calculation process of C is as follows. and represent the pixel values corresponding to bare soil and pure vegetation, respectively. These values are determined as the minimum and maximum NDVI within the region excluding water bodies ( ). The value is typically assigned according to different land use practices. The forest is commonly assigned a value of 1, signifying the absence of implemented soil conservation measures ( ). Given the predominant forested composition of the study area, a value of 1 is assigned. The construction years, road lengths, and distribution of forest disturbance of forest wind farms across six study areas are depicted in . These forest wind farms were initially deployed in 2009 and as late as 2019, with construction typically lasting 1 to 2 years. The extracted deployment date were visually verified using Google Historical Images, confirming the reliability of the method in this study for automatically extracting deployment dates. The extraction results indicate that roads were constructed in forested areas during wind farm deployment, with all study areas having main road lengths exceeding 25 km. Among them, HS exhibits the longest main road length, extending to 163 km. A significant amount of forest disturbance was detected around roads and wind turbines. illustrates the area of forest disturbance influenced by wind turbine and road construction across the six study areas. Specifically, the forest disturbance extents in JC (1519 ha) and HS (1280 ha) are substantially larger than those in the other four areas, with GB showing the smallest area of forest disturbance (294 ha). In terms of forest disturbance areas, the impact of road construction tends to surpass that of wind turbine construction. Across all study areas, the forest disturbance area caused by road construction is 2–7 times greater than that caused by wind turbine construction. To analyze variations in forest disturbance intensity caused by wind power development across different regions, the ratio of total installed capacity of wind farms to forest disturbance area was used to quantify the forest disturbance intensity of six study areas. In theory, it is a simply defined metric that describes the amount of forest disturbance area needed to support a MW of installed capacity. Additionally, the average disturbance area per wind turbine and per km of road construction was analyzed by calculating the ratio of the forest disturbance area to the number of wind turbines and the ratio of the forest disturbance area to the length of roads, respectively. The results indicate significant differences in forest disturbance intensity caused by wind power development across six study areas, ranging from 1.5 ha to 6.5 ha/MW, with an average of 4.3 ha/MW ( a). LE and JC exhibit lower forest disturbance intensity, potentially related to their individual wind turbines having higher installed capacities. The installed capacity of a single wind turbine in these two regions is 1.5–1.7 times the average capacity of all turbines. Moreover, the forest disturbance area per wind turbine ranged from 1.4 to 1.8 ha, showing minimal variation among the study areas ( b). The forest disturbance area per km of road construction ranged from 5.6 to 9.2 ha, with the JC exhibiting the highest disturbance and the GB exhibiting the lowest. In addition to the area of forest disturbance, the magnitude of forest disturbance was quantified by measuring the NDVI difference in the disturbed regions before and after wind farm construction. The results indicated that the NDVI in disturbed regions affected by wind turbine construction decreased by 0.06 to 0.33, while disturbed regions impacted by road construction experienced an NDVI decrease of 0.03 to 0.24. Notably, the most severe NDVI decline occurred in HS and DG. For each study area, the confusion matrix and kappa coefficient were calculated to assess the spatial accuracy of the final forest disturbance maps ( ). The OA of the identified forest disturbances across all study areas ranged from 80 % to 90 %, with kappa coefficients ranging from 0.60 to 0.83. These findings collectively indicate that the developed method in this study generally captured the spatial characteristics of forest disturbances caused by wind farm construction. However, for the disturbance class, the UA was relatively higher than the PA, indicating that the research results tend to overestimate the actual forest disturbance area. Furthermore, we compared the differences in terrain types and forest disturbance mapping accuracy of different study areas. It was observed that the accuracy of forest disturbance mapping in plain forests tended to be higher than in mountain forests using the method proposed in this study. The total soil erosion amount and average soil erosion per unit area before and after the construction of wind farms are illustrated in . Following the deployment of wind farms, vegetation cover decreased, leading to varying degrees of increase in the gross amount of erosion annually within the influence zones of six regions. Besides vegetation cover, the soil erosion changes also depended on local geographical conditions such as precipitation, soil type, and terrain. In regions with higher levels of precipitation and steeper terrain (LE, SC and JC), the increase in the gross amount of erosion was more pronounced. JC experienced the most significant increase in total soil erosion amount, with an annual increase of 353,261 tons, which is 92 times the pre-deployment level. DG exhibited the least variation in total soil erosion amount with an annual increase of 13,548 tons. Across all regions, the construction of wind farm roads resulted in a higher total soil erosion amount compared to the construction of wind turbines, averaging 2–7 times greater. In terms of impact magnitude, the increase in average soil erosion per unit area due to road construction ranges from 24.74 to 274.33 t/hm , while for wind turbine construction, it ranges from 26.52 to 263.46 t/hm . In regions a, c, d, and e, the increase in average soil erosion per unit area due to wind turbine construction exceeds that of road construction, averaging 1.1 times higher. However, the situation is reversed in regions b and d, with road construction resulting in average soil erosion per unit area increases 1.5 to 1.6 times higher than wind turbine construction. Our findings indicate that forest disturbances triggered by the boom of forest wind farms are substantial. Varied scales of forest disturbance were observed in six wind farm regions across the globe. The largest-scale forest disturbances were observed in two study areas: the forests of central Sweden (HS) and southern China (JC). In these regions, forest disturbances were primarily distributed along roads. Due to the remoteness of the deployment areas and the lack of existing roads, road networks extending over 70 km were constructed to access the wind turbines. Road construction led to extensive vegetation damage by disturbing the surface and soil layer structure ( ). The scale of road construction is considered the primary factor influencing the extent of forest disturbance, with over 70 % of forest disturbance in these two study areas being associated with road construction. In different regions, the forest disturbance intensity caused by wind farm construction also varies. In the United Kingdom, forest disturbance appears as large patches, with the highest forest disturbance intensity observed here, reaching 6.5 ha/MW. The higher forest disturbance intensity in this region may be related to the local wind energy development pattern. In Scottish, where the study area is located, onshore wind capacity currently exceeds the total of all other renewable energy sources and many wind farms are situated on windy and remote peatlands ( ). Due to the extensive afforestation of UK peatlands ( ), large-scale felling is often carried out to create space and increase wind turbine's wind power generation by reducing surface roughness ( ). The local large-scale logging activities to make way for wind turbines have sparked social controversies, but explanations suggest that many of the felled trees are part of a commercial crop and would be logged anyway at the end of the rotation ( ). Additionally, wind farm developers typically engage in on-site replanting or provide compensatory planting elsewhere to avoid a net loss of forest and reduce the negative impact of forest disturbance ( ). The environmental impacts of wind farm-induced forest disturbances exhibit spatial heterogeneity. In two mountainous regions of southern China (c, d), the forest disturbances by wind farms have led to significant soil erosion, with the increase in soil erosion modulus far exceeding that of other study areas. This is due to the high rainfall in southern China, which leads to greater rainfall erosivity, combined with steep mountainous terrain characterized by greater slope length and slope. The synergistic effect of these factors may result in a doubling of soil erosion when vegetation is disrupted, causing more severe damage compared to other forested areas ( ). The strength and novelty of our research method lie in integrating multiple-source spaceborne data, time series image analysis, and morphology operations to address challenges related to assessing forest disturbances caused by wind farm construction. This comprehensive approach enables a more accurate and efficient understanding of the forest disturbance caused by wind farm construction in regions lacking prior information (such as deployment date and road distribution). To illustrate the advantages of the proposed method, we compared our method with previous methods. In previous studies, two common methods for assessing the range of wind farms’ direct impact on vegetation were visual interpretation and buffer zone method ( ; ). While visual interpretation can delineate vegetation destruction with precise boundaries, it heavily relies on expert judgment and high-resolution imagery, incurring substantial human and economic costs ( ). The buffer zone method is more commonly used in practical applications due to its simplicity and convenience. Various studies have set the radius of wind turbine impact on vegetation ranging from 30 m to 1 km ( ; ). The larger buffer zone focuses more on how wind turbines, operating at a larger spatial scale, alter local climate by changing wind speeds, thereby affecting vegetation activity ( ; ). Qin et al.'s remote sensing analysis of numerous wind farm samples indicates that wind farm deployment has spatially heterogeneous impacts on vegetation at a large scale ( ). For instance, turbine-induced atmospheric drying may reduce grassland growth within wind farms ( ). Conversely, other study has found that wind farms in deserts had positive ecological effects on vegetation during operation by mitigating drought stress ( ). However, it overlooks the severe forest disturbance caused by wind turbine construction in the surrounding areas at smaller spatial scales ( ). Ma et al. conducted tests using buffer zones of different radii, narrowing the impact ranges of wind farm roads and wind turbines to within 60 m and 90 m, respectively ( ). However, the fixed-radius buffer zone is difficult to be applied flexibly to other areas, limiting the assessment of forest disturbance caused by wind farm construction on a large scale. For instance, temporary dumping of excavated soil during wind farm construction is also a significant factor causing severe negative impacts on vegetation ( ). These surface soil dumping areas alongside slopes can extend over 200 m (Fig.S4b). Fig. S4c illustrates a forest wind farm in Guangxi, China, where post-construction dumping of waste rock and debris along the mountain slopes led to extensive exposure of bedrock, causing severe vegetation destruction. The buffer zone range proposed by Ma et al. of 60 m cannot encompass the surface soil dumping areas extending outward near the side slopes (Fig. S4b), leading to potential underestimation of forest disturbance caused by wind farm construction. Conversely, our method can effectively identify this area (Fig. S4a). summarizes the advantages and disadvantages of our method compared to current methods. The limitations of the proposed method are further analyzed in the next section. Compared to previous studies that used low-resolution MODIS imagery on a large scale ( ; ), this study demonstrated that using higher-resolution imagery to track forest disturbances can more accurately depict the severe impact of wind farm construction on forest vegetation. Qin et al. found that wind farms built in forests caused an NDVI reduction of 0.014 over a large area ( ). However, this study found that the NDVI reduction in disturbed regions ranged from 0.1 to 0.3 for most study areas. This suggests that using low-resolution imagery for large-scale analysis might overlook significant NDVI changes around wind turbines and roads, leading to an optimistic assessment of the magnitude of forest disturbance caused by wind farm construction. Figure S5 illustrates how MODIS NDVI data fails to capture the significant NDVI decrease caused by wind farm construction on a small scale. Our proposed method tends to achieve higher accuracy in mapping forest disturbances in plain forests compared to mountain forests. This is primarily due to the complex terrain with many ravines and considerable variations in elevation in mountainous areas, leading to a fragmented distribution of vegetation types. Such fragmentation affects vegetation indices like NBR, causing unstable index signals and resulting in incorrect segments and fitted trajectories ( ). Additionally, mountain shading can also interfere with the performance of forest disturbance detection methods ( ). The UA of the forest disturbance category is relatively higher than that of PA, indicating an overestimation of the extracted forest disturbance area. To identify as many potential forest disturbances as possible for extracting the main roads of forest wind farms, five spectral indices were employed in the LandTrendr algorithm, and the chi-square probability threshold was lowered to minimize omission errors, making it more sensitive to forest disturbances. However, this also resulted in an increase in commission errors ( ). When applying this method to map forest disturbances caused by large-scale wind farm construction in the future, further classification of the forest disturbance map can be accomplished by collecting reference samples and using machine learning models like random forest to enhance the accuracy of the mapping results ( ). Based on the forest disturbance mapping results, an analysis was conducted regarding changes in soil erosion resulting from reduced vegetation cover. Due to the vulnerability of soil erosion to heavy rainfall ( ), forests become more sensitive to the impacts of climate change after deforestation. It is important for subsequent research to consider the influence of climate change on soil erosion ( ). In addition to increasing soil erosion, the reduction in vegetation cover also decreases vegetation productivity, leads to carbon sequestration loss ( ; ; ), and impacts biodiversity ( ). These factors need to be taken into account in future studies. Our findings also emphasize that road construction for forest wind farms is a primary factor causing forest disturbance. However, the construction of new overhead power lines to connect wind farms to the electricity network also requires major tree felling, which also needs attention in the future ( ). Additionally, in some regions like Scotland, deforested areas will be replanted or restored as habitats. Therefore, it is necessary to establish longer-term satellite observations to understand the long-term environmental impacts of forest wind farms and the recovery of vegetation after disturbance, providing information for environmental management and rehabilitation. This study primarily focuses on forest wind farms. Further validation is needed to determine the applicability of the methods and results for monitoring and analyzing vegetation disturbances caused by other types of wind farms, such as those deployed on grasslands. Additionally, it is recommended that future studies incorporate long-term field observations to enhance the preliminary RUSLE research to capture the actual on-site dynamics of soil erosion process in forest wind farms ( ). Monitoring the environmental impacts caused by the expansion of forest wind farms contributes to guiding the synergistic development of renewable energy and environmental protection. This study proposes an approach that utilizes multi-source satellite images to reliably detect the deployment date of forest wind farms and map their internal road distribution at fine scales. We applied the approach to six typical locations worldwide, and mapped forest disturbances and soil erosion changes caused by wind farm construction at 30 m resolution. The overall accuracy of the identified forest disturbances across all study areas exceeded 80 %. Satellite observations showed that the average forest disturbance intensity caused by wind farm construction in the study area is 4.3 ha/MW. Road construction is identified as the primary factor responsible for forest disturbance, resulting in a disturbance area 2 – 7 times larger than that caused by wind turbine construction. Construction activities exacerbate vegetation cover destruction, leading to increased soil erosion, particularly severe in mountainous and rainy forest areas. The NDVI in disturbed forest regions affected by road construction decreased by 0.06 to 0.33, with an average increase in soil erosion per unit area ranging from 24.74 to 274.33 t/hm . For disturbed forest regions affected by wind turbine construction, the NDVI decreased by 0.03 to 0.24, with an average increase in soil erosion per unit area ranging from 26.52 to 263.46 t/hm . Although the inter-comparison shows that multiple spaceborne data can accurately depict spatially explicit forest disturbances caused by wind farm construction, the proposed forest disturbance monitoring framework may face certain limitations and uncertainties. Given the limited representativeness of study areas in this study, further validation is needed to assess its applicability in other regions. Writing – original draft, Visualization, Methodology, Conceptualization. Writing – review & editing. Writing – review & editing, Methodology. Writing – review & editing. Writing – review & editing. Writing – review & editing. Writing – review & editing, Conceptualization. Writing – review & editing, Methodology, Funding acquisition, Conceptualization.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Used\n",
      "* **AIモデルの判断:** Not Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "\n",
      "####################  確認用プロンプト 193 / DOI: 10.1016/j.brainresbull.2025.111327  ####################\n",
      "\n",
      "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
      "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
      "\n",
      "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
      "\n",
      "---\n",
      "### 【判定基準】\n",
      "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
      "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
      "\n",
      "---\n",
      "### 【入力データ】\n",
      "\n",
      "**引用元データ論文のタイトル:**\n",
      "Systematic phenotyping and characterization of the 5xFAD mouse model of Alzheimer’s disease\n",
      "\n",
      "**被引用論文のタイトル:**\n",
      "Pad2 deletion induces anxiety‑like behaviors and memory deficits in mice\n",
      "\n",
      "**被引用論文の全文テキスト:**\n",
      "As one of the important ways to regulate protein function, protein posttranslational modifications (PMTs) provide new functional groups that can change the structure and functions of the modified proteins ( ). Citrullination was first reported in 1985, which is the deimination of peptidyl-arginine residues into peptidyl-citrulline, performed by peptidylarginine deiminase (PADs) ( ). There are five calcium-dependent PADs in humans that have been identified as PADs1–4 and PAD6 ( ). Of the five PADs, PAD2 is a main PAD enzyme expressed in the CNS, which is localized in oligodendrocytes and responsible for citrullinating myelin basic protein ( ). The involvement of citrullination in the disease process of Rheumatoid Arthritis was reported in 1998 firstly, and since then, abnormalities in citrullination mediated by PAD2 have been shown to be involved in the pathogenesis of various diseases ( ). It has been reported that PAD2 is over-expressed in brain and peripheral blood of patients with Multiple Sclerosis (MS), supporting the view that up-regulated PAD2 is an important change in pathogenesis of MS ( ). However, interestingly, knockout of did not attenuate disease in Experimental autoimmune encephalomyelitis (EAE), an animal model of MS ( ). Furthermore, clinical and animal studies have demonstrated that PAD2 plays a key role in the onset and progression of sepsis ( ), inflammatory arthritis ( ), Alzheimer’s disease ( ) and cancers ( ). A transcriptome sequencing result showed highest levels of PAD2 in mature astrocytes, oligodendrocytes, and microglia in CNS ( ). Elevated PAD2 levels were observed in embryonic day 16 to postnatal day 30, suggesting that PAD2 may play a role in brain development ( ). However, to date, little is known regarding the function of in CNS, as well as alterations in behaviors and changes in proteins caused by the loss of PAD2. In the present study, we reported the creation of the knockout (KO) mouse. Behavioral test including elevated plus maze, open field test, marble burying test, tail suspension test and forced swimming test were used to assess anxiety-like and depression-like behaviors in animals. Furthermore, we performed Morris water maze to examine the learning and memory capacity in KO mice. Then biochemistry, Western Blot, immunohistochemistry and proteomic strategy were conducted to reveal the changes in protein levels caused by the loss of . The behavioral examinations indicated that KO phenotype displayed anxiety-like behaviors and impaired hippocampal dependent learning and memory capacity. Consistently, loss of resulted in a significant decrease in dendritic spine density and alterations in synaptic proteins such as Drebrin, NMDARs and VGluT3 in hippocampus of KO mice. Our findings demonstrated direct evidence linking the change in PAD2 dosage to anxiety-like behavior and impairments in cognation, which was related with decreased spine density and altered synaptic proteins in KO mice. The construction of knockout mice were performed by deleting the promoter of exon 3 in allele via CRISPR/Cas9 ( A). Heterozygous knockout mice were backcrossed with C57BL/6 N mice respectively for one generation. Then, the heterozygous mice were intercrossed to obtain heterozygous ( ), homozygous mice ( ) and wild-type (WT)littermates ( B). The PCR products of WT mice, mice, and mice were 355 bp, 355 bp & 750 bp, and 750 bp, respectively ( C). Expression of PAD2 protein were downregulated in the hippocampus, cerebral cortex and cerebellum of mice compared to WT littermates ( D). Furthermore, mice did not show any difference in the cell number in hippocampal CA1 and CA3 compared with WT littermates ( E-F). To examine whether the KO mice demonstrate anxiety-like phenotypes, we compared KO mice and wild-type (WT) mice using open field test. In the open field test, mice showed difference in the average travel velocity compared to that of the WT mice ( B-C; WT: 23.69 ± 1.202 cm/s, n = 14; KO mice: 17.82 ± 0.556 cm/s, n = 15; = 0.0003), suggesting abnormal motor function. Similar reduction was detected in the total travelled distance in KO mice ( D; WT: 7293.67 ± 357.06 cm, KO: 5547.92 ± 167.6 cm; = 0.0003), which was caused by more frequent pausing ( E; WT: 107.22 ± 3.88 s, KO: 121.76 ± 5.18 s; = 0.035) and the decreased travel speed. Compared to the WT mice, KO mice showed significant decrease in crossing numbers ( F; WT: 45.07 ± 6.93, KO: 16.2 ± 1.94; = 0.001) and travelled distance ( G; WT: 970.89 ± 170.25 cm, KO: 330.75 ± 38.29 cm; = 0.001) in the center square. When the recordings were further analyzed by time, we found that KO mice spent less time in center square ( H-I; WT: 38.41 ± 7.65 s, 12.8 ± 2.55 %; KO: 16.39 ± 1.94 s, 5.47 ± 0.65 %; = 0.008) but more time in wall area ( J; WT: 261.59 ± 7.65 s, KO: 283.61 ± 1.94 s; = 0.008), compared with WT mice. For the open field test, the reduction of crossings, time and distance in the center of the arena are associated with increased anxiety. Taken together, these findings suggested that KO mice had abnormal locomotion with an increased level of anxiety. The elevated plus maze test is mainly composed of two open arms and two closed arms, which is widely used to evaluate the anxiety-like behaviors in rodents ( A). During the elevated plus maze test, there were no significant difference in total travelled distance (WT: 560.4 ± 53.8 cm, KO: 525.03 ± 39.06 cm; = 0.6), but a significant reduction in open arm-travelled distance (WT: 89.3 ± 14.14 cm, KO: 46.06 ± 9.35 cm; = 0.018) between WT and Pad2 KO mice ( B-D). Furthermore, we found that KO mice displayed similar level of time in open arms ( E; WT: 87.24 ± 21.89 s, KO: 43.09 ± 7.22 s; = 0.069), but spent more time in closed arms ( F; WT: 153.1 ± 18.25 s, KO: 206.4 ± 9.05 s; = 0.016). Reduction in open arm travelled distance and spent time indicated an increasing anxiety levels of Pad2 KO animals. In the marble burying test, WT or KO mice were individually placed into a cage with 20 marble balls and buried marbles was considered as a less than or equal to 1/3 exposed volume. Compared to the WT mice, KO mice buried significantly more marble balls, indicating a higher anxiety level ( G-H; WT: 5.64 ± 0.94 s, KO: 8.93 ± 0.90 s; = 0.018). Next, we assessed the depression-like behaviors in KO mice via forced swimming test and tail suspension test. The results indicated that KO mice showed similar duration of immobility as WT in forced swimming test ( I-J; WT: 175.64 ± 4.33 s, KO: 178.6 ± 6.91 s; = 0.724) and tail suspension test ( K-L; WT: 148.79 ± 11.26 s, KO: 153.53 ± 8.3 s; = 0.735), which suggested no significant change in depression-like behaviors. These findings suggested that KO mice had an increased level of anxiety. To assess the learning and memory capacity of KO mice, we performed Morris water maze to examine hippocampus-dependent spatial learning and memory. Briefly, the mice were trained to find the platform under the water and remember the location of the platform. It was observed that KO mice swam larger distance to find the platform during the place navigation period and traveled larger distance in the quadrant of the platform during spatial probe period compared with WT mice, as shown in A. Moreover, we found that the latency to platforms of mice of KO group was significant longer than that of WT group ( B; day 2: WT=28.65 ± 3.16 s, KO=38.56 ± 2.47 s, = 0.019; day 3, WT=23.48 ± 2.94 s, KO=35.90 ± 4.03 s, = 0.02; day 4, WT=16.88 ± 2.69 s, KO=36.72 ± 4.77 s, = 0.001; day 5, WT=18.25 ± 2.70 s, KO=37.24 ± 5.15 s, = 0.019). KO animals showed significant decrease in swimming speed, compared with WT mice ( C; WT: 20.72 ± 0.60 cm/s, KO: 15.85 ± 1.62 cm/s; = 0.011). Quantification of the swim track showed that KO mice had a decreased percentage of distance in the target quadrant to total distance ( D; WT: 31.07 ± 1.85 %, KO: 20.24 ± 2.93 %; = 0.0047) and KO animals showed a significant decrease in number of platform crossings ( E; WT: 4.214 ± 0.86, KO: 1.86 ± 0.48; = 0.022). Furthermore, there was a significant reduction in the time spent in target quadrant of KO animals, compared with WT mice ( F; WT: 19.33 ± 1.12 s, KO: 11.87 ± 1.93 cm; = 0.003). Given the decreased swimming speed, the recordings in place navigation phase were further analyzed by distance from start to platform to evaluate the learning and memory capacity of KO mice. We found that the distance to platforms of KO mice were longer than WT mice in place navigation phase ( G; day 3, WT=263.5 ± 93.46 cm, KO=490.4 ± 165.20 cm; day 4, WT=258.1 ± 49.63 cm, KO=493.4 ± 53.04 cm; day 5, WT=306.6 ± 44.77 cm, KO=497.3 ± 61.69 cm; < 0.05). To further evaluate the cognitive functions and memory of KO mice, we performed novel object recognition test. Consistent with the results of the MWM, KO mice also performed poorly in the novel object recognition test, showing a decreased object recognition index ( H; WT: 69.95 ± 2.51 %, KO: 54.83 ± 2.48 %; = 0.0004). Together, these results indicated that spatial learning and memory capacity were impaired in the absence of PAD2, suggesting major deficits in learning and memory of KO mice. Abnormal results of the Morris water maze experiment suggested that learning and memory capacity were severely impaired in KO animals. Substantial studies have shown that hippocampus play key roles in learning, memory, and emotional control ( ). To investigate the molecular changes involved in abnormal learning and memory in hippocampus of KO mice, Western Blot was performed to analyzed the synaptic and neuro-related protein levels ( A-C). We interestingly observed increases in GluNR1 ( = 0.0104) and GluN2B ( = 0.004), which were all Ionotropic glutamate receptors. Furthermore, compared with WT mice, reduction in the expression of Grik5 ( = 0.007) was found, one of the metabotropic glutamate receptors. The expressions of Drebrin ( = 0.0096) and BDNF ( = 0.0045) were downregulated in KO hippocampus. No changes were detected in the level of GluA1, GABAα1, PSD95, Synaptophysin, SNAP25 and SV2C ( > 0.05). Considering the possibility of a compensatory mechanism, we examined the expression of PAD4 in the KO hippocampus, another type of PAD enzyme mainly expressed in brain tissue. However, In the absence of PAD2, the protein expression of PAD4 did not change. Glutamate synapses play a crucial role in emotional regulation and learning and memory, with synaptic proteins related to glutamate transmission exhibiting abnormal changes in anxiety, depression, and memory deficits ( ). The down-regulation of puncta density for VGluT3 were detected by immunofluorescent staining in KO animals, which is responsible for the storage and release of glutamate ( D and I; CA1: WT: 0.0136 ± 0.0007/μm , KO: 0.0109 ± 0.0004/μm , = 0.0058; CA3: WT: 0.0207 ± 0.0008/μm , KO: 0.0162 ± 0.0006/μm , = 0.005). Previously, it has been proposed that PAD2-mediated citrullination is critical for neuronal myelination ( ). To determine the level of myelin-associated protein, we performed immunofluorescence staining for MBP in hippocampus and found a reduction of MBP in hippocampal CA3 region ( E and J; CA1: WT: 7.89 ± 0.536 %, KO: 8.77 ± 0.46 %, > 0.05; CA3: WT: 13.1 ± 0.515 %, KO: 15.3 ± 0.526 %, = 0.013). Given the downregulated expression of Drebrin, a protein primarily responsible for the development of dendritic spines in neurons, we hypothesized that dendritic spines may have been affected in the hippocampus of KO mice. According to reports, neurons in the CA3 region of the hippocampus play a crucial role in learning and memory, so we quantified the spine density in CA3 of hippocampus using Geen Gun. The results showed that the density of dendritic spines in KO mice was significant decreased, compared with WT mice ( F-G; WT: 10.68 ± 0.79 per 10 μm, KO: 6.77 ± 0.35 per 10 μm; = 0.001). Finally, considering that PAD2 protein primarily functions in post-translational modifications, we examined the protein citrulline modification in KO mice. The results indicated that in the absence of PAD2, the citrullinated proteins in the hippocampus of KO mice were significantly reduced ( H and K). Together, these data indicated that synaptic related proteins, GluNR1, GluN2B, Grik5 and vGluT3 were altered in KO mice, accompanied by decreased Drebrin and dendritic spines in hippocampus. To comprehensively understand the changes in protein expression after loss of in hippocampus, we conducted the proteomic analysis of hippocampus. Proteins from hippocampus in WT mice and KO mice were extracted, digested and then analyzed by LC-MS/MS ( A). A total of 5538 proteins have been identified and quantified. The heat map showed that there were significant differences in protein expression profiles between KO and WT mice ( B). Among the identified proteins, 148 proteins were up-regulated (Fold Change > 2.0, < 0.05) and 64 proteins were down regulated (Fold Change < 0.5, < 0.05, C). Among these differentially expressed proteins, we found many proteins associated with nervous function, including MBP, mGluR7, SNCG and CaMKIIδ (up-regulated) and Tau, Drebrin, SV2C, Grik5 and vGluT3 (down-regulated). Furthermore, the 64 down-regulated proteins were linked to 47 KEGG pathways, such as Transcriptional misregulation in cancer, Glutamatergic synapse and Neuroactive ligand-receptor interaction pathways ( D). The differentially abundant proteins cluster annotated by KEGG presents up-regulated enrichment pathways, such as ECM-receptor interaction pathway and Focal adhesion pathways ( E). According to Gene Ontology (GO) analysis( < 0.05), the enrichment values of differential proteins were related to BP (biological process, n = 40), CC (cell component, n = 27), and MF (molecular function, n = 56, ). Although the distribution of is generally well established, few studies have examined their function in the CNS. In this study, we developed KO mice. Several anxiety-like behaviors were detected in the KO mice. In open field test, we found that KO mice have an abnormal locomotion with an increased level of anxiety. Consistently, abnormal anxiety-like behavior was also observed in elevated plus maze test and marble burying test. Coincidentally, excessive and abnormal anxiety is associated with neurodegenerative disorders such as Alzheimer's disease(AD) ( ) and Multiple Sclerosis(MS) ( ), which have already been found to be associated with an abnormal function of . In addition to anxiety-like behaviors, the marble burying test has also been used to assess repetitive and compulsive-like behaviors in animals, which are typical symptoms of autism or obsessive-compulsive disorder ( ). In this study, we found that PAD2 KO mice exhibited a significant increase in the number of buried marbles. However, our equipment is unable to save the videos of open filed test which prevents us from further combining the self-grooming behavior in the open field to assess the animals' stereotypical and repetitive behaviors. Therefore, PAD2 KO mice may also display excessive stereotyped repetitive behaviors, which requires further evaluation in future. In order to comprehensively analyze the molecular-level changes caused by the loss of PAD2 protein, we performed proteomic sequencing on the hippocampus of KO mice. Notably, many proteins that were essential for normal neural function were identified. These differentially expressed proteins mainly include: glutamate-related (vGluT3, mGluR7 and Grik5), synaptic transmission (SV2C), dendritic spine development (Drebrin) and CamKII. Furthermore, elevated expressions of Tau and MBP were found in Pad2 KO hippocampus, which were all considered to be involved in the pathogenesis of neurodegenerative diseases ( ). Previous studies have shown that hippocampus serves as a major target of stress mediators leading to multiple psychiatric disorders, especially anxiety disorders ( ). As a specific marker of glutamatergic excitatory transmission, the vesicular glutamate transporter type 3(VGLUT3) is highly expressed in hippocampus ( ). Furthermore, abnormal expression and function of VGLUT3 have already been shown to be associated with neurodegenerative diseases, addiction and anxiety ( ). Previous reports using VGLUT3 knockout mice displayed anxiety-associated phenotype compared to wild type, suggesting that down-regulation of VGLUT3 leads to anxiety-like behavior ( ). Related to these findings, we found a significant reduction in VGLUT3 in hippocampus in KO mice, accompanied by alterations in glutamatergic synapse KEGG pathways. Taken together, these findings suggested that abnormalities in glutamatergic synapses in the hippocampus may be responsible for the anxiety-like behaviors in KO animals. In addition to typical anxiety-like behaviors, we found that KO animals displayed impaired hippocampus-dependent learning and memory capacity in MWM test. Notably, abnormal cognitive function was also found in the disorders with PAD2 abnormalities, such as AD ( )and MS ( ). In addition, proteomics sequencing results showed that the down-regulated KEGG pathway in KO mice was enriched in cognitive disorder Parkinson's disease ( ). The hippocampus is a key region responsible for learning and memory in the brain ( ). In addition, massive dendritic spines and synapses are the core structural basis for the cognitive function of the hippocampus. Additionally, it has been found that the number of thin and mushroom dendritic spines is significantly reduced in Alzheimer's disease patients with cognitive impairment, compared with normal subjects ( ). One recent study reveals that dendritic spine retraction in hippocampal CA3 neurons caused spatial learning deficits in rodents ( ). In this study, we found that spine density in the CA3 region of the hippocampus was significantly reduced, which is in line with the known finding that hippocampal spine abnormalities in cognitively impaired animals. Calcium/calmodulin-dependent protein kinase II (CaMKII) is a calcium/calmodulin (CaM)-dependent serine/threonine protein kinase, consists of four different isozymes, CamKIIα, CamKIIβ, CamKIIγ, and CamKIIδ ( ). CamKII α and β isoforms have been thought to play important functions in the nervous system, while CamKIIδ proteins expressed in the heart and has been associated with cardiac anomalies ( ). However, some recent studies indicated that CamKIIδ plays a key role in neurodevelopment in mice and in humans, and individuals harboring heterozygous variants in CaMKIIδ display symptoms of intellectual disability ( ). Animal studies have shown that CaMKIIδ functions in learning and memory in addition to its better defined non-neuronal functions ( ). Previous studies have reported that SV2C in the brain is mainly involved in neurotransmitter loading and vesicle transport ( ). More recently，SV2C are increasingly implicated in diseases such as Alzheimer’s disease and Parkinson’s disease ( ). Extensive SV2C loss in the brains of PD patients compared to controls has been reported ( ). Brain developmental modulator protein is an actin-binding protein mainly distributed in neuronal dendritic spines, which plays an important role in the development of dendritic spines, the regulation of synaptic morphology, and the maintenance of cognition ( ). As an important neuronal development related protein, Drebrin is found predominantly in dendritic spines of the hippocampus, playing a key role in regulating dendritic spine growth and development ( ). Previous studies have shown that decreased expression of Drebrin leads to a reduction in dendritic spine density, accompanied by impaired cognitive function in animals ( ). Additionally, the distribution and expression of Drebrin have been shown to change in the hippocampus of patients with Alzheimer 's disease ( ). In our study, we found abnormal spatial learning and memory in KO mice, accompanied by reduction in Drebrin and dendritic spines. Taken together, these findings and results suggested that abnormal Drebrin levels probably caused a decrease in dendritic spine density, resulting in impaired cognitive function in KO animals. In previous studies, Pad2 has been reported to be highly expressed in oligodendrocytes, astrocytes, and microglia ( ). Research has shown that glial cells, especially oligodendrocytes and astrocytes, are involved in the important regulatory functions of neurons ( ). Astrocytes can regulate the strength of synaptic transmission by clearing glutamate from the synaptic cleft ( ). Neurons have a unique feedback mechanism for changes in glutamate concentration, such as regulating mGluR signaling and adjusting glutamate receptor expression ( ). Oligodendrocytes regulate neuronal activity by forming myelin and providing metabolic support ( ). Therefore, we speculate that the loss of PAD2 may indirectly affect the expression of neuron-related genes and proteins via glial cells. On the other hand, it is possible that glial cells secrete Pad2 and that Pad2 is expressed at low levels in neurons. These Pad2 enzymes may directly regulate citrullination of neuron-related proteins, although this hypothesis currently lacks strong evidence. However, interestingly, we found in this study that the knockout of PAD2 resulted in abnormal learning and memory, which differed from the elevated PAD2 levels reported in the brains of AD patients. Coincidentally, this opposite trend of PAD2 in human disease and animal studies also existed in multiple sclerosis. Numerous studies have reported that PAD2 expression levels are elevated in the central nervous system of multiple patients, and abnormal citrullination modifications cause neuronal myelin damage ( ). However, a recent animal study showed that knockout of PAD2 resulted in a decreased number of myelinated axons ( ). Regrettably, no AD studies based on PAD2 knockout mice have been reported and the expression of PAD2 did not change in the AD mouse model ( ). However, in an AD model constructed using rats, the expression of PAD2 showed a positive correlation with disease progression ( ). Overall, the levels of PAD2 must be fine-tuned regulated since both its increase and decrease are potentially linked to memory deficits. In summary, this study demonstrates suggested that PAD2 may play a role in regulating emotion and cognitive abilities in the CNS. In this study, we found that deficiency of induced anxiety-like behaviors and memory deficits in mice. However, our findings did not provide sufficient insight into the molecular changes underlying anxiety-like behaviors in mice. Future studies will explore what brain regions PAD2 contributes to anxiety function. Based on the characteristics to all transcript spliceosomes of the gene sequence, the exon 3 of was selected as an ideal target. Then, the promoter of the exon 3 was knocked out with CRISPR/Cas9 to generate deletion heterozygous mice ( ), which were developed in the C57BL/6 N genetic backgrounds. Finally, heterozygous mice were intercrossed to obtain heterozygous, homozygous mice ( ) and wild-type (WT) littermates. We assessed the deletion of promoter to exon 3 of by performing genotyping in mouse tail DNA. Briefly, DNA was extracted by incubating tissue at 98°C for 20 min in 100 μl of extraction buffer (Coolaber, China). The reaction was neutralized by adding 20 μl of 40 mM Tris-HCl. After centrifugation for 5 min at 12,000 rpm, the genomic DNA were collected and used in PCR reaction with Mouse Direct PCR Master Mix (Yeasen, China) following manufacturer's instructions. Two sets of primers were used to in genotyping. Primer 1: 5′-CCTGTCTGGTTGGATTATCTGGGAAAA-3′ and 5′-CCACCCTGAAACCTGCCCTGAGTT-3’with an expected PCR fragment of 355 bp. Primer 2:5′-GAGTACCAGACTAACCCGAGCTAT-3′ and 5′-GTGACAGCACTCAGGACACTAGG-3′ with an expected PCR fragment of 750 bp. The PCR was performed with BIO-RAD T100 Thermal Cycler at the following conditions: hold at 94°C for 5 min, followed by 40 extension cycles of 10 s denaturing step at 94°C with annealing step for 20 s annealing at 60 °C and an extension step of 30 s at 72 °C, followed by a final extension step at 72 °C for 5 min. Finally, 10 μL amplified DNA was placed in a 1 % agarose gel for electrophoresis and visualized with ultraviolet light. All animal experimental procedures were approved by the Animal Care and Use Committee of Institute for Hygiene of Ordnance Industry. All animals were housed under a 12-hour-light/-dark cycle (7:00 am to 7:00 pm) at a temperature (23 ± 3°C) and relative humidity (55 % ± 10 %) with free access to food and water. Behavioral tests were performed on male mice at the age of P60 and performed between 8 am and 4 pm during the light cycle. The same cohort of mice underwent behavioral tests beginning with less stressful behavioral tasks, with a 24 h interval between each experiment: open field test, elevated plus maze test, marble burying test, morris water maze, tail suspension test and forced swimming test. The novel object recognition test was conducted for another cohort of mice. After each mouse was tested, the behavioural apparatus was cleaned thoroughly with 75 % (volume/volume; vol/vol) alcohol to remove the odor. Open field test (OFT) was performed to assess locomotor activity and anxiety-like behavior in rodents. The equipment is a 50 cm × 50 cm × 35 cm (L×W×H) open container, divided into nine equal squares. On the day of the test, mice were allowed to habituate the testing room at least 2 h before the experiment. During the test, a single mouse was placed in the center square of the box, and its activity was recorded for 5 min with Multi Conditioning System (TSE, German). The total distance traveled and the time spent in the central area of the box were analyzed blindly. The cross counts of squares were counted blindly as well. The Elevated plus maze test was conducted to assess anxiety levels in mice. The equipment is set in the shape of a cross-shaped elevated platform, which contains two open arms and two closed arms surrounded by walls (10 cm high). Animals are transferred to the test room at least 2 h before test. Mice were placed in the center, facing the open wall. Animal movement was recorded and analyzed individually. After the experiment of one mouse, the equipment was cleaned by 75 % ethanol for the next recording. Marble burying test was performed to assess anxiety-like and stereotyped behaviors in rodents. During the test, the mice were individually placed into a cage with 20 evenly distributed marbles for 5 mins. The marble balls covered with bedding more than 2/3 were considered buried by the mouse. Tail suspension test was conducted to assess despair behavior in rodents, one of the core depression-like behaviors. Briefly, mice were hanged by its tail using adhesive tape and suspended for 6 min. Next, immobility time of each animal were recorded, which reflect the despair level of mice. For data analysis, the immobility time of each mouse between 2nd and 6th min was analyzed blindly. The forced swimming test was conducted in a container filled with 23 °C water. Due to an aversion to water, rodents usually constantly struggle and try to escape. A hopeless state was represented as immobility. On the test day, each mouse was singly placed into the container and allowed to swim freely for 6 min. For data analysis, the immobility time of each mouse between 2nd and 6th min was analyzed blindly. Morris water maze was performed to assess the hippocampal dependent spatial learning and memory capacity of rodents ( ). The test was conducted in a pool measuring 100 cm in diameter and 60 cm in depth, which was filled with 22°C water stained with a white, non-toxic and odorless colorant. During the test, swimming pool was surrounded by a black curtain, which was hung specific shape markers as spatial orientation clue. Animals were transferred to the behavioral room at least 2 hours prior to the test to habituate to the testing room. The test was divided into two phases: place navigation (days 1–5) and spatial probe (day 6). During the place navigation, the escape platform was hidden under the water. Then, each mouse was placed into the pool from four quadrants and allowed to search the platform freely within 60 s. When the mouse failed to locate the platform within 60 s, it was guided to the platform and given a latency score of 60 s. During the spatial probe, the escape platform was removed from the pool. The animals were palced into the pool from quadrant opposite the platform, and allowed to probe the pool freely. The platform latency and swimming path was recorded. The novel object recognition test evaluates the animal's cognitive memory ability through a behavioral method that measures the time spent exploring familiar versus unfamiliar objects. Mice have a natural tendency to explore novel objects, so their preference for novel objects indicates intact memory of the familiar ones. First, the animals were allowed to freely explore the open filed for 10 minutes. Then, two objects (A) with identical colors and shapes are placed inside. The mice were allowed to explore freely for 5 minutes, until they accumulate a total of 20 seconds exploring object A. One hour later, one object A was replaced with object B, with different shape and color from object A. Time spent exploring object A (the familiar object) and object B (the novel object) were recorded within 5 minutes. The object recognition index was calculated according to the formula: time of novel object exploration/time of novel plus familiar object exploration × 100 %. The mice were sacrificed and the brain tissues were immediately kept in cold RIPA solution (Beyotime Biotechnology, China) supplemented with protease and phosphatase inhibitors (MedChemExpress, USA). All samples were lysed by an ultrasonic machine and the collected fluid was centrifuged at 12,000 rpm for 30 minutes. According to the manufacturer's instructions, protein concentrations were determined using the BCA Protein Assay Kit (Coolaber, China). Protein samples were separated by SDS-PAGE and then transferred to PVDF membranes (Millipore, USA), which were blocked for 1 h in 5 % milk at room temperature. After that, the membranes were incubated in primary antibodies against PAD2(Proteintech, 1:1000), GluA1(Cell Signaling Technology(CST), 1:1000), GluN2B (CST, 1:1000), GluNR1(Abcam, 1:1000), GABARα1(Abcam, 1:1000), PSD95(CST, 1:1000), Drebrin(Proteintech, 1:5000), Synaptophysin(Abcam, 1:1000), BDNF(Abcam, 1:1000), PAD4 (Proteintech, 1:1000), SNAP25 (CST, 1:1000), SV2C(Proteintech, 1:1000), Grik5 (Proteintech, 1:1000), β-Tubulin and GAPDH (Proteintech, 1:2000), followed by incubation with secondary antibodies conjugated with horseradish peroxidase(Proteintech, 1:2000). Finally, ECL reagents (Millipore, USA) were used to visualize the protein bands. The expression of target proteins was normalized to their corresponding GAPDH or β-Tubulin. Citrullinated proteins in animal hippocampus were detected using an anti-modified citrulline assay kit (Sigma-Aldrich, 17–347B). Briefly, 50 μg total proteins were separated by SDS-PAGE and transferred to PVDF membranes. Modification of citrulline residues was created by a chemical reaction with 2,3-butanedione monoxime and antipyrine in a strong acid solution at 37°C overnight. Blots were blocked in 5 % nonfat dry milk for 1 h at RT and then incubated with dilution of anti-modified Citrulline antibody (1:1000) for 2 h at RT. After washed twice with TBST buffer, the blot was incubated with dilution of anti-Human HRP-conjugated antibody (1:2000) for 1 h at RT. Finally, the protein bands were visualized by ECL chemical regents. All collected brain tissues were fixed in 4 % paraformaldehyde (PFA) overnight and sliced in 6 μm paraffin sections. Subsequently, the sections were deparaffinized in baths of xylene and rehydrated by ethanol and PBS. Antigen retrieval was performed by sodium citrate (Bioss, China) in microwave on medium power for 10 min. Then, brain slides were blocked in 5 % bovine serum albumin (Sigma, USA) and permeabilization with 4 ‰ Triton X-100 (MP Biomedicals, USA) in PBS for 40 min at room temperature. After that, the sections were incubated with primary antibody VGLUT3(Synaptic Systems, 1:200), MBP (Proteintech, 1:250) diluted in antibody diluent solution (Coolaber, China) at 4°C overnight. After rinsing with PBS for 30 min, sections were incubated with appropriate secondary antibody Alexa Fluor conjugated secondary antibodies (Thermo Fisher Scientific, 1:500) for 1 hour at room temperature, protected from light. Photograph acquisition was collected with Leica SP8 laser confocal microscope (Leica, Germany). Procedures were essentially identical to those previously described ( ). Mice brains were perfusion fixed with 4 % paraformaldehyde (PFA) followed by 30 min of postfixation in 4 % PFA. Brain slices were made using a vibratome, with coronal sections 100 μm/slice thickness and stored in 4°C precooled PBS. Tefzel tubes containing lipophilic Dil dye (ThermoFisher scientific, USA) were loaded onto a Gene Gun (Bio-Rad, USA). Then, the slices were spread on a 6-well culture plate and diolistically labeled with the Gene Gun at a height of 5 cm from the slices. After that, the labeled slices were postfixed by 4 % PFA for 5 min and washed with PBS for 15 min. Finally, the slices were sealed with anti-fluorescence quencher (Coolaber, China). Z-Stack images were acquired using Leica SP8 laser confocal microscope. Briefly, the hippocampal CA3 region was confirmed under 10 × objective lens, followed by the pyramidal cells were found by 20 × objective lens. The apical dendrite of pyramidal cells in the hippocampus play an important role in the long-term synaptic plasticity related to learning and memory ( ).Then, 20 z-sections of spines on the secondary and tertiary apical dendrites of pyramidal cell were acquired at 0.5 µm steps using a 60 × objective lens. Image J was used to perform statistical analysis of the number of dendritic spines. The 6 μm paraffin sections were deparaffinized in baths of xylene and rehydrated by ethanol and PBS. The slices were placed into 1 % cresyl violet solution and heated in a 56°C water bath for 1 hour. Next, the slices were washed with water and differentiated in alcohol solution for 1 min. Finally, the xylene and neutral gum were separately used for transparent and seal. 1. Total protein extraction The hippocampus was homogenized in ice-cold protein lysis buffer and destructed by ultrasonic on ice for 5 min. After reacting at 95°C for 15 min and ice-bath for 2 min, the samples were centrifuged at 4°C, 12000 × for 15 min. Iodoacetamide (Sigma, USA) was used to denature the proteins and incubated for 1 hour in the dark at room temperature. Four times volume of ice-cold acetone were added to protein samples and incubated at least 2 hours. The concentrations of samples were detected by Braford protein quantitative kit (Thermo Fisher Scientific, USA). Then, collected proteins were digested with trypsin overnight. The digested peptides were desalted using a C18 desalting column. Finally, the peptides were lyophilized and stored at −80℃ until use. 2. Proteomic analyses by LC-ESI-MS UHPLC-MS/MS analyses were performed using a nanoElute UHPLC system (Bruker, Germany) coupled with a tims TOF pro2 mass spectrometer (Bruker, Germany). At first, the peptide was dissolved in 10 μL buffer A (100 % water, 0.1 % formic acid), centrifuged at 14000 × g for 20 min at 4℃. 200 ng of the supernatant was injected into analytical column (Novogene, 15 cm ×100 μm, 1.9 μm) to separate with 60 min gradient, and flow rate was maintained at 300 nL/min. For mass spectrometry parameters, spray voltage was set to 2.1 kV, the range was set from 100 to 1700 Da for MS scan, the ramp time was 0.1 s. Furthermore, settings of parallel accumulation–serial fragmentation were as following: 10 MS/MS scans (a total cycle time of 1.17 s), ionic strength threshold of 2500, scheduling target intensity of 20000. 3. Data analysis The search parameters were set as followings: Oxidation of methionine was searched as variable modification, and Carbamidomethyl was searched as fixed modification with a maximum of two missed cleavages; The software PD or MaxQuant were used to further filter the data: identified Peptide Spectrum Matches (PSMs) were considered as identified PSMs with a credibility of more than 0.99. Proteins containing at least 1 unique peptide were considered as identified proteins. Correction for false discovery rate (FDR) was applied to correct p-value, and the FDR threshold of PSMs and identified proteins was set to 0.01. The differentially expressed proteins (DEP) were identified by unpaired two-tail Student’s -test with the significance threshold set as -value< 0.05 and fold change of quantitation calculated as KO/WT ratio > 2(up-regulated) or< 0.5 (down-regulated). Software Interproscan was used for Gene Ontology (GO) analysis. Furthermore, COG (Clusters of Orthologous Groups) and KEGG (Kyoto Encyclopedia of Genes and Genomes) were performed to analyze the functional protein family and pathways of the identified proteins. IBM SPSS was used for statistical analysis and GraphPad Prism 9 for the graphics. Before the calculation, tests for normality of data distribution were performed. Significant differences were evaluated by a two-tailed student's -test, Kolmogorov-Smirnov test or Two-Way ANOVA in this study. p < 0.05 was considered statistically significant. All data were presented as mean ± SEM. Methodology. Writing – review & editing, Supervision. Supervision. Writing – original draft, Funding acquisition. Methodology. Methodology. Data curation. Methodology. Writing – original draft, Methodology. Methodology. All animals’ tests have been reviewed by the Animal Care and Use Committee of Institute for Hygiene of Ordnance Industry for the scientific ethical issues that may be involved. All animals’ tests were considered to meet the ethical requirements for animal experiments and the management specifications for the safety of animal experiments and meet the scientific ethical principles, with the approval number of IACUC202205.\n",
      "\n",
      "---\n",
      "### 【状況】\n",
      "* **私の最初の判断:** Not Used\n",
      "* **AIモデルの判断:** Used\n",
      "\n",
      "---\n",
      "### 【あなたのタスク】\n",
      "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
      "\n",
      "######################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. ファイルパスの設定 ---\n",
    "GROUND_TRUTH_CSV = '../data/ground_truth/annotation_target_list.csv'\n",
    "LLM_PREDICTIONS_CSV = '../data/processed/prediction_llm.csv'\n",
    "SAMPLES_WITH_TEXT_CSV = '../data/processed/samples_with_text.csv'\n",
    "\n",
    "# --- 2. データの読み込みと準備 ---\n",
    "try:\n",
    "    df_gt = pd.read_csv(GROUND_TRUTH_CSV)\n",
    "    df_llm = pd.read_csv(LLM_PREDICTIONS_CSV)\n",
    "    df_text = pd.read_csv(SAMPLES_WITH_TEXT_CSV)\n",
    "\n",
    "    # 評価対象のモデルのカラム名\n",
    "    best_model_column = 'prediction_rule3_gemini-2_5-flash'\n",
    "\n",
    "    # 3つのファイルをDOIをキーにマージ\n",
    "    df_merged = pd.merge(df_gt, df_llm, on='citing_paper_doi', how='inner', suffixes=('_gt', ''))\n",
    "    df_review = pd.merge(df_merged, df_text, on='citing_paper_doi', how='inner', suffixes=('', '_text'))\n",
    "    \n",
    "    # -1やNaNを除外\n",
    "    df_review.dropna(subset=['is_data_used_gt', best_model_column], inplace=True)\n",
    "    df_review = df_review[df_review[best_model_column] != -1]\n",
    "    df_review[best_model_column] = df_review[best_model_column].astype(int)\n",
    "    df_review['is_data_used_gt'] = df_review['is_data_used_gt'].astype(int)\n",
    "    \n",
    "    # 判断の不一致を検出\n",
    "    disagreements = df_review[df_review['is_data_used_gt'] != df_review[best_model_column]].copy()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ エラー: データの読み込みに失敗しました。 {e}\")\n",
    "    disagreements = pd.DataFrame()\n",
    "\n",
    "# --- 3. 確認用プロンプトの自動生成 ---\n",
    "if not disagreements.empty:\n",
    "    print(\"=\"*70)\n",
    "    print(f\"【確認が必要な {len(disagreements)} 件の論文について、以下のプロンプトをコピーして私に送信してください】\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for index, row in disagreements.iterrows():\n",
    "        human_label = \"Used\" if row['is_data_used_gt'] == 1 else \"Not Used\"\n",
    "        llm_label = \"Used\" if row[best_model_column] == 1 else \"Not Used\"\n",
    "        \n",
    "        prompt_template = f\"\"\"\n",
    "私は、ある論文が指定されたデータ論文のデータを「実際に使用しているか」を手動でラベル付けしました。\n",
    "しかし、作成したAIモデルの判断と私の判断が食い違ったため、第三者の意見を参考に、私の判断が正しかったかを再確認したいです。\n",
    "\n",
    "以下の【入力データ】と【判定基準】を基に、この論文はデータを「使用している」と判断すべきか、「使用していない」と判断すべきか、あなたの専門的な見解を教えてください。\n",
    "\n",
    "---\n",
    "### 【判定基準】\n",
    "* **\"Used\":** 論文の主張や結論（性能評価、分析結果、比較など）を導き出すために、データセットが分析、訓練、評価、性能比較などのプロセスに直接的に用いられている状態。\n",
    "* **\"Not Used\":** 研究の背景説明、関連研究の紹介、あるいは今後の展望としてデータセット名に言及しているだけの状態。\n",
    "\n",
    "---\n",
    "### 【入力データ】\n",
    "\n",
    "**引用元データ論文のタイトル:**\n",
    "{row['cited_data_paper_title']}\n",
    "\n",
    "**被引用論文のタイトル:**\n",
    "{row['citing_paper_title']}\n",
    "\n",
    "**被引用論文の全文テキスト:**\n",
    "{row['full_text']}\n",
    "\n",
    "---\n",
    "### 【状況】\n",
    "* **私の最初の判断:** {human_label}\n",
    "* **AIモデルの判断:** {llm_label}\n",
    "\n",
    "---\n",
    "### 【あなたのタスク】\n",
    "上記の情報を踏まえ、最終的にどちらの判断がより妥当と考えられるか、その根拠となるテキスト中の箇所を引用しながら、あなたの分析結果を提示してください。\n",
    "\"\"\"\n",
    "        print(\"\\n\\n\" + \"#\"*20 + f\"  確認用プロンプト {index+1} / DOI: {row['citing_paper_doi']}  \" + \"#\"*20)\n",
    "        print(prompt_template)\n",
    "        print(\"#\"*70 + \"\\n\")\n",
    "else:\n",
    "    print(\"✅ 人間の判断とLLMの判断はすべて一致していました。確認作業は不要です。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44117a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. 設定項目 ---\n",
    "GROUND_TRUTH_CSV = '../data/ground_truth/annotation_target_list.csv'\n",
    "\n",
    "# --- 2. 【重要】ここに修正内容を記入してください ---\n",
    "# 例: corrections = {\n",
    "#         \"10.1016/j.jprot.2022.104578\": 1, # このDOIのラベルを1に修正\n",
    "#         \"10.1016/j.ccell.2021.04.015\": 0  # このDOIのラベルを0に修正\n",
    "#     }\n",
    "corrections = {\n",
    "    # ↓↓↓ ここに修正したいDOIと最終的な正しいラベルを記入 ↓↓↓\n",
    "    \n",
    "    \n",
    "    # ↑↑↑ ここまで ↑↑↑\n",
    "}\n",
    "\n",
    "# --- 3. 修正の実行と結果の確認 ---\n",
    "if not corrections:\n",
    "    print(\"修正対象が指定されていません。\")\n",
    "else:\n",
    "    try:\n",
    "        df_gt = pd.read_csv(GROUND_TRUTH_CSV)\n",
    "        print(f\"元の '{os.path.basename(GROUND_TRUTH_CSV)}' を読み込みました。\")\n",
    "\n",
    "        updated_count = 0\n",
    "        for doi, new_label in corrections.items():\n",
    "            # DOIに一致する行のインデックスを取得\n",
    "            target_indices = df_gt[df_gt['citing_paper_doi'] == doi].index\n",
    "            \n",
    "            if not target_indices.empty:\n",
    "                # is_data_used_gt列の値を新しいラベルで更新\n",
    "                df_gt.loc[target_indices, 'is_data_used_gt'] = new_label\n",
    "                print(f\"  - DOI: {doi} のラベルを {new_label} に更新しました。\")\n",
    "                updated_count += 1\n",
    "            else:\n",
    "                print(f\"  - 警告: DOI {doi} は見つかりませんでした。\")\n",
    "\n",
    "        if updated_count > 0:\n",
    "            # 更新された内容をファイルに上書き保存\n",
    "            df_gt.to_csv(GROUND_TRUTH_CSV, index=False, encoding='utf-8-sig')\n",
    "            print(f\"\\n✅ {updated_count}件の修正をファイルに保存しました。\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ エラーが発生しました: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataUsageValidator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
