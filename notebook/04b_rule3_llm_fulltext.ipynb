{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c091cfab",
   "metadata": {},
   "source": [
    "- zero shot\n",
    "- gemini 1.5 flash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "616f47d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… APIã‚­ãƒ¼ã‚’.envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\n",
      "âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ« 'zero_shot_fulltext.txt' ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\n",
      "åˆè¨ˆ 200 ä»¶ã®è«–æ–‡ã«å¯¾ã—ã¦LLM(Full-Text)ã®åˆ¤å®šã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c6e7527516458289dfb0a559f37018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM(Full-Text)ã§åˆ¤å®šä¸­:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å‡¦ç†å®Œäº†ã€‚LLM(Full-Text)ã®äºˆæ¸¬çµæœã‚’ '../data/processed/prediction_llm.csv' ã«è¿½åŠ ãƒ»æ›´æ–°ã—ã¾ã—ãŸã€‚\n",
      "\n",
      "--- ä¿å­˜ã•ã‚ŒãŸçµæœã®å†…è¨³ï¼ˆãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆç‰ˆï¼‰ ---\n",
      "prediction_rule3_fulltext\n",
      "0    141\n",
      "1     53\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­5ä»¶ ---\n",
      "     citing_paper_eid                 citing_paper_doi  \\\n",
      "0  2-s2.0-85211640167  10.1016/j.marpolbul.2024.117442   \n",
      "1  2-s2.0-85210281314   10.1016/j.jaridenv.2024.105282   \n",
      "2  2-s2.0-85171680307   10.1016/j.revpalbo.2023.104989   \n",
      "3  2-s2.0-85142708771     10.1016/j.foreco.2022.120653   \n",
      "4  2-s2.0-85194770743    10.1016/j.jnucmat.2024.155194   \n",
      "\n",
      "                                  citing_paper_title  \\\n",
      "0  Multi-indicator assessment of heavy metal poll...   \n",
      "1  Potential effects of climate change on cacti d...   \n",
      "2  Approaches to pollen taxonomic harmonisation i...   \n",
      "3  Allometric equations to estimate the dry mass ...   \n",
      "4  Microstructural evolution in doped high entrop...   \n",
      "\n",
      "                              cited_data_paper_title  \\\n",
      "0  Pollution load index for heavy metals in Mian-...   \n",
      "1  The World Checklist of Vascular Plants, a cont...   \n",
      "2  European pollen-based REVEALS land-cover recon...   \n",
      "3  A global map of root biomass across the world'...   \n",
      "4  Database on the mechanical properties of high ...   \n",
      "\n",
      "   prediction_rule3_abstract  prediction_rule3_fulltext  \n",
      "0                          0                          0  \n",
      "1                          0                          1  \n",
      "2                          0                          0  \n",
      "3                          0                          0  \n",
      "4                          0                          0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- 1. è¨­å®šã¨APIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿ ---\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if API_KEY:\n",
    "    print(\"âœ… APIã‚­ãƒ¼ã‚’.envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\")\n",
    "else:\n",
    "    print(\"âŒ .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰GEMINI_API_KEYãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "    exit()\n",
    "\n",
    "MODEL_NAME = \"gemini-1.5-flash\"\n",
    "API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={API_KEY}\"\n",
    "\n",
    "# å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«\n",
    "INPUT_CSV = '../data/processed/samples_with_text.csv'\n",
    "PROMPT_FILE_PATH = '../prompts/zero_shot_fulltext.txt'\n",
    "# æ—¢å­˜ã®LLMäºˆæ¸¬çµæœãƒ•ã‚¡ã‚¤ãƒ«ã«çµæœã‚’è¿½è¨˜ã™ã‚‹\n",
    "OUTPUT_FILE = '../data/processed/prediction_llm.csv'\n",
    "\n",
    "# --- 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ ---\n",
    "try:\n",
    "    with open(PROMPT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        PROMPT_TEMPLATE = f.read()\n",
    "    print(f\"âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ« '{os.path.basename(PROMPT_FILE_PATH)}' ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\")\n",
    "    \n",
    "    df_to_process = pd.read_csv(INPUT_CSV)\n",
    "    df_to_process.dropna(subset=['full_text'], inplace=True)\n",
    "    df_to_process.drop_duplicates(subset=['citing_paper_doi'], inplace=True, keep='first')\n",
    "    df_to_process.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f\"åˆè¨ˆ {len(df_to_process)} ä»¶ã®è«–æ–‡ã«å¯¾ã—ã¦LLM(Full-Text)ã®åˆ¤å®šã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ {e}\")\n",
    "    df_to_process = pd.DataFrame()\n",
    "\n",
    "# --- 3. LLMã«ã‚ˆã‚‹åˆ¤å®šã®æœ¬ç•ªå®Ÿè¡Œ ---\n",
    "predictions = []\n",
    "if PROMPT_TEMPLATE and not df_to_process.empty:\n",
    "    for index, row in tqdm(df_to_process.iterrows(), total=len(df_to_process), desc=\"LLM(Full-Text)ã§åˆ¤å®šä¸­\"):\n",
    "        \n",
    "        full_text = row.get('full_text', 'ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚')\n",
    "        \n",
    "        prompt = PROMPT_TEMPLATE.format(\n",
    "            cited_data_paper_title=row['cited_data_paper_title'],\n",
    "            citing_paper_title=row['citing_paper_title'],\n",
    "            full_text=full_text[:30000] # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ä¸Šé™ã‚’è€ƒæ…®\n",
    "        )\n",
    "        \n",
    "        payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(API_URL, json=payload, timeout=120)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            response_json = response.json()\n",
    "            response_text = response_json['candidates'][0]['content']['parts'][0]['text']\n",
    "            \n",
    "            match = re.search(r\"\\{.*\\}\", response_text, re.DOTALL)\n",
    "            json_text = match.group(0) if match else response_text\n",
    "            final_json = json.loads(json_text)\n",
    "            \n",
    "            decision = final_json.get('decision')\n",
    "            prediction = 1 if decision == \"Used\" else 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nDOI {row['citing_paper_doi']} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            prediction = -1\n",
    "            \n",
    "        predictions.append(prediction)\n",
    "        time.sleep(1)\n",
    "\n",
    "# --- 4. çµæœã®ä¿å­˜ ---\n",
    "if len(predictions) == len(df_to_process):\n",
    "    # ä»Šå›ã®äºˆæ¸¬çµæœã‚’DataFrameã«ã¾ã¨ã‚ã‚‹\n",
    "    df_fulltext_predictions = pd.DataFrame({\n",
    "        'citing_paper_doi': df_to_process['citing_paper_doi'],\n",
    "        'prediction_rule3_fulltext': predictions\n",
    "    })\n",
    "    \n",
    "    # æ—¢å­˜ã®LLMäºˆæ¸¬çµæœãƒ•ã‚¡ã‚¤ãƒ«(ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆç‰ˆ)ã‚’èª­ã¿è¾¼ã‚€\n",
    "    try:\n",
    "        output_df = pd.read_csv(OUTPUT_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"è­¦å‘Š: '{OUTPUT_FILE}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚­ãƒ¼æƒ…å ±ã®ã¿ã§æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚\")\n",
    "        output_df = df_to_process[['citing_paper_eid', 'citing_paper_doi', 'citing_paper_title', 'cited_data_paper_title']].copy()\n",
    "\n",
    "    # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã¨æ–°ã—ã„äºˆæ¸¬çµæœã‚’ãƒãƒ¼ã‚¸ï¼ˆçµåˆï¼‰\n",
    "    # æ—¢ã«åŒåã‚«ãƒ©ãƒ ãŒã‚ã‚Œã°ä¸Šæ›¸ãã—ã€ãªã‘ã‚Œã°è¿½åŠ ã™ã‚‹\n",
    "    if 'prediction_rule3_fulltext' in output_df.columns:\n",
    "        output_df.drop(columns=['prediction_rule3_fulltext'], inplace=True)\n",
    "        \n",
    "    output_df = pd.merge(output_df, df_fulltext_predictions, on='citing_paper_doi', how='left')\n",
    "    \n",
    "    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¸Šæ›¸ãä¿å­˜\n",
    "    output_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\nå‡¦ç†å®Œäº†ã€‚LLM(Full-Text)ã®äºˆæ¸¬çµæœã‚’ '{OUTPUT_FILE}' ã«è¿½åŠ ãƒ»æ›´æ–°ã—ã¾ã—ãŸã€‚\")\n",
    "    print(\"\\n--- ä¿å­˜ã•ã‚ŒãŸçµæœã®å†…è¨³ï¼ˆãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆç‰ˆï¼‰ ---\")\n",
    "    print(output_df['prediction_rule3_fulltext'].value_counts())\n",
    "    print(\"\\n--- ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­5ä»¶ ---\")\n",
    "    print(output_df.head())\n",
    "else:\n",
    "    print(\"å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e44bb4f",
   "metadata": {},
   "source": [
    "- few shot\n",
    "- cot\n",
    "- gemini 1.5 flash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8a5afa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… APIã‚­ãƒ¼ã‚’.envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\n",
      "âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ« 'few_shot_cot_fulltext.txt' ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\n",
      "åˆè¨ˆ 200 ä»¶ã®è«–æ–‡ã«å¯¾ã—ã¦LLM(Full-Text, Few-shot)ã®åˆ¤å®šã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3606e1b6e5d84f76b997beb5807a73e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM(Full-Text, Few-shot)ã§åˆ¤å®šä¸­:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å‡¦ç†å®Œäº†ã€‚LLM(Full-Text, Few-shot)ã®äºˆæ¸¬çµæœã‚’ '../data/processed/prediction_llm.csv' ã«è¿½åŠ ãƒ»æ›´æ–°ã—ã¾ã—ãŸã€‚\n",
      "\n",
      "--- ä¿å­˜ã•ã‚ŒãŸçµæœã®å†…è¨³ï¼ˆFew-shotç‰ˆï¼‰ ---\n",
      "prediction_rule3_fulltext_few_shot\n",
      "0    123\n",
      "1     77\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- 1. è¨­å®šã¨APIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿ ---\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if API_KEY:\n",
    "    print(\"âœ… APIã‚­ãƒ¼ã‚’.envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\")\n",
    "else:\n",
    "    print(\"âŒ .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰GEMINI_API_KEYãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "    exit()\n",
    "\n",
    "MODEL_NAME = \"gemini-1.5-flash\"\n",
    "API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={API_KEY}\"\n",
    "\n",
    "# å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«\n",
    "INPUT_CSV = '../data/processed/samples_with_text.csv'\n",
    "# ã€å¤‰æ›´ç‚¹ã€‘Few-shot & CoTãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª­ã¿è¾¼ã‚€\n",
    "PROMPT_FILE_PATH = '../prompts/few_shot_cot_fulltext.txt' \n",
    "# æ—¢å­˜ã®LLMäºˆæ¸¬çµæœãƒ•ã‚¡ã‚¤ãƒ«ã«çµæœã‚’è¿½è¨˜ã™ã‚‹\n",
    "OUTPUT_FILE = '../data/processed/prediction_llm.csv'\n",
    "\n",
    "# --- 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ ---\n",
    "try:\n",
    "    with open(PROMPT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        PROMPT_TEMPLATE = f.read()\n",
    "    print(f\"âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ« '{os.path.basename(PROMPT_FILE_PATH)}' ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\")\n",
    "    \n",
    "    df_to_process = pd.read_csv(INPUT_CSV)\n",
    "    df_to_process.dropna(subset=['full_text'], inplace=True)\n",
    "    df_to_process.drop_duplicates(subset=['citing_paper_doi'], inplace=True, keep='first')\n",
    "    df_to_process.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f\"åˆè¨ˆ {len(df_to_process)} ä»¶ã®è«–æ–‡ã«å¯¾ã—ã¦LLM(Full-Text, Few-shot)ã®åˆ¤å®šã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ {e}\")\n",
    "    df_to_process = pd.DataFrame()\n",
    "\n",
    "# --- 3. LLMã«ã‚ˆã‚‹åˆ¤å®šã®æœ¬ç•ªå®Ÿè¡Œ ---\n",
    "predictions = []\n",
    "if PROMPT_TEMPLATE and not df_to_process.empty:\n",
    "    for index, row in tqdm(df_to_process.iterrows(), total=len(df_to_process), desc=\"LLM(Full-Text, Few-shot)ã§åˆ¤å®šä¸­\"):\n",
    "        \n",
    "        full_text = row.get('full_text', 'ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚')\n",
    "        \n",
    "        prompt = PROMPT_TEMPLATE.format(\n",
    "            cited_data_paper_title=row['cited_data_paper_title'],\n",
    "            citing_paper_title=row['citing_paper_title'],\n",
    "            full_text=full_text[:30000] # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ä¸Šé™ã‚’è€ƒæ…®\n",
    "        )\n",
    "        \n",
    "        payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(API_URL, json=payload, timeout=120)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            response_json = response.json()\n",
    "            response_text = response_json['candidates'][0]['content']['parts'][0]['text']\n",
    "            \n",
    "            match = re.search(r\"\\{.*\\}\", response_text, re.DOTALL)\n",
    "            json_text = match.group(0) if match else response_text\n",
    "            final_json = json.loads(json_text)\n",
    "            \n",
    "            decision = final_json.get('decision')\n",
    "            prediction = 1 if decision == \"Used\" else 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nDOI {row['citing_paper_doi']} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            prediction = -1\n",
    "            \n",
    "        predictions.append(prediction)\n",
    "        time.sleep(1)\n",
    "\n",
    "# --- 4. çµæœã®ä¿å­˜ ---\n",
    "if len(predictions) == len(df_to_process):\n",
    "    # ä»Šå›ã®äºˆæ¸¬çµæœã‚’DataFrameã«ã¾ã¨ã‚ã‚‹\n",
    "    df_new_predictions = pd.DataFrame({\n",
    "        'citing_paper_doi': df_to_process['citing_paper_doi'],\n",
    "        # ã€å¤‰æ›´ç‚¹ã€‘æ–°ã—ã„ã‚«ãƒ©ãƒ åã§çµæœã‚’ä¿å­˜\n",
    "        'prediction_rule3_fulltext_few_shot': predictions \n",
    "    })\n",
    "    \n",
    "    # æ—¢å­˜ã®LLMäºˆæ¸¬çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "    try:\n",
    "        output_df = pd.read_csv(OUTPUT_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"è­¦å‘Š: '{OUTPUT_FILE}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚­ãƒ¼æƒ…å ±ã®ã¿ã§æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚\")\n",
    "        output_df = df_to_process[['citing_paper_eid', 'citing_paper_doi', 'citing_paper_title', 'cited_data_paper_title']].copy()\n",
    "\n",
    "    # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã¨æ–°ã—ã„äºˆæ¸¬çµæœã‚’ãƒãƒ¼ã‚¸ï¼ˆçµåˆï¼‰\n",
    "    if 'prediction_rule3_fulltext_few_shot' in output_df.columns:\n",
    "        output_df.drop(columns=['prediction_rule3_fulltext_few_shot'], inplace=True)\n",
    "        \n",
    "    output_df = pd.merge(output_df, df_new_predictions, on='citing_paper_doi', how='left')\n",
    "    \n",
    "    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¸Šæ›¸ãä¿å­˜\n",
    "    output_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\nå‡¦ç†å®Œäº†ã€‚LLM(Full-Text, Few-shot)ã®äºˆæ¸¬çµæœã‚’ '{OUTPUT_FILE}' ã«è¿½åŠ ãƒ»æ›´æ–°ã—ã¾ã—ãŸã€‚\")\n",
    "    print(\"\\n--- ä¿å­˜ã•ã‚ŒãŸçµæœã®å†…è¨³ï¼ˆFew-shotç‰ˆï¼‰ ---\")\n",
    "    print(output_df['prediction_rule3_fulltext_few_shot'].value_counts())\n",
    "else:\n",
    "    print(\"å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e149544f",
   "metadata": {},
   "source": [
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc51691d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ« 'few_shot_cot_fulltext.txt' ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\n",
      "\n",
      "==================================================\n",
      " singolo APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã™\n",
      "==================================================\n",
      "\n",
      "âœ… 1. LLMã¸ã®é€ä¿¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæœ€çµ‚ç‰ˆï¼‰\n",
      "----------------------------------------\n",
      "You are an expert AI assistant specializing in the analysis of academic papers.\n",
      "\n",
      "### Instructions\n",
      "Following the ã€Thought Processã€‘ and ã€Criteriaã€‘ below, determine whether the \"Citing Paper\" provided in the ã€Input Dataã€‘ actually uses the data from the specified \"Cited Data Paper\".\n",
      "Refer to the ã€Examplesã€‘ and ensure the final output is in JSON format, containing only the \"Used\" or \"Not Used\" decision.\n",
      "\n",
      "### ã€Thought Processã€‘\n",
      "First, read the full text of the Citing Paper, paying close attention to sections like \"Methods\", \"Experiment\", and \"Results\" to understand the research objective and methodology.\n",
      "\n",
      "Next, search for any mentions related to the \"Cited Data Paper Title\" within that context.\n",
      "\n",
      "Carefully determine if the mention is used directly to support the paper's conclusions (e.g., for performance evaluation, analysis, or comparison) or if it is merely a background reference, based on the ã€Criteriaã€‘.\n",
      "\n",
      "Based on this thought process, decide on a final determination of either \"Used\" or \"Not Used\".\n",
      "\n",
      "### ã€Criteriaã€‘\n",
      "\"Used\": The state where the dataset is directly used in processes such as analysis, training, evaluation, or performance comparison to derive the paper's claims or conclusions.\n",
      "\n",
      "\"Not Used\": The state where the dataset is only mentioned as part of the research background, related work, or for future work.\n",
      "\n",
      "### ã€Example 1ã€‘\n",
      "Input:\n",
      "\n",
      "Cited Data Paper Title: \"ImageNet Large Scale Visual Recognition Challenge\"\n",
      "\n",
      "Citing Paper Full Text: \"...[Methods] We evaluated our proposed algorithm on the publicly available 'ImageNet' dataset... [Results] Our method achieved 85% accuracy on ImageNet...\"\n",
      "\n",
      "Output:\n",
      "```json\n",
      "{\n",
      "\"decision\": \"Used\"\n",
      "}\n",
      "```\n",
      "\n",
      "### ã€Example 2ã€‘\n",
      "Input:\n",
      "\n",
      "Cited Data Paper Title: \"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\"\n",
      "\n",
      "Citing Paper Full Text: \"...[Related Work] Many state-of-the-art models, such as BERT, have demonstrated strong performance on comprehensive benchmarks like GLUE...\"\n",
      "\n",
      "Output:\n",
      "```json\n",
      "{\n",
      "\"decision\": \"Not Used\"\n",
      "}\n",
      "```\n",
      "\n",
      "### ã€Your Taskã€‘\n",
      "ã€Input Dataã€‘\n",
      "Cited Data Paper Title: ImageNet Large Scale Visual Recognition Challenge\n",
      "\n",
      "Citing Paper Title: Deep Residual Learning for Image Recognition\n",
      "\n",
      "Citing Paper Full Text: 4. Experiments. We conduct comprehensive experiments on the ImageNet 2012 classification dataset that consists of 1.28 million training images and 50k validation images. We evaluate both training error and validation error. Our ResNet-152 model has a top-1 validation error of 19.38%. This result won the 1st place in the ILSVRC 2015.\n",
      "\n",
      "ã€Outputã€‘\n",
      "```json\n",
      "\n",
      "```\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ” 2. APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ä¸­...\n",
      "\n",
      "âœ… 3. LLMã‹ã‚‰ã®RAWãƒ¬ã‚¹ãƒãƒ³ã‚¹ (JSONå…¨ä½“)\n",
      "----------------------------------------\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"```json\\n{\\n\\\"decision\\\": \\\"Used\\\"\\n}\\n```\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 603,\n",
      "    \"candidatesTokenCount\": 15,\n",
      "    \"totalTokenCount\": 1036,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 603\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 418\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"zKSHaPbvMdCJqtsPlbjBsAs\"\n",
      "}\n",
      "----------------------------------------\n",
      "\n",
      "âœ… 4. ãƒ‘ãƒ¼ã‚¹ï¼ˆè§£æï¼‰å¾Œã®ä¸»è¦ãªçµæœ\n",
      "----------------------------------------\n",
      "{\n",
      "  \"decision\": \"Used\"\n",
      "}\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ‰ 5. æœ€çµ‚åˆ¤å®š\n",
      "----------------------------------------\n",
      "Decision: 'Used'  =>  Prediction: 1\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- 1. è¨­å®šã¨APIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿ ---\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if not API_KEY:\n",
    "    print(\"âŒ .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰GEMINI_API_KEYãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "    exit()\n",
    "\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={API_KEY}\"\n",
    "PROMPT_FILE_PATH = '../prompts/few_shot_cot_fulltext.txt' # ã”è‡ªèº«ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹\n",
    "\n",
    "# --- 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®èª­ã¿è¾¼ã¿ ---\n",
    "try:\n",
    "    with open(PROMPT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        PROMPT_TEMPLATE = f.read()\n",
    "    print(f\"âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ« '{os.path.basename(PROMPT_FILE_PATH)}' ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. ç¢ºèªã—ãŸã„ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ ---\n",
    "# ã“ã“ã®å†…å®¹ã‚’æ›¸ãæ›ãˆã‚‹ã“ã¨ã§ã€è‰²ã€…ãªã‚±ãƒ¼ã‚¹ã‚’ãƒ†ã‚¹ãƒˆã§ãã¾ã™ã€‚\n",
    "sample_row = {\n",
    "    'cited_data_paper_title': \"ImageNet Large Scale Visual Recognition Challenge\",\n",
    "    'citing_paper_title': \"Deep Residual Learning for Image Recognition\",\n",
    "    'full_text': (\n",
    "        \"4. Experiments. We conduct comprehensive experiments on the ImageNet 2012 classification dataset \"\n",
    "        \"that consists of 1.28 million training images and 50k validation images. \"\n",
    "        \"We evaluate both training error and validation error. \"\n",
    "        \"Our ResNet-152 model has a top-1 validation error of 19.38%. This result won the 1st place in the ILSVRC 2015.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# --- 4. å˜ä¸€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" singolo APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã™\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 4.1. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç”Ÿæˆ\n",
    "# ãƒˆãƒ¼ã‚¯ãƒ³æ•°ä¸Šé™ã‚’è€ƒæ…®ã—ã€å®Ÿéš›ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«åˆã‚ã›ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ‡ã‚Šè©°ã‚ã¾ã™\n",
    "final_prompt = PROMPT_TEMPLATE.format(\n",
    "    cited_data_paper_title=sample_row['cited_data_paper_title'],\n",
    "    citing_paper_title=sample_row['citing_paper_title'],\n",
    "    full_text=sample_row['full_text'][:30000] \n",
    ")\n",
    "\n",
    "print(\"\\nâœ… 1. LLMã¸ã®é€ä¿¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæœ€çµ‚ç‰ˆï¼‰\")\n",
    "print(\"-\" * 40)\n",
    "print(final_prompt)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# 4.2. APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å®Ÿè¡Œ\n",
    "payload = {\"contents\": [{\"parts\": [{\"text\": final_prompt}]}]}\n",
    "print(\"\\nğŸ” 2. APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ä¸­...\")\n",
    "\n",
    "try:\n",
    "    response = requests.post(API_URL, json=payload, timeout=120)\n",
    "    response.raise_for_status()  # ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°ã“ã“ã§ä¾‹å¤–ã‚’ç™ºç”Ÿã•ã›ã‚‹\n",
    "    \n",
    "    print(\"\\nâœ… 3. LLMã‹ã‚‰ã®RAWãƒ¬ã‚¹ãƒãƒ³ã‚¹ (JSONå…¨ä½“)\")\n",
    "    print(\"-\" * 40)\n",
    "    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®JSONã‚’æ•´å½¢ã—ã¦è¡¨ç¤º\n",
    "    raw_response_json = response.json()\n",
    "    print(json.dumps(raw_response_json, indent=2, ensure_ascii=False))\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # 4.3. ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è§£æ\n",
    "    response_text = raw_response_json['candidates'][0]['content']['parts'][0]['text']\n",
    "    \n",
    "    # JSONéƒ¨åˆ†ã‚’æŠ½å‡º\n",
    "    match = re.search(r\"\\{.*\\}\", response_text, re.DOTALL)\n",
    "    json_text = match.group(0) if match else response_text\n",
    "    final_json = json.loads(json_text)\n",
    "    \n",
    "    print(\"\\nâœ… 4. ãƒ‘ãƒ¼ã‚¹ï¼ˆè§£æï¼‰å¾Œã®ä¸»è¦ãªçµæœ\")\n",
    "    print(\"-\" * 40)\n",
    "    print(json.dumps(final_json, indent=2, ensure_ascii=False))\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 4.4. æœ€çµ‚åˆ¤å®š\n",
    "    decision = final_json.get('decision', 'N/A')\n",
    "    prediction = 1 if decision == \"Used\" else (0 if decision == \"Not Used\" else -1)\n",
    "    \n",
    "    print(f\"\\nğŸ‰ 5. æœ€çµ‚åˆ¤å®š\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Decision: '{decision}'  =>  Prediction: {prediction}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"\\nâŒ APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e9a64",
   "metadata": {},
   "source": [
    "- few shot\n",
    "- cot\n",
    "- gemini 2.5 flash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7004a7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… APIã‚­ãƒ¼ã‚’.envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\n",
      "âœ… ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: gemini-2.5-flash\n",
      "âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ« 'few_shot_cot_fulltext.txt' ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\n",
      "åˆè¨ˆ 200 ä»¶ã®è«–æ–‡ã«å¯¾ã—ã¦LLM(gemini-2.5-flash)ã®åˆ¤å®šã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1dd9e3cfc64828b6d08be6d944c841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM(gemini-2.5-flash)ã§åˆ¤å®šä¸­:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DOI 10.1016/j.oneear.2025.101197 ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: HTTPSConnectionPool(host='generativelanguage.googleapis.com', port=443): Read timed out. (read timeout=180)\n",
      "\n",
      "å‡¦ç†å®Œäº†ã€‚LLM(gemini-2.5-flash)ã®äºˆæ¸¬çµæœã‚’ '../data/processed/prediction_llm.csv' ã«è¿½åŠ ãƒ»æ›´æ–°ã—ã¾ã—ãŸã€‚\n",
      "\n",
      "--- ä¿å­˜ã•ã‚ŒãŸçµæœã®å†…è¨³ï¼ˆgemini-2.5-flashç‰ˆï¼‰ ---\n",
      "prediction_rule3_gemini-2_5-flash\n",
      " 0    124\n",
      " 1     75\n",
      "-1      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- 1. è¨­å®šã¨APIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿ ---\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if API_KEY:\n",
    "    print(\"âœ… APIã‚­ãƒ¼ã‚’.envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\")\n",
    "else:\n",
    "    print(\"âŒ .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰GEMINI_API_KEYãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "    exit()\n",
    "\n",
    "# ã€å¤‰æ›´ç‚¹ã€‘å°†æ¥ã®ãƒ¢ãƒ‡ãƒ«åã«å¤‰æ›´\n",
    "MODEL_NAME = \"gemini-2.5-flash\" \n",
    "# æ³¨æ„: ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ç¾æ™‚ç‚¹ã§ã¯å­˜åœ¨ã—ãªã„ãŸã‚ã€å®Ÿè¡Œã™ã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã«ãªã‚Šã¾ã™ã€‚\n",
    "# å°†æ¥çš„ã«ãƒ¢ãƒ‡ãƒ«ãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸéš›ã«ã€ã“ã®åå‰ãŒæ­£ã—ã„ã‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={API_KEY}\"\n",
    "print(f\"âœ… ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {MODEL_NAME}\")\n",
    "\n",
    "# å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«\n",
    "INPUT_CSV = '../data/processed/samples_with_text.csv'\n",
    "PROMPT_FILE_PATH = '../prompts/few_shot_cot_fulltext.txt'\n",
    "OUTPUT_FILE = '../data/processed/prediction_llm.csv'\n",
    "\n",
    "# --- 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ ---\n",
    "try:\n",
    "    with open(PROMPT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        PROMPT_TEMPLATE = f.read()\n",
    "    print(f\"âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ« '{os.path.basename(PROMPT_FILE_PATH)}' ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\")\n",
    "    \n",
    "    df_to_process = pd.read_csv(INPUT_CSV)\n",
    "    df_to_process.dropna(subset=['full_text'], inplace=True)\n",
    "    df_to_process.drop_duplicates(subset=['citing_paper_doi'], inplace=True, keep='first')\n",
    "    df_to_process.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f\"åˆè¨ˆ {len(df_to_process)} ä»¶ã®è«–æ–‡ã«å¯¾ã—ã¦LLM({MODEL_NAME})ã®åˆ¤å®šã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ {e}\")\n",
    "    df_to_process = pd.DataFrame()\n",
    "\n",
    "# --- 3. LLMã«ã‚ˆã‚‹åˆ¤å®šã®æœ¬ç•ªå®Ÿè¡Œ ---\n",
    "predictions = []\n",
    "if PROMPT_TEMPLATE and not df_to_process.empty:\n",
    "    for index, row in tqdm(df_to_process.iterrows(), total=len(df_to_process), desc=f\"LLM({MODEL_NAME})ã§åˆ¤å®šä¸­\"):\n",
    "        \n",
    "        full_text = row.get('full_text', 'ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚')\n",
    "        \n",
    "        prompt = PROMPT_TEMPLATE.format(\n",
    "            cited_data_paper_title=row['cited_data_paper_title'],\n",
    "            citing_paper_title=row['citing_paper_title'],\n",
    "            full_text=full_text[:30000] # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ä¸Šé™ã‚’è€ƒæ…®\n",
    "        )\n",
    "        \n",
    "        payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(API_URL, json=payload, timeout=180)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            response_json = response.json()\n",
    "            response_text = response_json['candidates'][0]['content']['parts'][0]['text']\n",
    "            \n",
    "            match = re.search(r\"\\{.*\\}\", response_text, re.DOTALL)\n",
    "            json_text = match.group(0) if match else response_text\n",
    "            final_json = json.loads(json_text)\n",
    "            \n",
    "            decision = final_json.get('decision')\n",
    "            prediction = 1 if decision == \"Used\" else 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nDOI {row['citing_paper_doi']} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            prediction = -1\n",
    "            \n",
    "        predictions.append(prediction)\n",
    "        time.sleep(1.5)\n",
    "\n",
    "# --- 4. çµæœã®ä¿å­˜ ---\n",
    "if len(predictions) == len(df_to_process):\n",
    "    # ä»Šå›ã®äºˆæ¸¬çµæœã‚’DataFrameã«ã¾ã¨ã‚ã‚‹\n",
    "    new_column_name = f\"prediction_rule3_{MODEL_NAME.replace('.', '_')}\" # ãƒ¢ãƒ‡ãƒ«åã‹ã‚‰ã‚«ãƒ©ãƒ åã‚’ç”Ÿæˆ\n",
    "    df_new_predictions = pd.DataFrame({\n",
    "        'citing_paper_doi': df_to_process['citing_paper_doi'],\n",
    "        new_column_name: predictions\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        output_df = pd.read_csv(OUTPUT_FILE)\n",
    "    except FileNotFoundError:\n",
    "        output_df = df_to_process[['citing_paper_eid', 'citing_paper_doi', 'citing_paper_title', 'cited_data_paper_title']].copy()\n",
    "\n",
    "    if new_column_name in output_df.columns:\n",
    "        output_df.drop(columns=[new_column_name], inplace=True)\n",
    "        \n",
    "    output_df = pd.merge(output_df, df_new_predictions, on='citing_paper_doi', how='left')\n",
    "    \n",
    "    output_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\nå‡¦ç†å®Œäº†ã€‚LLM({MODEL_NAME})ã®äºˆæ¸¬çµæœã‚’ '{OUTPUT_FILE}' ã«è¿½åŠ ãƒ»æ›´æ–°ã—ã¾ã—ãŸã€‚\")\n",
    "    print(f\"\\n--- ä¿å­˜ã•ã‚ŒãŸçµæœã®å†…è¨³ï¼ˆ{MODEL_NAME}ç‰ˆï¼‰ ---\")\n",
    "    print(output_df[new_column_name].value_counts())\n",
    "else:\n",
    "    print(\"å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd3a50c",
   "metadata": {},
   "source": [
    "ãƒªãƒˆãƒ©ã‚¤ç”¨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fab28aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… APIã‚­ãƒ¼ã‚’.envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\n",
      "âœ… ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: gemini-2.5-flash\n",
      "âœ… ã‚¨ãƒ©ãƒ¼(-1)ãŒè¨˜éŒ²ã•ã‚ŒãŸ 1 ä»¶ã‚’ç‰¹å®šã—ã¾ã—ãŸã€‚å†è©¦è¡Œã‚’é–‹å§‹ã—ã¾ã™ã€‚\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed29300b9ed4c749cc1729eeeee3a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemini-2.5-flashã§å†è©¦è¡Œä¸­:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å†è©¦è¡Œå®Œäº†ã€‚'../data/processed/prediction_llm.csv' ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚\n",
      "\n",
      "--- æœ€æ–°ã®äºˆæ¸¬çµæœã®å†…è¨³ï¼ˆgemini-2.5-flashç‰ˆï¼‰ ---\n",
      "prediction_rule3_gemini-2_5-flash\n",
      "0    125\n",
      "1     75\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- 1. è¨­å®šã¨APIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿ ---\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if API_KEY:\n",
    "    print(\"âœ… APIã‚­ãƒ¼ã‚’.envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\")\n",
    "else:\n",
    "    print(\"âŒ .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰GEMINI_API_KEYãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "    exit()\n",
    "\n",
    "# --- ã€é‡è¦ã€‘å†è©¦è¡Œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­å®š ---\n",
    "# ãƒ¢ãƒ‡ãƒ«åã‚’ 'gemini-1.5-flash' ã¾ãŸã¯ 'gemini-1.5-pro' ãªã©ã€å†è©¦è¡Œã—ãŸã„ãƒ¢ãƒ‡ãƒ«åã«å¤‰æ›´ã—ã¦ãã ã•ã„\n",
    "MODEL_NAME = \"gemini-2.5-flash\" \n",
    "# å†è©¦è¡Œã«ä½¿ç”¨ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„\n",
    "PROMPT_FILE_PATH = '../prompts/few_shot_cot_fulltext.txt' \n",
    "# å†è©¦è¡Œã™ã‚‹å¯¾è±¡ã®ã‚«ãƒ©ãƒ åã‚’æŒ‡å®šã—ã¦ãã ã•ã„\n",
    "COLUMN_TO_RETRY = 'prediction_rule3_gemini-2_5-flash'\n",
    "\n",
    "API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={API_KEY}\"\n",
    "print(f\"âœ… ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {MODEL_NAME}\")\n",
    "\n",
    "# å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«\n",
    "SAMPLES_CSV = '../data/processed/samples_with_text.csv'\n",
    "PREDICTIONS_CSV = '../data/processed/prediction_llm.csv'\n",
    "\n",
    "# --- 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ ---\n",
    "try:\n",
    "    with open(PROMPT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        PROMPT_TEMPLATE = f.read()\n",
    "    \n",
    "    df_samples = pd.read_csv(SAMPLES_CSV)\n",
    "    df_predictions = pd.read_csv(PREDICTIONS_CSV)\n",
    "\n",
    "    # --- å†è©¦è¡Œå¯¾è±¡ã®è«–æ–‡ã‚’ç‰¹å®š ---\n",
    "    if COLUMN_TO_RETRY not in df_predictions.columns:\n",
    "        print(f\"âŒ ã‚¨ãƒ©ãƒ¼: '{COLUMN_TO_RETRY}' ã‚«ãƒ©ãƒ ãŒäºˆæ¸¬çµæœãƒ•ã‚¡ã‚¤ãƒ«ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
    "        df_to_retry = pd.DataFrame()\n",
    "    else:\n",
    "        # äºˆæ¸¬çµæœãŒ-1ã®è¡Œã®DOIã‚’å–å¾—\n",
    "        retry_dois = df_predictions[df_predictions[COLUMN_TO_RETRY] == -1]['citing_paper_doi']\n",
    "        # samples_with_text.csvã‹ã‚‰ã€ãã®DOIã«ä¸€è‡´ã™ã‚‹è¡Œã‚’æŠ½å‡º\n",
    "        df_to_retry = df_samples[df_samples['citing_paper_doi'].isin(retry_dois)].copy()\n",
    "    \n",
    "    if not df_to_retry.empty:\n",
    "        print(f\"âœ… ã‚¨ãƒ©ãƒ¼(-1)ãŒè¨˜éŒ²ã•ã‚ŒãŸ {len(df_to_retry)} ä»¶ã‚’ç‰¹å®šã—ã¾ã—ãŸã€‚å†è©¦è¡Œã‚’é–‹å§‹ã—ã¾ã™ã€‚\")\n",
    "    else:\n",
    "        print(\"âœ… ã‚¨ãƒ©ãƒ¼(-1)ãŒè¨˜éŒ²ã•ã‚ŒãŸè«–æ–‡ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã¯ä¸è¦ã§ã™ã€‚\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ {e}\")\n",
    "    df_to_retry = pd.DataFrame()\n",
    "\n",
    "# --- 3. å¤±æ•—ã—ãŸã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦ã®ã¿LLMåˆ¤å®šã‚’å†å®Ÿè¡Œ ---\n",
    "retry_results = []\n",
    "if not df_to_retry.empty and PROMPT_TEMPLATE:\n",
    "    for index, row in tqdm(df_to_retry.iterrows(), total=len(df_to_retry), desc=f\"{MODEL_NAME}ã§å†è©¦è¡Œä¸­\"):\n",
    "        \n",
    "        full_text = row.get('full_text', 'ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚')\n",
    "        \n",
    "        prompt = PROMPT_TEMPLATE.format(\n",
    "            cited_data_paper_title=row['cited_data_paper_title'],\n",
    "            citing_paper_title=row['citing_paper_title'],\n",
    "            full_text=full_text[:30000]\n",
    "        )\n",
    "        \n",
    "        payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(API_URL, json=payload, timeout=180)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            response_json = response.json()\n",
    "            response_text = response_json['candidates'][0]['content']['parts'][0]['text']\n",
    "            \n",
    "            match = re.search(r\"\\{.*\\}\", response_text, re.DOTALL)\n",
    "            json_text = match.group(0) if match else response_text\n",
    "            final_json = json.loads(json_text)\n",
    "            \n",
    "            prediction = 1 if final_json.get('decision') == \"Used\" else 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nDOI {row['citing_paper_doi']} ã®å†è©¦è¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            prediction = -1 # å†è©¦è¡Œã—ã¦ã‚‚å¤±æ•—ã—ãŸå ´åˆã¯-1ã®ã¾ã¾\n",
    "            \n",
    "        retry_results.append({\n",
    "            'citing_paper_doi': row['citing_paper_doi'],\n",
    "            COLUMN_TO_RETRY: prediction\n",
    "        })\n",
    "        time.sleep(1.5)\n",
    "\n",
    "# --- 4. çµæœã®æ›´æ–°ã¨ä¿å­˜ ---\n",
    "if retry_results:\n",
    "    df_retry_results = pd.DataFrame(retry_results)\n",
    "    \n",
    "    # å…ƒã®äºˆæ¸¬çµæœã®DOIã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨­å®š\n",
    "    df_predictions.set_index('citing_paper_doi', inplace=True)\n",
    "    # å†è©¦è¡Œçµæœã®DOIã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨­å®š\n",
    "    df_retry_results.set_index('citing_paper_doi', inplace=True)\n",
    "    \n",
    "    # å†è©¦è¡Œã®çµæœã§å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°\n",
    "    df_predictions.update(df_retry_results)\n",
    "    \n",
    "    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆ\n",
    "    df_predictions.reset_index(inplace=True)\n",
    "    \n",
    "    df_predictions.to_csv(PREDICTIONS_CSV, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\nå†è©¦è¡Œå®Œäº†ã€‚'{PREDICTIONS_CSV}' ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚\")\n",
    "    print(f\"\\n--- æœ€æ–°ã®äºˆæ¸¬çµæœã®å†…è¨³ï¼ˆ{MODEL_NAME}ç‰ˆï¼‰ ---\")\n",
    "    print(df_predictions[COLUMN_TO_RETRY].value_counts())\n",
    "else:\n",
    "    print(\"å†è©¦è¡Œã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c99985",
   "metadata": {},
   "source": [
    "- zero shot\n",
    "- gemini 2.5 flash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0788ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… APIã‚­ãƒ¼ã‚’.envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\n",
      "âœ… ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: gemini-2.5-flash\n",
      "âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ« 'zero_shot_fulltext.txt' ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\n",
      "åˆè¨ˆ 200 ä»¶ã®è«–æ–‡ã«å¯¾ã—ã¦LLM(gemini-2.5-flash, Zero-shot)ã®åˆ¤å®šã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10395474fe2a4d2f80ed94c6268cbe2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM(gemini-2.5-flash, Zero-shot)ã§åˆ¤å®šä¸­:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å‡¦ç†å®Œäº†ã€‚LLM(gemini-2.5-flash, Zero-shot)ã®äºˆæ¸¬çµæœã‚’ '../data/processed/prediction_llm.csv' ã«è¿½åŠ ãƒ»æ›´æ–°ã—ã¾ã—ãŸã€‚\n",
      "\n",
      "--- ä¿å­˜ã•ã‚ŒãŸçµæœã®å†…è¨³ï¼ˆgemini-2.5-flash, Zero-shotç‰ˆï¼‰ ---\n",
      "prediction_rule3_gemini-2_5-flash_zeroshot\n",
      "0    107\n",
      "1     93\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- 1. è¨­å®šã¨APIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿ ---\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if API_KEY:\n",
    "    print(\"âœ… APIã‚­ãƒ¼ã‚’.envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\")\n",
    "else:\n",
    "    print(\"âŒ .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰GEMINI_API_KEYãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "    exit()\n",
    "\n",
    "# ã€é‡è¦ã€‘å®Ÿè¡Œã™ã‚‹ãƒ¢ãƒ‡ãƒ«åã¨ã€ãã‚Œã«å¯¾å¿œã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®š\n",
    "MODEL_NAME = \"gemini-2.5-flash\" \n",
    "PROMPT_FILE_PATH = '../prompts/zero_shot_fulltext.txt' \n",
    "\n",
    "# APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ\n",
    "API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={API_KEY}\"\n",
    "print(f\"âœ… ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {MODEL_NAME}\")\n",
    "\n",
    "# å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã¨å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«\n",
    "INPUT_CSV = '../data/processed/samples_with_text.csv'\n",
    "OUTPUT_FILE = '../data/processed/prediction_llm.csv'\n",
    "\n",
    "# --- 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ ---\n",
    "try:\n",
    "    with open(PROMPT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        PROMPT_TEMPLATE = f.read()\n",
    "    print(f\"âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ« '{os.path.basename(PROMPT_FILE_PATH)}' ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\")\n",
    "    \n",
    "    df_to_process = pd.read_csv(INPUT_CSV)\n",
    "    df_to_process.dropna(subset=['full_text'], inplace=True)\n",
    "    df_to_process.drop_duplicates(subset=['citing_paper_doi'], inplace=True, keep='first')\n",
    "    df_to_process.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f\"åˆè¨ˆ {len(df_to_process)} ä»¶ã®è«–æ–‡ã«å¯¾ã—ã¦LLM({MODEL_NAME}, Zero-shot)ã®åˆ¤å®šã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ {e}\")\n",
    "    df_to_process = pd.DataFrame()\n",
    "\n",
    "# --- 3. LLMã«ã‚ˆã‚‹åˆ¤å®šã®æœ¬ç•ªå®Ÿè¡Œ ---\n",
    "predictions = []\n",
    "if PROMPT_TEMPLATE and not df_to_process.empty:\n",
    "    for index, row in tqdm(df_to_process.iterrows(), total=len(df_to_process), desc=f\"LLM({MODEL_NAME}, Zero-shot)ã§åˆ¤å®šä¸­\"):\n",
    "        \n",
    "        full_text = row.get('full_text', 'ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚')\n",
    "        \n",
    "        prompt = PROMPT_TEMPLATE.format(\n",
    "            cited_data_paper_title=row['cited_data_paper_title'],\n",
    "            citing_paper_title=row['citing_paper_title'],\n",
    "            full_text=full_text[:30000] # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ä¸Šé™ã‚’è€ƒæ…®\n",
    "        )\n",
    "        \n",
    "        payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(API_URL, json=payload, timeout=180)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            response_json = response.json()\n",
    "            response_text = response_json['candidates'][0]['content']['parts'][0]['text']\n",
    "            \n",
    "            match = re.search(r\"\\{.*\\}\", response_text, re.DOTALL)\n",
    "            json_text = match.group(0) if match else response_text\n",
    "            final_json = json.loads(json_text)\n",
    "            \n",
    "            decision = final_json.get('decision')\n",
    "            prediction = 1 if decision == \"Used\" else 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nDOI {row['citing_paper_doi']} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            prediction = -1\n",
    "            \n",
    "        predictions.append(prediction)\n",
    "        time.sleep(1.5) # APIã¸ã®è² è·ã‚’è€ƒæ…®\n",
    "\n",
    "# --- 4. çµæœã®ä¿å­˜ ---\n",
    "if len(predictions) == len(df_to_process):\n",
    "    # ä»Šå›ã®äºˆæ¸¬çµæœã‚’DataFrameã«ã¾ã¨ã‚ã‚‹\n",
    "    new_column_name = f\"prediction_rule3_{MODEL_NAME.replace('.', '_')}_zeroshot\"\n",
    "    df_new_predictions = pd.DataFrame({\n",
    "        'citing_paper_doi': df_to_process['citing_paper_doi'],\n",
    "        new_column_name: predictions\n",
    "    })\n",
    "    \n",
    "    # æ—¢å­˜ã®LLMäºˆæ¸¬çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "    try:\n",
    "        output_df = pd.read_csv(OUTPUT_FILE)\n",
    "    except FileNotFoundError:\n",
    "        output_df = df_to_process[['citing_paper_eid', 'citing_paper_doi', 'citing_paper_title', 'cited_data_paper_title']].copy()\n",
    "\n",
    "    # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã¨æ–°ã—ã„äºˆæ¸¬çµæœã‚’ãƒãƒ¼ã‚¸ï¼ˆçµåˆï¼‰\n",
    "    if new_column_name in output_df.columns:\n",
    "        output_df.drop(columns=[new_column_name], inplace=True)\n",
    "        \n",
    "    output_df = pd.merge(output_df, df_new_predictions, on='citing_paper_doi', how='left')\n",
    "    \n",
    "    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¸Šæ›¸ãä¿å­˜\n",
    "    output_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\nå‡¦ç†å®Œäº†ã€‚LLM({MODEL_NAME}, Zero-shot)ã®äºˆæ¸¬çµæœã‚’ '{OUTPUT_FILE}' ã«è¿½åŠ ãƒ»æ›´æ–°ã—ã¾ã—ãŸã€‚\")\n",
    "    print(f\"\\n--- ä¿å­˜ã•ã‚ŒãŸçµæœã®å†…è¨³ï¼ˆ{MODEL_NAME}, Zero-shotç‰ˆï¼‰ ---\")\n",
    "    print(output_df[new_column_name].value_counts())\n",
    "else:\n",
    "    print(\"å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸã€‚\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataUsageValidator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
