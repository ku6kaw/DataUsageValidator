{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c091cfab",
   "metadata": {},
   "source": [
    "- zero shot\n",
    "- gemini 1.5 flash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "616f47d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ APIキーを.envファイルから正常に読み込みました。\n",
      "✅ プロンプトファイル 'zero_shot_fulltext.txt' を正常に読み込みました。\n",
      "合計 200 件の論文に対してLLM(Full-Text)の判定を実行します。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c6e7527516458289dfb0a559f37018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM(Full-Text)で判定中:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "処理完了。LLM(Full-Text)の予測結果を '../data/processed/prediction_llm.csv' に追加・更新しました。\n",
      "\n",
      "--- 保存された結果の内訳（フルテキスト版） ---\n",
      "prediction_rule3_fulltext\n",
      "0    141\n",
      "1     53\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- 保存されたデータの先頭5件 ---\n",
      "     citing_paper_eid                 citing_paper_doi  \\\n",
      "0  2-s2.0-85211640167  10.1016/j.marpolbul.2024.117442   \n",
      "1  2-s2.0-85210281314   10.1016/j.jaridenv.2024.105282   \n",
      "2  2-s2.0-85171680307   10.1016/j.revpalbo.2023.104989   \n",
      "3  2-s2.0-85142708771     10.1016/j.foreco.2022.120653   \n",
      "4  2-s2.0-85194770743    10.1016/j.jnucmat.2024.155194   \n",
      "\n",
      "                                  citing_paper_title  \\\n",
      "0  Multi-indicator assessment of heavy metal poll...   \n",
      "1  Potential effects of climate change on cacti d...   \n",
      "2  Approaches to pollen taxonomic harmonisation i...   \n",
      "3  Allometric equations to estimate the dry mass ...   \n",
      "4  Microstructural evolution in doped high entrop...   \n",
      "\n",
      "                              cited_data_paper_title  \\\n",
      "0  Pollution load index for heavy metals in Mian-...   \n",
      "1  The World Checklist of Vascular Plants, a cont...   \n",
      "2  European pollen-based REVEALS land-cover recon...   \n",
      "3  A global map of root biomass across the world'...   \n",
      "4  Database on the mechanical properties of high ...   \n",
      "\n",
      "   prediction_rule3_abstract  prediction_rule3_fulltext  \n",
      "0                          0                          0  \n",
      "1                          0                          1  \n",
      "2                          0                          0  \n",
      "3                          0                          0  \n",
      "4                          0                          0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- 1. 設定とAPIキーの読み込み ---\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if API_KEY:\n",
    "    print(\"✅ APIキーを.envファイルから正常に読み込みました。\")\n",
    "else:\n",
    "    print(\"❌ .envファイルからGEMINI_API_KEYが見つかりませんでした。\")\n",
    "    exit()\n",
    "\n",
    "MODEL_NAME = \"gemini-1.5-flash\"\n",
    "API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={API_KEY}\"\n",
    "\n",
    "# 入力ファイルとプロンプトファイル\n",
    "INPUT_CSV = '../data/processed/samples_with_text.csv'\n",
    "PROMPT_FILE_PATH = '../prompts/zero_shot_fulltext.txt'\n",
    "# 既存のLLM予測結果ファイルに結果を追記する\n",
    "OUTPUT_FILE = '../data/processed/prediction_llm.csv'\n",
    "\n",
    "# --- 2. プロンプトとデータの準備 ---\n",
    "try:\n",
    "    with open(PROMPT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        PROMPT_TEMPLATE = f.read()\n",
    "    print(f\"✅ プロンプトファイル '{os.path.basename(PROMPT_FILE_PATH)}' を正常に読み込みました。\")\n",
    "    \n",
    "    df_to_process = pd.read_csv(INPUT_CSV)\n",
    "    df_to_process.dropna(subset=['full_text'], inplace=True)\n",
    "    df_to_process.drop_duplicates(subset=['citing_paper_doi'], inplace=True, keep='first')\n",
    "    df_to_process.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f\"合計 {len(df_to_process)} 件の論文に対してLLM(Full-Text)の判定を実行します。\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ エラー: データの読み込みに失敗しました。 {e}\")\n",
    "    df_to_process = pd.DataFrame()\n",
    "\n",
    "# --- 3. LLMによる判定の本番実行 ---\n",
    "predictions = []\n",
    "if PROMPT_TEMPLATE and not df_to_process.empty:\n",
    "    for index, row in tqdm(df_to_process.iterrows(), total=len(df_to_process), desc=\"LLM(Full-Text)で判定中\"):\n",
    "        \n",
    "        full_text = row.get('full_text', 'フルテキストが見つかりませんでした。')\n",
    "        \n",
    "        prompt = PROMPT_TEMPLATE.format(\n",
    "            cited_data_paper_title=row['cited_data_paper_title'],\n",
    "            citing_paper_title=row['citing_paper_title'],\n",
    "            full_text=full_text[:30000] # トークン数上限を考慮\n",
    "        )\n",
    "        \n",
    "        payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(API_URL, json=payload, timeout=120)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            response_json = response.json()\n",
    "            response_text = response_json['candidates'][0]['content']['parts'][0]['text']\n",
    "            \n",
    "            match = re.search(r\"\\{.*\\}\", response_text, re.DOTALL)\n",
    "            json_text = match.group(0) if match else response_text\n",
    "            final_json = json.loads(json_text)\n",
    "            \n",
    "            decision = final_json.get('decision')\n",
    "            prediction = 1 if decision == \"Used\" else 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nDOI {row['citing_paper_doi']} の処理でエラー: {e}\")\n",
    "            prediction = -1\n",
    "            \n",
    "        predictions.append(prediction)\n",
    "        time.sleep(1)\n",
    "\n",
    "# --- 4. 結果の保存 ---\n",
    "if len(predictions) == len(df_to_process):\n",
    "    # 今回の予測結果をDataFrameにまとめる\n",
    "    df_fulltext_predictions = pd.DataFrame({\n",
    "        'citing_paper_doi': df_to_process['citing_paper_doi'],\n",
    "        'prediction_rule3_fulltext': predictions\n",
    "    })\n",
    "    \n",
    "    # 既存のLLM予測結果ファイル(アブストラクト版)を読み込む\n",
    "    try:\n",
    "        output_df = pd.read_csv(OUTPUT_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"警告: '{OUTPUT_FILE}' が見つかりません。キー情報のみで新しいファイルを作成します。\")\n",
    "        output_df = df_to_process[['citing_paper_eid', 'citing_paper_doi', 'citing_paper_title', 'cited_data_paper_title']].copy()\n",
    "\n",
    "    # 既存のデータと新しい予測結果をマージ（結合）\n",
    "    # 既に同名カラムがあれば上書きし、なければ追加する\n",
    "    if 'prediction_rule3_fulltext' in output_df.columns:\n",
    "        output_df.drop(columns=['prediction_rule3_fulltext'], inplace=True)\n",
    "        \n",
    "    output_df = pd.merge(output_df, df_fulltext_predictions, on='citing_paper_doi', how='left')\n",
    "    \n",
    "    # ファイルに上書き保存\n",
    "    output_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\n処理完了。LLM(Full-Text)の予測結果を '{OUTPUT_FILE}' に追加・更新しました。\")\n",
    "    print(\"\\n--- 保存された結果の内訳（フルテキスト版） ---\")\n",
    "    print(output_df['prediction_rule3_fulltext'].value_counts())\n",
    "    print(\"\\n--- 保存されたデータの先頭5件 ---\")\n",
    "    print(output_df.head())\n",
    "else:\n",
    "    print(\"処理が正常に完了しませんでした。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e44bb4f",
   "metadata": {},
   "source": [
    "- few shot\n",
    "- cot\n",
    "- gemini 1.5 flash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8a5afa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ APIキーを.envファイルから正常に読み込みました。\n",
      "✅ プロンプトファイル 'few_shot_cot_fulltext.txt' を正常に読み込みました。\n",
      "合計 200 件の論文に対してLLM(Full-Text, Few-shot)の判定を実行します。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3606e1b6e5d84f76b997beb5807a73e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM(Full-Text, Few-shot)で判定中:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "処理完了。LLM(Full-Text, Few-shot)の予測結果を '../data/processed/prediction_llm.csv' に追加・更新しました。\n",
      "\n",
      "--- 保存された結果の内訳（Few-shot版） ---\n",
      "prediction_rule3_fulltext_few_shot\n",
      "0    123\n",
      "1     77\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- 1. 設定とAPIキーの読み込み ---\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if API_KEY:\n",
    "    print(\"✅ APIキーを.envファイルから正常に読み込みました。\")\n",
    "else:\n",
    "    print(\"❌ .envファイルからGEMINI_API_KEYが見つかりませんでした。\")\n",
    "    exit()\n",
    "\n",
    "MODEL_NAME = \"gemini-1.5-flash\"\n",
    "API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={API_KEY}\"\n",
    "\n",
    "# 入力ファイルとプロンプトファイル\n",
    "INPUT_CSV = '../data/processed/samples_with_text.csv'\n",
    "# 【変更点】Few-shot & CoTプロンプトを読み込む\n",
    "PROMPT_FILE_PATH = '../prompts/few_shot_cot_fulltext.txt' \n",
    "# 既存のLLM予測結果ファイルに結果を追記する\n",
    "OUTPUT_FILE = '../data/processed/prediction_llm.csv'\n",
    "\n",
    "# --- 2. プロンプトとデータの準備 ---\n",
    "try:\n",
    "    with open(PROMPT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        PROMPT_TEMPLATE = f.read()\n",
    "    print(f\"✅ プロンプトファイル '{os.path.basename(PROMPT_FILE_PATH)}' を正常に読み込みました。\")\n",
    "    \n",
    "    df_to_process = pd.read_csv(INPUT_CSV)\n",
    "    df_to_process.dropna(subset=['full_text'], inplace=True)\n",
    "    df_to_process.drop_duplicates(subset=['citing_paper_doi'], inplace=True, keep='first')\n",
    "    df_to_process.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f\"合計 {len(df_to_process)} 件の論文に対してLLM(Full-Text, Few-shot)の判定を実行します。\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ エラー: データの読み込みに失敗しました。 {e}\")\n",
    "    df_to_process = pd.DataFrame()\n",
    "\n",
    "# --- 3. LLMによる判定の本番実行 ---\n",
    "predictions = []\n",
    "if PROMPT_TEMPLATE and not df_to_process.empty:\n",
    "    for index, row in tqdm(df_to_process.iterrows(), total=len(df_to_process), desc=\"LLM(Full-Text, Few-shot)で判定中\"):\n",
    "        \n",
    "        full_text = row.get('full_text', 'フルテキストが見つかりませんでした。')\n",
    "        \n",
    "        prompt = PROMPT_TEMPLATE.format(\n",
    "            cited_data_paper_title=row['cited_data_paper_title'],\n",
    "            citing_paper_title=row['citing_paper_title'],\n",
    "            full_text=full_text[:30000] # トークン数上限を考慮\n",
    "        )\n",
    "        \n",
    "        payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(API_URL, json=payload, timeout=120)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            response_json = response.json()\n",
    "            response_text = response_json['candidates'][0]['content']['parts'][0]['text']\n",
    "            \n",
    "            match = re.search(r\"\\{.*\\}\", response_text, re.DOTALL)\n",
    "            json_text = match.group(0) if match else response_text\n",
    "            final_json = json.loads(json_text)\n",
    "            \n",
    "            decision = final_json.get('decision')\n",
    "            prediction = 1 if decision == \"Used\" else 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nDOI {row['citing_paper_doi']} の処理でエラー: {e}\")\n",
    "            prediction = -1\n",
    "            \n",
    "        predictions.append(prediction)\n",
    "        time.sleep(1)\n",
    "\n",
    "# --- 4. 結果の保存 ---\n",
    "if len(predictions) == len(df_to_process):\n",
    "    # 今回の予測結果をDataFrameにまとめる\n",
    "    df_new_predictions = pd.DataFrame({\n",
    "        'citing_paper_doi': df_to_process['citing_paper_doi'],\n",
    "        # 【変更点】新しいカラム名で結果を保存\n",
    "        'prediction_rule3_fulltext_few_shot': predictions \n",
    "    })\n",
    "    \n",
    "    # 既存のLLM予測結果ファイルを読み込む\n",
    "    try:\n",
    "        output_df = pd.read_csv(OUTPUT_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"警告: '{OUTPUT_FILE}' が見つかりません。キー情報のみで新しいファイルを作成します。\")\n",
    "        output_df = df_to_process[['citing_paper_eid', 'citing_paper_doi', 'citing_paper_title', 'cited_data_paper_title']].copy()\n",
    "\n",
    "    # 既存のデータと新しい予測結果をマージ（結合）\n",
    "    if 'prediction_rule3_fulltext_few_shot' in output_df.columns:\n",
    "        output_df.drop(columns=['prediction_rule3_fulltext_few_shot'], inplace=True)\n",
    "        \n",
    "    output_df = pd.merge(output_df, df_new_predictions, on='citing_paper_doi', how='left')\n",
    "    \n",
    "    # ファイルに上書き保存\n",
    "    output_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\n処理完了。LLM(Full-Text, Few-shot)の予測結果を '{OUTPUT_FILE}' に追加・更新しました。\")\n",
    "    print(\"\\n--- 保存された結果の内訳（Few-shot版） ---\")\n",
    "    print(output_df['prediction_rule3_fulltext_few_shot'].value_counts())\n",
    "else:\n",
    "    print(\"処理が正常に完了しませんでした。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e149544f",
   "metadata": {},
   "source": [
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc51691d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ プロンプトファイル 'few_shot_cot_fulltext.txt' を読み込みました。\n",
      "\n",
      "==================================================\n",
      " singolo APIリクエストのテストを開始します\n",
      "==================================================\n",
      "\n",
      "✅ 1. LLMへの送信プロンプト（最終版）\n",
      "----------------------------------------\n",
      "You are an expert AI assistant specializing in the analysis of academic papers.\n",
      "\n",
      "### Instructions\n",
      "Following the 【Thought Process】 and 【Criteria】 below, determine whether the \"Citing Paper\" provided in the 【Input Data】 actually uses the data from the specified \"Cited Data Paper\".\n",
      "Refer to the 【Examples】 and ensure the final output is in JSON format, containing only the \"Used\" or \"Not Used\" decision.\n",
      "\n",
      "### 【Thought Process】\n",
      "First, read the full text of the Citing Paper, paying close attention to sections like \"Methods\", \"Experiment\", and \"Results\" to understand the research objective and methodology.\n",
      "\n",
      "Next, search for any mentions related to the \"Cited Data Paper Title\" within that context.\n",
      "\n",
      "Carefully determine if the mention is used directly to support the paper's conclusions (e.g., for performance evaluation, analysis, or comparison) or if it is merely a background reference, based on the 【Criteria】.\n",
      "\n",
      "Based on this thought process, decide on a final determination of either \"Used\" or \"Not Used\".\n",
      "\n",
      "### 【Criteria】\n",
      "\"Used\": The state where the dataset is directly used in processes such as analysis, training, evaluation, or performance comparison to derive the paper's claims or conclusions.\n",
      "\n",
      "\"Not Used\": The state where the dataset is only mentioned as part of the research background, related work, or for future work.\n",
      "\n",
      "### 【Example 1】\n",
      "Input:\n",
      "\n",
      "Cited Data Paper Title: \"ImageNet Large Scale Visual Recognition Challenge\"\n",
      "\n",
      "Citing Paper Full Text: \"...[Methods] We evaluated our proposed algorithm on the publicly available 'ImageNet' dataset... [Results] Our method achieved 85% accuracy on ImageNet...\"\n",
      "\n",
      "Output:\n",
      "```json\n",
      "{\n",
      "\"decision\": \"Used\"\n",
      "}\n",
      "```\n",
      "\n",
      "### 【Example 2】\n",
      "Input:\n",
      "\n",
      "Cited Data Paper Title: \"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\"\n",
      "\n",
      "Citing Paper Full Text: \"...[Related Work] Many state-of-the-art models, such as BERT, have demonstrated strong performance on comprehensive benchmarks like GLUE...\"\n",
      "\n",
      "Output:\n",
      "```json\n",
      "{\n",
      "\"decision\": \"Not Used\"\n",
      "}\n",
      "```\n",
      "\n",
      "### 【Your Task】\n",
      "【Input Data】\n",
      "Cited Data Paper Title: ImageNet Large Scale Visual Recognition Challenge\n",
      "\n",
      "Citing Paper Title: Deep Residual Learning for Image Recognition\n",
      "\n",
      "Citing Paper Full Text: 4. Experiments. We conduct comprehensive experiments on the ImageNet 2012 classification dataset that consists of 1.28 million training images and 50k validation images. We evaluate both training error and validation error. Our ResNet-152 model has a top-1 validation error of 19.38%. This result won the 1st place in the ILSVRC 2015.\n",
      "\n",
      "【Output】\n",
      "```json\n",
      "\n",
      "```\n",
      "----------------------------------------\n",
      "\n",
      "🔍 2. APIにリクエストを送信中...\n",
      "\n",
      "✅ 3. LLMからのRAWレスポンス (JSON全体)\n",
      "----------------------------------------\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"```json\\n{\\n\\\"decision\\\": \\\"Used\\\"\\n}\\n```\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 603,\n",
      "    \"candidatesTokenCount\": 15,\n",
      "    \"totalTokenCount\": 1036,\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 603\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 418\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-flash\",\n",
      "  \"responseId\": \"zKSHaPbvMdCJqtsPlbjBsAs\"\n",
      "}\n",
      "----------------------------------------\n",
      "\n",
      "✅ 4. パース（解析）後の主要な結果\n",
      "----------------------------------------\n",
      "{\n",
      "  \"decision\": \"Used\"\n",
      "}\n",
      "----------------------------------------\n",
      "\n",
      "🎉 5. 最終判定\n",
      "----------------------------------------\n",
      "Decision: 'Used'  =>  Prediction: 1\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- 1. 設定とAPIキーの読み込み ---\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if not API_KEY:\n",
    "    print(\"❌ .envファイルからGEMINI_API_KEYが見つかりませんでした。\")\n",
    "    exit()\n",
    "\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={API_KEY}\"\n",
    "PROMPT_FILE_PATH = '../prompts/few_shot_cot_fulltext.txt' # ご自身のプロンプトファイルへのパス\n",
    "\n",
    "# --- 2. プロンプトテンプレートの読み込み ---\n",
    "try:\n",
    "    with open(PROMPT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        PROMPT_TEMPLATE = f.read()\n",
    "    print(f\"✅ プロンプトファイル '{os.path.basename(PROMPT_FILE_PATH)}' を読み込みました。\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ プロンプトファイルの読み込みに失敗しました: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. 確認したいサンプルデータ ---\n",
    "# ここの内容を書き換えることで、色々なケースをテストできます。\n",
    "sample_row = {\n",
    "    'cited_data_paper_title': \"ImageNet Large Scale Visual Recognition Challenge\",\n",
    "    'citing_paper_title': \"Deep Residual Learning for Image Recognition\",\n",
    "    'full_text': (\n",
    "        \"4. Experiments. We conduct comprehensive experiments on the ImageNet 2012 classification dataset \"\n",
    "        \"that consists of 1.28 million training images and 50k validation images. \"\n",
    "        \"We evaluate both training error and validation error. \"\n",
    "        \"Our ResNet-152 model has a top-1 validation error of 19.38%. This result won the 1st place in the ILSVRC 2015.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# --- 4. 単一リクエストのテスト実行 ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" singolo APIリクエストのテストを開始します\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 4.1. プロンプトの生成\n",
    "# トークン数上限を考慮し、実際のスクリプトに合わせてテキストを切り詰めます\n",
    "final_prompt = PROMPT_TEMPLATE.format(\n",
    "    cited_data_paper_title=sample_row['cited_data_paper_title'],\n",
    "    citing_paper_title=sample_row['citing_paper_title'],\n",
    "    full_text=sample_row['full_text'][:30000] \n",
    ")\n",
    "\n",
    "print(\"\\n✅ 1. LLMへの送信プロンプト（最終版）\")\n",
    "print(\"-\" * 40)\n",
    "print(final_prompt)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# 4.2. APIリクエストの実行\n",
    "payload = {\"contents\": [{\"parts\": [{\"text\": final_prompt}]}]}\n",
    "print(\"\\n🔍 2. APIにリクエストを送信中...\")\n",
    "\n",
    "try:\n",
    "    response = requests.post(API_URL, json=payload, timeout=120)\n",
    "    response.raise_for_status()  # エラーがあればここで例外を発生させる\n",
    "    \n",
    "    print(\"\\n✅ 3. LLMからのRAWレスポンス (JSON全体)\")\n",
    "    print(\"-\" * 40)\n",
    "    # レスポンスのJSONを整形して表示\n",
    "    raw_response_json = response.json()\n",
    "    print(json.dumps(raw_response_json, indent=2, ensure_ascii=False))\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # 4.3. レスポンスの解析\n",
    "    response_text = raw_response_json['candidates'][0]['content']['parts'][0]['text']\n",
    "    \n",
    "    # JSON部分を抽出\n",
    "    match = re.search(r\"\\{.*\\}\", response_text, re.DOTALL)\n",
    "    json_text = match.group(0) if match else response_text\n",
    "    final_json = json.loads(json_text)\n",
    "    \n",
    "    print(\"\\n✅ 4. パース（解析）後の主要な結果\")\n",
    "    print(\"-\" * 40)\n",
    "    print(json.dumps(final_json, indent=2, ensure_ascii=False))\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 4.4. 最終判定\n",
    "    decision = final_json.get('decision', 'N/A')\n",
    "    prediction = 1 if decision == \"Used\" else (0 if decision == \"Not Used\" else -1)\n",
    "    \n",
    "    print(f\"\\n🎉 5. 最終判定\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Decision: '{decision}'  =>  Prediction: {prediction}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"\\n❌ APIリクエストでエラーが発生しました: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ 処理中に予期せぬエラーが発生しました: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e9a64",
   "metadata": {},
   "source": [
    "- few shot\n",
    "- cot\n",
    "- gemini 2.5 flash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7004a7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ APIキーを.envファイルから正常に読み込みました。\n",
      "✅ 使用モデル: gemini-2.5-flash\n",
      "✅ プロンプトファイル 'few_shot_cot_fulltext.txt' を正常に読み込みました。\n",
      "合計 200 件の論文に対してLLM(gemini-2.5-flash)の判定を実行します。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1dd9e3cfc64828b6d08be6d944c841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM(gemini-2.5-flash)で判定中:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DOI 10.1016/j.oneear.2025.101197 の処理でエラー: HTTPSConnectionPool(host='generativelanguage.googleapis.com', port=443): Read timed out. (read timeout=180)\n",
      "\n",
      "処理完了。LLM(gemini-2.5-flash)の予測結果を '../data/processed/prediction_llm.csv' に追加・更新しました。\n",
      "\n",
      "--- 保存された結果の内訳（gemini-2.5-flash版） ---\n",
      "prediction_rule3_gemini-2_5-flash\n",
      " 0    124\n",
      " 1     75\n",
      "-1      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- 1. 設定とAPIキーの読み込み ---\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if API_KEY:\n",
    "    print(\"✅ APIキーを.envファイルから正常に読み込みました。\")\n",
    "else:\n",
    "    print(\"❌ .envファイルからGEMINI_API_KEYが見つかりませんでした。\")\n",
    "    exit()\n",
    "\n",
    "# 【変更点】将来のモデル名に変更\n",
    "MODEL_NAME = \"gemini-2.5-flash\" \n",
    "# 注意: このモデルは現時点では存在しないため、実行するとエラーになります。\n",
    "# 将来的にモデルがリリースされた際に、この名前が正しいかを確認してください。\n",
    "\n",
    "API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={API_KEY}\"\n",
    "print(f\"✅ 使用モデル: {MODEL_NAME}\")\n",
    "\n",
    "# 入力ファイルとプロンプトファイル\n",
    "INPUT_CSV = '../data/processed/samples_with_text.csv'\n",
    "PROMPT_FILE_PATH = '../prompts/few_shot_cot_fulltext.txt'\n",
    "OUTPUT_FILE = '../data/processed/prediction_llm.csv'\n",
    "\n",
    "# --- 2. プロンプトとデータの準備 ---\n",
    "try:\n",
    "    with open(PROMPT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        PROMPT_TEMPLATE = f.read()\n",
    "    print(f\"✅ プロンプトファイル '{os.path.basename(PROMPT_FILE_PATH)}' を正常に読み込みました。\")\n",
    "    \n",
    "    df_to_process = pd.read_csv(INPUT_CSV)\n",
    "    df_to_process.dropna(subset=['full_text'], inplace=True)\n",
    "    df_to_process.drop_duplicates(subset=['citing_paper_doi'], inplace=True, keep='first')\n",
    "    df_to_process.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f\"合計 {len(df_to_process)} 件の論文に対してLLM({MODEL_NAME})の判定を実行します。\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ エラー: データの読み込みに失敗しました。 {e}\")\n",
    "    df_to_process = pd.DataFrame()\n",
    "\n",
    "# --- 3. LLMによる判定の本番実行 ---\n",
    "predictions = []\n",
    "if PROMPT_TEMPLATE and not df_to_process.empty:\n",
    "    for index, row in tqdm(df_to_process.iterrows(), total=len(df_to_process), desc=f\"LLM({MODEL_NAME})で判定中\"):\n",
    "        \n",
    "        full_text = row.get('full_text', 'フルテキストが見つかりませんでした。')\n",
    "        \n",
    "        prompt = PROMPT_TEMPLATE.format(\n",
    "            cited_data_paper_title=row['cited_data_paper_title'],\n",
    "            citing_paper_title=row['citing_paper_title'],\n",
    "            full_text=full_text[:30000] # トークン数上限を考慮\n",
    "        )\n",
    "        \n",
    "        payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(API_URL, json=payload, timeout=180)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            response_json = response.json()\n",
    "            response_text = response_json['candidates'][0]['content']['parts'][0]['text']\n",
    "            \n",
    "            match = re.search(r\"\\{.*\\}\", response_text, re.DOTALL)\n",
    "            json_text = match.group(0) if match else response_text\n",
    "            final_json = json.loads(json_text)\n",
    "            \n",
    "            decision = final_json.get('decision')\n",
    "            prediction = 1 if decision == \"Used\" else 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nDOI {row['citing_paper_doi']} の処理でエラー: {e}\")\n",
    "            prediction = -1\n",
    "            \n",
    "        predictions.append(prediction)\n",
    "        time.sleep(1.5)\n",
    "\n",
    "# --- 4. 結果の保存 ---\n",
    "if len(predictions) == len(df_to_process):\n",
    "    # 今回の予測結果をDataFrameにまとめる\n",
    "    new_column_name = f\"prediction_rule3_{MODEL_NAME.replace('.', '_')}\" # モデル名からカラム名を生成\n",
    "    df_new_predictions = pd.DataFrame({\n",
    "        'citing_paper_doi': df_to_process['citing_paper_doi'],\n",
    "        new_column_name: predictions\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        output_df = pd.read_csv(OUTPUT_FILE)\n",
    "    except FileNotFoundError:\n",
    "        output_df = df_to_process[['citing_paper_eid', 'citing_paper_doi', 'citing_paper_title', 'cited_data_paper_title']].copy()\n",
    "\n",
    "    if new_column_name in output_df.columns:\n",
    "        output_df.drop(columns=[new_column_name], inplace=True)\n",
    "        \n",
    "    output_df = pd.merge(output_df, df_new_predictions, on='citing_paper_doi', how='left')\n",
    "    \n",
    "    output_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\n処理完了。LLM({MODEL_NAME})の予測結果を '{OUTPUT_FILE}' に追加・更新しました。\")\n",
    "    print(f\"\\n--- 保存された結果の内訳（{MODEL_NAME}版） ---\")\n",
    "    print(output_df[new_column_name].value_counts())\n",
    "else:\n",
    "    print(\"処理が正常に完了しませんでした。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd3a50c",
   "metadata": {},
   "source": [
    "リトライ用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fab28aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ APIキーを.envファイルから正常に読み込みました。\n",
      "✅ 使用モデル: gemini-2.5-flash\n",
      "✅ エラー(-1)が記録された 1 件を特定しました。再試行を開始します。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed29300b9ed4c749cc1729eeeee3a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemini-2.5-flashで再試行中:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "再試行完了。'../data/processed/prediction_llm.csv' を更新しました。\n",
      "\n",
      "--- 最新の予測結果の内訳（gemini-2.5-flash版） ---\n",
      "prediction_rule3_gemini-2_5-flash\n",
      "0    125\n",
      "1     75\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- 1. 設定とAPIキーの読み込み ---\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if API_KEY:\n",
    "    print(\"✅ APIキーを.envファイルから正常に読み込みました。\")\n",
    "else:\n",
    "    print(\"❌ .envファイルからGEMINI_API_KEYが見つかりませんでした。\")\n",
    "    exit()\n",
    "\n",
    "# --- 【重要】再試行するモデルとプロンプトを設定 ---\n",
    "# モデル名を 'gemini-1.5-flash' または 'gemini-1.5-pro' など、再試行したいモデル名に変更してください\n",
    "MODEL_NAME = \"gemini-2.5-flash\" \n",
    "# 再試行に使用するプロンプトファイルを指定してください\n",
    "PROMPT_FILE_PATH = '../prompts/few_shot_cot_fulltext.txt' \n",
    "# 再試行する対象のカラム名を指定してください\n",
    "COLUMN_TO_RETRY = 'prediction_rule3_gemini-2_5-flash'\n",
    "\n",
    "API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={API_KEY}\"\n",
    "print(f\"✅ 使用モデル: {MODEL_NAME}\")\n",
    "\n",
    "# 入力ファイル\n",
    "SAMPLES_CSV = '../data/processed/samples_with_text.csv'\n",
    "PREDICTIONS_CSV = '../data/processed/prediction_llm.csv'\n",
    "\n",
    "# --- 2. プロンプトとデータの準備 ---\n",
    "try:\n",
    "    with open(PROMPT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        PROMPT_TEMPLATE = f.read()\n",
    "    \n",
    "    df_samples = pd.read_csv(SAMPLES_CSV)\n",
    "    df_predictions = pd.read_csv(PREDICTIONS_CSV)\n",
    "\n",
    "    # --- 再試行対象の論文を特定 ---\n",
    "    if COLUMN_TO_RETRY not in df_predictions.columns:\n",
    "        print(f\"❌ エラー: '{COLUMN_TO_RETRY}' カラムが予測結果ファイルに見つかりません。\")\n",
    "        df_to_retry = pd.DataFrame()\n",
    "    else:\n",
    "        # 予測結果が-1の行のDOIを取得\n",
    "        retry_dois = df_predictions[df_predictions[COLUMN_TO_RETRY] == -1]['citing_paper_doi']\n",
    "        # samples_with_text.csvから、そのDOIに一致する行を抽出\n",
    "        df_to_retry = df_samples[df_samples['citing_paper_doi'].isin(retry_dois)].copy()\n",
    "    \n",
    "    if not df_to_retry.empty:\n",
    "        print(f\"✅ エラー(-1)が記録された {len(df_to_retry)} 件を特定しました。再試行を開始します。\")\n",
    "    else:\n",
    "        print(\"✅ エラー(-1)が記録された論文はありませんでした。処理は不要です。\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ エラー: データの読み込みに失敗しました。 {e}\")\n",
    "    df_to_retry = pd.DataFrame()\n",
    "\n",
    "# --- 3. 失敗したタスクに対してのみLLM判定を再実行 ---\n",
    "retry_results = []\n",
    "if not df_to_retry.empty and PROMPT_TEMPLATE:\n",
    "    for index, row in tqdm(df_to_retry.iterrows(), total=len(df_to_retry), desc=f\"{MODEL_NAME}で再試行中\"):\n",
    "        \n",
    "        full_text = row.get('full_text', 'フルテキストが見つかりませんでした。')\n",
    "        \n",
    "        prompt = PROMPT_TEMPLATE.format(\n",
    "            cited_data_paper_title=row['cited_data_paper_title'],\n",
    "            citing_paper_title=row['citing_paper_title'],\n",
    "            full_text=full_text[:30000]\n",
    "        )\n",
    "        \n",
    "        payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(API_URL, json=payload, timeout=180)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            response_json = response.json()\n",
    "            response_text = response_json['candidates'][0]['content']['parts'][0]['text']\n",
    "            \n",
    "            match = re.search(r\"\\{.*\\}\", response_text, re.DOTALL)\n",
    "            json_text = match.group(0) if match else response_text\n",
    "            final_json = json.loads(json_text)\n",
    "            \n",
    "            prediction = 1 if final_json.get('decision') == \"Used\" else 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nDOI {row['citing_paper_doi']} の再試行中にエラー: {e}\")\n",
    "            prediction = -1 # 再試行しても失敗した場合は-1のまま\n",
    "            \n",
    "        retry_results.append({\n",
    "            'citing_paper_doi': row['citing_paper_doi'],\n",
    "            COLUMN_TO_RETRY: prediction\n",
    "        })\n",
    "        time.sleep(1.5)\n",
    "\n",
    "# --- 4. 結果の更新と保存 ---\n",
    "if retry_results:\n",
    "    df_retry_results = pd.DataFrame(retry_results)\n",
    "    \n",
    "    # 元の予測結果のDOIをインデックスに設定\n",
    "    df_predictions.set_index('citing_paper_doi', inplace=True)\n",
    "    # 再試行結果のDOIをインデックスに設定\n",
    "    df_retry_results.set_index('citing_paper_doi', inplace=True)\n",
    "    \n",
    "    # 再試行の結果で元のデータを更新\n",
    "    df_predictions.update(df_retry_results)\n",
    "    \n",
    "    # インデックスをリセット\n",
    "    df_predictions.reset_index(inplace=True)\n",
    "    \n",
    "    df_predictions.to_csv(PREDICTIONS_CSV, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\n再試行完了。'{PREDICTIONS_CSV}' を更新しました。\")\n",
    "    print(f\"\\n--- 最新の予測結果の内訳（{MODEL_NAME}版） ---\")\n",
    "    print(df_predictions[COLUMN_TO_RETRY].value_counts())\n",
    "else:\n",
    "    print(\"再試行するデータがありませんでした。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c99985",
   "metadata": {},
   "source": [
    "- zero shot\n",
    "- gemini 2.5 flash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0788ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ APIキーを.envファイルから正常に読み込みました。\n",
      "✅ 使用モデル: gemini-2.5-flash\n",
      "✅ プロンプトファイル 'zero_shot_fulltext.txt' を正常に読み込みました。\n",
      "合計 200 件の論文に対してLLM(gemini-2.5-flash, Zero-shot)の判定を実行します。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10395474fe2a4d2f80ed94c6268cbe2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM(gemini-2.5-flash, Zero-shot)で判定中:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "処理完了。LLM(gemini-2.5-flash, Zero-shot)の予測結果を '../data/processed/prediction_llm.csv' に追加・更新しました。\n",
      "\n",
      "--- 保存された結果の内訳（gemini-2.5-flash, Zero-shot版） ---\n",
      "prediction_rule3_gemini-2_5-flash_zeroshot\n",
      "0    107\n",
      "1     93\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- 1. 設定とAPIキーの読み込み ---\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if API_KEY:\n",
    "    print(\"✅ APIキーを.envファイルから正常に読み込みました。\")\n",
    "else:\n",
    "    print(\"❌ .envファイルからGEMINI_API_KEYが見つかりませんでした。\")\n",
    "    exit()\n",
    "\n",
    "# 【重要】実行するモデル名と、それに対応するプロンプトファイルを指定\n",
    "MODEL_NAME = \"gemini-2.5-flash\" \n",
    "PROMPT_FILE_PATH = '../prompts/zero_shot_fulltext.txt' \n",
    "\n",
    "# APIエンドポイント\n",
    "API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={API_KEY}\"\n",
    "print(f\"✅ 使用モデル: {MODEL_NAME}\")\n",
    "\n",
    "# 入力ファイルと出力ファイル\n",
    "INPUT_CSV = '../data/processed/samples_with_text.csv'\n",
    "OUTPUT_FILE = '../data/processed/prediction_llm.csv'\n",
    "\n",
    "# --- 2. プロンプトとデータの準備 ---\n",
    "try:\n",
    "    with open(PROMPT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        PROMPT_TEMPLATE = f.read()\n",
    "    print(f\"✅ プロンプトファイル '{os.path.basename(PROMPT_FILE_PATH)}' を正常に読み込みました。\")\n",
    "    \n",
    "    df_to_process = pd.read_csv(INPUT_CSV)\n",
    "    df_to_process.dropna(subset=['full_text'], inplace=True)\n",
    "    df_to_process.drop_duplicates(subset=['citing_paper_doi'], inplace=True, keep='first')\n",
    "    df_to_process.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f\"合計 {len(df_to_process)} 件の論文に対してLLM({MODEL_NAME}, Zero-shot)の判定を実行します。\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ エラー: データの読み込みに失敗しました。 {e}\")\n",
    "    df_to_process = pd.DataFrame()\n",
    "\n",
    "# --- 3. LLMによる判定の本番実行 ---\n",
    "predictions = []\n",
    "if PROMPT_TEMPLATE and not df_to_process.empty:\n",
    "    for index, row in tqdm(df_to_process.iterrows(), total=len(df_to_process), desc=f\"LLM({MODEL_NAME}, Zero-shot)で判定中\"):\n",
    "        \n",
    "        full_text = row.get('full_text', 'フルテキストが見つかりませんでした。')\n",
    "        \n",
    "        prompt = PROMPT_TEMPLATE.format(\n",
    "            cited_data_paper_title=row['cited_data_paper_title'],\n",
    "            citing_paper_title=row['citing_paper_title'],\n",
    "            full_text=full_text[:30000] # トークン数上限を考慮\n",
    "        )\n",
    "        \n",
    "        payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(API_URL, json=payload, timeout=180)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            response_json = response.json()\n",
    "            response_text = response_json['candidates'][0]['content']['parts'][0]['text']\n",
    "            \n",
    "            match = re.search(r\"\\{.*\\}\", response_text, re.DOTALL)\n",
    "            json_text = match.group(0) if match else response_text\n",
    "            final_json = json.loads(json_text)\n",
    "            \n",
    "            decision = final_json.get('decision')\n",
    "            prediction = 1 if decision == \"Used\" else 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nDOI {row['citing_paper_doi']} の処理でエラー: {e}\")\n",
    "            prediction = -1\n",
    "            \n",
    "        predictions.append(prediction)\n",
    "        time.sleep(1.5) # APIへの負荷を考慮\n",
    "\n",
    "# --- 4. 結果の保存 ---\n",
    "if len(predictions) == len(df_to_process):\n",
    "    # 今回の予測結果をDataFrameにまとめる\n",
    "    new_column_name = f\"prediction_rule3_{MODEL_NAME.replace('.', '_')}_zeroshot\"\n",
    "    df_new_predictions = pd.DataFrame({\n",
    "        'citing_paper_doi': df_to_process['citing_paper_doi'],\n",
    "        new_column_name: predictions\n",
    "    })\n",
    "    \n",
    "    # 既存のLLM予測結果ファイルを読み込む\n",
    "    try:\n",
    "        output_df = pd.read_csv(OUTPUT_FILE)\n",
    "    except FileNotFoundError:\n",
    "        output_df = df_to_process[['citing_paper_eid', 'citing_paper_doi', 'citing_paper_title', 'cited_data_paper_title']].copy()\n",
    "\n",
    "    # 既存のデータと新しい予測結果をマージ（結合）\n",
    "    if new_column_name in output_df.columns:\n",
    "        output_df.drop(columns=[new_column_name], inplace=True)\n",
    "        \n",
    "    output_df = pd.merge(output_df, df_new_predictions, on='citing_paper_doi', how='left')\n",
    "    \n",
    "    # ファイルに上書き保存\n",
    "    output_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\n処理完了。LLM({MODEL_NAME}, Zero-shot)の予測結果を '{OUTPUT_FILE}' に追加・更新しました。\")\n",
    "    print(f\"\\n--- 保存された結果の内訳（{MODEL_NAME}, Zero-shot版） ---\")\n",
    "    print(output_df[new_column_name].value_counts())\n",
    "else:\n",
    "    print(\"処理が正常に完了しませんでした。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataUsageValidator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
