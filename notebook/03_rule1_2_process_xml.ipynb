{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66534f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "アノテーション対象 200 件のXMLを解析します...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f812a07f1f406ab0ec8036f4f464a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "特徴量抽出中:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kurokawa\\AppData\\Local\\Temp\\ipykernel_6312\\1032045506.py:71: DeprecationWarning: Testing an element's truth value will always return True in future versions.  Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  if not top_level_sections:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "処理完了。特徴量抽出結果を '../data/processed\\features_for_evaluation.csv' に保存しました。\n",
      "\n",
      "--- 保存されたデータの出力例（先頭5件）---\n",
      "     citing_paper_eid                 citing_paper_doi  \\\n",
      "0  2-s2.0-85211640167  10.1016/j.marpolbul.2024.117442   \n",
      "1  2-s2.0-85210281314   10.1016/j.jaridenv.2024.105282   \n",
      "2  2-s2.0-85171680307   10.1016/j.revpalbo.2023.104989   \n",
      "3  2-s2.0-85142708771     10.1016/j.foreco.2022.120653   \n",
      "4  2-s2.0-85194770743    10.1016/j.jnucmat.2024.155194   \n",
      "\n",
      "                                  citing_paper_title  \\\n",
      "0  Multi-indicator assessment of heavy metal poll...   \n",
      "1  Potential effects of climate change on cacti d...   \n",
      "2  Approaches to pollen taxonomic harmonisation i...   \n",
      "3  Allometric equations to estimate the dry mass ...   \n",
      "4  Microstructural evolution in doped high entrop...   \n",
      "\n",
      "                              cited_data_paper_title  mention_count  \\\n",
      "0  Pollution load index for heavy metals in Mian-...              1   \n",
      "1  The World Checklist of Vascular Plants, a cont...              2   \n",
      "2  European pollen-based REVEALS land-cover recon...              1   \n",
      "3  A global map of root biomass across the world'...              0   \n",
      "4  Database on the mechanical properties of high ...              1   \n",
      "\n",
      "                                  mentioned_sections  prediction_rule1  \\\n",
      "0                       [Pollution Load Index (PLI)]                 0   \n",
      "1                               [– Data preparation]                 1   \n",
      "2  [Pollen taxonomic harmonisation – Top-down and...                 0   \n",
      "3                                                 []                 0   \n",
      "4                                     [Introduction]                 0   \n",
      "\n",
      "   prediction_rule2  \n",
      "0                 0  \n",
      "1                 1  \n",
      "2                 0  \n",
      "3                 0  \n",
      "4                 0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- 設定項目 ---\n",
    "ANNOTATION_LIST_CSV = '../data/ground_truth/annotation_target_list.csv'\n",
    "MASTER_LIST_CSV = '../data/processed/citing_papers_with_paths.csv'\n",
    "OUTPUT_DIR = '../data/processed'\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_DIR, 'features_for_evaluation.csv')\n",
    "\n",
    "# XMLの名前空間\n",
    "namespaces = {'ce': 'http://www.elsevier.com/xml/common/dtd', 'sb': 'http://www.elsevier.com/xml/common/struct-bib/dtd', 'ja': 'http://www.elsevier.com/xml/ja/dtd'}\n",
    "\n",
    "# --- 1. XML解析用の関数群を定義 ---\n",
    "\n",
    "def get_citation_map_et(root_element):\n",
    "    \"\"\"参考文献リストから {ref_id: '文献情報'} の辞書を作成する\"\"\"\n",
    "    citation_map = {}\n",
    "    references = root_element.findall('.//ce:bibliography/ce:bibliography-sec/ce:bib-reference', namespaces)\n",
    "    for ref in references:\n",
    "        ref_id = ref.get('id')\n",
    "        source_text_element = ref.find('.//ce:source-text', namespaces)\n",
    "        citation_text = source_text_element.text if source_text_element is not None else ''.join(ref.itertext())\n",
    "        if ref_id:\n",
    "            citation_map[ref_id] = citation_text.strip() if citation_text else 'N/A'\n",
    "    return citation_map\n",
    "\n",
    "def find_target_ref_id(citation_map, target_title):\n",
    "    \"\"\"参考文献マップとデータ論文タイトルから、対応するRef IDを見つける\"\"\"\n",
    "    for ref_id, full_citation in citation_map.items():\n",
    "        if target_title.lower() in full_citation.lower():\n",
    "            return ref_id\n",
    "    return None\n",
    "\n",
    "def parse_sections_recursive(element):\n",
    "    \"\"\"XML要素から、全セクションのタイトルと、各セクション内の引用IDリストを抽出する\"\"\"\n",
    "    sections_data = []\n",
    "    for section in element.findall('./ce:section', namespaces):\n",
    "        title_tag = section.find('./ce:section-title', namespaces)\n",
    "        sec_title = title_tag.text.strip() if title_tag is not None and title_tag.text else 'No Title'\n",
    "        \n",
    "        citations_in_section = []\n",
    "        # 直下のparaタグ内の引用のみを対象とする\n",
    "        paragraphs = section.findall('./ce:para', namespaces)\n",
    "        for p in paragraphs:\n",
    "            cross_refs = p.findall('.//ce:cross-ref', namespaces)\n",
    "            for xref in cross_refs:\n",
    "                if xref.get('refid'):\n",
    "                    ref_ids = xref.get('refid').split()\n",
    "                    for ref_id in ref_ids:\n",
    "                        citations_in_section.append(ref_id)\n",
    "                        \n",
    "        sections_data.append({'title': sec_title, 'citations': citations_in_section})\n",
    "        # 再帰呼び出しでサブセクションも探索\n",
    "        sections_data.extend(parse_sections_recursive(section))\n",
    "    return sections_data\n",
    "\n",
    "def analyze_single_xml(xml_path, target_data_paper_title):\n",
    "    \"\"\"1つのXMLファイルを解析し、特徴量と判定結果を抽出するメイン関数\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        citation_map = get_citation_map_et(root)\n",
    "        target_ref_id = find_target_ref_id(citation_map, target_data_paper_title)\n",
    "        if not target_ref_id:\n",
    "            return 0, [], 0, 0\n",
    "\n",
    "        top_level_sections = root.find('.//ja:body/ce:sections', namespaces)\n",
    "        if not top_level_sections:\n",
    "            return 0, [], 0, 0\n",
    "        \n",
    "        all_sections_data = parse_sections_recursive(top_level_sections)\n",
    "        \n",
    "        mention_count = 0\n",
    "        mentioned_sections = []\n",
    "        keywords_to_check = ['data', 'method', 'experiment']\n",
    "        contains_keyword = False\n",
    "        \n",
    "        for section in all_sections_data:\n",
    "            count_in_section = section['citations'].count(target_ref_id)\n",
    "            if count_in_section > 0:\n",
    "                mention_count += count_in_section\n",
    "                section_title = section['title']\n",
    "                mentioned_sections.append(section_title)\n",
    "                if any(keyword in section_title.lower() for keyword in keywords_to_check):\n",
    "                    contains_keyword = True\n",
    "        \n",
    "        # 手法1とprediction_rule2を計算\n",
    "        prediction_method1 = 1 if mention_count >= 2 else 0\n",
    "        prediction_method2 = 1 if contains_keyword else 0\n",
    "        \n",
    "        return mention_count, list(set(mentioned_sections)), prediction_method1, prediction_method2\n",
    "\n",
    "    except Exception:\n",
    "        return -1, ['parsing_error'], -1, -1\n",
    "\n",
    "# --- 2. メイン処理 ---\n",
    "try:\n",
    "    df_targets = pd.read_csv(ANNOTATION_LIST_CSV)\n",
    "    df_master = pd.read_csv(MASTER_LIST_CSV)\n",
    "    \n",
    "    # 2つのDataFrameをマージして、アノテーション対象の論文情報（パスを含む）を取得\n",
    "    # マージのキーをタプルにして、複数の列でマージする\n",
    "    merge_keys = ['citing_paper_eid', 'citing_paper_doi', 'citing_paper_title', 'cited_data_paper_title']\n",
    "    df_to_process = pd.merge(df_targets, df_master.drop_duplicates(subset=merge_keys), on=merge_keys, how='left')\n",
    "    \n",
    "    results_list = []\n",
    "    print(f\"アノテーション対象 {len(df_to_process)} 件のXMLを解析します...\")\n",
    "    \n",
    "    for index, row in tqdm(df_to_process.iterrows(), total=len(df_to_process), desc=\"特徴量抽出中\"):\n",
    "        xml_path = row['fulltext_xml_path']\n",
    "        target_title = row['cited_data_paper_title']\n",
    "        \n",
    "        if pd.notna(xml_path) and os.path.exists(xml_path):\n",
    "            count, sections, pred1, pred2 = analyze_single_xml(xml_path, target_title)\n",
    "        else:\n",
    "            count, sections, pred1, pred2 = -1, ['file_not_found'], -1, -1\n",
    "\n",
    "        # 抽出した特徴量と判定結果を元の情報に追加\n",
    "        result_row = row.to_dict()\n",
    "        result_row['mention_count'] = count\n",
    "        result_row['mentioned_sections'] = sections\n",
    "        result_row['prediction_rule1'] = pred1\n",
    "        result_row['prediction_rule2'] = pred2\n",
    "        results_list.append(result_row)\n",
    "\n",
    "    # --- 3. 結果の保存と表示 ---\n",
    "    df_final = pd.DataFrame(results_list)\n",
    "    \n",
    "    # 指定されたカラムのみを保存\n",
    "    columns_to_save = [\n",
    "        'citing_paper_eid', \n",
    "        'citing_paper_doi', \n",
    "        'citing_paper_title', \n",
    "        'cited_data_paper_title',\n",
    "        'mention_count',\n",
    "        'mentioned_sections',\n",
    "        'prediction_rule1',\n",
    "        'prediction_rule2'\n",
    "    ]\n",
    "    df_to_save = df_final[columns_to_save]\n",
    "    \n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    df_to_save.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\n処理完了。特徴量抽出結果を '{OUTPUT_FILE}' に保存しました。\")\n",
    "    \n",
    "    print(\"\\n--- 保存されたデータの出力例（先頭5件）---\")\n",
    "    print(df_to_save.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nメイン処理中にエラーが発生しました: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataUsageValidator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
